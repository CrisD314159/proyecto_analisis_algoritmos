@Filtered Article{00475aca-0fb0-410c-8c46-da8ed020880c,
 abstract = {Given the need for cleaner energy sources associated with the ESG (Environmental, social and corporate governance) policy, the work in question is a case study referring to the project to expand biomethane production in national territory, especially for industrial commercialization. By applying the Value Focused Thinking (VFT) methodology, the study initially seeks the approach based on the values of decision-makers, these being three professionals in the energy sector. After the central objective of supporting decision-making, the hybrid method PROMETHEE-SAPEVO-M1 was used, characterized by the possibility of evaluating quantitative data and qualitative variables. To this end, the modeling occurred through the software of the PROMETHEE-SAPEVO-M1 method to clarify the best plants, because of the range of possibilities in the national territory, for project implementation and subsequent production of biomethane for industrial use. As a result, we verified that São Paulo is the best alternative for applying the investment in biomethane production.},
 authors = {['Wallace L.T. Costa', 'Igor P.A. Costa', 'Adilson V. Terra', 'Miguel Â.L. Moreira', 'Carlos', 'F.S. Gomes', 'Marcos Santos']},
 journal = {IFAC-PapersOnLine},
 keywords = {['Multicriteria', 'PROMETHEE', 'SAPEVO', 'Energy', 'Biomethane']},
 title = {Multicriteria analysis by PROMETHEE-SAPEVO-M1 method: choice of Brazilian sugar and ethanol plants for biomethane production},
 year = {2022}
}

@Filtered Article{00726a39-5e4a-4b40-a4da-55ada043ebec,
 abstract = {Although by 11years children demonstrate impressive performance on various tasks that assess symbolic thinking in language development, research suggests that few young adolescents demonstrate evidence of symbolic processing when reading literature. This study investigated whether the difficulty might be due to a lack of adequate exposure to domain-specific knowledge. Students in the experimental groups in three age groups — preadolescence, middle adolescence and later adolescence — received concrete scaffolds designed to foster domain-specific knowledge of the symbolic process. A comparison of the experimental and control groups showed that students at all three ages who had experienced the scaffolds demonstrated significantly greater symbolic interpretation. Furthermore, despite concerns that the scaffolds might dampen the readers' personal response, the experimental groups at all three ages provided significantly higher enjoyment ratings of the test poems.},
 authors = {['Joan Peskin', 'Rebecca Wells-Jopling']},
 journal = {Journal of Applied Developmental Psychology},
 keywords = {['Adolescence', 'Symbolic interpretation', 'Domain-specific knowledge', 'Poetry', 'Concrete scaffolds', 'Computational skills']},
 title = {Fostering symbolic interpretation during adolescence},
 year = {2012}
}

@Filtered Article{0073d178-8735-468c-9319-872164462a5b,
 abstract = {The complexity of performance-based building design stems from the evaluation of numerous candidate design options, driven by the plethora of variables, objectives, and constraints inherent in multi-disciplinary projects. This necessitates optimization approaches to support the identification of well performing designs while reducing the computational time of performance evaluation. In response, this paper proposes and evaluates a sequential approach for multi-objective design optimization of building geometry, fabric, HVAC system and controls for building performance. This approach involves sequential optimizations with optimal solutions from previous stages passed to the next. The performance of the sequential approach is benchmarked against a full factorial search, assessing its effectiveness in finding global optima, solution quality, reliability to scale and variations of problem formulations, and computational efficiency compared to the NSGA-II algorithm. 24 configurations of the sequential approach are tested on a multi-scale case study, simulating 874 to 4,147,200 design options for an office building, aiming to minimize energy demand while maintaining thermal comfort. A two-stage sequential process-(building geometry + fabric) and (HVAC system + controls) identified the same Pareto-optimal solutions as the full factorial search across all four scales and variations of problem formulations, demonstrating 100 % effectiveness and reliability. This approach required 100,700 function evaluations, representing a 91.2 % reduction in computational effort compared to the full factorial search. In contrast, NSGA-II achieved only 73.5 % of the global optima with the same number of function evaluations. This research indicates that a sequential optimization approach is a highly efficient and robust alternative to the standard NSGA-II algorithm.},
 authors = {['Riccardo Talami', 'Jonathan Wright', 'Bianca Howard']},
 journal = {Energy and Buildings},
 keywords = {['Building optimization', 'Building performance', 'Algorithm', 'Building simulation', 'Building design']},
 title = {Evaluating the effectiveness, reliability and efficiency of a multi-objective sequential optimization approach for building performance design},
 year = {2025}
}

@Filtered Article{0088a94f-e4cc-45d1-9453-199a1415837f,
 abstract = {Association Rule Mining (ARM) is a significant task for discovering frequent patterns in data mining. It has achieved great success in a plethora of applications such as market basket, computer networks, recommendation systems, and healthcare. In the past few years, evolutionary computation-based ARM has emerged as one of the most popular research areas for addressing the high computation time of traditional ARM. Although numerous papers have been published, there is no comprehensive analysis of existing evolutionary ARM methodologies. In this paper, we review emerging research of evolutionary computation for ARM. We discuss the applications on evolutionary computations for different types of ARM approaches including numerical rules, fuzzy rules, high-utility itemsets, class association rules, and rare association rules. Evolutionary ARM algorithms were classified into four main groups in terms of the evolutionary approach, including evolution-based, swarm intelligence-based, physics-inspired, and hybrid approaches. Furthermore, we discuss the remaining challenges of evolutionary ARM and discuss its applications and future topics.},
 authors = {['Akbar Telikani', 'Amir H. Gandomi', 'Asadollah Shahbahrami']},
 journal = {Information Sciences},
 keywords = {['Data mining', 'Association rule mining', 'Evolutionary computation', 'Swarm intelligent']},
 title = {A survey of evolutionary computation for association rule mining},
 year = {2020}
}

@Filtered Article{00945f8d-9643-4b05-a6a9-07dcd686c748,
 abstract = {This paper presents a strategy for fostering digital citizenship at school that transcends the mere use of digital devices or instructional methods focused solely on their use. The core premise of this proposal rests on the need for an ethical-political debate concerning digitization in education. In addition, it emphasizes the need to cultivate a form of digital literacy that blends science and technology with the humanities, and erases the traditional boundaries between making and thinking. The proposed approach encapsulates two primary concerns: firstly, it asserts that digital literacy serves as a foundation for meaningful participation in digital societies; secondly, it underscores the importance of democratizing digital technologies by incorporating the perspectives, needs, and concerns of children. Drawing inspiration from the theories of pragmatism and responsible research and innovation (RRI), we present a conceptual framework for digital citizenship. To operationalize this approach, we adapt John Dewey's pragmatic model of inquiry as a method that can be applied within the school setting. This pragmatic methodology serves as a conduit for developing hands-on experience geared towards developing digital citizenship. The practical implementation of this methodology is illustrated through an actualized experience with 10- and 11-year-old children in a public primary school, regarding the issue of care robots. This paper advocates for a symbiotic relationship between theoretical understanding and practical application, and puts forward a concrete proposal for the integration of digital citizenship in schools in the form of a four-phase procedural model, based on the creation of what we term ‘the encounter’ between the educational community and the research and development community.},
 authors = {['Núria Vallès-Peris', 'Miquel Domènech']},
 journal = {Technology in Society},
 keywords = {['Science and technology studies', 'Digital citizenship', 'Responsible research and innovation', 'Democracy', 'Pragmatism']},
 title = {Digital citizenship at school: Democracy, pragmatism and RRI},
 year = {2024}
}

@Filtered Article{009e76ba-a259-47f3-becd-9c72d07a26e7,
 abstract = {The investigation of how users make sense of the data provided by information systems is very important for human computer interaction. In this context, understanding the interaction processes of users plays an important role. The analysis of interaction sequences, for example, can provide a deeper understanding about how users solve problems. In this paper we present an analysis of sequences of interactions within a visualization system and compare the results to previous research. We used log file analysis and thinking aloud as methods. There is some indication based on log file analysis that there are interaction patterns which can be generalized. Thinking aloud indicates that some cognitive processes occur together with a higher probability than others.},
 authors = {['Margit Pohl', 'Günter Wallner', 'Simone Kriglstein']},
 journal = {International Journal of Human-Computer Studies},
 keywords = {['Interaction sequences', 'Lag-sequential analysis', 'Visualization', 'Log files', 'Thinking aloud']},
 title = {Using lag-sequential analysis for understanding interaction sequences in visualizations},
 year = {2016}
}

@Filtered Article{00c4dd1b-c5ca-482c-b156-9630f08506f1,
 abstract = {This paper has proposed an integrated healthcare monitoring solution for the soldiers deployed in adverse environmental conditions, using the internet of things (IoT) with distributed computing. For these soldiers, the health parameters of every individual need to be monitored on a real-time basis and subsequent analysis of the dataset to be made for initiating appropriate medical support with the lowest possible delay. In this paper, a three-layer service-oriented IoT architecture has been proposed where the computational functionalities are distributed among all the layers. The proposed distributed computing mechanism has implemented two levels of filtration of redundant information that belongs to safe soldiers. The first level of filtering is done at the end-node using the Fuzzy classification approach and the second level of filtering is done at the intermediate node using the time-series pattern analysis approach. This layer-wise filtration process results in a reduction in data flooding and computational burden on the cloud due to which system response time improves to suit emergency applications. A prototype has been developed to validate the effectiveness of the proposed solution.},
 authors = {['Shuvabrata Bandopadhaya', 'Rajiv Dey', 'Ashok Suhag']},
 journal = {Sustainable Computing: Informatics and Systems},
 keywords = {['Healthcare monitoring system', 'Internet of things (IoT)', 'Distributed computing', 'Fuzzy classification', 'Partten recognistion', 'Long Range wide area network (LoRaWAN)']},
 title = {Integrated healthcare monitoring solutions for soldier using the internet of things with distributed computing},
 year = {2020}
}

@Filtered Article{00ef215b-b316-4fb3-b200-30dec818f6bf,
 abstract = {An entire generation of children is growing up with machine learning (ML) systems that are greatly disrupting job markets as well as changing people’s everyday lives. Yet, that development and its societal effects have been given minor attention in computing education in schools, which mainly focuses on rule-based programming. This article presents a pedagogical framework for supporting middle schoolers to become co-designers and makers of their own machine learning applications. It presents a case study conducted in the 6th grade of a Finnish elementary school and analyzes students’ (N=34) evolving ML ideas and explanations. Data consists of a children’s artwork, students’ design ideas and co-designed applications, and structured group interviews organized at the end of the ML project. The qualitative content analysis revealed how hands-on exploration with ML-based technologies supported students in developing various kinds of design ideas that harnessed face recognition, gestures, or voice recognition for solving real-life problems. The results of the study further indicated that co-designing ML applications provided a promising entry point for students to develop their conceptual understanding of ML principles, its workflows, and its role in their everyday practices. The article concludes with a discussion on how to support students to become innovators and software designers in the age of machine learning.},
 authors = {['Henriikka Vartiainen', 'Tapani Toivonen', 'Ilkka Jormanainen', 'Juho Kahila', 'Matti Tedre', 'Teemu Valtonen']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['AI', 'Machine learning', 'K-12', 'Computational thinking', 'Design-oriented pedagogy', 'Design-based research']},
 title = {Machine learning for middle schoolers: Learning through data-driven design},
 year = {2021}
}

@Filtered Article{011e1bc7-ba1f-4514-a175-37ef5e178746,
 abstract = {Although there is no direct connection, the incidence of Legionnaire’s disease has increased concurrently with increased usage of energy efficient domestic hot water (DHW) systems, which serve as ideal growth environments for Legionella pneumophila, the bacteria responsible for Legionnaire’s disease. The Duck Foot Heat Exchange Model (DFHXM) was developed to aid design of energy efficient thermal pasteurization systems with Legionella control specifically in mind. The model simulates a system design imitating the countercurrent heat exchange in the feet of ducks, an evolutionary adaption reducing environmental heat losses in cold climates. Such systems use a heat exchanger to preheat fluids prior to pasteurization and cool the same fluid after pasteurization. Thus, the design requires minimal addition of heat to achieve pasteurization temperatures and to cover environmental heat losses. This article describes the underlying principles and use of the freely available Microsoft Excel model, as well as compares results from the DFHXM to measurements of an experimental pilot system. Simulation outputs agreed well with experimental results for transient and steady-state temperatures, the largest discrepancy in steady-state temperatures being 4.6%. Lastly, we discuss the flexibility of the DFHXM to simulate a wide variety of designs with special emphasis on Legionella control and solar-thermal water disinfection.},
 authors = {['Samuel Knapp', 'Bo Nordell']},
 journal = {Applied Thermal Engineering},
 keywords = {['Thermal model', 'Heat exchanger', 'Pasteurization', 'Legionnaire’s disease', 'Microsoft Excel']},
 title = {Energy-efficient Legionella control that mimics nature and an open-source computational model to aid system design},
 year = {2017}
}

@Filtered Article{0141b55c-7530-4e0d-afc7-e8839d942ab5,
 abstract = {A multi-granularity knowledge space is a computational model that simulates human thinking and solves complex problems. However, as the amount of data increases, the multi-granularity knowledge space will have a larger number of layers, which will reduce its problem-solving ability. Therefore, we define a knowledge space distance measurement and propose two algorithms to select k representative layers from the multi-granularity knowledge space, where k is specified by the user according to the needs in problem solving, and k representative layers are approximately equidistant. First, we propose a knowledge space distance to measure the distance between any two layers in a multi-granularity knowledge space with superset-subset relationships, and the rationality of the knowledge space distance is verified by theory and experiment. Second, relying on the knowledge space distance and knowledge space distance variance, we propose two algorithms (i.e., a deterministic algorithm and a heuristic algorithm) to select an approximate equidistant k-layer multi-granularity knowledge space. Third, in addition to verifying the effectiveness of the knowledge space distance, the knowledge space distance variance, the deterministic algorithm and the heuristic algorithm, we verify that the equidistant k-layer multi-granularity knowledge space is more efficient than the original multi-granularity knowledge space.},
 authors = {['Jiangli Duan', 'Guoyin Wang', 'Xin Hu']},
 journal = {Knowledge-Based Systems},
 keywords = {['Granular computing', 'Multi-granularity knowledge space', 'Knowledge space distance', 'Hierarchical quotient space']},
 title = {Equidistant k-layer multi-granularity knowledge space},
 year = {2021}
}

@Filtered Article{0194f0d2-f659-4ddc-999b-01eb29caffa4,
 abstract = {Abstract:
Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material-independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.},
 authors = {['A.P. Vassilopoulos', 'E.F. Georgopoulos']},
 journal = {Woodhead Publishing},
 keywords = {['fatigue', 'composites', 'artificial neural network', 'genetic programming', 'ANFIS', 'S–N curves']},
 title = {5 - Novel computational methods for fatig life modeling of composite materials},
 year = {2010}
}

@Filtered Article{01e2f07c-f399-4042-8d00-226a5cdc07a7,
 authors = {['S. Becker', 'A. Chan', 'P. Fletcher', 'A. Smith', 'S. Kapur']},
 journal = {Schizophrenia Research},
 title = {A computational model of the role of dopamine and psychotropic drugs in modulating motivated action},
 year = {2003}
}

@Filtered Article{0263a846-d8cb-45cc-bdee-6e2a6c5d893f,
 abstract = {Yuxuan Zhao, associate professor, Enmeng Lu, research engineer, and Yi Zeng, professor and lab director, have proposed a brain-inspired bodily self-perception model based on biological findings on monkeys and humans. This model can reproduce various rubber hand illusion (RHI) experiments, which helps reveal the RHI’s computational and biological mechanisms. They talk about their view of data science and research plans for brain-inspired robot self-modeling and ethical robots.},
 authors = {['Yuxuan Zhao', 'Enmeng Lu', 'Yi Zeng']},
 journal = {Patterns},
 title = {Meet the authors: Yuxuan Zhao, Enmeng Lu, and Yi Zeng},
 year = {2023}
}

@Filtered Article{02b2fb8f-b1da-4c85-ba5d-bdc08ac3ab39,
 abstract = {Many people report a form of internal language known as inner speech (IS). This review examines recent growth of research interest in the phenomenon, which has broadly supported a theoretical model in which IS is a functional language process that can confer benefits for cognition in a range of domains. A key insight to have emerged in recent years is that IS is an embodied experience characterized by varied subjective qualities, which can be usefully modeled in artificial systems and whose neural signals have the potential to be decoded through advancing brain–computer interface technologies. Challenges for future research include understanding individual differences in IS and mapping form to function across IS subtypes.},
 authors = {['Charles Fernyhough', 'Anna M. Borghi']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['inner dialogue', 'inner monologue', 'verbal thinking', 'self-talk', 'self-regulation', 'phenomenology']},
 title = {Inner speech as language process and cognitive tool},
 year = {2023}
}

@Filtered Article{02e74846-0580-4c79-8ac8-c3ca508a97bb,
 abstract = {Organismal behavior, with its tremendous complexity and diversity, is generated by numerous physiological systems acting in coordination. Understanding how these systems evolve to support differences in behavior within and among species is a longstanding goal in biology that has captured the imagination of researchers who work on a multitude of taxa, including humans. Of particular importance are the physiological determinants of behavioral evolution, which are sometimes overlooked because we lack a robust conceptual framework to study mechanisms underlying adaptation and diversification of behavior. Here, we discuss a framework for such an analysis that applies a “systems view” to our understanding of behavioral control. This approach involves linking separate models that consider behavior and physiology as their own networks into a singular vertically integrated behavioral control system. In doing so, hormones commonly stand out as the links, or edges, among nodes within this system. To ground our discussion, we focus on studies of manakins (Pipridae), a family of Neotropical birds. These species have numerous physiological and endocrine specializations that support their elaborate reproductive displays. As a result, manakins provide a useful example to help imagine and visualize the way systems concepts can inform our appreciation of behavioral evolution. In particular, manakins help clarify how connectedness among physiological systems—which is maintained through endocrine signaling—potentiate and/or constrain the evolution of complex behavior to yield behavioral differences across taxa. Ultimately, we hope this review will continue to stimulate thought, discussion, and the emergence of research focused on integrated phenotypes in behavioral ecology and endocrinology.},
 authors = {['Matthew J. Fuxjager', 'T. Brandt Ryder', 'Nicole M. Moody', 'Camilo Alfonso', 'Christopher N. Balakrishnan', 'Julia Barske', 'Mariane Bosholn', 'W. Alice Boyle', 'Edward L. Braun', 'Ioana Chiver', 'Roslyn Dakin', 'Lainy B. Day', 'Robert Driver', 'Leonida Fusani', 'Brent M. Horton', 'Rebecca T. Kimball', 'Sara Lipshutz', 'Claudio V. Mello', 'Eliot T. Miller', 'Michael S. Webster', 'Morgan Wirthlin', 'Roy Wollman', 'Ignacio T. Moore', 'Barney A. Schlinger']},
 journal = {Hormones and Behavior},
 keywords = {['Systems biology', 'Animal behavior', 'Organismal physiology', 'Adaptive evolution', 'Manakin birds', 'Androgenic hormones', 'Robustness']},
 title = {Systems biology as a framework to understand the physiological and endocrine bases of behavior and its evolution—From concepts to a case study in birds},
 year = {2023}
}

@Filtered Article{03145211-733d-4708-9835-70301c6e2ea1,
 abstract = {Particle swarm optimization (PSO) is considered among the best seminal meta-heuristic algorithms,boasting merits of minimal parameter requirements, straightforward implementation, and highly accelerated convergence capacity, lower computational complexity, etc. Nevertheless, it also has drawbacks, for instance, it tends to converge prematurely at local optima, lack of diversity, and low accuracy. In order to effectively overcome these shortcomings, this paper presents a multi-strategy fusion enhanced PSO called MSFPSO algorithm. Firstly,It motivated by the black-winged kite algorithm, a migration mechanism based on Cauchy's variation is introduced. This mechanism contributes to the efficiency and effectiveness of the algorithm in exploiting the present search area. Also, it effectively balances the dynamics relationship between exploration and exploitation, boosting the algorithm's global and local search capabilities.Second, a joint-opposition selection strategy is introduced for expanding the solution search range. Our approach is designed to avoid getting stuck in local optima. Specifically, selective opposition obtains the proximity dimension of a candidate solution through a linearly decreasing threshold. Dynamic opposition further extends the process of investigating the solution space. The algorithm is fully incorporated with the differential creative search algorithm for dual-strategy scenarios to enhance the performance of the decision-making effectiveness, population diversity, exploitation capability of the PSO. Finally, an attraction-rejection optimization strategy is introduced to further obtain a good exploitation-exploration balance capability and avoid stagnation of the algorithm. In addition, the comparison results with eight advanced optimization algorithms and six improved particle swarm optimization algorithms on CEC2020 test sets, and the statistical analysis was conducted by Wilcoxon rank sum test. It illustrate the features of the MSFPSO developed within this research strong competitiveness. The convergence of the algorithm was verified at maximum iterations of 10000 on the CEC2017 test set. Meanwhile, the experimental outcomes of applying MSFPSO to 50 practical engineering design challenges prove its effectiveness and strong applicability. The test results and numerical computations manifest that the MSFPSO algorithm with strong competitiveness will become a preferred class of meta-heuristic algorithms to tackle issues within the realm of engineering optimization.},
 authors = {['Bin Shu', 'Gang Hu', 'Mao Cheng', 'Cunxia Zhang']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Particle swarm optimization', 'Cauchy variation', 'Joint adversarial selection', 'Differential creative search', 'Attraction-rejection', 'Algorithm fusion']},
 title = {MSFPSO: Multi-algorithm integrated particle swarm optimization with novel strategies for solving complex engineering design problems},
 year = {2025}
}

@Filtered Article{031b87ec-e76d-4cb9-adda-ba5da60e0336,
 abstract = {Sweden's pulp and paper sector accounts for a significant proportion of national energy usage, besides generating wastewater that causes eutrophication of nearby sinks. In this paper, the possibility of optimizing biological wastewater treatment at the Stora Enso Skoghall mill south of the city of Karlstad in central Sweden, with respect to electricity usage and the addition of nutrients, has been investigated. A computational model of the treatment process was developed, based on process data obtained from the said mill, and nine different scenarios were compared subsequently, with energy use, environmental impacts and operational expenses, as criteria. The most energy-efficient and cost-effective alternative was a combination of measures such as lowering the oxygen level in the MBBR (Moving Bed Bio-Reactor) from 3 mg/l to 2 mg/l and using the Hyperclassic aerator in the aerated lagoon. This arrangement yielded a 48.5 % reduction in operational expenses, and a 60 % decrease in the energy use, vis-à-vis the reference case, without affecting the efficiency of the treatment process. This also uncovered an opportunity to mitigate the annual global warming and eutrophication impacts, by approximately 100 tons CO2-eq. and 140 kg PO43−-eq. respectively. All attempts to optimise the use of resources and decrease the anthropogenic environmental footprint ought to be made to come closer to the targets set by the United Nations' sustainable development goals (SDGs). The authors' conclusion predicated on the results of the modelling and analysis done in this study is that the potential of seemingly small process modifications, such as lowering the oxygen level in the MBBR, and applying a more optimal dosage of nutrient salts, must not be overlooked by wastewater treatment plants in general (and those in pulp and paper mills in particular).},
 authors = {['Selma Zetterlund', 'Olivia Schwartz', 'Maria Sandberg', 'G. Venkatesh']},
 journal = {Journal of Water Process Engineering},
 keywords = {['Aeration', 'Biological wastewater treatment', 'Energy use optimisation', 'Nutrients', 'Pulp and paper mills']},
 title = {Computational modelling to advise and inform optimization for aeration and nutrient-dosing in wastewater treatment: Case study from pulp and paper mill in south-central Sweden},
 year = {2023}
}

@Filtered Article{034b5f1b-cd08-4bd6-ae2b-405ebb6126ca,
 abstract = {This study is the first to compare in the same subjects the specific spatial distribution and the functional and anatomical connectivity of the neuronal resources that activate and integrate syntactic representations during music and language processing. Combining functional magnetic resonance imaging with functional connectivity and diffusion tensor imaging-based probabilistic tractography, we examined the brain network involved in the recognition and integration of words and chords that were not hierarchically related to the preceding syntax; that is, those deviating from the universal principles of grammar and tonal relatedness. This kind of syntactic processing in both domains was found to rely on a shared network in the left hemisphere centered on the inferior part of the inferior frontal gyrus (IFG), including pars opercularis and pars triangularis, and on dorsal and ventral long association tracts connecting this brain area with temporo-parietal regions. Language processing utilized some adjacent left hemispheric IFG and middle temporal regions more than music processing, and music processing also involved right hemisphere regions not activated in language processing. Our data indicate that a dual-stream system with dorsal and ventral long association tracts centered on a functionally and structurally highly differentiated left IFG is pivotal for domain–general syntactic competence over a broad range of elements including words and chords.},
 authors = {['Mariacristina Musso', 'Cornelius Weiller', 'Andreas Horn', 'Volkmer Glauche', 'Roza Umarova', 'Jürgen Hennig', 'Albrecht Schneider', 'Michel Rijntjes']},
 journal = {NeuroImage},
 title = {A single dual-stream framework for syntactic computations in music and language},
 year = {2015}
}

@Filtered Article{036582a4-af99-44e5-98b8-a22f6ff7242f,
 abstract = {A large variety of models have been developed in the last two decades aiming at supply chain (SC) stress-testing and resilience. New digital and artificial intelligence (AI) technologies allow to develop novel approaches and tools in this area for the transition from standalone models to intelligent decision-support systems (DSSs). However, the literature lacks concepts and guidelines for the design of such systems. In this paper, we offer a generalized decision-making framework for using digital twins in SC stress-testing and resilience analysis as well as delineate how digital twins can contribute to theory development in SC resilience and viability. We position our proposed approach as an intelligent digital twin (iDT) – a human–AI system which visualizes physical SCs in digital form, collects and processes data for modelling using analytics methods, mimics human decision-making rules, and creates new knowledge and decision-making algorithms through human–AI collaboration. We conclude that the iDT supports monitoring, disruption prediction (early signals), event-driven responses, learning, and proactive thinking, integrating proactive and reactive approaches to SC resilience. The iDT helps to make the unknown known and so contributes to the development of a proactive, adaptation-based view on SC resilience and viability. This research can be used to solve existing problems in the industry, and it develops new methods and infrastructures for solutions to future problems.},
 authors = {['Dmitry Ivanov']},
 journal = {International Journal of Production Economics},
 keywords = {['Supply chain resilience', 'Intelligent digital twin', 'Data analytics', 'Stress-test', 'Ripple effect', 'anyLogistix']},
 title = {Intelligent digital twin (iDT) for supply chain stress-testing, resilience, and viability},
 year = {2023}
}

@Filtered Article{0386e933-53cf-4ef8-b731-8dd7d2bb941e,
 abstract = {Effective chronic disease prevention requires a systems approach to the design, implementation, and refinement of interventions that account for the complexity and interdependence of factors influencing health outcomes. This paper proposes the Participatory Implementation Systems Mapping (PISM) process, which combines participatory systems modeling with implementation strategy development to enhance intervention design and implementation planning. PISM leverages the collaborative efforts of researchers and community partners to analyze complex health systems, identify key determinants, and develop tailored interventions and strategies that are both adaptive and contextually relevant. The phases of the PISM process include strategize, innovate, operationalize, and assess. We describe and demonstrate how each phase contributes to the overall goal of effective and sustainable intervention implementation. We also address the challenges of data availability, model complexity, and resource constraints. We offer solutions such as innovative data collection methods and participatory model development to enhance the robustness and applicability of systems models. Through a case study on the development of a chronic disease prevention intervention, the paper illustrates the practical application of PISM and highlights its potential to guide epidemiologists and implementation scientists in developing interventions that are responsive to the complexities of real-world health systems. The conclusion calls for further research to refine participatory systems modeling techniques, overcome existing challenges in data availability, and expand the use of PISM in diverse public health contexts.},
 authors = {['Travis R. Moore', 'Erin Hennessy', 'Yuilyn Chang Chusan', 'Laura Ellen Ashcraft', 'Christina D. Economos']},
 journal = {Annals of Epidemiology},
 keywords = {['Implementation science', 'Implementation mapping', 'Community-engaged research', 'Systems thinking', 'Epidemiology', 'Systems science']},
 title = {Considerations for using participatory systems modeling as a tool for implementation mapping in chronic disease prevention},
 year = {2025}
}

@Filtered Article{0394a903-7d0d-40eb-9bff-f3470488844b,
 abstract = {The introduction of Artificial Intelligence (AI), specifically Generative AI (GenAI), has significantly transformed the higher education landscape. Despite the opportunities GenAI offers to students, they pose significant challenges for academic integrity. Thus, it is crucial for higher education institutions (HEI) to balance the use of GenAI for enhancing the learning experience of students with its ethical and responsible use in their educational journey. The present study proposes a comprehensive academic integrity framework focusing on three key stakeholders: students, educators, and institutions. We propose eight strategies ranging from collaborative learning for students to developing a comprehensive GenAI policy for institutions in maintaining academic integrity among students in HEI. Furthermore, we identified four challenges, namely financial, strategic, operational, and cultural, in the implementing a comprehensive academic integrity framework in the GenAI era. This study offers significant insights for HEI to maintain academic integrity among students in the GenAI era.},
 authors = {['Tareq Rasul', 'Sumesh Nair', 'Diane Kalendra', 'M.S. Balaji', 'Fernando de Oliveira Santini', 'Wagner Junior Ladeira', 'Raouf Ahmad Rather', 'Naveed Yasin', 'Raul V. Rodriguez', 'Panagiotis Kokkalis', 'Md Wahid Murad', 'Md Uzir Hossain']},
 journal = {The International Journal of Management Education},
 keywords = {['Generative AI', 'Academic integrity', 'Higher education', 'Students', 'Stakeholders']},
 title = {Enhancing academic integrity among students in GenAI Era:A holistic framework},
 year = {2024}
}

@Filtered Article{03a69463-b48c-4f0a-875f-83c76a304a01,
 abstract = {We introduce a novel detailed conductance-based model of the bursting activity in external tufted (ET) cells of the olfactory bulb. We investigate the mechanisms underlying their bursting, and make experimentally-testable predictions. The ionic currents included in the model are specific to ET cells, and their kinetic and other parameters are based on experimental recordings. We validate the model by showing that its bursting characteristics under various conditions (e.g. blocking various currents) are consistent with experimental observations. Further, we identify the bifurcation structure and dynamics that explain bursting behavior. This analysis allows us to make predictions of the response of the cell to current pulses at different burst phases. We find that depolarizing (but not hyperpolarizing) inputs received during the interburst interval can advance burst timing, creating the substrate for synchronization by excitatory connections. It has been hypothesized that such synchronization among the ET cells within one glomerulus might help coordinate the glomerular output. Next we investigate model parameter sensitivity and identify parameters that play the most prominent role in controlling each burst characteristic, such as the burst frequency and duration. Finally, the response of the cell to periodic inputs is examined, reflecting the sniffing-modulated input that these cell receive in vivo. We find that individual cells can be better entrained by inputs with higher, rather than lower, frequencies than the intrinsic bursting frequency of the cell. Nevertheless, a heterogeneous population of ET cells (as may be found in a glomerulus) is able to produce reliable periodic population responses even at lower input frequencies.},
 authors = {['Ryan Viertel', 'Alla Borisyuk']},
 journal = {Journal of Theoretical Biology},
 keywords = {['External tufted cell', 'Bursting', 'Glomerulus', 'Olfactory bulb', 'Hodgkin Huxley model']},
 title = {A Computational model of the mammalian external tufted cell},
 year = {2019}
}

@Filtered Article{03d6a1e1-4577-40b9-87b0-58e0acbf6c35,
 abstract = {This paper presents the integration of a cognitive architecture with an intelligent decision support model (IDSM) that is embedded into an autonomous non-deterministic safety critical system. The IDSM will integrate multi-criteria decision making via intelligent technologies like expert systems, fuzzy logic, machine learning and genetic algorithms. Cognitive technology is currently simulated in safety–critical systems to highlight variables of interest, interface with intelligent technologies, and provide an environment that improves a system’s cognitive performance. In this study, the IDSM is being applied to an actual safety–critical system, an unmanned surface vehicle (USV) with embedded artificial intelligence (AI) software. The USV’s safety performance is being researched in a simulated and a real world nautical based environment. The objective is to build a dynamically changing model to evaluate a cognitive architecture’s ability to ensure safe performance of an intelligent safety–critical system. The IDSM does this by finding a set of key safety performance parameters that can be critiqued via safety measurements, mechanisms and methodologies. The uniqueness of this research will be on bounding the decision making associated with the cognitive architecture’s key safety parameters (KSP). Other real-time applications that could benefit from advancing the safety of cognitive technologies are unmanned platforms, transportation technologies, and service robotics. The results will provide cognitive science researchers a reference for safety engineering artificially intelligent safety–critical systems.},
 authors = {['Harry H. Dreany', 'Robert Roncace', 'Paul Young']},
 journal = {Safety Science},
 keywords = {['Safety engineering', 'Artificial intelligence', 'Cognitive architecture', 'Decision support model', 'Intelligent technologies']},
 title = {Safety engineering of computational cognitive architectures within safety-critical systems},
 year = {2018}
}

@Filtered Article{044456ff-7fb8-43b6-8ce0-90281426941b,
 abstract = {The world is changing and business schools are struggling to keep up. Theories of reflective practice developed by the likes of Schon (1983), Gibbs (1988), Driscoll (1994, 2007) and Kolb (1984, 2015) are outdated and unfit for current purposes. Problems include the chronology of events, the orientation of the observer, the impact of external inputs, and the fact that neither education nor the workplace follow a structured, linear path. In response to these challenges, we propose a new ‘solution’: John Boyd's OODA loop. We argue that OODA loops offer the chance to reshape reflective practice and work-based learning for a world in which individuals must cope with ‘an unfolding evolving reality that is uncertain, ever changing and unpredictable’ (Boyd, 1995, slide 1). By embracing the philosophy of John Boyd and his OODA loop theory, business schools can develop greater resilience and employability in graduates, preparing them to embrace change while also embedding the concept of life-long learning to make them better equipped to face the uncertainty that the modern world brings.},
 authors = {['Mike Ryder', 'Carolyn Downs']},
 journal = {The International Journal of Management Education},
 keywords = {['OODA', 'Reflective practice', 'Experiential learning', 'Work-based learning', 'Employability', 'Business education']},
 title = {Rethinking reflective practice: John Boyd’s OODA loop as an alternative to Kolb},
 year = {2022}
}

@Filtered Article{04567fe8-9fdb-4842-b633-528da20cb811,
 abstract = {The needs of everyday life, such as counting and measuring, are roots of theoretical mathematics. I believe these roots are why mathematical ideas ground research so amazingly well within many scientific fields. Initially trained as a theoretical mathematician and having collaborated with non-mathematicians in the field of bone research, I address the advantages and challenges of collaborations across fields of research among investigators trained in different disciplines. I report on the mathematical ideas that have guided my research on the mechanics of bone tissue. I explain how the mathematical ideas of local vs. global properties influence my research. Polarized light microscopy (PLM) is a tool that I use consistently, in association with other microscopy techniques, to investigate bone in its healthy state and in the presence of bone disease, in humans and in animal models. I review the results that I and investigators around the world have obtained with PLM. Applied to thin bone sections, PLM yields extinct (black) and bright (white) signals that are interpreted in terms of the orientation of collagen type I, by means of other microscopy techniques. Collagen type I is an elementary component of bone tissue. Its orientation is important for the mechanical function of bone. Images obtained by PLM at a specific bone site yield big data sets regarding collagen orientation. Multiple data sets in respect of multiple sites are often needed for research because the bone tissue differs by location in response to the distinct forces acting on it. Mathematics, defined by philosophers as the theory of patterns, offers the backdrop for pattern identification in the big data sets regarding collagen orientation. I also discuss the computational aspect of the research, pursuant to which the patterns identified are incorporated in simulations of mechanical behaviors of bone. These mathematical ideas serve to understand the role of collagen orientation in bone fracture risk.},
 authors = {['Maria-Grazia Ascenzi']},
 journal = {Bone},
 keywords = {['Biomechanics', 'Lamella', 'Low-trauma fracture', 'Mathematics', 'Osteon', 'Pathology']},
 title = {Theoretical mathematics, polarized light microscopy and computational models in healthy and pathological bone},
 year = {2020}
}

@Filtered Article{04af10b4-fe14-4a52-9884-9dd3ecb3ccf2,
 authors = {['BRIAN MACWHINNEY', 'PING LI']},
 journal = {Elsevier},
 title = {CHAPTER 22 - Neurolinguistic Computational Models},
 year = {2008}
}

@Filtered Article{050542b5-daf6-4a7e-b99a-c443e2009853,
 abstract = {Financial attitude influences the financial behavior of retail investors. Although the extant research has acknowledged and examined this relationship, the measures of financial attitude and behavior still vary widely and are generally posed as a series of questions rather than statements. In addition to this, there is insufficient knowledge regarding retail investors' behavior in the face of a health crisis, such as the current COVID-19 pandemic. This study addresses these gaps in the prior literature by examining the relative influence of six dimensions of financial attitude, namely, financial anxiety, optimism, financial security, deliberative thinking, interest in financial issues, and needs for precautionary savings, on the trading activity of retail investors during the pandemic. Data were collected from 404 respondents and analyzed using the artificial neural network (ANN) method. The results revealed that all six dimensions had a positive influence on trading activity, with interest in financial issues exerting the strongest influence, followed by deliberative thinking. The study thus contributes important inferences for researchers and managers.},
 authors = {['Manish Talwar', 'Shalini Talwar', 'Puneet Kaur', 'Naliniprava Tripathy', 'Amandeep Dhir']},
 journal = {Journal of Retailing and Consumer Services},
 keywords = {['Artificial neural network', 'COVID-19', 'Financial behavior', 'Financial attitude', 'Financial anxiety', 'Pandemic']},
 title = {Has financial attitude impacted the trading activity of retail investors during the COVID-19 pandemic?},
 year = {2021}
}

@Filtered Article{05259e44-aa60-4425-bd1b-997fe4fa77c5,
 abstract = {There is an urgent need to identify new therapies that prevent SARS-CoV-2 infection and improve the outcome of COVID-19 patients. This pandemic has thus spurred intensive research in most scientific areas and in a short period of time, several vaccines have been developed. But, while the race to find vaccines for COVID-19 has dominated the headlines, other types of therapeutic agents are being developed. In this mini-review, we report several databases and online tools that could assist the discovery of anti-SARS-CoV-2 small chemical compounds and peptides. We then give examples of studies that combined in silico and in vitro screening, either for drug repositioning purposes or to search for novel bioactive compounds. Finally, we question the overall lack of discussion and plan observed in academic research in many countries during this crisis and suggest that there is room for improvement.},
 authors = {['Natesh Singh', 'Bruno O. Villoutreix']},
 journal = {Computational and Structural Biotechnology Journal},
 title = {Resources and computational strategies to advance small molecule SARS-CoV-2 discovery: Lessons from the pandemic and preparing for future health crises},
 year = {2021}
}

@Filtered Article{0534904a-840b-4897-9c00-3bac2323e3c2,
 abstract = {The National Science Foundation estimates that 80% of the jobs available during the next decade will require math and science skills, dictating that programs in biochemistry and molecular biology must be transformative and use new pedagogical approaches and experiential learning for careers in industry, research, education, engineering, health-care professions, and other interdisciplinary fields. These efforts require an environment that values the individual student and integrates recent advances from the primary literature in the discipline, experimentally directed research, data collection and analysis, and scientific writing. Current trends shaping these efforts must include critical thinking, experimental testing, computational modeling, and inferential logic. In essence, modern biochemistry and molecular biology education must be informed by, and integrated with, cutting-edge research. This environment relies on sustained research support, commitment to providing the requisite mentoring, access to instrumentation, and state-of-the-art facilities. The academic environment must establish a culture of excellence and faculty engagement, leading to innovation in the classroom and laboratory. These efforts must not lose sight of the importance of multidimensional programs that enrich science literacy in all facets of the population, students and teachers in K-12 schools, nonbiochemistry and molecular biology students, and other stakeholders. As biochemistry and molecular biology educators, we have an obligation to provide students with the skills that allow them to be innovative and self-reliant. The next generation of biochemistry and molecular biology students must be taught proficiencies in scientific and technological literacy, the importance of the scientific discourse, and skills required for problem solvers of the 21st century.},
 authors = {['Paul N. Black']},
 journal = {Journal of Biological Chemistry},
 keywords = {['biochemistry', 'molecular biology', 'teaching', 'learning', 'primary research', 'leadership', 'environment', 'inclusive excellence', 'STEM education', 'biochemistry and molecular biology teaching and learning']},
 title = {A revolution in biochemistry and molecular biology education informed by basic research to meet the demands of 21st century career paths},
 year = {2020}
}

@Filtered Article{05b5268e-a383-45a4-b270-ba0ebe66e878,
 abstract = {Background
Evolving research practices and new forms of research enabled by technological advances require a redesigned research oversight system that respects and protects human research participants.
Objective
Our objective was to generate creative ideas for redesigning our current human research oversight system.
Methods
A total of 11 researchers and institutional review board (IRB) professionals participated in a January 2015 design thinking workshop to develop ideas for redesigning the IRB system.
Results
Ideas in 5 major domains were generated. The areas of focus were (1) improving the consent form and process, (2) empowering researchers to protect their participants, (3) creating a system to learn from mistakes, (4) improving IRB efficiency, and (5) facilitating review of research that leverages technological advances.
Conclusions
We describe the impetus for and results of a design thinking workshop to reimagine a human research protections system that is responsive to 21st century science.},
 authors = {['Cinnamon Bloss', 'Camille Nebeker', 'Matthew Bietz', 'Deborah Bae', 'Barbara Bigby', 'Mary Devereaux', 'James Fowler', 'Ann Waldo', 'Nadir Weibel', 'Kevin Patrick', 'Scott Klemmer', 'Lori Melichar']},
 journal = {Journal of Medical Internet Research},
 keywords = {['ethics committees', 'research', 'biomedical research', 'telemedicine', 'informed consent', 'behavioral research']},
 title = {Reimagining Human Research Protections for 21st Century Science},
 year = {2016}
}

@Filtered Article{05cd8c9a-a32f-4da6-afa4-1ce65accf9c3,
 abstract = {Quantum computing (QC) is a new area of research which incorporates elements from mathematics, physics, and computing. Quantum computing has generated a growing interest among scientists, technologists, and industrialists. Over the past decade it provided a platform for research to people in the scientific, technical, and industrial fields. Quantum physics concepts have been used in developing the basics of QC. In QC, the parallel processing feature has reduced the algorithm complexities which are being used. This feature helped find solutions to several optimization problems and issues that were related to it. Quantum inspired intelligent computational methods have been used in several application areas. Image segmentation is one such area and the exploration of this feature in image segmentation is the primary focus of this chapter.},
 authors = {['D.P. Hudedagaddi', 'B.K. Tripathy']},
 journal = {Morgan Kaufmann},
 keywords = {['Quantum computing', 'Computing intelligence', 'Image segmentation', 'Evolutionary algorithms']},
 title = {Chapter 7 - Quantum inspired computational intelligent techniques in image segmentation},
 year = {2017}
}

@Filtered Article{05f48fef-1e1a-480f-b3fc-22649f597fde,
 abstract = {Determining RNA secondary structure is a core problem in computational biology. Fast algorithms for predicting secondary structure are fundamental to this task. We describe a modified formulation of the Zuker-Stiegler algorithm with coaxial stacking, a stabilising interaction in which the ends of helices in multi-loops are stacked. In particular, optimal coaxial stacking is computed as part of the dynamic programming state, rather than in an inner loop. We introduce a new notion of sparsity, which we call replaceability. Replaceability is a more general condition and applicable in more places than the triangle inequality that is used by previous sparse folding methods. We also introduce non-monotonic candidate lists as an additional sparsification tool. Existing usages of the triangle inequality for sparsification can be thought of as an application of both replaceability and monotonicity together. The modified recurrences along with replaceability allows sparsification to be applied to coaxial stacking as well, which increases the speed of the algorithm. We implemented this algorithm in software we call memerna, which we show to have the fastest exact (non–heuristic) implementation of RNA folding under the complete Turner 2004 model with coaxial stacking, out of several popular RNA folding tools supporting coaxial stacking. We also introduce a new notation for secondary structure which includes coaxial stacking, terminal mismatches, and dangles (CTDs) information. The memerna package 0.1 release is available at https://github.com/Edgeworth/memerna/tree/release/0.1.},
 authors = {['Eliot Courtney', 'Amitava Datta', 'David H. Mathews', 'Max Ward']},
 journal = {Journal of Molecular Biology},
 keywords = {['RNA secondary structure', 'sparsification', 'dynamic programming', 'nearest neighbor', 'energy model']},
 title = {memerna: Sparse RNA folding including coaxial stacking},
 year = {2025}
}

@Filtered Article{06160854-a1df-4bb5-acc2-c665588f1d43,
 abstract = {New sixteen uric acid metal complexes of different stoichiometry, stereo-chemistries and modes of interactions were synthesized using different metals Cr, Mn, Fe, Co, Ni, Cu, Cd, UO2, Na and K. The synthesized complexes were characterized by elemental analysis, spectral (IR, UV–Vis and ESR) methods, thermal analysis (TG, DTA and DSC) and magnetic susceptibility studies. Molecular modeling calculations were used to characterize the ligation sites of the free ligand. Furthermore, quantum chemical parameters of uric acid such as the energies of highest occupied molecular orbital (EHOMO), energies of lowest unoccupied molecular orbital (ELUMO), the separation energy (ΔE=ELUMO−EHOMO), the absolute electronegativity, χ, the chemical potential, Pi, the absolute hardness, η and the softness (σ) were obtained for uric acid. Eight different microbial categories were used to study the antimicrobial activity of the free ligand and ten of its complexes. The results indicate that the ligand and its metal complexes possess antimicrobial properties. The stoichiometry of iron–uric acid complex was studied by using different spectrophotometric methods.},
 authors = {['Mamdouh S. Masoud', 'Alaa E. Ali', 'Medhat A. Shaker', 'Gehan S. Elasala']},
 journal = {Spectrochimica Acta Part A: Molecular and Biomolecular Spectroscopy},
 keywords = {['Uric', 'Complexes', 'Synthesis', 'Spectroscopy', 'Thermal analysis', 'Computational']},
 title = {Synthesis, computational, spectroscopic, thermal and antimicrobial activity studies on some metal–urate complexes},
 year = {2012}
}

@Filtered Article{06689595-4305-4117-906d-768a3e524dd3,
 abstract = {Abstract reasoning is associated with the ability to detect relations among objects, ideas, events. It underlies the understanding of other individuals’ thoughts and intentions. In natural settings, individuals have to infer relevant associations that have proven to be reliable or precise predictors. Salience theory suggests that the attribution of meaning to stimulus depends on their contingency, saliency, and relevance to adaptation. So far, subjective estimates of relevance have mostly been explored in motivation and implicit learning. Mechanisms underlying formation of associations in abstract thinking with regard to their subjective relevance, or salience, are not clear. Applying novel computational methods, we investigated relevance detection in categorization tasks in 17 healthy individuals. Two models of relevance detection were developed: a conventional one with nouns from the same semantic category, an aberrant one based on an insignificant common feature. Control condition introduced non-related words. The participants were to detect either a relevant principle or an insignificant feature to group presented words. In control condition they inferred that the stimuli were irrelevant to any grouping idea. Cross-frequency phase coupling analysis revealed statistically distinct patterns of synchronization representing search and decision in the models of normal and aberrant relevance detection. Significantly distinct frontotemporal functional networks with central and parietal components in the theta and alpha frequency bands may reflect differences in relevance detection.},
 authors = {['Aleksandra Miasnikova', 'Gleb Perevoznyuk', 'Olga Martynova', 'Mikhail Baklushev']},
 journal = {Neuroscience Research},
 keywords = {['Abstract reasoning', 'Salience', 'Phase synchronization', 'Cross-frequency coupling', 'Phase-to-phase coupling', 'EEG']},
 title = {Cross-frequency phase coupling of brain oscillations and relevance attribution as saliency detection in abstract reasoning},
 year = {2021}
}

@Filtered Article{067c67b4-754d-44ea-93e1-3c1f13b4160e,
 abstract = {We identify a metaphor for the design activity: we view design as bricolage. We start from describing bricolage, and we proceed to the relationship of design to art. We obtain a characterisation of design that enables us to show that both traditional and contemporary design are forms of bricolage. We examine the consequences of `design as bricolage' for the relationship between design and science and for the extent of the design activity.},
 authors = {['Panagiotis Louridas']},
 journal = {Design Studies},
 keywords = {['aesthetics', 'design activity', 'design cognition', 'metaphor', 'psychology of design']},
 title = {Design as bricolage: anthropology meets design thinking},
 year = {1999}
}

@Filtered Article{06d3245f-f6ff-4010-ba7d-4538228c2b26,
 abstract = {This paper explores a rough set-based approach for supporting insightful reasoning in Intelligent Systems (ISs). The novelty lies in the introduction of a new concept for approximate reasoning processes based on granular computations. Although many rough set theory extensions developed over time focus on reasoning about (partial) set inclusion, these approximation spaces sometimes fall short when dealing with crucial aspects of approximate reasoning within ISs. Specifically, these systems aim to construct high-quality approximations of compound decision granules that represent solutions. Here, we present the basis for insightful reasoning realized through approximate reasoning processes grounded in granular computations. By doing so, we provide a sufficiently rich basis for designing IS problem solvers. This basis allows ISs to restructure or adapt their reasoning based on the generated granular computations, ultimately leading to high-quality granular solutions.},
 authors = {['Andrzej Skowron', 'Jaroslaw Stepaniuk']},
 journal = {Information Sciences},
 keywords = {['Artificial intelligence', 'Granular computing', 'Rough sets', 'Granular approximation process', 'Reasoning over granular computations']},
 title = {Toward rough set based insightful reasoning in intelligent systems},
 year = {2025}
}

@Filtered Article{06d4c191-346e-4483-ab8a-f887d2f84707,
 abstract = {The state-of-the-art of membrane technology is characterized by a number of mature applications such as sterile filtration, hemodialysis, water purification and gas separation, as well as many more niche applications of successful membrane-based separation and processing of fluid mixtures. The membrane industry is currently employing a portfolio of established materials, mostly standard polymers or inorganic materials (not originally developed for membranes), and easily scalable manufacturing processes such as phase inversion, interfacial polymerization and coating. Innovations in membranes and their manufacturing processes must meet the desired intrinsic properties that determine selectivity and flux, for specific applications. However, tunable and stable performance, as well as sustainability over the entire life cycle of membrane products are becoming increasingly important. Membrane manufacturers are progressively required to share the carbon footprint of their membrane modules with their customers. Environmental awareness among the world's population is a growing phenomenon and finds its reflection in product development and manufacturing processes. In membrane technology one can see initial steps in this direction with the replacement of hazardous solvents, the utilization of renewable materials for membrane production and the reuse of membrane modules. Other examples include increasing the stability of organic membrane polymers and lowering the cost of inorganic membranes. In a long-term perspective, many more developments in materials science will be required for making new, advanced membranes. These include “tools” such as self-assembly or micro- and nano-fabrication, and “building blocks”, e.g. tailored block copolymers or 1D, 2D and 3D materials. Such membranes must be fabricated in a simpler manner and be more versatile than existing ones. In this perspective paper, a vision of such LEGO®-like membranes with precisely adjustable properties will be illustrated with, where possible, examples that already demonstrate feasibility. These include the possibility to switch properties using an external stimulus, adapting a membrane's selectivity to a given separation, or providing the ability to assemble, disassemble and reassemble the membrane on a suitable support as scaffold, in situ, in place and on-demand. Overall, it is foreseen that the scope of future membrane applications will become much wider, based on improved existing membrane materials and manufacturing processes, as well as the combination of novel, tailor-made “building blocks” and “tools” for the fabrication of next-generation membranes tuned to specific applications.},
 authors = {['Suzana P. Nunes', 'P. Zeynep Culfaz-Emecen', 'Guy Z. Ramon', 'Tymen Visser', 'Geert Henk Koops', 'Wanqin Jin', 'Mathias Ulbricht']},
 journal = {Journal of Membrane Science},
 title = {Thinking the future of membranes: Perspectives for advanced and new membrane materials and manufacturing processes},
 year = {2020}
}

@Filtered Article{07662508-4f49-4b06-99b8-9e0e0d6e7efd,
 abstract = {Abstracts:
The collection and analysis of data for intelligence purposes is vital to national security. There are a number of hurdles including the exponentially increasing volume of available data, the need for increased cooperation between national and international agencies due to the increasingly globalized nature of threats to citizens and nations, and the need to be flexible in identifying new threats. Increasing reliance on computers is necessary, but complications arise due to such issues as incompatible data formats, multiple natural languages, and data privacy concerns. However, a potential solution to solving some of these problems for national security and law enforcement agencies is C2LG (Command and Control Lexical Grammar), which was originally developed for use within NATO, and is being adapted for use in crisis management and the fight against international organized crime.},
 authors = {['Kellyn Rein']},
 journal = {Butterworth-Heinemann},
 keywords = {['agencies', 'analysis', 'data', 'fusion', 'human', 'intelligence', 'language', 'management', 'national', 'security', 'sources', 'text']},
 title = {Chapter 16 - Re-thinking Standardization for Interagency Information Sharing},
 year = {2013}
}

@Filtered Article{077c88a0-2d19-4ac8-93b0-5fa9a5c83f25,
 abstract = {Much of the rich internal world constructed by humans is derived from, and experienced through, visual mental imagery. Despite growing appreciation of visual exploration in guiding episodic memory processes, extant theories of prospection have yet to accommodate the precise role of visual mental imagery in the service of future-oriented thinking. We propose that the construction of future events relies on the assimilation of perceptual details originally experienced, and subsequently reinstantiated, predominantly in the visual domain. Individual differences in the capacity to summon discrete aspects of visual imagery can therefore account for the diversity of content generated by humans during future simulation. Our integrative framework provides a novel testbed to query alterations in future thinking in health and disease.},
 authors = {['Federica Conti', 'Muireann Irish']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['future thinking', 'visual mental imagery', 'episodic memory', 'imagination', 'hippocampus', 'default mode network']},
 title = {Harnessing Visual Imagery and Oculomotor Behaviour to Understand Prospection},
 year = {2021}
}

@Filtered Article{07985276-a07b-4082-905d-18372155e694,
 authors = {['M. Myerscough', 'A. Chalmers']},
 journal = {Atherosclerosis},
 title = {Tracking the development of atherosclerosis in silico: a computational model for early inflammatory events},
 year = {2014}
}

@Filtered Article{07c77318-7903-48bc-9583-b9bd5822823c,
 abstract = {Developing Scientific Reasoning (SR) competencies at an early age, are challenging to meet expectation of the 4th sustainable development goal. Hence, educators and educational decision-makers try to embed these competencies into such subjects as the arts, language, technology, economics, mathematics and science, using an inter-disciplinary approach. In this context, this paper proposes a fuzzy knowledge-based solution to build practical pupils, educators, and decision-makers recommender system to support the development of SR competencies in a data driver manner. Our system consists of:(1) inferring and computational module that calculates in a fuzzy manner the global appreciation to each SR-competencies. (2) recommendation module that aims to help learners, educators and decision makers to assess the degree of development of SR competencies and to get alternative suggestion of remediation. The proposed solution has been tested on the last two levels of science education in four Tunisian elementary schools in different regions. A preliminary analysis showed that the learning process should be more focused on Tunisian pupil's profile, and that investigation and collaborative based learning should be applied further in Tunisian classroom.},
 authors = {['Manel BenSassi', 'Henda Ben Ghezala']},
 journal = {Procedia Computer Science},
 keywords = {['Learning analytics', 'educational recommendations', 'fuzzy competencies assessment', 'knowledge representation', 'fuzzy ontology', 'Scientific Reasoning competencies']},
 title = {Fuzzy knowledge based assessment system for K-12 Scientific Reasoning Competencies},
 year = {2023}
}

@Filtered Article{07f79dc9-7ea7-4f8b-9548-7a36e7d8efde,
 abstract = {In this work I introduce a simple model to study how natural selection acts upon aging, which focuses on the viability of each individual. It is able to reproduce the Gompertz law of mortality and can make predictions about the relation between the level of mutation rates (beneficial/deleterious/neutral), age at reproductive maturity and the degree of biological aging. With no mutations, a population with low age at reproductive maturity R stabilizes at higher density values, while with mutations it reaches its maximum density, because even for large pre-reproductive periods each individual evolves to survive to maturity. Species with very short pre-reproductive periods can only tolerate a small number of detrimental mutations. The probabilities of detrimental (Pd) or beneficial (Pb) mutations are demonstrated to greatly affect the process. High absolute values produce peaks in the viability of the population over time. Mutations combined with low selection pressure move the system towards weaker phenotypes. For low values in the ratio Pd/Pb, the speed at which aging occurs is almost independent of R, while higher values favor significantly species with high R. The value of R is critical to whether the population survives or dies out. The aging rate is controlled by Pd and Pb and the amount of the viability of each individual is modified, with neutral mutations allowing the system more “room” to evolve. The process of aging in this simple model is revealed to be fairly complex, yielding a rich variety of results.},
 authors = {['Aristotelis Kittas']},
 journal = {Journal of Theoretical Biology},
 keywords = {['Evolution', 'Aging', 'Computer simulations', 'Age-structured populations', 'Modelling']},
 title = {Evolution of the rate of biological aging using a phenotype based computational model},
 year = {2010}
}

@Filtered Article{084e4075-6307-4e9f-9487-1cb570ff846f,
 abstract = {Herbert Simon made overlapping substantive contributions to the fields of economics, psychology, cognitive science, artificial intelligence, decision theory, and organization theory. Simon’s work was motivated by the belief that neither the human mind, human thinking and decision making, nor human creativity need be mysterious. It was after he helped create “thinking” machines that Simon came to understand human intuition as subconscious pattern recognition. In doing so he showed that intuition need not be associated with magic and mysticism, and that it is complementary with analytical thinking. This paper will show how the overlaps in his work and especially his work on AI affected his view towards intuition.},
 authors = {['Roger Frantz']},
 journal = {Journal of Economic Psychology},
 keywords = {['Herbert Simon', 'Intuition', 'Artificial intelligence', 'Bounded rationality', 'Economics and psychology']},
 title = {Herbert Simon. Artificial intelligence as a framework for understanding intuition},
 year = {2003}
}

@Filtered Article{08546427-72b0-4889-a508-df74bc38534f,
 abstract = {Serena Nik-Zainal, MD, PhD, is professor of genomic medicine and bioinformatics and an honorary consultant in clinical genetics at the University of Cambridge. Prof. Nik-Zainal has dedicated her career to studying the physiology of cancer mutagenesis via a combination of computational and experimental work, as well as validation with clinical data. Among the many awards she has earned for her work, she has recently received the 2024 ESMO Award for Translational Research, for the research in the field of mutational signatures and her efforts in translating their use into clinics.},
 authors = {['Serena Nik-Zainal']},
 journal = {Cell Reports Medicine},
 title = {Prof. Serena Nik-Zainal},
 year = {2024}
}

@Filtered Article{0871f322-3714-4701-a18e-d6f28da70fe7,
 abstract = {This paper summarizes some of Robert Mare’s major contributions as a sociologist, demographer, and social statistician; as a pioneer who advanced the multi-generational perspective in social science research; as a leader who introduced demographic thinking to social mobility studies; and as a trailblazer who developed new approaches to studying multi-generational processes},
 authors = {['Xi Song']},
 journal = {Research in Social Stratification and Mobility},
 title = {Robert Mare’s legacy: Multi-generational processes},
 year = {2023}
}

@Filtered Article{08755152-8094-4fe6-9537-573d933099c0,
 abstract = {Publisher Summary
This chapter describes system modeling and its relationship to models of computation. It compares several different models of computation and evaluates their usefulness at various stages in system design. It also describes the modeling environment for software and hardware (MESH) environment for hardware and software modeling. Models of computation (MoCs) are abstract representations of computing systems. Computer modeling can be separated into three areas—formal MoCs, computer artifacts, and computer design tools. A formal MoC is generally considered to be one with a mathematical basis. Simulations of formal models may be more efficient for large systems; however, the properties of formal models permit the representation of the system to be manipulated purely mathematically. Computer artifacts are the objects of computer architects. They include software, hardware, or both. Design tools are computer programs that are used to assist the construction of instances of computers as well as the conceptualization of computer artifacts. Design tools may be considered synonymous with design artifacts because they are objects, or entities, used to facilitate the design process. They may have a formal mathematical basis.},
 authors = {['JoAnn M. Paul', 'Donald E. Thomas']},
 journal = {Morgan Kaufmann},
 title = {Chapter 15 - Models of Computation for Systems-on-Chips},
 year = {2005}
}

@Filtered Article{08d58b8a-a8dd-4c79-ac27-15a9b0678f3a,
 abstract = {Accurate measurements of the acidities and basicities of nucleic bases and nucleic base derivatives is essential for understanding issues of fundamental importance in biological systems. Hydrogen bonding modulates recognition of DNA and RNA bases, and the interaction energy between two bonded complementary nucleobases is dependent on the intrinsic basicity and acidity of the acceptor and donor groups. In addition, understanding the intrinsic reactivity of nucleic bases can shed light on key biosynthetic mechanisms for which nucleobases are substrates. In this review, we highlight advances in our lab toward understanding the fundamental reactivity of DNA and RNA. In particular, we focus on our investigation of the gas phase acidities and basicities of natural and unnatural nucleobases, and the implications of our results for the mechanisms of nucleotide biosynthetic and repair enzymes.},
 authors = {['Jeehiun K. Lee']},
 journal = {International Journal of Mass Spectrometry},
 keywords = {['Nucleic acids', 'RNA', 'DNA', 'Enzyme nucleobase', 'Acidity', 'Proton affinity']},
 title = {Insights into nucleic acid reactivity through gas-phase experimental and computational studies},
 year = {2005}
}

@Filtered Article{09705fa8-4dfc-42c2-9dcd-4767f66fa8a2,
 abstract = {This chapter reviews models of choice on two levels: The first concerns the descriptions of choice and their evolution from normative models of how choices should be make to more behaviorally realistic models, more consistent with data showing that choice depends heavily on context. We present brief overviews of risky and riskless choice models and data and for choice over time. We then turn to computational process models, a more recent class of models that make prediction for multiple properties of the decision process beyond simply what is chosen, including predicting the distribution of errors and decision times.These models are typically applied to simpler choices, but have found great use in contemporary neuroscience.},
 authors = {['Eric J. Johnson', 'Roger Ratcliff']},
 journal = {Academic Press},
 keywords = {['Computation Process Models', 'Decision Neuroscience', 'Drift-Diffusion Models', 'economic theory', 'Intertemporal Choice', 'Riskless Choice', 'Risky Choice']},
 title = {Chapter 3 - Computational and Process Models of Decision Making in Psychology and Behavioral Economics},
 year = {2014}
}

@Filtered Article{09adfb3d-d51c-4de3-b56f-6c0ec98a5527,
 abstract = {Technological advances and high-throughput bio-chemical assays are rapidly changing ways how we formulate and test biological hypotheses, and how we treat patients. Most complex diseases arise on a background of genetics, lifestyle and environment factors, and manifest themselves as a spectrum of symptoms. To fathom intricate biological processes and their changes from healthy to disease states, we need to systematically integrate and analyze multi-omics datasets, ontologies, and diverse annotations. Without proper management of such complex biological and clinical data, artificial intelligence (AI) algorithms alone cannot be effectively trained, validated, and successfully applied to provide trustworthy and patient-centric diagnosis, prognosis and treatment. Precision medicine requires to use multi-omics approaches effectively, and offers many opportunities for using AI, “big data” analytics, and integrative computational biology workflows. Advances in optical and biochemical assay technologies including sequencing, mass spectrometry and imaging modalities have transformed research by empowering us to simultaneously view all genes expressed, identify proteome-wide changes, and assess interacting partners of each individual protein within a dynamically changing biological system, at an individual cell level. While such views are already having an impact on our understanding of healthy and disease conditions, it remains challenging to extract useful information comprehensively and systematically from individual studies, ensure that signal is separated from noise, develop models, and provide hypotheses for further research. Data remain incomplete and are often poorly connected using fragmented biological networks. In addition, statistical and machine learning models are developed at a cohort level and often not validated at the individual patient level. Combining integrative computational biology and AI has the potential to improve understanding and treatment of diseases by identifying biomarkers and building explainable models characterizing individual patients. From systematic data analysis to more specific diagnostic, prognostic and predictive biomarkers, drug mechanism of action, and patient selection, such analyses influence multiple steps from prevention to disease characterization, and from prognosis to drug discovery. Data mining, machine learning, graph theory and advanced visualization may help identify diagnostic, prognostic and predictive biomarkers, and create causal models of disease. Intertwining computational prediction and modeling with biological experiments leads to faster, more biologically and clinically relevant discoveries. However, computational analysis results and models are going to be only as accurate and useful as correct and comprehensive are the networks, ontologies and datasets used to build them. High quality, curated data portals provide the necessary foundation for translational research. They help to identify better biomarkers, new drugs, precision treatments, and should lead to improved patient outcomes and their quality of life. Intertwining computational prediction and modeling with biological experiments, efficiently and effectively leads to more useful findings faster.},
 authors = {['I Jurisica']},
 journal = {Best Practice & Research Clinical Rheumatology},
 keywords = {['Precision medicine', 'Rheumatoid arthritis', 'Artificial intelligence', 'Integrative computational biology']},
 title = {Explainable biology for improved therapies in precision medicine: AI is not enough},
 year = {2024}
}

@Filtered Article{09ebd6db-fa97-4c49-a331-f54c0a2f851c,
 abstract = {A series of 10‐[(4‐halo‐2,6‐diisopropylphenyl)imino]phenanthren‐9‐ones and derivatives of the phenanthrene‐9,10‐dione ligand have been synthesised and structurally characterised to explore two types of noncovalent interactions, namely the influence of the steric bulk upon the resulting C–H···π and π‐stacking interactions and halogen bonding. Selected noncovalent interactions have additionally been analysed by DFT and AIM techniques. No halogen bonding has been observed in these systems, but X lone pair···π, C–H···O=C and C–H···π interactions are the prevalent ones in the halogenated systems. Removal of the steric bulk in N‐(2,4,6‐trimethylphenyl)‐9,10‐iminophenanthrenequinone affords different noncovalent interactions, but the C–H···O=C hydrogen bonds are observed. Surprisingly, in N‐(2,6‐dimethylphenyl)‐9,10‐iminophenanthrenequinone and N‐(phenyl)‐9,10‐iminophenanthrenequinone these C–H···O=C hydrogen bonds are not observed. However, they are observed in the related 2,6‐di‐tert‐butylphenanthrene‐9,10‐dione. The π‐interactions in dimers extracted from the crystal structures have been analysed by DFT and AIM. Spectroscopic investigations are also presented and these show only small perturbations to the O=C–C=N fragment.},
 authors = {['David Farrell', 'Samuel J. Kingston', 'Dmitry Tungulin', 'Stefano Nuzzo', 'Brendan Twamley', 'James A. Platts', 'Robert J. Baker']},
 journal = {European Journal of Organic Chemistry},
 keywords = {['Stacking interactions', 'Noncovalent interactions', 'Density functional calculations']},
 title = {N‐Aryl‐9,10‐phenanthreneimines as Scaffolds for Exploring Noncovalent Interactions: A Structural and Computational Study},
 year = {2017}
}

@Filtered Article{0a47b67a-09ab-4877-bf7b-0dceaa6633e5,
 abstract = {OMiLAB is a community of practice which offers a digital ecosystem bringing together open technologies to investigate and apply conceptual modeling methods for varying purposes and domains. One of the core value propositions is a dedicated Digital Innovation environment comprising several toolkits and workspaces, designed to support Product-Service Systems (PSS) prototyping – a key ingredient for PSS lifecycle management. At the core of this environment is a notion of Agile Digital Twin – a conceptual representation that can be tailored with knowledge engineering means to bridge the semantic and functional gap between a business perspective (focusing on value creation) and an engineering perspective (focusing on cyber-physical proofs-of-concept). To facilitate this bridging, the hereby proposed environment orchestrates, across three abstraction layers, methods such as Design Thinking, Agile Modeling Method Engineering and Model-driven Engineering to turn Ideation into smart Product-Service Systems experiments, in a laboratory setting. The proposed environment was built following Design Science principles. It addresses the problem of historically-disconnected skills required for Digital Innovation projects and it provides a testbed for feasibility experimentation. For design-oriented, artifact building research, a higher Technology Readiness Level can thus be achieved (compared to the level that idea development methods typically attain).},
 authors = {['Dimitris Karagiannis', 'Robert Andrei Buchmann', 'Wilfrid Utz']},
 journal = {Computers in Industry},
 keywords = {['Digital twin', 'Physical twin', 'Smart Product-service Systems', 'Agile modeling method engineering', 'OMiLAB', 'Domain-specific conceptual modeling']},
 title = {The OMiLAB Digital Innovation environment: Agile conceptual models to bridge business value with Digital and Physical Twins for Product-Service Systems development},
 year = {2022}
}

@Filtered Article{0ab65260-f74f-47b2-99da-5f5c060afe5c,
 abstract = {Packaging design, as an important element in product appearance, can directly affect customers' sensory perception of the product. Many universities even offer packaging design majors, which mainly use natural science and aesthetic knowledge to promote product sales. However, many old brands remain complacent and their packaging design still adopts traditional thinking, which to some extent affects their sales. Therefore, this article decided to use genetic algorithms as a tool to parameterize the 3D fractal images in packaging design, aiming to create more creative and eye-catching packaging designs. At the end of this article, an experiment was conducted on two branches of a certain brand. Branch 1 tried out the new design provided in this article, while Branch 2 continued to use the original design. After Branch 1 fully adopted the design, sales skyrocketed, from the original daily sales of 50-60 units to 70-85 units. Branch 2 remained unchanged, with a sharp contrast.},
 authors = {['Jinxia Wang']},
 journal = {Procedia Computer Science},
 keywords = {['Genetic Algorithm', 'Packaging Design', '3D Fractal Image', 'NSGA - II']},
 title = {Parameterization Design of 3D Fractal Images in Packaging Design Based on Genetic Algorithm},
 year = {2023}
}

@Filtered Article{0ad548cd-df2e-4e11-85a9-7c0bc177af96,
 abstract = {Utilizing a case study approach, this research investigates the transformation of elementary students' Maker mindsets within the context of Maker education through a lesson study cycle. The study focuses on the Maker mindsets transformation of three students with varying abilities, deliberately chosen as information-rich participants. A project-specific questionnaire, the Maker Mindsets Scale, was employed to facilitate self-assessment of Maker mindsets before and after intervention. Additionally, teachers' post-lesson discussion meetings were observed, and semi-structured interviews with participating teachers were conducted to gauge their perceptions of students' Maker mindsets transformation. The analysis encompassed students' semi-structured reflection logs and interviews to uncover the underlying factors driving Maker mindsets transformation. The results revealed distinct variations in how students of different abilities perceived their Maker mindsets transformation. Nonetheless, participant teachers consistently observed transformations in STEM (Science, Technology, Engineering, Mathematics) thinking skills, self-efficacy, motivation, and collaborative learning across all students. The study further identifies a collaborative convergence of multiple factors contributing to Maker mindsets transformation, spanning teacher, student, and pedagogical perspectives. These findings carry significant implications for educators, advocating for the implementation of customized strategies, authentic contextualization, structured methodologies, and collaborative frameworks to holistically nurture Maker mindsets evolution. Moreover, our study underscores the practicality of the LS approach in fostering collaborative development of innovative pedagogical strategies aimed at fostering Maker mindsets formation.},
 authors = {['Jiajia Li', 'Zhuang Li', 'Huixin Gao', 'Tianying Yun']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Maker mindsets', 'STEM learning', 'Maker education', 'Lesson study']},
 title = {Transforming maker mindsets: A case study of elementary students in a maker education context during lesson study},
 year = {2024}
}

@Filtered Article{0ad79d09-f81b-414e-bcce-22aeddc9bc40,
 abstract = {The success of our actions often depends on what others are doing. How does the brain discern predictions of others’ actions when situations are ambiguous? Recent work by Ma and colleagues suggests that the brain solves this problem by entertaining multiple predictions of others’ actions, ranked by their likelihood.},
 authors = {['Yongling Lin', 'Marco K. Wittmann']},
 journal = {Trends in Neurosciences},
 keywords = {['social cognition', 'neuroimaging', 'decision making', 'prediction', 'theory of mind', 'computational modelling']},
 title = {Multiple predictions of others’ actions in the human brain},
 year = {2025}
}

@Filtered Article{0ae90737-d3bc-40a4-800c-65c5d3f6b4a3,
 abstract = {Plant growth-promoting rhizobacteria (PGPR) can improve crop yields, nutrient use efficiency, plant tolerance to stressors, and confer benefits to future generations of crops grown in the same soil. Unlocking the potential of microbial communities in the rhizosphere and endosphere is therefore of great interest for sustainable agriculture advancements. Before plant microbiomes can be engineered to confer desirable phenotypic effects on their plant hosts, a deeper understanding of the interacting factors influencing rhizosphere community structure and function is needed. Dealing with this complexity is becoming more feasible using computational approaches. In this review, we discuss recent advances at the intersection of experimental and computational strategies for the investigation of plant–microbiome interactions and the engineering of desirable soil microbiomes.},
 authors = {['Chiara A. Berruto', 'Gozde S. Demirer']},
 journal = {Trends in Microbiology},
 keywords = {['rhizosphere engineering', 'plant microbiome', 'machine learning', 'community modeling', 'host–microbe interactions', 'microbiome-associated phenotype']},
 title = {Engineering agricultural soil microbiomes and predicting plant phenotypes},
 year = {2024}
}

@Filtered Article{0bbe1a7d-f67c-4adc-a9ad-1ea19820d144,
 abstract = {Sensitivity analysis is a key tool in the study of the relationships between the input parameters of a model and the output solution. Although sensitivity analysis is extensively addressed in the literature, little attention has been brought to the methodological aspects of the sensitivity of nonlinear parametric solutions computed through a continuation technique. This paper proposes four combinations of sensitivity analysis with continuation and homotopy methods, including sensitivity analysis along solution branches or at a particular point. Theoretical aspects are discussed in the higher order continuation framework Diamant. The sensitivity methods are applied to a thermal ignition problem and some free vibration problems. Remarkable eigenvalue maps are produced for the complex nonlinear eigenvalue problems.},
 authors = {['Isabelle Charpentier', 'Komlanvi Lampoh']},
 journal = {Applied Mathematical Modelling},
 keywords = {['Continuation', 'Homotopy', 'Sensitivity', 'Automatic differentiation', 'Diamant', 'Complex nonlinear eigenvalue problem', '']},
 title = {Sensitivity computations in higher order continuation methods},
 year = {2016}
}

@Filtered Article{0c46290d-2643-4ba8-a690-6e9df25b859e,
 abstract = {In the 20th century the interpretation of the human mind and brain as a computer has replaced the 18th century metaphor of “man as a machine”. This paper traces the development of the computational metaphor with some attention to its 18th century roots, and then argues that its employment need not lead to the mechanization of thinking and the autonomy of technique. An awareness of the metaphoric and, therefore, hypothetical status of the computational metaphor will prevent technique from escaping intentional human control. This is a shortened version of a paper included in C. Mitcham and Alois Huning, eds., Philosophy and Technology II: Information Technology and Computers in Theory and Practice (Boston: D. Reidel, in press), and is included here with permission.},
 authors = {['Earl R. MacCormac']},
 journal = {Technology in Society},
 title = {Men and machines: The computational metaphor},
 year = {1984}
}

@Filtered Article{0c5a6bed-46d0-49c9-96b2-7e4dda83a651,
 abstract = {The aim of this paper is twofold. First, we provide a methodological pathway from theories of situated, embodied cognition to simulations with an eye to empirical evidence, and suggest a possible cross-fertilization between cognitive robotics and psychology. Psychological theories, in particular those formulated at an abstract level, include models which are often severely underspecified at the level of mechanisms. This is true in the synchronic, constructive perspective (how can the effects observed in experiments be concretely generated by the model's mechanisms?) and in the diachronic, developmental perspective (how can such mechanisms be learned and developed?). The synthetic method of artificial cognitive systems research, and in particular of cognitive robotics, can complement research in psychology (and neurosciences) by exploring the constructive and developmental aspects of theories. Our second aim is to provide an example of such a methodology by describing simulations aiming at developing a perceptual symbol system (PSS) (Barsalou, 1999). We then describe the two main theoretical constructs of the PSS, perceptual symbols and simulators, illustrate their development in an artificial system, and test the system in prediction, categorization, and abstraction tasks.},
 authors = {['Giovanni Pezzulo', 'Gianguglielmo Calvi']},
 journal = {New Ideas in Psychology},
 keywords = {['Perceptual symbol systems', 'Schemas', 'Embodiment', 'Anticipation', 'Simulation']},
 title = {Computational explorations of perceptual symbol systems theory},
 year = {2011}
}

@Filtered Article{0cabb24e-f2f3-47fb-8b7e-0a0b500d9afb,
 abstract = {One of the main challenges of cognitive science is to explain the representation of conceptual knowledge and the mechanisms involved in evaluating the similarities between these representations. Theories that attempt to explain this phenomenon should account for the fact that conceptual knowledge is not static. In line with this thinking, many studies suggest that the representation of a concept changes depending on context. Traditionally, concepts have been studied as vectors within a geometric space, sometimes called Semantic-Vector Space Models (S-VSMs). However, S-VSMs have certain limitations in emulating human biases or context effects when the similarity of concepts is judged. Such limitations are related to the use of a classical geometric approach that represents a concept as a point in space. Recently, some theories have proposed the use of sequential projections of subspaces based on Quantum Probability Theory (Busemeyer and Bruza, 2012; Pothos et al., 2013). They argue that this theoretical approach may facilitate accounting for human similarity biases and context effects in a more natural way. More specifically, Pothos and Busemeyer (2011) proposed the Quantum Similarity Model (QSM) to determine expectation in conceptual spaces in a non-monotonic logic frame. To the best of our knowledge, previous data-driven studies have used the QSM subspaces in a unidimensional way. In this paper, we present a data-driven method to generate these conceptual subspaces in a multidimensional manner using a traditional S-VSM. We present an illustration of the method taking Tversky’s classical examples to explain the effects of Asymmetry, Triangular Inequality, and the Diagnosticity by means of sequential projections of those conceptual subspaces.},
 authors = {['Alejandro Martínez-Mingo', 'Guillermo Jorge-Botana', 'José Ángel Martinez-Huertas', 'Ricardo {Olmos Albacete}']},
 journal = {Cognitive Systems Research},
 keywords = {['Quantum similarity model', 'Semantic-vector space models', 'Computational linguistics', 'Similarity']},
 title = {Quantum projections on conceptual subspaces},
 year = {2023}
}

@Filtered Article{0d84aabf-3c36-4de2-a076-e6b8f02e6e2b,
 abstract = {Considerable cost and effort are invested in government and private-sector activities aimed at providing a societally tolerable level of fire safety of the built environment. This is particularly true with respect to fire safety of new building construction. On the government side, this includes activities associated with building and fire regulations, material performance and test standards, design guidance, competency requirements, review and approvals, and more. On the private side, activities include product development, analysis and design, construction and installation, as well as education and training of practitioners. In some cases there are overlaps (e.g., private building control). However, once buildings become occupied, the system faces several challenges. Oversight of building use and modification often gets lost. Different actors come into play. Competing objectives become more significant. Occupants often lack understanding and ability to recognize problems and make adjustments. The net result is an increase in fire safety risk over the life of a building, with less opportunities for the regulatory system to make interventions prior to an unwanted fire event. However, this can be changed if the approach to regulating existing buildings changes, and importantly, embodies whole-of-life, multi-agency, holistic, systems-based thinking.},
 authors = {['Brian J. Meacham']},
 journal = {Fire Safety Journal},
 keywords = {['Regulatory system', 'Existing buildings', 'Fire risk', 'Systems thinking']},
 title = {Fire safety of existing residential buildings: Building regulatory system gaps and needs},
 year = {2023}
}

@Filtered Article{0de24163-2f0b-4f5d-9b29-89005dbdbc63,
 authors = {['Gerald Penn']},
 journal = {North-Holland},
 title = {Computational Linguistics},
 year = {2012}
}

@Filtered Article{0e0e5881-e9ab-4792-8c6c-5cfa9a1053b2,
 abstract = {The computation of concept distances aids in understanding the interrelations among entities within knowledge graphs and uncovering implicit information. The existing studies predominantly focus on the conceptual distance of specific hierarchical levels without offering a unified framework for comprehensive exploration. To overcome the limitations of unidimensional approaches, this paper proposes a method for calculating concept distances at multiple granularities based on a three-way partial order structure. Specifically: (1) this study introduces a methodology for calculating inter-object similarity based on the three-way attribute partial order structure (APOS); (2) It proposes the application of the similarity matrix to delineate the structure of categories; (3) Based on the similarity matrix describing the three-way APOS of categories, we establish a novel method for calculating inter-category distance. The experiments on eight datasets demonstrate that this approach effectively differentiates various concepts and computes their distances. When applied to classification tasks, it exhibits outstanding performance.},
 authors = {['Enliang Yan', 'Pengfei Zhang', 'Tianyong Hao', 'Tao Zhang', 'Jianping Yu', 'Yuncheng Jiang', 'Yuan Yang']},
 journal = {International Journal of Approximate Reasoning},
 keywords = {['Partial order structure', 'Concept-cognitive learning', 'Knowledge distance', 'Three-way decision', 'Granular computing', 'Concept graph']},
 title = {An approach to calculate conceptual distance across multi-granularity based on three-way partial order structure},
 year = {2025}
}

@Filtered Article{0e0ff7db-dfd5-47f9-b6c8-6db2aa976aaf,
 abstract = {Mathematics teachers are called on to craft instruction that centers students’ mathematical ideas and creates consistent, pervasive opportunities for meaning-making through discourse. In the context of collaborative problem solving, teachers can use eliciting and probing to uncover student thinking while students work together to develop mathematical ideas and strategies. After eliciting and probing, teachers can further respond to the student thinking that has been revealed. This study explored the discursive pathways two fourth grade mathematics teachers used after eliciting student thinking, when their aim was to be responsive to and advance student thinking. Drawing on interactions (n = 97) from nine lessons, qualitative analysis identified five distinct discursive pathways after eliciting, two of which, praise and funneling, were associated with the nature of student understanding uncovered during eliciting. Implications for future research and professional development on teacher-student discourse are discussed.},
 authors = {['Jen Munson']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Classroom discourse', 'Eliciting', 'Responsiveness', 'Student understanding']},
 title = {After eliciting: Variation in elementary mathematics teachers’ discursive pathways during collaborative problem solving},
 year = {2019}
}

@Filtered Article{0e34024a-74cb-4190-9f72-5c4d3138eb79,
 abstract = {Neuroscientists are searching for the engram within the conceptual framework established by John Locke's theory of mind. This framework was elaborated before the development of information theory, before the development of information processing machines and the science of computation, before the discovery that molecules carry hereditary information, before the discovery of the codon code and the molecular machinery for editing the messages written in this code and translating it into transcription factors that mark abstract features of organic structure such as anterior and distal. The search for the engram needs to abandon Locke's conceptual framework and work within a framework informed by these developments. The engram is the medium by which information extracted from past experience is transmitted to the computations that inform future behavior. The information-conveying symbols in the engram are rapidly generated in the course of computations, which implies that they are molecules.},
 authors = {['C.R. Gallistel']},
 journal = {Cognition},
 keywords = {['Engram', 'Communication channel', 'Plastic synapse', 'Molecules']},
 title = {The physical basis of memory},
 year = {2021}
}

@Filtered Article{0e497e71-ddd6-4eef-bf6e-7130d8e956c0,
 abstract = {Epistemological foundations for modeling of cognitive evolution are characterized. Cognitive evolution is the evolution of cognitive abilities of biological organisms. The important result of this evolution is the human thinking, which is used at scientific cognition of nature. The related epistemological viewpoints of David Hume, Immanuel Kant, Konrad Lorenz, and Eugene Wigner are outlined. The sketch program for future investigations of cognitive evolution is proposed; initial models of these studies are outlined. According to the presented analysis, it is possible to believe the following. Investigations of cognitive evolution are directed to analyze the fundamental problems: “Why is human thinking applicable to cognition of nature?”, “How did human thinking origin in the process of biological evolution?” There are powerful backgrounds for considered investigations: (1) models of autonomous cognitive agents, (2) biological investigations of animal cognitive features. Studies of cognitive evolution would have broad interdisciplinary relations. These studies should contribute significantly to the development of the scientific point of view.},
 authors = {['Vladimir G. Red’ko']},
 journal = {Biologically Inspired Cognitive Architectures},
 keywords = {['Modeling of cognitive evolution', 'Cognitive agents', 'Animal cognitive features', 'Epistemological foundations']},
 title = {Epistemological foundations of investigation of cognitive evolution},
 year = {2016}
}

@Filtered Article{0e5ea7d0-6f0d-43ea-b0d2-3f70dc665854,
 abstract = {This paper addresses some of the challenges involved in implementing the new approach established in the Spanish National Curriculum in 2006, which brought as a major change a focus on the development of key competencies. The paper focuses on scientific competency and the challenges involved in the itinerary from policy documents to classrooms are addressed in three sections: (i) an analysis is made of the changes in the science curriculum as a consequence of the emphasis on scientific competency, comparing the assessment criteria in the previous and current steering documents; (ii) trends in teacher education are discussed; (iii) the findings of the diagnostic evaluation are analyzed. The paper is framed in a theoretical approach, viewing students’ participation in scientific practices, and the development of higher-order thinking as necessary goals of science education. We argue that the focus on competencies, characterized as the ability to apply knowledge and skills in new contexts, involves a major change towards knowledge transfer and higher-order thinking skills. Some issues emerging from the analysis relate to the implications of assessment criteria and the challenges involved in its implementation, to the trends in teacher professional development and the difficulties related to the current economic crisis and to the results of the diagnostic evaluation and time frame needed for reforms to have an impact. It is argued that the development of both competencies and higher-order thinking requires students’ prolonged engagement.},
 authors = {['Beatriz Crujeiras', 'María Pilar Jiménez-Aleixandre']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Scientific competency', 'Epistemic practices', 'Higher-order thinking', 'Policy']},
 title = {Challenges in the implementation of a competency-based curriculum in Spain},
 year = {2013}
}

@Filtered Article{0e71f82c-ab30-4249-866d-db7fc967cf91,
 abstract = {Previously, doctors interpreted diseases and their outcomes according to their experience in diagnosis. However, with the rapid increase in technology and population, the task of examining the patient becomes cumbersome and sometimes human efforts produce inconsistent results. Several research is being done for healthcare in terms of improving visualization and accuracy by using machine learning models. The current research targets to explore quantum computing as a different way of processing information compared to classical computer systems such as the use of quantum bits (qubits) along with superposition and entanglement for extending the computation capabilities at an unprecedented level of thinking in the healthcare domain. Quantum computing systems provide exponential benefits in terms of high-speed processing, faster and easier diagnostic assistance, unimaginable reduction in processing throughput, and many more. An extensive comparative analysis of existing approaches has been made which benchmarks the need for quantum healthcare computing. The objective of this work is to interpret whether Quantum computers prove to be more trusted when it comes to patient diagnosis, and faster analysis leading to cost optimization. In order to accelerate patient diagnosis, different approaches have been presented. The authors have proposed a precision-based granular approach for patient diagnosis that incorporates diagnosing the disease with enhanced precision and granularity. It involves reporting symptoms by the patient, encountering by healthcare expert on multiple factors, precise examination, granular health status (understanding past and present medical history), followed by a precise intervention by understanding biomolecular simulations. The algorithm has been presented to describe the flow process for patient diagnosis modeling using quantum computing. It involves qubits initialization, pairing the values, assigning probabilistic values, cross-validation, and quantum circuit formation. Precision-based granular approach has been implemented for a scenario (consisting of medical parameters such as oxygen and heart rate level, with the functionality of diagnosing oxygen level and heart range which lies as either normal or not normal (high/low)). Precision-based granular approach deals specifically with the individual ‘biomolecular simulation by understanding variations in the individual body whereas the umbrella-based approach does not deal with specifically to individual mechanisms. Granular level of encounter is not possible in umbrella-based treatment. Python Jupyter notebook and IBM Composer tool is used for the implementation of results. Bloch sphere and computational state graph are obtained as an output for better visualization and understanding. Falcon r5.11H processor is used with the version of 1.0.24 of IBM Composer to simulate the experiment. The methodology using precision based granular approach provides timely encounter of disease along with umbrella diagnosis and precise treatment. The time is taken and frequency of qubits have been presented with promising results. The diagnosis process and optimizing cost efficiency can aid in an early detection of the disease.},
 authors = {['Lakshita Aggarwal', 'Shelly Sachdeva', 'Puneet Goswami']},
 journal = {Applied Soft Computing},
 keywords = {['Quantum computing', 'Qubits', 'Healthcare', 'Diagnosis', 'Classical computing', 'Precision']},
 title = {Quantum healthcare computing using precision based granular approach},
 year = {2023}
}

@Filtered Article{0e74d390-a12a-4dab-84f7-08bb94726c4b,
 abstract = {Publisher Summary
This chapter deals with a computational model of conscious and conscious strategy discovery and advocates a triangulation strategy for attaining a better understanding of change mechanisms. This triangulation strategy involves going back and forth among traditional studies of age-related change, microgenetic studies of children's gleaming, and computer simulations that generate the changes documented in the other two approaches. The chapter describes a new computational model of conscious and unconscious strategy discovery. Apart from being a crucial component of one of the examples of the triangulation strategy, this simulation significantly extends previous models of strategy choice and discovery. A large majority of studies of cognitive development have been devoted to describe age-related changes. The studies of age-related change have succeeded in providing excellent descriptions of many aspects of cognitive growth. Each of these three approaches—descriptions of age-related change, descriptions of learning, and formal modeling—provides unique information critical to a well-grounded account of developmental change.},
 authors = {['Robert Siegler', 'Roberto Araya']},
 journal = {JAI},
 title = {A computational model of conscious and unconscious strategy discovery},
 year = {2005}
}

@Filtered Article{0ee0c474-9062-4645-a1b2-8ca98617df5c,
 abstract = {Abstract
One of the major computational challenges that online businesses face today is to make sense of huge amount of information. Granular computing has emerged as an important conceptual and computational paradigm of information processing. As an emerging field of study, it has been suggested that granular computing at philosophical level concerns with structural thinking and at the application level concerns with structured problem solving. It has been further suggested that fuzzy information granulation, as a special case of granular computing, is likely to play an important role in the evolution of fuzzy logic and may eventually have a far-reaching impact on its applications. Responding to the calls for future studies dealing with the applications of fuzzy information granulation, this paper presents an application of the theory of fuzzy information granulation in an emerging and important area where there has not yet been any application, showing how online sports services can make sense of huge amount of data in a structured way and how they can structure their decisions. The empirical results show that despite the huge amount of data that needs to be processed, fuzzy information granulation can help online sports services make sense of it and identify meaningful granules for easier decision making.},
 authors = {['Muammer Ozer']},
 journal = {Expert Systems with Applications},
 keywords = {['Information granulation', 'Fuzzy granulation', 'Application', 'Online business']},
 title = {An application of fuzzy information granulation in the emerging area of online sports},
 year = {2011}
}

@Filtered Article{0f2330a6-db3e-4aa5-9425-010703a49724,
 abstract = {Assembly of normally soluble proteins into amyloid fibrils is a cause or associated symptom of numerous human disorders. Although some progress toward understanding the molecular‐level details of fibril structure has been made through in vitro experiments, the insoluble nature of fibrils make them difficult to study experimentally. We describe two computational approaches used to investigate fibril formation and structure: intermediate‐resolution discontinuous molecular dynamics simulations and atomistic molecular dynamics simulations. Each method has its strengths and weaknesses, but taken together the two approaches provide a useful molecular‐level picture of fibril structure and formation.},
 authors = {['Carol K. Hall', 'Victoria A. Wagoner']},
 journal = {Academic Press},
 title = {Computational Approaches to Fibril Structure and Formation},
 year = {2006}
}

@Filtered Article{0f23ace0-fe68-4283-8169-b4c77274ff22,
 abstract = {With the development of the times, celestial body search has become increasingly common, and people want to enhance their understanding of the universe through further search of celestial bodies. Nowadays, many software and hardware have been invented to assist in celestial body search, but the computational efficiency and efficiency of current software are still insufficient. So this article focuses on the research and application of Deep Learning (DL) in the search and analysis of "a certain celestial body", aiming to improve celestial search through DL. Through experiments, this article uses DL to achieve a maximum computational rate of 78% and a minimum of 70% for celestial search. The computational efficiency of celestial search without DL can reach up to 68% and 57%, respectively. After using DL, the efficiency of celestial search reaches up to 82% and 70%, while before using DL, the efficiency of celestial search reaches up to 62% and 50%, respectively. From this data, it can be seen that DL can achieve good results in celestial search.},
 authors = {['Yang Lyu', 'Donglin Su']},
 journal = {Procedia Computer Science},
 keywords = {['Deep Learning', 'Search Analysis', 'Research Applications', 'a Certain Celestial Body']},
 title = {Application of Deep Learning in the Search and a Certain Celestial Body},
 year = {2023}
}

@Filtered Article{0f63b14c-0d07-427f-be3b-628c2f42b651,
 abstract = {Existing models that integrate emotion and cognition generally do not fully specify why cognition needs emotion and conversely why emotion needs cognition. In this paper, we present a unified computational model that combines an abstract cognitive theory of behavior control (PEACTIDM) and a detailed theory of emotion (based on an appraisal theory), integrated in a theory of cognitive architecture (Soar). The theory of cognitive control specifies a set of required computational functions and their abstract inputs and outputs, while the appraisal theory specifies in more detail the nature of these inputs and outputs and an ontology for their representation. We argue that there is a surprising functional symbiosis between these two independently motivated theories that leads to a deeper theoretical integration than has been previously obtained in other computational treatments of cognition and emotion. We use an implemented model in Soar to test the feasibility of the resulting integrated theory, and explore its implications and predictive power in several task domains.},
 authors = {['Robert P. Marinier', 'John E. Laird', 'Richard L. Lewis']},
 journal = {Cognitive Systems Research},
 title = {A computational unification of cognitive behavior and emotion},
 year = {2009}
}

@Filtered Article{0fbb26d1-212a-4ed5-adaa-7351b8c3e24e,
 abstract = {The resurgence of computer programming in the school curriculum brings a promise of preparing students for the future that goes beyond just learning how to code. This study reviewed research to analyse educational outcomes for children learning to code at school. A systematic review was applied to identify relevant articles and a thematic analysis to synthesise the findings. Ten articles were included in the synthesis and an overarching model was developed which depicts the themes. The results demonstrate that although students are learning to code, a range of other educational outcomes can be learnt or practiced through the teaching of coding. These included mathematical problem-solving, critical thinking, social skills, self-management and academic skills. The review also identified the importance of instructional design for developing these educational outcomes through coding.},
 authors = {['Shahira Popat', 'Louise Starkey']},
 journal = {Computers & Education},
 keywords = {['Coding', 'Programming', 'School', 'Computer', 'Outcome', 'Skills']},
 title = {Learning to code or coding to learn? A systematic review},
 year = {2019}
}

@Filtered Article{102ac650-c46b-401a-8c99-efb93f789383,
 abstract = {We propose an approach for partitioning an irregular application problem in computational biology called Molecular Dynamics (MD) of Macromolecules. We model the application as a task graph which we call a compact MD graph. Such a modeling allows existing mapping heuristics to be applied to this problem. We then provide a parallel algorithm for this application, by using an efficient mapping heuristic called Allocation By Recursive Mincut (ARM) to map the compact MD graph to a hypercube connected parallel computer, the nCUBE 2S. A canonical model for executing parallel computations modeled as graphs is described. Thus, we attempt to provide the missing link between the mapping research and application implementation research, and demonstrate that the execution time can be sufficiently reduced by considering formal mapping techniques, while designing parallel programs for important applications.},
 authors = {['Vamsee Lakamsani', 'Laxmi N. Bhuyan', 'D.Scott Linthicum']},
 journal = {Parallel Computing},
 keywords = {['Mapping problem', 'Recursive mincut', 'Molecular dynamics', 'Compact MD graph', 'Hypercube']},
 title = {Mapping molecular dynamics computations on to hypercubes},
 year = {1995}
}

@Filtered Article{1047425a-6182-4e94-b54a-7db4fee3f6e6,
 abstract = {While the burden of disease from well-studied drinking water contaminants is declining, risks from emerging chemical and microbial contaminants arise because of social, technological, demographic and climatological developments. At present, emerging chemical and microbial drinking water contaminants are not assessed in a systematic way, but reactively and incidence based. Furthermore, they are assessed separately despite similar pollution sources. As a result, risks might be addressed ineffectively. Integrated risk assessment approaches are thus needed that elucidate the uncertainties in the risk evaluation of emerging drinking water contaminants, while considering risk assessors’ values. This study therefore aimed to (1) construct an assessment hierarchy for the integrated evaluation of the potential risks from emerging chemical and microbial contaminants in drinking water and (2) develop a decision support tool, based on the agreed assessment hierarchy, to quantify (uncertain) risk scores. A multi-actor approach was used to construct the assessment hierarchy, involving chemical and microbial risk assessors, drinking water experts and members of responsible authorities. The concept of value-focused thinking was applied to guide the problem-structuring and model-building process. The development of the decision support tool was done using Decisi-o-rama, an open-source Python library. With the developed decision support tool (uncertain) risk scores can be calculated for emerging chemical and microbial drinking water contaminants, which can be used for the evidence-based prioritisation of actions on emerging chemical and microbial drinking water risks. The decision support tool improves existing prioritisation approaches as it combines uncertain indicator levels with a multi-stakeholder approach and integrated the risk assessment of chemical and microbial contaminants. By applying the concept of value-focused thinking, this study addressed difficulties in evidence-based decision-making related to emerging drinking water contaminants. Suggestions to improve the model were made to guide future research in assisting policy makers to effectively protect public health from emerging drinking water risks.},
 authors = {['Julia Hartmann', 'Juan Carlos Chacon-Hurtado', 'Eric Verbruggen', 'Jack Schijven', 'Emiel Rorije', 'Susanne Wuijts', 'Ana Maria {de Roda Husman}', 'Jan Peter {van der Hoek}', 'Lisa Scholten']},
 journal = {Journal of Environmental Management},
 keywords = {['Multi criteria analysis', 'MCA', 'Stakeholder consultation', 'Water contaminants', 'Pathogen']},
 title = {Model development for evidence-based prioritisation of policy action on emerging chemical and microbial drinking water risks},
 year = {2021}
}

@Filtered Article{10706f89-84d2-4ffc-93c8-32f557d984ea,
 abstract = {Purpose
This paper examines how mathematics teacher educators (MTEs) learn to enact equitable mathematics instruction using technology through lesson study (LS).
Design/methodology/approach
A LS team with three MTEs conducted three iterations of LS on teaching the Pythagorean Theorem in an in-person, technology-mediated environment. Many forms of data were collected: Desmos activities, videos of research lessons (RLs), videos of MTE RL debriefings, artifacts of student learning in the Desmos Dashboard, and MTEs' written self-reflection. The authors investigate the teacher educators' learning through LS by analyzing the MTE debriefings of the RLs using Bannister’s (2015) framework for teacher learning in communities of practice.
Findings
The MTEs learned to enact equitable mathematics instruction using technology through addressing emerging issues related to intellectual authority and use of student thinking. Throughout the LS, the MTEs sought ways of promoting students' mathematical authority and using student thinking through features of the Desmos platform.
Research limitations/implications
This study focuses on MTEs' learning without examining participating preservice teachers' learning. It demonstrates the benefits of LS for MTEs' professional learning.
Practical implications
This study showcases how a research-based Desmos activity is used and refined to promote MTE learning how to implement equitable mathematics instruction.
Originality/value
The study contributes to better understanding of how LS could be used to develop MTEs' professional learning. Moreover, the dual process of participation and reification was concretized through diagnostic and prognostic frames in the LS context, which enriches the concept of community of practice.},
 authors = {['RongjinRongjin HuangHuang', 'Christopher T.Christopher T. BonnesenBonnesen', 'Amanda LakeAmanda Lake HeathHeath', 'Jennifer M.Jennifer M. SuhSuh']},
 journal = {International Journal for Lesson and Learning Studies},
 keywords = {['Equitable instruction', 'Technology', 'Teacher educator learning', 'Lesson study']},
 title = {Teacher educator learning to implement equitable mathematics teaching using technology through lesson study},
 year = {2023}
}

@Filtered Article{10b93c7f-2d4a-4711-b143-d3f30e77bf07,
 abstract = {Three-dimensional (3D) simulations and precise landscape visualizations are crucial for various applications, like landscape management and planning, computer and connection of the landscape, evaluation, and tracking of land use. The consequences of several plans and a large scene cannot be communicated using older methods of comprehensive environmental planning and development in a timely, rational, and coordinated manner. Architects have trouble incorporating ideas into other comprehensive planning implementation processes. Architects did not thoroughly investigate the neighbourhood's demographics and matching behavioural needs and lacked critical thinking. The 3D dynamic landscape simulation is a detailed computerized three-dimensional simulation of the environment that can be dynamically presented. With the aid of Artificial Intelligence (AI) technology, the system possesses a strong sense of reality, a user-friendly interface, and interactive features that can be tailored to the requirements of the contemporary urban environmental landscape. Regarding exterior publicity, domestic assistance, environmental land use planning, and information systems. The novelty of the proposed Interactive Design System based on AI (IDS-AI) is to create a 3D dynamic landscape model based on a real-life environmental scene, utilizing a Geographic Information System (GIS) to optimize landscape vision. Secondly, 3D environmental landscape design simulation was implemented using GIS spatial analysis in conjunction with the Fuzzy Analytical Hierarchical Process (FAHP) to reduce the data overlap rate and help make an accurate decision. Finally, the design incorporates the development of the interactive interface system application of landscape design and environmental resources for viewing the landscape, the factors that affect them, and the area coverage ratio of various land cover types. The experimental outcomes show that the suggested IDS model increases the gradient sensitivity level of 98.3 % and area coverage ratio of 93.4 % compared to other existing models.},
 authors = {['Binbin Shi']},
 journal = {Heliyon},
 keywords = {['Artificial intelligence', 'Environmental landscape design', 'Fuzzy analytical hierarchical process', 'Geographical information system', '3D dynamic landscape', 'Interactive design system']},
 title = {3D dynamic landscape simulation of artificial intelligence in environmental landscape design},
 year = {2024}
}

@Filtered Article{1105bc1b-bdfb-4c01-ba37-60b9e3b5a9d6,
 abstract = {We examined the factors that produce differences in generating scenarios on the near future using the scanning method. Participants were asked to briefly read (scan) 151 articles about new technology, the latest customs, fashion, social change, value system transition, or emerging social problems, and then to generate three scenarios about the near future based on the articles. We compared the generated scenarios between scanning method experts and non-experts with no prior experience with the scanning method. We found that experts generated more unique scenarios than non-experts did, and that experts and non-experts differed in the diversity of articles referenced when generating scenarios. We discuss the relationship between the present findings and previous findings on divergent thinking.},
 authors = {['Hidehito Honda', 'Yuichi Washida', 'Akihito Sudo', 'Yuichiro Wajima', 'Keigo Awata', 'Kazuhiro Ueda']},
 journal = {Technological Forecasting and Social Change},
 keywords = {['Foresight', 'Scanning method', 'Divergent thinking', 'Difference between experts and non-experts', 'Creativity']},
 title = {The difference in foresight using the scanning method between experts and non-experts},
 year = {2017}
}

@Filtered Article{1209a7e9-a0e1-4b87-8346-0c25ec7c46e5,
 abstract = {Although solutions based on artificial and computational intelligence have made life easier, the fast development of technology also raises questions about near future and log term human cognition and social interaction. Through a survey of the literature and qualitative analysis, our work examined current research on how the AI/CI affects human cognitive functions and social interactions. We discuss how AI and CI are influencing e.g. how we humans gather information, build relationships, and communicate with others, with and without the new frontline technologies. Additionally, proposals for future advances are discussed along with the ethical and societal ramifications these technologies have, could and might bring into our lives. We think that by developing a deeper knowledge of how AI/CI affects human cognition and social interaction, new contributions are made to a positive conversation and encourage a responsible approach to incorporating new technologies into our daily lives.},
 authors = {['Usman Ahmad Usmani', 'Ari Happonen', 'Junzo Watada']},
 journal = {Procedia Computer Science},
 keywords = {['Artificial intelligence', 'Computational intelligence', 'Digitalization', 'Digital transformation', 'Human cognition', 'Social interaction', 'Industry 4.0', 'Digital capability', 'Social transformation', 'Human computer interaction']},
 title = {The Digital Age: Exploring the Intersection of AI/CI and Human Cognition and Social Interactions},
 year = {2024}
}

@Filtered Article{12139258-b275-4b5e-8887-50684184ad50,
 abstract = {Activist short sellers (AShSs) and financial analysts are information intermediaries who analyze firm disclosures as well as produce and disseminate influential investment narratives. This study aims to better understand narrative challenges surrounding the legitimate expertise of financial analysts. Specifically, we examine how AShSs challenge sell-side financial analysts' narrative authority (i.e., the perception that they produce expert knowledge) in interpreting firms' performance and future prospects. We investigate how analysts respond (or do not respond) to this challenge. We use 442 AShS reports, 12 interviews with AShSs and analysts, and analysts' stock recommendations and target prices. In their criticisms of analysts (found in one-third of reports), AShSs frequently frame analysts as lacking market expertise and critical thinking – two core dimensions of analysts' narrative authority. Sixty-six percent of analysts, although explicitly criticized in AShS reports, do not engage in written responses in their equity research reports because they reportedly either adopt a renunciation attitude to the challenge or they engage in off-the-record discussions with certain market participants. However, 34% of analysts respond overtly by counter-framing AShSs as lacking market expertise and objectivity. After the dissemination of AShS reports, analysts, on average, do not revise their highly visible stock recommendations but they revise target prices downward. Theoretically, this study extends our understanding of the construction of narrative authority in capital markets as we examine a challenge to the expertise of influential information intermediaries.},
 authors = {['Hervé Stolowy', 'Luc Paugam', 'Yves Gendron']},
 journal = {Accounting, Organizations and Society},
 keywords = {['Activist short sellers', 'Expertise', 'Financial analysts', 'Framing', 'Narrative authority']},
 title = {Competing for narrative authority in capital markets: Activist short sellers vs. financial analysts},
 year = {2022}
}

@Filtered Article{1279d260-6835-4286-bf79-e9b08a44cd29,
 abstract = {The electrical activity of diverse brain cells is modulated across states of vigilance, namely wakefulness, non-rapid eye movement (NREM) sleep, and rapid eye movement (REM) sleep. Enhanced activity of neuronal circuits during NREM sleep impacts on subsequent awake behaviors, yet the significance of their activation, or lack thereof, during REM sleep remains unclear. This review focuses on feeding-promoting cells in the lateral hypothalamus (LH) that express the vesicular GABA and glycine transporter (vgat) as a model to further understand the impact of REM sleep on neural encoding of goal-directed behavior. It emphasizes both spatial and temporal aspects of hypothalamic cell dynamics across awake behaviors and REM sleep, and discusses a role for REM sleep in brain plasticity underlying energy homeostasis and behavioral optimization.},
 authors = {['Lukas T. Oesch', 'Antoine R. Adamantidis']},
 journal = {Trends in Neurosciences},
 keywords = {['sleep', 'feeding', 'goal-directed behavior', 'hypothalamus', 'population coding']},
 title = {How REM sleep shapes hypothalamic computations for feeding behavior},
 year = {2021}
}

@Filtered Article{12d9f096-d5ae-49e4-af8c-e808ef5e545c,
 abstract = {The evolution of the genome has led to very sophisticated and complex regulation. Because of the abundance of non-coding RNA (ncRNA) in the cell, different species will promiscuously associate with each other, suggesting collective dynamics similar to artificial neural networks. A simple mechanism is proposed allowing ncRNA to perform computations equivalent to neural network algorithms such as Boltzmann machines and the Hopfield model. The quantities analogous to the neural couplings are the equilibrium constants between different RNA species. The relatively rapid equilibration of RNA binding and unbinding is regulated by a slower process that degrades and creates new RNA. The model requires that the creation rate for each species be an increasing function of the ratio of total to unbound RNA. Similar mechanisms have already been found to exist experimentally for ncRNA regulation. With the overall concentration of RNA regulated, equilibrium constants can be chosen to store many different patterns, or many different input–output relations. The network is also quite insensitive to random mutations in equilibrium constants. Therefore one expects that this kind of mechanism will have a much higher mutation rate than ones typically regarded as being under evolutionary constraint.},
 authors = {['J.M. Deutsch']},
 journal = {Journal of Theoretical Biology},
 title = {Computational mechanisms in genetic regulation by RNA},
 year = {2018}
}

@Filtered Article{13481abf-7ff8-495d-8624-ba5b18cdfa9f,
 abstract = {I discuss a multi-level analysis approach in the study of cognitive development in cultural context. In this approach, culture is conceived of as a system and a process of symbolic mediation, where values, norms, and beliefs manifest in and through customs, rituals, and practices in directing and regulating both intrapersonal and interpersonal psychological functions. To capture the dynamic process in which cognitive development unfolds in cultural context, this approach examines the influence of culture on the developing cognitive skills between groups – group level analysis, between the child and socialization agents – dyadic level analysis, at the level of the child – individual level analysis, and within the child – situation level analysis. The temporal level analysis further situates cognitive development in historical time. By utilizing different analytical and methodological strategies, the multi-level analysis approach provides converging evidence for the development of cognitive skills in cultural context. Important challenges in this approach include the development of a “big picture” to guide the investigation, methodological training for research assistants, and difficulties in collecting, managing, and analyzing diverse datasets. I discuss the conceptual and methodological issues by using our work on the development of episodic thinking as an example.},
 authors = {['Qi Wang']},
 journal = {Developmental Review},
 keywords = {['Culture', 'Cognition', 'Episodic thinking', 'Memory', 'Multiple levels of analysis']},
 title = {Studying cognitive development in cultural context: A multi-level analysis approach},
 year = {2018}
}

@Filtered Article{1363d45e-1b24-4f9e-82c5-cbc1700d00a8,
 abstract = {Methods for inferring species trees from gene trees motivated by incomplete lineage sorting typically use either rooted gene trees to infer a rooted species tree, or use unrooted gene trees to infer an unrooted species tree, which is then typically rooted using one or more outgroups. Theoretically, however, it has been known since 2011 that it is possible to consistently infer the root of the species tree directly from unrooted gene trees without assuming an outgroup. Here, we use approximate Bayesian computation to infer the root of the species tree from unrooted gene trees assuming the multispecies coalescent model. It is hoped that this approach will be useful in cases where an appropriate outgroup is difficult to find and gene trees do not follow a molecular clock. We use approximate Bayesian computation to infer the root of the species tree from unrooted gene trees. This approach could also be useful when there is prior information that makes a small number of root locations plausible in an unrooted species tree.},
 authors = {['Ayed R.A. Alanzi', 'James H. Degnan']},
 journal = {Molecular Phylogenetics and Evolution},
 keywords = {['Multispecies coalescent', 'Outgroup', 'Midpoint rooting', 'Molecular clock', 'Identifiability', 'Sufficiency']},
 title = {Inferring rooted species trees from unrooted gene trees using approximate Bayesian computation},
 year = {2017}
}

@Filtered Article{136af46c-2f24-4423-b367-84e747e407dd,
 abstract = {In this paper, we review the milestones in the development of heuristic methods for optimization over the last 50 years. We propose a critical analysis of the main findings and contributions, mainly from a European perspective. Starting with the roots of the area that can be traced back to the classical philosophers, we follow the historical path of heuristics and metaheuristics in the field of operations research and list the main milestones, up to the latest proposals to hybridize metaheuristics with machine learning. We pay special attention to the theories that changed our way of thinking about problem solving, and to the role played by the European Journal of Operational Research in the development of these theories. Our approach emphasizes methodologies and their connections with related areas, which permits to identify potential lines of future research.},
 authors = {['Rafael Martí', 'Marc Sevaux', 'Kenneth Sörensen']},
 journal = {European Journal of Operational Research},
 keywords = {['Heuristics', 'Combinatorial optimization', 'Critical review', 'Metaheuristics']},
 title = {Fifty years of metaheuristics},
 year = {2025}
}

@Filtered Article{13812403-c20d-4b26-b3ee-4a829f0cd32a,
 abstract = {Publisher Summary
This chapter discusses the process of thinking and knowing. Forming concepts, propositions, problems, and directions are examples of thinking. Thinking can be visual, verbal, or abstract. It can be chaotic or orderly, creative, or routine. Thinking of any kind is an activity of some plastic neural systems. Of all mental activities, thinking is probably the one most affected by chemical changes and changes in basic properties of neurons. For example, humans unable to oxidize the amino acid phenylalanine cannot think, thyroid hypofunction produces cretinism, and normal subjects cannot think straight when in states of extreme stress, which are often states of hormonal imbalance, or when under the action of psychotropic drugs. The chapter also discusses the concept of cognition. All cognition is learned but not every learned item is of a cognitive nature. All cognition is cognition of some object, concrete or conceptual, and it consists in some information about its object—complete or partial, true or false. Cognition can be behavioral, perceptual, or conceptual.},
 authors = {['MARIO BUNGE']},
 journal = {Pergamon},
 title = {CHAPTER 7 - Thinking and Knowing},
 year = {1980}
}

@Filtered Article{13ae4b60-e324-4604-9baa-8b94e60d3a4f,
 abstract = {In nature-inspired computation, different intelligent computation modes of agents usually have different extrinsic forms; but can they take on some relative uniform characteristics? To validate this idea, further systematic study on nature-inspired computation from a more macroscopical angle is made in this article and the uniform framework mode of nature-inspired computation is consequently summarized and presented, as well as described with feedback neural network and swarm intelligence algorithms. On the basis of the defined general mode framework, agents of the algorithms in a nature-inspired computation field can show a type of uniform intelligent computation mode.},
 authors = {['Lei WANG', 'Qi KANG', 'Qi-di WU']},
 journal = {Systems Engineering - Theory & Practice},
 keywords = {['nature-inspired computation', 'general mode', 'uniform framework mode', 'neural networks', 'swarm intelligence']},
 title = {Nature-inspired Computation — Effective Realization of Artificial Intelligence},
 year = {2007}
}

@Filtered Article{13cdee41-fd0b-4e80-9a3d-47bc70cb2e2d,
 abstract = {Using computational methods to produce and interpret multiple scientific representations is now a common practice in many science disciplines. Research has shown students have difficulty in moving across, connecting, and sensemaking from multiple representations. There is a need to develop task-specific representational competencies for students to reason and conduct scientific investigations using multiple representations. In this study, we focus on three representational competencies: 1) linking between representations, 2) disciplinary sensemaking from multiple representations, and 3) conceptualizing domain-relevant content derived from multiple representations. We developed a block code-based computational modeling environment with three different representations and embedded it within an online activity for students to carry out investigations around the earthquake cycle. The three representations include a procedural representation of block codes, a geometric representation of land deformation build-up, and a graphical representation of deformation build-up over time. We examined the extent of students' representational competencies and which competencies are most correlated with students’ future performance in a computationally supported geoscience investigation. Results indicate that a majority of the 431 students showed at least some form of representational competence. However, a relatively small number of students showed sophisticated levels of linking, sensemaking, and conceptualizing from the representations. Five of seven representational competencies, the most prominent being code sensemaking (η2 = 0.053, p < 0.001), were significantly correlated to student performance on a summative geoscience investigation.},
 authors = {['Christopher Lore', 'Hee-Sun Lee', 'Amy Pallant', 'Jie Chao']},
 journal = {Computers & Education},
 keywords = {['Teaching/learning strategies', 'Simulations', 'Pedagogical issues', 'Applications in subject areas']},
 title = {Using multiple, dynamically linked representations to develop representational competency and conceptual understanding of the earthquake cycle},
 year = {2024}
}

@Filtered Article{13f2ee80-255a-46e4-b50d-36216e711a78,
 abstract = {The subject of metaphors is introduced with a definition, stressing its role as a universal process in the development of thinking and language in human beings. A discussion follows on the differences between metaphor and analogy in this thinking process. The classification of metaphors is then addressed, while the body of the text is dedicated to reviewing explanations given from the standpoint of Psychology of Creativity on the nature of this process and its role in creative output. Lastly, the usefulness of metaphors and their dependence on domain specificity is analysed.},
 authors = {['Manuela Romo']},
 journal = {Academic Press},
 keywords = {['Analogical thought', 'Combinatory process', 'Computational creativity', 'Constitutive metaphor', 'Darwin', 'Einstein', 'Lorca', 'Pedagogical metaphor', 'Poetry', 'Scientific discovery']},
 title = {Metaphor},
 year = {2020}
}

@Filtered Article{146998a2-fda0-47f6-a1b8-0a43f22d6deb,
 abstract = {Natural computing, inspired by biological course of action, is an interdisciplinary field that formalizes processes observed in living organisms to design computational methods for solving complex problems, or designing artificial systems with more natural behaviour. Based on the tasks abstracted from natural phenomena, such as brain modelling, self-organization, self-repetition, self evaluation, Darwinian survival, granulation and perception, nature serves as a source of inspiration for the development of computational tools or systems that are used for solving complex problems. Nature inspired main computing paradigms used for such development include artificial neural networks, fuzzy logic, rough sets, evolutionary algorithms, fractal geometry, DNA computing, artificial life and granular or perception-based computing. Information granulation in granular computing is an inherent characteristic of human thinking and reasoning process performed in everyday life. The present article provides an overview of the significance of natural computing with respect to the granulation-based information processing models, such as neural networks, fuzzy sets and rough sets, and their hybridization. We emphasize on the biological motivation, design principles, application areas, open research problems and challenging issues of these models.},
 authors = {['Sankar K. Pal', 'Saroj K. Meher']},
 journal = {Applied Soft Computing},
 keywords = {['Natural computing', 'Granular computing', 'Soft computing', 'Hybrid model', 'Decision systems']},
 title = {Title Paper: Natural computing: A problem solving paradigm with granular information processing},
 year = {2013}
}

@Filtered Article{1474f9ca-8523-4062-b58a-cf9f3de8fca0,
 abstract = {Today's scientific progress would be unthinkable without theoretical and computational assistance. This holds true also for the solid-state sciences – which are without doubt a fundamental part of modern inorganic chemistry. This chapter is concerned mainly with first principles or ab initio quantum-chemical methods; the fundamental goal of solving Schrödinger's equation does not change upon going to extended systems, but there are some very important new ideas to consider. First, we describe these essential concepts; however, we do not, nor attempt to, provide an exhaustive overview of electronic-structure theory. Subsequently, we deal with simplifications, which are necessary to make quantum-chemical computations tractable and which possess special importance in the solid state. Simplifying ‘well’ is thus a vital part of any theorist's work. Finally, we describe applications – how chemists ‘see’ bonds in complicated structures, and how the computational toolkit may complement and enhance chemical concepts. They illustrate our most important message: how beautifully rock-solid theories and chemists' ingenious models blend in the solid state.},
 authors = {['V.L. Deringer', 'R. Dronskowski']},
 journal = {Elsevier},
 keywords = {['Ab initio calculations', 'Band structure', 'Bonding indicators', 'Chemical bonding', 'Computational chemistry', 'Crystal orbitals', 'Density-functional theory', 'Electronic-structure calculations', 'Magnetism', 'Materials science', 'Plane-wave basis sets', 'Pseudopotentials', 'Quantum chemistry', 'Solid-state chemistry', 'Theoretical chemistry', 'Thermochemistry']},
 title = {9.02 - Computational Methods for Solids},
 year = {2013}
}

@Filtered Article{157b37b0-591e-4635-a917-377f3823d8dd,
 abstract = {This paper aimed at constructing a systematic framework and examining the effect of gamification in programming education through a meta-analysis conducted on 21 empirical studies published in the last decade. We examined the effects of game types, gamification applications, pedagogical agents, programming types, and schooling levels on students' academic achievement, cognitive load, motivation, and thinking skills in programming education by cross-tabulation analysis. Results verified the positive impact of gamification in programming education. Gamification has the largest effect on students' motivation, followed by academic achievement, whereas it has the least effect on students' cognitive load. As for game types, the reasoning strategy game is most effective on academic achievement, while the puzzle game is most effective on motivation. As for gamification application, the games as a competitive mechanism has the greatest impact on students’ thinking skills and motivation. However, when games were adopted as teaching tools or student works, the effects are mainly represented in academic achievement. Pedagogical agents have a limited effect on programming education. With regard to programming types, the effect of gamification is more pronounced in text-based programming rather than graphical programming. This study provided an analytic framework and shed light on potential directions for further studies in the field.},
 authors = {['Zehui Zhan', 'Luyao He', 'Yao Tong', 'Xinya Liang', 'Shihao Guo', 'Xixin Lan']},
 journal = {Computers and Education: Artificial Intelligence},
 keywords = {['Programming education', 'Gamification', 'Meta-analysis', 'Game-based learning']},
 title = {The effectiveness of gamification in programming education: Evidence from a meta-analysis},
 year = {2022}
}

@Filtered Article{163c18f4-b134-4a70-afe8-6cf5fbd6b1f1,
 abstract = {Publisher Summary
Molecular modeling covers a wide range of techniques and the calculation of an even wider range of properties. Although for polymers, the possibility of treating a polymer chain quantum mechanically is formidable, it is clear that the modeling approach allows calculations on monomers, dimmers, and oligomers to guide the interpretation of many spectroscopic observations with great success. For those systems, where longer times scales and larger size scales are important, molecular mechanics and molecular dynamics methods are available, but the issue of the force field and the approximations that it introduces remain significant. The key to the change in attitude to modeling and its role have to lie in the availability of mature algorithms with well-known and well-understood properties. The density functional theory method in quantum mechanics has introduced a new era in applications of quantum mechanical methods.},
 authors = {['John Kendrick']},
 journal = {Elsevier},
 title = {Chapter 17 The Supporting Role of Molecular Modelling and Computational Chemistry in Polymer Analysis},
 year = {2008}
}

@Filtered Article{164a67a2-b138-477d-9f4d-e3cc025b1bec,
 abstract = {This Voices piece will highlight the impact of artificial intelligence on algorithm development among computational biologists. How has worldwide focus on AI changed the path of research in computational biology? What is the impact on the algorithmic biology research community?},
 authors = {['Mona Singh', 'Cenk Sahinalp', 'Jianyang Zeng', 'Wei Vivian Li', 'Carl Kingsford', 'Qiangfeng Zhang', 'Teresa Przytycka', 'Joshua Welch', 'Jian Ma', 'Bonnie Berger']},
 journal = {Cell Systems},
 title = {How has the AI boom impacted algorithmic biology?},
 year = {2024}
}

@Filtered Article{1692b2ac-75c9-46d6-a30e-e1efdcc8dab0,
 abstract = {Since the early attempts to understand the brain made by Greek philosophers more than 2000 years ago, one of the main questions in neuroscience has been how the brain perceives all the stimuli in the environment and uses this information to implement a response. Recent hypotheses of the neural code rely on the existence of an ideal observer, whether on specific areas of the cerebral cortex or distributed network composed of cortical and subcortical elements. The Neurobehavioral State hypothesis stipulates that neurons are in a quasi-stable state due to the dynamic interaction of their molecular components. This increases their computational capabilities and electrophysiological behavior further than a binary active/inactive state. Together, neuronal populations across the brain learn to identify and associate internal and external stimuli with actions and emotions. Furthermore, such associations can be stored through the regulation of neuronal components as new quasi-stable states. Using this framework, behavior arises as the result of the dynamic interaction between internal and external stimuli together with previously established quasi-stable states that delineate the behavioral response. Finally, the Neurobehavioral State hypothesis is firmly grounded on present evidence of the complex dynamics within the brain, from the molecular to the network level, and avoids the need for a central observer by proposing the brain configures itself through experience-driven associations.},
 authors = {['Luis Fernando Ontiveros-Araiza']},
 journal = {BioSystems},
 keywords = {['Brain networks', 'Neuronal dynamics', 'Neural code', 'Neurotransmitter', 'Electrophysiology', 'Neuronal computation', 'Behavior']},
 title = {The Neurobehavioral State hypothesis},
 year = {2025}
}

@Filtered Article{16a4f7f5-f6b2-49ff-8229-2ebf7178f8af,
 abstract = {A finite element method for computational fluid dynamics has been implemented on the Connection Machine systems CM-2 and CM-200. An implicit iterative solution strategy, based on the preconditioned matrix-free GMRES algorithm, is employed. Parallel data structures built on both nodal and elemental sets are used to achieve maximum parallelization. Communication primitives provided through the Connection Machine Scientific Software Library substantially improved the overall performance of the program. Computations of three-dimensional compressible flows using unstructured meshes having close to one million elements, such as a complete airplane, demonstrate that the Connection Machine systems are suitable for these applications. Performance comparisons are also carried out with the vector computers Cray Y-MP and Convex C-1.},
 authors = {['Zdeněk Johan', 'Thomas J.R. Hughes', 'Kapil K. Mathur', 'S.Lennart Johnsson']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 title = {A data parallel finite element method for computational fluid dynamics on the Connection Machine system},
 year = {1992}
}

@Filtered Article{16dd648e-090c-420e-8646-524a555b830d,
 abstract = {Dealing with students' attitudinal problems related to statistics is an important aspect of statistics instruction. Employing the appropriate learning strategies may have a relationship with anxiety during the process of statistics learning. Thus, the present study investigated multivariate relationships between self-regulated learning strategies and statistical anxiety using canonical correlation analysis (CCA). Three hundred twenty Turkish college students responded to the Motivated Strategies for Learning Questionnaire and the Statistical Anxiety Rating Scale. Of the group, 189 (59.1%) were women and 131 (40.9%) were men. Participants' ages ranged from 18 to 33years with a mean of 21.28years (SD=1.53). Bivariate correlation coefficients showed significant relationships between the dimensions of learning strategies and statistical anxiety. CCA showed that students who used more rehearsal, elaboration, organization, critical thinking, metacognitive regulation, time and study environment management, and effort regulation strategies experienced lower computational anxiety and had more positive attitudes toward statistics. Additionally, a combination of effort regulation and help seeking strategies is associated with test/class anxiety.},
 authors = {['Şahin Kesici', 'Mustafa Baloğlu', 'M. Engin Deniz']},
 journal = {Learning and Individual Differences},
 keywords = {['Statistical anxiety', 'Statistics learning', 'Learning strategies', 'Metacognition']},
 title = {Self-regulated learning strategies in relation with statistics anxiety},
 year = {2011}
}

@Filtered Article{16f74c88-8aa0-49e5-a8d5-820f142f99fa,
 abstract = {This parallel randomized controlled trial evaluated the effect of acceptance and commitment therapy (ACT) focused on repetitive negative thinking (RNT) versus a waitlist control (WLC) in improving interpersonal skills in adolescents with problems of social and school adaptation. Forty-two adolescents (11–17 years) agreed to participate. Participants were allocated through simple randomization to the intervention condition or the waitlist control condition. The intervention was a 3-session, group-based, RNT-focused ACT protocol. The primary outcome was the performance on a test of interpersonal skills (Interpersonal Conflict Resolution Assessment, ESCI). At posttreatment, repeated measures ANOVA showed that the intervention was efficacious in increasing overall interpersonal skills (d = 2.62), progress in values (d = 1.23), and reducing emotional symptoms (d = 0.98). No adverse events were found. A brief RNT-focused ACT intervention was highly efficacious in improving interpersonal skills and reducing emotional symptoms in adolescents.},
 authors = {['Koryn N. Bernal-Manrique', 'María B. García-Martín', 'Francisco J. Ruiz']},
 journal = {Journal of Contextual Behavioral Science},
 keywords = {['Acceptance and commitment therapy', 'Interpersonal skills', 'Emotional disorders', 'Psychological flexibility', 'Repetitive negative thinking']},
 title = {Effect of acceptance and commitment therapy in improving interpersonal skills in adolescents: A randomized waitlist control trial},
 year = {2020}
}

@Filtered Article{170d18b4-70fc-4f2d-9b74-dab7ad6da8ca,
 abstract = {Information technology is deeply ingrained in most aspects of everyday life and can be designed to influence users to behave in a certain way. Influencing students to improve their study behaviour would be a useful application of this technology. As a preamble to the design of a persuasive system for learning, we collected data to identify the study behaviours of students and recent alumni. We then developed two models to measure which behaviours have the most significant impact on learning performance. Current students reported more foundational behaviours whereas alumni demonstrated more higher-order thinking traits.},
 authors = {['Justin Filippou', 'Christopher Cheong', 'France Cheong']},
 journal = {Information & Management},
 keywords = {['Study behaviour', 'Persuasive systems', 'Linear modelling', 'Higher education']},
 title = {Modelling the impact of study behaviours on academic performance to inform the design of a persuasive system},
 year = {2016}
}

@Filtered Article{172ffa27-17f0-4066-8167-1fa97fe65a20,
 abstract = {In this work, the interaction between Kynurenic acid (KYNA) and several natural and modified cyclodextrins (CDs) is carried out. Among all the CD tested, HPβ-CD showed the strongest complexation constant (KF), with a value of 270.94 ± 29.80 M−1. Between natural (α- and β-) CDs, the complex of KYNA with β-CD was the most efficient. The inclusion complex of KYNA with CDs showed a strong influence of pH and temperature. The KF value decreased at high pH values, when the pKa was passed. Moreover, an increase of the temperature caused a decrease in the KF values. The thermodynamic parameters of the complexation (ΔH°, ΔS° and ΔG°) were studied with negative entropy, enthalpy and spontaneity of the process at 25 °C. Moreover, the inclusion complex was also characterized using FTIR and TGA. Finally, molecular docking calculations provided different interactions and their influence in the complexation constant.},
 authors = {['Adrián Matencio', 'Fabrizio Caldera', 'Alberto {Rubin Pedrazzo}', 'Yousef {Khazaei Monfared}', 'Nilesh {K. Dhakar}', 'Francesco Trotta']},
 journal = {Food Chemistry},
 keywords = {['Kynurenic acid', 'Cyclodextrin', 'Inclusion complex', 'Physicochemical', 'Stability']},
 title = {A physicochemical, thermodynamical, structural and computational evaluation of kynurenic acid/cyclodextrin complexes},
 year = {2021}
}

@Filtered Article{17548783-83a5-4221-a467-0d75e1da5759,
 abstract = {This paper reviews 15 years of exploration and development in Digital Gastronomy (DG), tracing its progression from foundational frameworks to AI-integrated culinary systems. The journey begins with integrating computational tools like laser cooking, 3D printing, CNC milling, and modular molds, which expand the possibilities of creativity and precision in the kitchen. Building on these technologies, the Meta-Recipe (MR) framework introduces a structured approach to recipe design, allowing chefs to adapt dishes dynamically while maintaining culinary coherence. The concept of “Digital Alchemy” extends this foundation, blending AI-driven methods with traditional healing and sustainable practices to emphasize well-being and environmental consciousness. These advancements culminate in the vision of an AI-augmented kitchen, conceptualized as a collaborative and adaptive space that bridges culinary artistry with algorithmic precision. This research highlights DG's potential as an evolving interdisciplinary field, offering new gastronomy, creativity, and sustainability directions.},
 authors = {['Amit Raphael Zoran']},
 journal = {International Journal of Gastronomy and Food Science},
 title = {Digital gastronomy 2.0: A 15-year transformative journey in culinary-tech evolution and interaction},
 year = {2025}
}

@Filtered Article{1795fdf1-fd03-4305-8be3-116a62d93faa,
 abstract = {Analysing the processes and products of creativity to better understand and support individuals and teams, is a difficult and elusive challenge despite years of research in creativity. In this article, we are particularly interested in social creativity in communities of interest. Building on Guilford's classic model of Divergent Thinking of fluency, flexibility, originality and elaboration, we employ Social Network Analysis to model the creative design process. The creative process in the current study takes place in a technological environment called the ‘MC-squared platform’, in which members of a community of interest collaborate in a social, co-creative process for designing digital, mathematical textbooks. Both the technological environment and the methodology are exemplified through two case examples, one on the design process of a digital book about a bioclimatic amusement park and one on the design process of a digital book about fractions. We conclude that, for these examples, both the technological tool and the data analysis approach provide insight into the social creativity process of the community of interest.},
 authors = {['Christian Bokhove', 'Marios Xenos', 'Manolis Mavrikis']},
 journal = {Social Sciences & Humanities Open},
 keywords = {['Technological environment', 'Co-creation', 'Social creativity', 'Social network analysis']},
 title = {Using Social Network Analysis to gain insight into social creativity while designing digital mathematics books},
 year = {2023}
}

@Filtered Article{17d675ee-d389-4086-8db1-b2a3a72e7d98,
 authors = {['Lawrence K. Cormack']},
 journal = {Academic Press},
 title = {4.1 - Computational Models of Early Human Vision},
 year = {2005}
}

@Filtered Article{183a5da0-56df-4508-9ff7-3e9202306243,
 abstract = {Digital transformation is a multidimensional challenge that is requiring to change consolidated approaches and managerial models. New organized entities are emerging because of this ongoing transformation and new competences are required for managing and living them. Thanks to the interpretative contribution provided by the systems thinking, the paper focuses the attention on the paradigm shift required for defining t-shaped professionals able to master digital transformation in emerging dynamics. A conceptual model is proposed and discussed building upon the T-shaped model with the aim to enrich current theoretical and managerial debates about strategies for supporting both individuals and organizations in facing the challenges imposed by the digital transformation.},
 authors = {['Francesco Caputo', 'Valentina Cillo', 'Fabio Fiano', 'Marco Pironti', 'Marco Romano']},
 journal = {Journal of Business Research},
 keywords = {['Digital transformation', 'T-shaped professionals', 'Systems thinking', 'Soft skills', 'Cognitive domain']},
 title = {Building T-shaped professionals for mastering digital transformation},
 year = {2023}
}

@Filtered Article{189cc81c-5330-42b3-9c54-7ac6f1e5b5b8,
 abstract = {New job skills required by the professional market have been causing significant changes in the learning process of undergraduate students. Different learning methodologies can be adopted to assist in the development of those skills, and the process of choosing the most suitable learning methodology for each situation may be complex, involving multiple and conflicting criteria. In order to support the choice of learning methodologies for the development of the “4C skills”, i.e, collaboration, communication, creativity and critical thinking, we propose a new framework based on the multiple criteria decision-making approach PROMETHEE II (Preference Ranking Organization Method for Enrichment of Evaluations), considering as criteria the “4C skills”, student motivation, level of learning, student comfort, decision-making capacity and time required for class preparation. Passive methods and active learning methodologies such as Guided Reciprocal Peer Questioning (GRPQ), Think-Pair-Share (TPS), and Problem Based Learning (PBL) are compared. Each methodology was applied to three groups of students of Industrial Engineering of a Brazilian University, totaling 138 students. As a result, PBL obtained the best assessment in the three groups, followed by GRPQ. The proposed framework validates the assessment of learning methodologies, providing a structure and guideline for its replication in other educational institutions.},
 authors = {['Rafaela Heloisa Carvalho Machado', 'Samuel Vieira Conceição', 'Renata Pelissari', 'Sarah Ben Amor', 'Thiago Lombardi Resende']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Active learning methodologies', 'Skills', 'Multiple criteria decision making', 'MCDA', 'MCDM']},
 title = {A multiple criteria framework to assess learning methodologies},
 year = {2023}
}

@Filtered Article{19358bf1-f9a4-4b61-be65-64839d4d6df6,
 abstract = {If O2 is for I, then what is O2 for AI? What is oxygen for humans is what is object-oriented thinking for artificial intelligent systems. Much the same way as humans need O2 knowingly or unknowingly, the first step in designing an AI system requires the application of object-oriented principles either explicitly or implicitly. The basis of the definition of state in AI is a description of the concept of interest as an object with properties. The idea of an object extends beyond typical noun forms that describe elements of the real world. The verb forms are included as well with -able suffixes such as runnable, serializable, and executable. In the software world, the first step in modeling a business requirement is the identification of objects of interest and defining their properties and interactions. For instance, in the case of web services, a service is an object; in the case of database systems, a table or a transaction is an object; or in the case of large-scale integration, electronic components are objects. The concept of an object extends beyond the software realm and into the mathematical world in an implicit form. Functional analysis is an old topic in mathematics where each function is an object indeed. The concept of space in mathematics relates closely to the possible value ranges of all attributes of an object. Mathematical operators are the same as methods of objects. It is day-to-day practical life in any modern operating system software dealing with process objects and applications such as Python scripts involving function objects. In this chapter, the application of object-oriented thinking to convert a business requirement to a machine learning (ML) formulation is presented with examples. The five steps of supervised ML formulation based on vector representations of input and output, mapping function, loss function, and data set are clarified. The scope and limitation of ML formulation as against general AI methodology are discussed to demystify popular myths. This chapter also reveals the secret behind the success of deep learning methodology as automatic differentiation involving function objects.},
 authors = {['Kalidas Yeturu']},
 journal = {Elsevier},
 keywords = {['Object oriented', 'Vector representation', 'Induction', 'Deduction', 'Machine learning', 'Artificial intelligence']},
 title = {Chapter 1 - Object-oriented basis of artificial intelligence methodologies},
 year = {2023}
}

@Filtered Article{19c5b301-d66a-4840-8fcf-2fee29360ad3,
 abstract = {Thought disorder, as inferred from disorganized and incoherent speech, is an important part of the clinical presentation in schizophrenia. Traditional measurement approaches essentially count occurrences of certain speech events which may have restricted their usefulness. Applying speech technologies in assessment can help automate traditional clinical rating tasks and thereby complement the process. Adopting these computational approaches affords clinical translational opportunities to enhance the traditional assessment by applying such methods remotely and scoring various parts of the assessment automatically. Further, digital measures of language may help detect subtle clinically significant signs and thus potentially disrupt the usual manner by which things are conducted. If proven beneficial to patient care, methods where patients’ voice are the primary data source could become core components of future clinical decision support systems that improve risk assessment. However, even if it is possible to measure thought disorder in a sensitive, reliable and efficient manner, there remain many challenges to then translate into a clinically implementable tool that can contribute towards providing better care. Indeed, embracing technology - notably artificial intelligence - requires vigorous standards for reporting underlying assumptions so as to ensure a trustworthy and ethical clinical science.},
 authors = {['Brita Elvevåg']},
 journal = {Psychiatry Research},
 keywords = {['Assessment', 'Language', 'Memory']},
 title = {Reflections on measuring disordered thoughts as expressed via language},
 year = {2023}
}

@Filtered Article{1a40e20e-0ca0-4973-b4fb-7fa3d911dc5d,
 abstract = {Specific environmental features, such as natural settings or spatial design, can foster creativity. The effect of object-context congruency on creativity has not yet been investigated. While congruence between an object and its visual context provides meaning to the object, it may hamper creativity due to mental fixation effects. In the current study, virtual reality technology (VR) was employed to examine the hypothesis that people display more cognitive flexibility - a key element of creativity, representing the ability to overcome mental fixation - when thinking about an object while being in an incongruent than in a congruent environment. Participants (N = 184) performed an Alternative Uses Task, in which they had to name as many uses for a book as possible, while being immersed in a virtual environment that was either object-context congruent (i.e., places where you would expect a book; e.g., a library or a living room; n = 91) or object-context incongruent (i.e., places where a book is not expected; e.g., a clothing store or a car workshop; n = 93). The effect of object (in)congruency was also assessed for three other indices of creativity: fluency (i.e., the number of ideas generated), originality and usefulness. In line with our hypothesis, participants scored higher on pure cognitive flexibility in the object-context incongruent than in the object-context congruent environment. Moreover, participants in the object-context incongruent environment condition generated more original ideas. The theoretical and practical implications of the current findings are discussed.},
 authors = {['Mare {van Hooijdonk}', 'Simone M. Ritter', 'Marcel Linka', 'Evelyn Kroesbergen']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Creativity', 'Spatial context', 'Object congruence', 'Cognitive flexibility', 'Stimulating creativity']},
 title = {Creativity and change of context: The influence of object-context (in)congruency on cognitive flexibility},
 year = {2022}
}

@Filtered Article{1a43dbf5-a3b2-4f42-91ce-69a26dfaef69,
 abstract = {Summary
Scientific thinking about the minds of humans and other animals has been transformed by the idea that the brain is Bayesian. A cornerstone of this idea is that agents set the balance between prior knowledge and incoming evidence based on how reliable or ‘precise’ these different sources of information are — lending the most weight to that which is most reliable. This concept of precision has crept into several branches of cognitive science and is a lynchpin of emerging ideas in computational psychiatry — where unusual beliefs or experiences are explained as abnormalities in how the brain estimates precision. But what precisely is precision? In this Primer we explain how precision has found its way into classic and contemporary models of perception, learning, self-awareness, and social interaction. We also chart how ideas around precision are beginning to change in radical ways, meaning we must get more precise about how precision works.},
 authors = {['Daniel Yon', 'Chris D. Frith']},
 journal = {Current Biology},
 title = {Precision and the Bayesian brain},
 year = {2021}
}

@Filtered Article{1af65d07-0b83-4031-8842-db4e2b71226b,
 abstract = {The present work develops a flexible and robust solution strategy to resolve coupled systems comprised of large numbers of flowing particles embedded within a fluid. A model problem, consisting of particles which may undergo inelastic collisions in the presence of near-field forces, is considered. The particles are surrounded by a continuous interstitial fluid which is assumed to obey the compressible Navier–Stokes equations. Thermal effects are also considered. Such particle/fluid systems are strongly coupled, due to the mechanical forces and heat transfer induced by the fluid onto the particles and vice-versa. Because the coupling of the various particle and fluid fields can dramatically change over the course of a flow process, a primary focus of this work is the development of a recursive “staggering” solution scheme, whereby the time-steps are adaptively adjusted to control the error associated with the incomplete resolution of the coupled interaction between the various solid particulate and continuum fluid fields. A central feature of the approach is the ability to account for the presence of particles within the fluid in a straightforward manner that can be easily incorporated within any standard computational fluid mechanics code based on finite difference, finite element or finite volume type discretization. A three dimensional example is provided to illustrate the overall approach.},
 authors = {['T.I. Zohdi']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Particle–fluid interaction', 'Multiple fields', 'Iterative methods']},
 title = {Computation of strongly coupled multifield interaction in particle–fluid systems},
 year = {2007}
}

@Filtered Article{1b2d291c-0638-4114-b736-ad213bc76e0c,
 abstract = {Modern advances in computational language processing methods have enabled new approaches to the measurement of mental processes. However, the field has primarily focused on model accuracy in predicting performance on a task or a diagnostic category. Instead the field should be more focused on determining which computational analyses align best with the targeted neurocognitive/psychological functions that we want to assess. In this paper we reflect on two decades of experience with the application of language-based assessment to patients' mental state and cognitive function by addressing the questions of what we are measuring, how it should be measured and why we are measuring the phenomena. We address the questions by advocating for a principled framework for aligning computational models to the constructs being assessed and the tasks being used, as well as defining how those constructs relate to patient clinical states. We further examine the assumptions that go into the computational models and the effects that model design decisions may have on the accuracy, bias and generalizability of models for assessing clinical states. Finally, we describe how this principled approach can further the goal of transitioning language-based computational assessments to part of clinical practice while gaining the trust of critical stakeholders.},
 authors = {['Peter W. Foltz', 'Chelsea Chandler', 'Catherine Diaz-Asper', 'Alex S. Cohen', 'Zachary Rodriguez', 'Terje B. Holmlund', 'Brita Elvevåg']},
 journal = {Schizophrenia Research},
 keywords = {['Natural language processing', 'Speech technologies', 'Artificial intelligence']},
 title = {Reflections on the nature of measurement in language-based automated assessments of patients' mental state and cognitive function},
 year = {2023}
}

@Filtered Article{1b550d4b-ceb6-47c8-8d85-331aee96d2df,
 abstract = {Product innovation design process involves a great deal of discrete engineering knowledge, limiting the ability of designers to quickly utilize this knowledge to support design innovation. Nowadays, innovation design based on knowledge graphs has enhanced the ability to explore design knowledge, improving the efficiency of knowledge retrieval. Previous studies have focused on mining more design knowledge to enrich the knowledge graph overlooks the implicit relationships with potential value among design knowledge, wasting design resources. To address these issues, an approach for product innovation design based on implicit knowledge relationship completion in the patent knowledge graph is proposed, which explores the implicit relationships between design knowledge to provide new knowledge satisfying design preferences and enhance the innovativeness of solutions. First, a requirements-function-structure-benefit (RFSB) knowledge ontology is constructed and extracted from the benefit knowledge of patents to build the knowledge graph. Second, an implicit relationship completion model based on the similarity of function or benefit entities explores the implicit relationships, replacing structure entities directly connected to similar function or benefit entities to generate new relationships and outputs novel ideas. Third, a scheme improvement process based on the co-occurrence frequency of requirement and structure knowledge supplements neglected design preferences. Final, a pipeline inspection robot case study is further employed to verify the proposed approach, and a patent knowledge graph assisted design solution prototype system is developed to assist in the utilization of innovative design knowledge. Evaluation results show the significant design potential of the proposed approach in inspiring innovative thinking and knowledge reuse.},
 authors = {['Shaofei Jiang', 'Jingwei Yang', 'Jing Xie', 'Xuesong Xu', 'Yubo Dou', 'Liting Jing']},
 journal = {Advanced Engineering Informatics},
 keywords = {['Product innovation design', 'Patent text', 'Knowledge graph', 'RFSB ontology model', 'Implicit relationship completion']},
 title = {Product innovation design approach driven by implicit relationship completion via patent knowledge graph},
 year = {2024}
}

@Filtered Article{1b9a6b99-7d5d-42e6-bcfb-11fd13f1e21c,
 abstract = {System xc− represents an intriguing target in attempts to understand the pathological states of the central nervous system. Also called a cystine-glutamate antiporter, system xc− typically functions by exchanging one molecule of extracellular cystine for one molecule of intracellular glutamate. Nonvesicular glutamate released during cystine-glutamate exchange activates extrasynaptic glutamate receptors in a manner that shapes synaptic activity and plasticity. These findings contribute to the intriguing possibility that extracellular glutamate is regulated by a complex network of release and reuptake mechanisms, many of which are unique to glutamate and rarely depicted in models of excitatory signaling. Because system xc− is often expressed on non-neuronal cells, the study of cystine-glutamate exchange may advance the emerging viewpoint that glia are active contributors to information processing in the brain. It is noteworthy that system xc− is at the interface between excitatory signaling and oxidative stress, because the uptake of cystine that results from cystine-glutamate exchange is critical in maintaining the levels of glutathione, a critical antioxidant. As a result of these dual functions, system xc− has been implicated in a wide array of central nervous system diseases ranging from addiction to neurodegenerative disorders to schizophrenia. In the current review, we briefly discuss the major cellular components that regulate glutamate homeostasis, including glutamate release by system xc−. This is followed by an in-depth discussion of system xc− as it relates to glutamate release, cystine transport, and glutathione synthesis. Finally, the role of system xc− is surveyed across a number of psychiatric and neurodegenerative disorders.},
 authors = {['Richard Bridges', 'Victoria Lutgen', 'Doug Lobner', 'David A. Baker']},
 journal = {Pharmacological Reviews},
 title = {Thinking Outside the Cleft to Understand Synaptic Activity: Contribution of the Cystine-Glutamate Antiporter (System xc−) to Normal and Pathological Glutamatergic Signaling},
 year = {2012}
}

@Filtered Article{1baee054-2dc8-4d6e-96e6-62dde4fad244,
 abstract = {This paper takes a narrative seam through the design discipline, attempting to explain how design methodology, one of the three types of Nigel Cross' designerly ways of knowing, has changed over the 40 years of Design Studies. Specifically, the paper identifies the point when a ‘social turn’ in the discipline occurred, allowing more nuanced and critical studies of designing, and shifting the balance from an objective (‘scientific’) perspective to one more based on relativist approaches. The paper concludes by noting the plurality of present-day study, arguably enabled by design thinking, and sketches what this holds for the future of the discipline. The references in the paper are mainly restricted to those published in, or strongly relating to, Design Studies.},
 authors = {['Peter Lloyd']},
 journal = {Design Studies},
 keywords = {['design methods', 'design studies', 'design research', 'design process', 'design thinking']},
 title = {You make it and you try it out: Seeds of design discipline futures},
 year = {2019}
}

@Filtered Article{1bb4bc4f-a102-4858-a146-bc5852eec3f3,
 abstract = {The work reported in this paper addresses the paradoxical state of the construction industry (also known as A/E/C, for Architecture, Engineering and Construction), where the design of highly integrated facilities is undertaken by severely fragmented teams, leading to diminished performance of both processes and products. The construction industry has been trying to overcome this problem by partitioning the design process hierarchically or temporally. While these methods are procedurally efficient, their piecemeal nature diminishes the overall performance of the project. Computational methods intended to facilitate collaboration in the construction industry have, so far, focused primarily on improving the flow of information among the participants. They have largely met their stated objective of improved communication, but have done little to improve joint decision-making, and therefore have not significantly improved the quality of the design project itself. We suggest that the main impediment to effective collaboration and joint decision-making in the A/E/C industry is the divergence of disciplinary `world-views', which are the product of educational and professional processes through which the individuals participating in the design process have been socialized into their respective disciplines. To maximize the performance of the overall project, these different world-views must be reconciled, possibly at the expense of individual goals. Such reconciliation can only be accomplished if the participants find the attainment of the overall goals of the project more compelling than their individual disciplinary goals. This will happen when the participants have become cognizant and appreciative of world-views other than their own, including the objectives and concerns of other participants. To achieve this state of knowledge, we propose to avail to the participants of the design team highly specific, contextualized information, reflecting each participant's valuation of the proposed design actions. P3 is a semantically-rich computational environment, which is intended to fulfill this mission. It consists of: (1) a shared representation of the evolving design project, connected (through the World Wide Web) to (2) individual experts and their discipline-specific knowledge repositories; and (3) a computational project manager makes the individual valuations visible to all the participants, and helps them deliberate and negotiate their respective positions for the purpose of improving the overall performance of the project. The paper discusses the theories on which the three components are founded, their function, and the principles of their implementation.},
 authors = {['Yehuda E Kalay']},
 journal = {Automation in Construction},
 keywords = {['Collaborative design', 'Design environment', 'Product model', 'Performance model', 'Process model']},
 title = {P3: Computational environment to support design collaboration},
 year = {1998}
}

@Filtered Article{1c013b7c-5fd3-43c4-8ab0-decedf65c02e,
 abstract = {One of the most influential descriptions of design activity emphasizes how problems and solutions “co-evolve.” This concept has somehow escaped critical review and cross-disciplinary comparison, resulting in a fragmented approach to the subject. Reviewing the published literature on design co-evolution reveals that the term is used to refer to a range of distinct concepts, and the study of co-evolution has generated a number of elaborations and alternatives. Reviewing the broader literature in design and other disciplines further reveals that discussions of design co-evolution are disconnected from the history of relevant concepts in design research, and disconnected from a range of relevant concepts in other disciplines that describe creative work. Here I examine what the different concepts of design co-evolution are, how they have been modified and what they are related to. This leads to questioning the distinction between problems and solutions, defining them in relative terms, and drawing a connection between design co-evolution and design fixation.},
 authors = {['Nathan Crilly']},
 journal = {She Ji: The Journal of Design, Economics, and Innovation},
 keywords = {['Design process', 'Design thinking', 'Creativity', 'Design history', 'Interdisciplinarity']},
 title = {The Evolution of “Co-evolution” (Part I): Problem Solving, Problem Finding, and Their Interaction in Design and Other Creative Practices},
 year = {2021}
}

@Filtered Article{1cacd290-e4e5-4785-8cb8-957b57cb68da,
 abstract = {Background
Clinical trials demonstrate the efficacy and tolerability of medications targeting calcitonin gene–related peptide (CGRP) signaling for migraine prevention. However, these trials may not accurately reflect the real-world experiences of more diverse and heterogeneous patient populations, who often have higher disease burden and more comorbidities. Therefore, postmarketing safety surveillance is warranted. Regulatory organizations encourage marketing authorization holders to screen digital media for suspected adverse reactions, applying the same requirements as for spontaneous reports. Real-world data from social media platforms constitute a potential venue to capture diverse patient experiences and help detect treatment-related adverse events. However, while social media holds promise for this purpose, its use in pharmacovigilance is still in its early stages. Computational linguistics, which involves the automatic manipulation and quantitative analysis of oral or written language, offers a potential method for exploring this content.
Objective
This study aims to characterize adverse events related to monoclonal antibodies targeting CGRP signaling on Reddit, a large online social media forum, by using computational linguistics.
Methods
We examined differences in word frequencies from medication-related posts on the Reddit subforum r/Migraine over a 10-year period (2010-2020) using computational linguistics. The study had 2 phases: a validation phase and an application phase. In the validation phase, we compared posts about propranolol and topiramate, as well as posts about each medication against randomly selected posts, to identify known and expected adverse events. In the application phase, we analyzed posts discussing 2 monoclonal antibodies targeting CGRP signaling—erenumab and fremanezumab—to identify potential adverse events for these medications.
Results
From 22,467 Reddit r/Migraine posts, we extracted 402 (2%) propranolol posts, 1423 (6.33%) topiramate posts, 468 (2.08%) erenumab posts, and 73 (0.32%) fremanezumab posts. Comparing topiramate against propranolol identified several expected adverse events, for example, “appetite,” “weight,” “taste,” “foggy,” “forgetful,” and “dizziness.” Comparing erenumab against a random selection of terms identified “constipation” as a recurring keyword. Comparing erenumab against fremanezumab identified “constipation,” “depression,” “vomiting,” and “muscle” as keywords. No adverse events were identified for fremanezumab.
Conclusions
The validation phase of our study accurately identified common adverse events for oral migraine preventive medications. For example, typical adverse events such as “appetite” and “dizziness” were mentioned in posts about topiramate. When we applied this methodology to monoclonal antibodies targeting CGRP or its receptor—fremanezumab and erenumab, respectively—we found no definite adverse events for fremanezumab. However, notable flagged words for erenumab included “constipation,” “depression,” and “vomiting.” In conclusion, computational linguistics applied to social media may help identify potential adverse events for novel therapeutics. While social media data show promise for pharmacovigilance, further work is needed to improve its reliability and usability.},
 authors = {['Pengfei Zhang', 'Brad K Kamitaki', 'Thien Phu Do']},
 journal = {JMIR Formative Research},
 keywords = {['internet', 'patient reported outcome', 'headache', 'health information', 'Reddit', 'registry', 'monoclonal antibody', 'crowdsourcing', 'postmarketing', 'safety', 'surveillance', 'migraine', 'preventives', 'prevention', 'self-reported', 'calcitonin gene–related peptide', 'calcitonin', 'therapeutics', 'social media', 'medication-related', 'posts', 'propranolol', 'topiramate', 'erenumab', 'fremanezumab', 'cross-sectional', 'surveys']},
 title = {Crowdsourcing Adverse Events Associated With Monoclonal Antibodies Targeting Calcitonin Gene–Related Peptide Signaling for Migraine Prevention: Natural Language Processing Analysis of Social Media},
 year = {2024}
}

@Filtered Article{1cd1291d-7aa1-4d63-9bc1-4b2efce5e7d2,
 abstract = {A number of cognitive, causal mapping and simulation techniques exist for dealing with the growing importance of environmental uncertainty. After briefly commenting on some of the more salient extant approaches, this paper offers a new one for consideration by the scenario planning community. Comprehensive Situation Mapping (CSM) is a powerful analytical tool combined with a process for framing and debating strategic situations. The CSM approach combines the problem framing features of causal mapping with a dialectical inquiry process patterned after Churchman's. Like the better approaches to planning through cognitive mapping, it facilitates the “backward analysis” of the underlying strategic assumptions. Its novelty is that it also allows the “forward analysis” of a situation by computing the potential change scenarios. Initially developed for manual application, the principles of CSM were originally tested in appropriate case studies. The contribution of the present paper is to present its theory and point out that its future potential is even greater: in concluding we indicate that, by using recent distributed artificial intelligence (DAI) technology, a fully computerized and interactive prototype is now being set up for commercial applications.},
 authors = {['William Acar', 'Douglas Druckenmiller']},
 journal = {Futures},
 title = {Endowing cognitive mapping with computational properties for strategic analysis},
 year = {2006}
}

@Filtered Article{1cd321c8-bdc9-4975-bcc2-0b0e97013dc5,
 authors = {['Fairouz Kamareddine', 'Twan Laan', 'Robert Constable']},
 journal = {North-Holland},
 title = {Russell's Orders in Kripke's Theory of Truth and Computational Type Theory},
 year = {2012}
}

@Filtered Article{1d0bf435-cb4b-469e-9b13-1ed1583dbad9,
 abstract = {The relationship between nonword repetition ability and vocabulary size and vocabulary learning has been a topic of intense research interest and investigation over the last two decades, following the demonstration that nonword repetition accuracy is predictive of vocabulary size (Gathercole & Baddeley, 1989). However, the nature of this relationship is not well understood. One prominent account posits that phonological short-term memory (PSTM) is a causal determinant both of nonword repetition ability and of phonological vocabulary learning, with the observed correlation between the two reflecting the effect of this underlying third variable (e.g., Baddeley, Gathercole, & Papagno, 1998). An alternative account proposes the opposite causality: that it is phonological vocabulary size that causally determines nonword repetition ability (e.g., Snowling, Chiat, & Hulme, 1991). We present a theory of phonological vocabulary learning, instantiated as a computational model. The model offers a precise account of the construct of PSTM, of performance in the nonword repetition task, of novel word form learning, and of the relationship between all of these. We show through simulation not only that PSTM causally affects both nonword repetition accuracy and phonological vocabulary size, but also that phonological vocabulary size causally affects nonword repetition ability. The plausibility of the model is supported by the fact that its nonword repetition accuracy displays effects of phonotactic probability and of nonword length, which have been taken as evidence for causal effects on nonword repetition accuracy of phonological vocabulary knowledge and PSTM, respectively. Thus the model makes explicit how the causal links posited by the two theoretical perspectives are both valid, in the process reconciling the two perspectives, and indicating that an opposition between them is unnecessary.},
 authors = {['Prahlad Gupta', 'Jamie Tisdale']},
 journal = {Journal of Memory and Language},
 keywords = {['Nonword repetition', 'Vocabulary learning', 'Computational modeling', 'Phonological memory', 'Word learning', 'Short-term memory', 'Language']},
 title = {Does phonological short-term memory causally determine vocabulary learning? Toward a computational resolution of the debate},
 year = {2009}
}

@Filtered Article{1d29e975-03c6-40b3-b65a-c1b798d3ef36,
 abstract = {We present a series of numerical experiments on the accuracy of approximate radiation boundary conditions for computational aeroacoustics based on Padé approximants. Our test problem is described by an infinite periodic array of pressure pulses, for which we can independently evaluate the exact solution by numerical quadrature. It is demonstrated that acceptable long time accuracy can be achieved, but only if conditions of high order are employed. As predicted by theory, the order required for a given accuracy is proportional to the time of the simulation.},
 authors = {['Thomas Hagstrom', 'John Goodrich']},
 journal = {Applied Numerical Mathematics},
 keywords = {['Nonreflecting Boundary Conditions', 'Aeroacoustics']},
 title = {Experiments with approximate radiation boundary conditions for computational aeroacoustics},
 year = {1998}
}

@Filtered Article{1d834dec-1f3e-478f-8ffe-6ba03c38e838,
 abstract = {When designing research to examine the variables underlying creative thinking and problem solving success, one must not only consider (a) the demands of the task being performed, but (b) the characteristics of the individual performing the task and (c) the constraints of the skill execution environment. In the current paper we describe methodologies that allow one to effectively study creative thinking by capturing interactions among the individual, task, and problem solving situation. In doing so, we demonstrate that the relation between executive functioning and problem solving success is not always as straightforward as one might initially believe.},
 authors = {['Marci S. DeCaro', 'Mareike Wieth', 'Sian L. Beilock']},
 journal = {Methods},
 keywords = {['Working memory', 'Performance', 'Pressure', 'Individual differences', 'Problem solving', 'Creativity', 'Short term memory', 'Stress', 'Math']},
 title = {Methodologies for examining problem solving success and failure},
 year = {2007}
}

@Filtered Article{1db48351-585b-4934-9c57-3a3ece433322,
 abstract = {The present paper describes a method to enhance the capability of, or to broaden the scope of computational mechanics by using deep learning, which is one of the machine learning methods and is based on the artificial neural network. The method utilizes deep learning to extract rules inherent in a computational mechanics application, which usually are implicit and sometimes too complicated to grasp from the large amount of available data A new method of numerical quadrature for the FEM stiffness matrices is developed by using the proposed method, where a kind of optimized quadrature rule superior in accuracy to the standard Gauss–Legendre quadrature is obtained on the element-by-element basis. The detailed formulation of the proposed method is given with the sample application above, and an acceleration technique for the proposed method is discussed},
 authors = {['Atsuya Oishi', 'Genki Yagawa']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Deep learning', 'Artificial neural network', 'Numerical quadrature', 'Element stiffness matrix']},
 title = {Computational mechanics enhanced by deep learning},
 year = {2017}
}

@Filtered Article{1dbf7a3a-b4df-45b5-bfa2-6a657700fe8f,
 abstract = {Construction robotics has emerged as a leading technology in the construction industry. This paper conducts an innovative dual-track quantitative comprehensive method to analyze the current literature and assess future trends. First, a bibliometric review of 955 journal articles published between 1974 and 2023 was performed, exploring keywords, journals, countries, and clusters. Furthermore, a neural topic model based on BERTopic addresses topic modeling repetition issues. The study identifies building information modeling (BIM), human–robot collaboration (HRC), and deep reinforcement learning (DRL) as “three pillars” in the field. Additionally, we systematically reviewed the relevant literature and nested symbiotic relationships. The outcome of this study is twofold: first, the findings provide quantitative and qualitative scientific guidance for future research on trends; second, the innovative dual-track quantitative analysis research methodology simultaneously stimulates critical thinking about the modeling of other similarly trending topics characterized to avoid high degree of homogeneity and corpus overlap.},
 authors = {['Yuming Liu', 'Aidi Hizami Bin Alias', 'Nuzul Azam Haron', 'Nabilah Abu Bakar', 'Hao Wang']},
 journal = {Automation in Construction},
 keywords = {['Construction robotics', 'BERTopic model', 'BIM', 'Human–robot collaboration', 'Deep reinforcement learning', 'Dual-track quantitative analysis']},
 title = {Exploring three pillars of construction robotics via dual-track quantitative analysis},
 year = {2024}
}

@Filtered Article{1dc1dfe1-02da-4aeb-850e-0f38e91e4210,
 abstract = {The aim of this article is to motivate and describe the parameter ecology program, which studies how different parameters contribute to the difficulty of classical problems. We call for a new type of race in parameterized analysis, with the purpose of uncovering the boundaries of tractability by finding the smallest possible parameterizations which admit FPT-algorithms or polynomial kernels. An extensive overview of recent advances on this front is presented for the Vertex Cover problem. Moving even beyond the parameter ecology program we advocate the principle of model enrichment, which raises the challenge of generalizing positive results to problem definitions with greater modeling power. The computational intractability which inevitably emerges can be deconstructed by introducing additional parameters, leading towards a theory of fully multivariate algorithmics.},
 authors = {['Michael R. Fellows', 'Bart M.P. Jansen', 'Frances Rosamond']},
 journal = {European Journal of Combinatorics},
 title = {Towards fully multivariate algorithmics: Parameter ecology and the deconstruction of computational complexity},
 year = {2013}
}

@Filtered Article{1dcc3c2c-67c4-4320-9440-d4e4fd57c0c3,
 abstract = {Technology is constantly supporting in the innovation of the teaching-learning process. Today’s students are more demanding actors when it comes to the environment they have at their disposal to learn, experiment and develop their critical thinking. The area of Mathematics has successively suffered from students’ learning difficulties, whether due to lack of motivation, low abstraction ability or lack of new tools for teachers to bring innovation into the classroom and outside it. While being true that digitalization has entered schools, it often follows a basic and simple process of digital replication of approaches and materials that were previously only available on physical media. This work focuses on the use of Extended Realities, more precisely, Mixed Reality, for teaching Mathematics, and very particularly in the teaching of Geometry, through the proposition of a conceptual model that combines the use of Extended Reality and Machine Learning. The proposed model was subject to prototyping, which is presented as a form of laboratory validation as a contribution to innovate the way of how the geometry teaching-learning process is developed and to promote the integration of Extended Reality technologies into the Education Sector as practical tools, as well due to its potential use to obtain useful insights for teachers, and students, throughout the process.},
 authors = {['Carlos R. Cunha', 'André Moreira', 'Sílvia Coelho', 'Vítor Mendonça', 'João Pedro Gomes']},
 journal = {Journal of Engineering Research},
 keywords = {['Geometry', 'Teaching', 'Learning', 'Education', 'Extended reality', 'Mixed reality', 'Machine learning']},
 title = {Converging extended reality and Machine Learning to improve the lecturing of geometry in basic education},
 year = {2024}
}

@Filtered Article{1dd8e446-aaca-4c44-a5ac-3e634e79fc47,
 abstract = {Creativity is necessary to human survival, human prosperity, civilization and well-being. Visual creativity is an important part of creativity and is the ability to create products of novel and useful visual forms, playing important role in many fields such as art, painting and sculpture. There have been several neuroimaging studies exploring the neural basis of visual creativity. However, to date, little is known about the relationship between cortical structure and visual creativity as measured by the Torrance Tests of Creative Thinking. Here, we investigated the association between cortical thickness and visual creativity in a large sample of 310 healthy adults. We used multiple regression to analyze the correlation between cortical thickness and visual creativity, adjusting for gender, age and general intelligence. The results showed that visual creativity was significantly negatively correlated with cortical thickness in the left middle frontal gyrus (MFG), right inferior frontal gyrus (IFG), right supplementary motor cortex (SMA) and the left insula. These observations have implications for understanding that a thinner prefrontal cortex (PFC) (e.g. IFG, MFG), SMA and insula correspond to higher visual creative performance, presumably due to their role in executive attention, cognitive control, motor planning and dynamic switching.},
 authors = {['Fang Tian', 'Qunlin Chen', 'Wenfeng Zhu', 'Yongming Wang', 'Wenjing Yang', 'Xingxing Zhu', 'Xue Tian', 'Qinglin Zhang', 'Guikang Cao', 'Jiang Qiu']},
 journal = {Neuroscience Letters},
 keywords = {['Visual creativity', 'Cortical thickness', 'Prefrontal cortex', 'Supplementary motor cortex', 'Insula']},
 title = {The association between visual creativity and cortical thickness in healthy adults},
 year = {2018}
}

@Filtered Article{1df1a6a8-b793-4c4e-b4a2-52c1ffad7984,
 abstract = {Background
The COVID-19 pandemic has accelerated the digitalization of modern society, extending digital transformation to daily life and psychological evaluation and treatment. However, the development of competencies and literacy in handling digital technology has not kept pace, resulting in a significant disparity among individuals. Existing measurements of digital literacy were developed before widespread information and communications technology device adoption, mainly focusing on one’s perceptions of their proficiency and the utility of device operation. In the contemporary landscape, digital transformation is evolving within specialized domains, necessitating a comprehensive evaluation of digital competencies, attitudes, and proficiency in technology application to bridge the digital divide and ensure digital compliance.
Objective
This study was designed to address the shortcomings of existing scales and formulate a digital sensitivity scale tailored to the requirements of today’s society.
Methods
Initial items of the Yongin Severance Digital Sensitivity Scale (YI-DSS) were collected through a literature review, and expert opinions were gathered to ensure content validity. An exploratory and confirmatory factor analysis included 986 adult participants evaluating 14 digital literacy items and 6 digital efficacy items. The Cronbach α confirmed internal consistency reliability, and 2-tailed t tests, ANOVAs, and post hoc tests analyzed demographic differences in digital literacy and efficacy.
Results
A robust 4-factor digital literacy solution was identified: digital application, digital communication, critical thinking, and digital ethics (Kaiser-Meyer-Olkin=0.891; Bartlett × 2=9829.713; P<.001; Cronbach α=0.782-0.947). A 2-factor solution defined digital efficacy: digital confidence and digital anxiety (Kaiser-Meyer-Olkin=0.735; Bartlett × 2=3282.217; P<.001; Cronbach α=0.787-0.912). Confirmatory factor analysis was conducted for each model (digital literacy model: χ271=676.0, comparative fit index=0.938, Tucker-Lewis index=0.921, standardized root mean square residual=0.73, and root mean square error of approximation=0.093; digital efficacy model: χ28=81.9, comparative fit index=0.977, Tucker-Lewis index=0.958, standardized root mean square residual=0.73, and root mean square error of approximation=0.097), which indicated a good fit. The YI-DSS also showed high correlation with the previously developed Digital Literacy Scale (r=0.809; P<.001).
Conclusions
The YI-DSS, as a self-assessment tool, has the potential to bridge the generational information gap by promoting acceptance, motivation, and adaptation to digital technology. Furthermore, given the remote nature of digital therapeutics, an individual’s familiarity with required technologies and digital communication strongly influences their acceptance of digital treatments and the efficacy thereof. This scale can play a pivotal role in enhancing compliance with digital therapeutics by preemptively assessing individuals’ technological literacy and competency.},
 authors = {['Hae In Park', 'Minjeong Jeon', 'Ji Seon Ahn', 'Kyungmi Chung', 'Jin Young Park']},
 journal = {Journal of Medical Internet Research},
 keywords = {['information literacy', 'health literacy', 'computer literacy', 'self-efficacy', 'attitude', 'digital divide']},
 title = {Development and Validation of the Digital Sensitivity Scale for Adults: Cross-Sectional Observational Study},
 year = {2025}
}

@Filtered Article{1e338d48-b547-4639-a2c0-dd182b6021e6,
 abstract = {During face-to-face interactions typically developing individuals use gaze aversion (GA), away from their questioner, when thinking. GA is also used when individuals with autism (ASD) and Williams syndrome (WS) are thinking during question-answer interactions. We investigated GA strategies during face-to-face social style interactions with familiar and unfamiliar interlocutors. Participants with WS and ASD used overall typical amounts/patterns of GA with all participants looking away most while thinking and remembering (in contrast to listening and speaking). However there were a couple of specific disorder related differences: participants with WS looked away less when thinking and interacting with unfamiliar interlocutors; in typical development and WS familiarity was associated with reduced gaze aversion, however no such difference was evident in ASD. Results inform typical/atypical social and cognitive phenotypes. We conclude that gaze aversion serves some common functions in typical and atypical development in terms of managing the cognitive and social load of interactions. There are some specific idiosyncracies associated with managing familiarity in ASD and WS with elevated sociability with unfamiliar others in WS and a lack of differentiation to interlocutor familiarity in ASD. Regardless of the familiarity of the interlocutor, GA is associated with thinking for typically developing as well as atypically developing groups. Social skills training must take this into account.},
 authors = {['Gwyneth Doherty-Sneddon', 'Lisa Whittle', 'Deborah M. Riby']},
 journal = {Research in Developmental Disabilities},
 keywords = {['Eye contact', 'Gaze', 'Williams syndrome', 'Gaze aversion', 'Autism spectrum disorder']},
 title = {Gaze aversion during social style interactions in autism spectrum disorder and Williams syndrome},
 year = {2013}
}

@Filtered Article{1e8d67c1-3731-4071-b2a0-8ab4292c2c68,
 abstract = {Configurable Computing Machines (CCMs) have been able to provide orders of magnitude increases in execution rates for applications such as image processing, signal processing, and automatic target recognition. This paper describes the use of CCMs to accelerate complex, large-scale scientific computations. These applications present a challenge for CCMs because of their large size, hundreds of thousands of lines of code, and the unstructured nature of the computations. This paper describes strategies for accelerating scientific computations on CCMs and demonstrates the effectiveness of one such strategy on the Annapolis Micro Systems WildForce board. Results from this implementation are analyzed.},
 authors = {['M.T Jones', 'K Ramachandran']},
 journal = {Advances in Engineering Software},
 keywords = {['Configurable computing', 'Floating point', 'Finite element']},
 title = {Unstructured mesh computations on CCMs},
 year = {2000}
}

@Filtered Article{1e9cfec5-4b05-429b-8c07-f86dbb04d6fc,
 abstract = {In the first author's thesis [Marcial-Romero, J. R., “Semantics of a sequential language for exact real-number computation”, PhD thesis at the University of Birmingham, 2004)], a sequential language, LRT, for real number computation is investigated. The thesis includes a proof that all polynomials are programmable, but that work comes short of giving a complete characterization of the expressive power of the language even for first-order functions. The technical problem is that LRT is non-deterministic. So a natural characterization of its expressive power should be in terms of relations rather than functions. In [Brattka, V., Recursive characterization of computable real-valued functions and relations, Theoretical Computer Science 162 (1) (1996) 45–77], Brattka investigates a formalization of recursive relations in the style of Kleene's recursive functions on the natural numbers. This paper establishes the expressive power of LRTp, a variant of LRT, in terms of Brattka's recursive relations. Because Brattka already did the work of establishing the precise connection between his recursive relations and Type 2 Theory of Effectivity, we thus obtain a complete characterization of first-order definability in LRTp.},
 authors = {['J. Raymundo Marcial-Romero', 'M. Andrew Moshier']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['exact real-number computation', 'sequential computation', 'recursive relations', 'semantics', 'non-determinism', 'PCF']},
 title = {Sequential Real Number Computation and Recursive Relations},
 year = {2008}
}

@Filtered Article{1ed790c4-5c65-4d73-915c-9d36168314e7,
 abstract = {This article sets out to dispel two widespread economic superstitions—the belief in an inherent equilibrium of the economic system, and the perception of international trade as a zero-sum game. The author argues that morphogenetic causal loops disprove the first assumption, and should be used to aid policy making; and that positive-sum results could be obtained by lifting import restrictions and adapting products for foreign markets.},
 authors = {['Magoroh Maruyama']},
 journal = {Futures},
 title = {New economic thinking: Morphogenetic causal loops and product adaptation strategy},
 year = {1987}
}

@Filtered Article{1f69c249-ce8b-4cab-a9fd-ab88baea135a,
 abstract = {This chapter provides an overview of the potentials of employing computational design methods and digital fabrication tools for the creation of novel, material-based design. Just as in the early days of architecture, when the master builder was responsible for all areas of building, these new technologies allow a return to the exploration of experimental design methods and the direct exchange with different materials. Designing for and through digital production techniques thus shifts the focus from formal design representations toward the physically realized. As such, material and tectonic thinking are reintroduced as the very base of the design approach. Due to this a new type of design becomes possible with a formerly unknown degree of complexity—both on a formal and on a functional level. This chapter gives an overview of the history of design, speaks about the so-called “digital continuum,” highlights the benefits of customization and individual production, stresses the nuisance of new esthetic formalizations and the importance of education to mediate such understanding to students of design and architecture.},
 authors = {['Manuel Kretzer']},
 journal = {Butterworth-Heinemann},
 keywords = {['Digital', 'crafting', 'design', 'architecture', 'production', 'fabrication', 'parametric', 'generative', 'Industry 4.0']},
 title = {Chapter 4 - Digital crafting: a new frontier for material design},
 year = {2021}
}

@Filtered Article{1f7c911b-250e-4346-9628-5271022201be,
 abstract = {The remarkable advances of high-performance computing to facilitate and increase efficiency in helping to resolve or support assessments on the toxic effects of chemicals on tissues and genomic material have led to development of novel in silico methods. These methods can support risk assessment via integration of study data that can be translated into meaningful predictive information. This chapter describes some methods in computational toxicology and how to integrate experimental data with computational assessments for supporting risk assessment.},
 authors = {['Luis G. Valerio']},
 journal = {Academic Press},
 keywords = {['toxicology', 'methods', 'translational research', 'QSAR', 'computational toxicology', 'drug safety', 'safety assessment']},
 title = {Chapter 6 - Computational Translation and Integration of Test Data to Meet Risk Assessment Goals},
 year = {2013}
}

@Filtered Article{1fc0d518-4e8f-4e1f-a02d-1225d09cfe6d,
 journal = {Butterworth-Heinemann},
 title = {Appendix A - Scientific chaos: a new way of thinking about dynamics},
 year = {1991}
}

@Filtered Article{201a4654-93a9-48ec-9d66-a74920203449,
 authors = {['Alla Karpova', 'Roozbeh Kiani']},
 journal = {Current Opinion in Neurobiology},
 title = {Editorial overview: Neurobiology of cognitive behavior: Complexity of neural computation and cognition},
 year = {2016}
}

@Filtered Article{205172ba-635e-4f10-9d33-4fe35061b0e4,
 abstract = {Children who hear large amounts of diverse speech learn language more quickly than children who do not. However, high correlations between the amount and the diversity of the input in speech samples makes it difficult to isolate the influence of each. We overcame this problem by controlling the input to a computational model so that amount of exposure to linguistic input (quantity) and the quality of that input (lexical diversity) were independently manipulated. Sublexical, lexical, and multi-word knowledge were charted across development (Study 1), showing that while input quantity may be important early in learning, lexical diversity is ultimately more crucial, a prediction confirmed against children’s data (Study 2). The model trained on a lexically diverse input also performed better on nonword repetition and sentence recall tests (Study 3) and was quicker to learn new words over time (Study 4). A language input that is rich in lexical diversity outperforms equivalent richness in quantity for learned sublexical and lexical knowledge, for well-established language tests, and for acquiring words that have never been encountered before.},
 authors = {['Gary Jones', 'Caroline F. Rowland']},
 journal = {Cognitive Psychology},
 keywords = {['Input quantity', 'Lexical diversity', 'Vocabulary acquisition', 'CLASSIC', 'Language acquisition']},
 title = {Diversity not quantity in caregiver speech: Using computational modeling to isolate the effects of the quantity and the diversity of the input on vocabulary growth},
 year = {2017}
}

@Filtered Article{20726d38-e413-4379-aad7-0e41c475b1d5,
 abstract = {The nickel catalyzed regio- and stereoselective condensation of boronic acids to allenamides is documented as a novel synthetic route to stereochemically defined tri-substituted enamides. The protocol has been implemented into a three-component variant intercepting the in situ formed allyl-Ni intermediate with a range of aldehydes. Additionally, evidence for the effective extension of this methodology to Me2Zn is documented. Full rationale on the mechanism as well as its stereochemical outcome is provided by a synergistic experimental/computational approach.},
 authors = {['Yang Liu', 'Alessandro Cerveri', 'Assunta {De Nisi}', 'Magda Monari', 'Olalla {Nieto Faza}', 'Carlos Silva Lopez', 'Marco Bandini']},
 journal = {Organic Chemistry Frontiers},
 title = {Nickel catalyzed regio- and stereoselective arylation and methylation of allenamides via coupling reactions. An experimental and computational study11Electronic supplementary information (ESI) available. CCDC 1548725 and 1817608. For ESI and crystallographic data in CIF or other electronic format see DOI: 10.1039/c8qo00729b},
 year = {2018}
}

@Filtered Article{20d6dd9a-2d10-43a1-ad67-5b337fd9c438,
 abstract = {ABSTRACT
Objective. To explore pharmacy students’ perceptions of a novel web application tool (AcaWriter) implemented in a Master of Pharmacy curriculum to support reflective thinking in scientific research. Methods. A qualitative research design involving a 50-minute focus group (n=12) was used. The focus group session was audio-taped, transcribed verbatim, and analyzed thematically using the Braun and Clarke framework. Results. Analysis generated four themes related to AcaWriter’s utility in enhancing students’ research thinking and capacity. The themes identified included: ease of use to prompt reflection, tangible tool with non-judgmental capacity; benefits for enhancing self and peer reflection on research techniques and group dynamics; benefits of the reflective writing process to enhance research capacity compared with engaging in reflective dialogue; and benefits beyond the writing process: cultivating self-improvement and self-confidence. Conclusion. The findings of this study show that a novel web application implemented within a pharmacy curriculum can assist students’ self and peer reflection on a research task. Further research is needed to explore the impact of using this tool and its relationship with academic performance and outcomes.},
 authors = {['Cherie Lucas', 'Simon Buckingham Shum', 'Ming Liu', 'Mary Bebawy']},
 journal = {American Journal of Pharmaceutical Education},
 keywords = {['reflection', 'formative feedback', 'pharmacy education', 'pharmaceutical research']},
 title = {Implementing a Novel Software Program to Support Pharmacy Students’ Reflective Practice in Scientific Research},
 year = {2021}
}

@Filtered Article{20e23ed2-66ec-4b53-8f25-c3d69860a787,
 abstract = {Dog experts, ornithologists, radiologists and other specialists are noted for their remarkable abilities at categorizing, identifying and recognizing objects within their domain of expertise. A complete understanding of the development of perceptual expertise requires a combination of thorough empirical research and carefully articulated computational theories that formalize specific hypotheses about the acquisition of expertise. A comprehensive computational theory of the development of perceptual expertise remains elusive, but we can look to existing computational models from the object-recognition, perceptual-categorization, automaticity and related literatures for possible starting points. Arguably, hypotheses about the development of perceptual expertise should first be explored within the context of existing computational models of visual object understanding before considering the creation of highly modularized adaptations for particular domains of perceptual expertise.},
 authors = {['Thomas J. Palmeri', 'Alan C-N. Wong', 'Isabel Gauthier']},
 journal = {Trends in Cognitive Sciences},
 title = {Computational approaches to the development of perceptual expertise},
 year = {2004}
}

@Filtered Article{211af329-f18b-452e-bd82-46f4131632db,
 abstract = {The aim of this study is to explore factors causing failures in innovative thought processes in engineering design. An innovation process is here understood as a complex and multi-phased thinking and problem solving process generating new and mostly unforeseeable solutions. The phases are partly overlapping and simultaneous. This complicated nature of innovation process demands a lot from innovation management, and thus it is not unusual that innovation processes fail. Identifying problems and shortcomings is important because it helps organizations to eliminate them in the future. This study focus on thought processes of individual participants in an innovation process, which is referred by us as microinnovation approach. This approach understands innovations as being based on human thinking.This study shows that factors related to knowledge, management and interaction are causing failures in engineering design. We found haste to be the most common reason for failures. Other contributing factors were lack of long-term thinking and inability to understand others’ perspective.},
 authors = {['Minna Pärttö', 'Pertti Saariluoma']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Microinnovation processes', 'engineering design', 'thought errors', 'thought failures']},
 title = {Explaining failures in innovative thought processes in engineering design},
 year = {2012}
}

@Filtered Article{2193a1bc-90d4-42ba-9562-e51e46c6a6df,
 abstract = {In recent years, the continuous development of computer science and AI technology makes the application prospect of artificial intelligence in fault diagnosis emerge. As a simulation technology of human thinking pattern, intelligent diagnosis technology can check and manage the monitoring target in real time to ensure the accuracy of data information. This paper introduces the basic principles of key artificial intelligence technologies in the field of sports, such as convolutional neural network, object detection, object tracking and action recognition. Then it analyzes the application status of intelligent diagnosis technology and artificial intelligence network under intelligent diagnosis, and puts forward the application of artificial intelligence neural network in automobile fault diagnosis based on examples. In the construction of the neural network system, the real-time collection of vehicle operation data can be analyzed, once the fault is found, the driver can be notified in time to avoid safety accidents. The author summarizes the existing research results on the application of artificial intelligence algorithm in intelligent diagnosis, in order to provide help for the subsequent research.},
 authors = {['Yukun Li']},
 journal = {Procedia Computer Science},
 keywords = {['Intelligent diagnosis', 'artificial intelligence network', 'automobile fault diagnosis', 'the neural network']},
 title = {Application Analysis of Artificial Intelligent Neural Network Based on Intelligent Diagnosis},
 year = {2022}
}

@Filtered Article{22600c45-bb5e-4d9e-a51d-e70974f017af,
 abstract = {A cell interacts with its environment through adhesion complexes. These are protein complexes that form through noncovalent interactions between adhesion receptors in the cell membrane and similar receptors in neighboring cells or ligand molecules in the surrounding extracellular matrix. Cell adhesions are crucial to maintain tissue integrity and cellular communication. Communication and sensing occur through the transmittal of forces through adhesions. This relevant role motivated researchers to develop theoretical models of adhesion. Initial models were based on studies of association kinetics of proteins, which later were expanded to explicitly include the role of force in determining bond strength. The introduction of techniques that allowed measurements of force in the range of a single adhesion produced models that describe the inner workings of the adhesion molecules themselves. Despite the relative simplicity of these models, they are still relevant. Not only were these studies novel and creative, they have been integrated into models describing larger cellular aggregates, unraveling the role of mechanics in biology. These models have been used in the study of cell migration, developmental biology, and cancer biology.},
 authors = {['Diego A. Vargas', 'Hans {Van Oosterwyck}']},
 journal = {Elsevier},
 keywords = {['Adherens junction', 'Adhesion dynamics', 'Bond lifetime', 'Cell adhesion', 'Cellularized material', 'Focal adhesion', 'Force spectroscopy', 'Mathematical modeling', 'Mechanotransduction', 'Multiscale modeling', 'Rate constant', 'Vertex model']},
 title = {Cell Adhesion: Basic Principles and Computational Modeling},
 year = {2019}
}

@Filtered Article{22a089be-55d9-4f1d-aa92-aa96c5a2f7c3,
 abstract = {The primate brain has evolved specialized visual capacities to navigate complex physical and social environments. Researchers studying cortical circuits underlying these capacities have traditionally favored the use of simplified tasks and brief stimulus presentations in order to isolate cognitive variables with tight experimental control. As a result, operational theories about visual brain function have come to emphasize feature detection, hierarchical stimulus encoding, top-down task modulation, and functional segregation in distinct cortical areas. Recently, however, experimental paradigms combining natural behavior with electrophysiological recordings have begun to offer a distinctly different portrait of how the brain takes in and analyzes its visual surroundings. The present article reviews recent work in this area, highlighting some of the more surprising findings in domains of social vision and spatial navigation along with shifts in thinking that have begun to emanate from this approach.},
 authors = {['David A. Leopold']},
 journal = {Current Opinion in Neurobiology},
 title = {The big mixup: Neural representation during natural modes of primate visual behavior},
 year = {2024}
}

@Filtered Article{22a4d2a4-0ad8-4209-836c-7c3b037436dd,
 abstract = {This paper explores the issues of feasibility and performance within predictive control. Conventional thinking is that there is typically a trade off between performance and the volume of the feasible region. However, this paper seeks to show that the trade off is often not as stark as might be expected and in fact one can sometimes gain huge amounts in feasibility with an almost negligible loss in performance while using a simple and conventional MPC algorithm.},
 authors = {['J.A. Rossiter', 'Yihang Ding']},
 journal = {IFAC Proceedings Volumes},
 keywords = {['Constraints', 'Feasibility', 'Performance', 'Computational Efficiency', 'Contours']},
 title = {Compromises between feasibility and performance within linear MPC},
 year = {2008}
}

@Filtered Article{22a5ae7e-e821-46d3-a158-7d4949a3bc53,
 abstract = {3D game development can be an enticing way to attract K-12 students to computer science, but designing and programming 3D games is far from trivial. Students need to achieve a certain level of 3D fluency in modeling, animation, and programming to be able to create compelling 3D content. The combination of innovative end-user development tools and standards-based curriculum that promotes IT fluency by shifting the pedagogical focus from programming to design, can address motivational aspects without sacrificing principled educational goals. The AgentCubes 3D game-authoring environment raises the ceiling of end-user development without raising the threshold. Our formal user study shows that with Incremental 3D, the gradual approach to transition from 2D to 3D authoring, middle school students can build sophisticated 3D games including 3D models, animations, and programming.},
 authors = {['Andri Ioannidou', 'Alexander Repenning', 'David C. Webb']},
 journal = {Journal of Visual Languages & Computing},
 keywords = {['Incremental 3D', 'Game design', 'Visual programming', 'End-user development', 'IT fluency', 'Computational thinking']},
 title = {AgentCubes: Incremental 3D end-user development},
 year = {2009}
}

@Filtered Article{22f12a8d-756a-46af-bd62-7ffe4bd75618,
 abstract = {Cognitive neuroscientists sometimes apply formal models to investigate how the brain implements cognitive processes. These models describe behavioral data in terms of underlying, latent variables linked to hypothesized cognitive processes. A goal of model-based cognitive neuroscience is to link these variables to brain measurements, which can advance progress in both cognitive and neuroscientific research. However, the details and the philosophical approach for this linking problem can vary greatly. We propose a continuum of approaches that differ in the degree of tight, quantitative, and explicit hypothesizing. We describe this continuum using four points along it, which we dub qualitative structural, qualitative predictive, quantitative predictive, and single model linking approaches. We further illustrate by providing examples from three research fields (decision making, reinforcement learning, and symbolic reasoning) for the different linking approaches.},
 authors = {['Gilles {de Hollander}', 'Birte U. Forstmann', 'Scott D. Brown']},
 journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
 keywords = {['Cognition', 'Computational models', 'Functional neuroimaging', 'Joint modeling', 'Linking', 'Mathematical models']},
 title = {Different Ways of Linking Behavioral and Neural Data via Computational Cognitive Models},
 year = {2016}
}

@Filtered Article{22f6453b-e155-4473-a6df-9eb8ee00d629,
 abstract = {Mathematical modeling plays a crucial role in understanding and managing urban water systems (UWS), with mechanistic models often serving as the foundation for their design and operations. Despite the wide adoptions, mechanistic models are challenged by the complexity of dynamic processes and high computational demands. Data-driven models bring opportunities to capture system complexities and reduce computational cost, by leveraging the abundant data made available by recent advance in sensor technologies. However, the interpretability and data availability hinder their wider adoption. This paper advocates for a paradigm shift in the application of data-driven models within the context of UWS. Integrating existing mechanistic knowledge into data-driven modeling offers a unique solution that reduces data requirements and enhances model interpretability. The knowledge-informed approach balances model complexity with dataset size, enabling more efficient and interpretable modeling in UWS. Furthermore, the integration of mechanistic and data-driven models offers a more accurate representation of UWS dynamics, addressing lingering uncertainties and advancing modelling capabilities. This paper presents perspectives and conceptual framework on developing and implementing knowledge-informed data-driven modeling, highlighting their potential to improve UWS management in the digital era.},
 authors = {['Haoran Duan', 'Jiuling Li', 'Zhiguo Yuan']},
 journal = {Water Research X},
 keywords = {['Modelling', 'Data-driven', 'Machine learning', 'Hybrid model', 'Urban water systems']},
 title = {Making waves: Knowledge and data fusion in urban water modelling},
 year = {2024}
}

@Filtered Article{232ff1da-3dbf-4e10-8b35-94401b7f58bc,
 abstract = {Summary
Despite the widespread use of personalization of eHealth technologies, there is a lack of comprehensive understanding regarding its application. This systematic review aims to bridge this gap by identifying and clustering different personalization approaches based on the type of variables used for user segmentation and the adaptations to the eHealth technology and examining the role of computational methods in the literature. From the 412 included reports, we identified 13 clusters of personalization approaches, such as behavior + channeling and environment + recommendations. Within these clusters, 10 computational methods were utilized to match segments with technology adaptations, such as classification-based methods and reinforcement learning. Several gaps were identified in the literature, such as the limited exploration of technology-related variables, the limited focus on user interaction reminders, and a frequent reliance on a single type of variable for personalization. Future research should explore leveraging technology-specific features to attain individualistic segmentation approaches.},
 authors = {['Iris ten Klooster', 'Hanneke Kip', 'Lisette {van Gemert-Pijnen}', 'Rik Crutzen', 'Saskia Kelders']},
 journal = {iScience},
 keywords = {['Health sciences', 'Health technology']},
 title = {A systematic review on eHealth technology personalization approaches},
 year = {2024}
}

@Filtered Article{2350bd6f-f773-4c84-ab47-761c51ec459d,
 abstract = {Fourth graders who had been in a standards-based elementary curriculum since kindergarten were individually interviewed and administered a whole-class test that probed their knowledge of facts and multidigit computation. Standard algorithms are not taught as part of the curriculum, which instead emphasizes student-invented procedures and discussions of solution methods. Of interest were the types of student-invented procedures that were used as well as their computational accuracy. Students used several procedures that involved sophisticated mental calculation strategies, such as decomposing numbers or adding from left to right. Many students also used the standard written algorithms. Both invented and standard algorithms used by the students were highly accurate, although invented procedures often indicated better mental flexibility and awareness of place value. On the written test, students' computational abilities were above national normative levels.},
 authors = {['William M. Carroll']},
 journal = {The Journal of Mathematical Behavior},
 title = {Invented Computational Procedures of Students in a Standards-Based Curriculum},
 year = {1999}
}

@Filtered Article{23772947-60f5-4ee9-b646-3aebc61c7d56,
 abstract = {This project has developed a geometry learning software that integrates multiple computer technologies to address the challenges of deep analysis of knowledge points and establishing connections in learning software. The software combines Long Short-Term Memory (LSTM) and Residual Neural Network (ResNet101) to encode text and image features. A self-attention mechanism is used to fuse information from both modalities, enabling decoding of geometric models and classification of corresponding knowledge points.This project uses LSTM and ResNet101 models to extract text and visual features for problem-solving using the Multi Mode Thinking Chain (CoT) method. Classification labels are utilized to generate text responses for problem-solving ideas. Furthermore, a recommendation module is proposed, which combines knowledge tracking and neural collaborative filtering algorithms to capture student behavior and knowledge point vectors. Implicit factors representing students' mastery of different knowledge points are used as inputs in neural collaborative filtering for personalized recommendations. The results demonstrate improvements in accuracy using the ResNet + LSTM multimodal algorithm, achieving a 13 % increase compared to single-modal classification. The multimodal CoT approach also outperforms language models like GPT3.5 and VisualBert by 10 %. Additionally, the combined algorithm of knowledge tracking and neural collaborative filtering shows a 13.3 % higher F1 value compared to ordinary algorithms, confirming the superiority of the adopted method in this project.},
 authors = {['Xiyin Zeng', 'Shouqiang Liu']},
 journal = {Expert Systems with Applications},
 keywords = {['Personalized learing', 'Pedagogy', 'Interactive learning environments', 'Applications']},
 title = {Research on the application of knowledge mapping and knowledge structure construction based on adaptive learning model},
 year = {2024}
}

@Filtered Article{237ea556-3c3f-492e-8c82-bebd5bb429d4,
 abstract = {Evolutionary Algorithms (EAs) are often challenging to apply in real-world settings since evolutionary computations involve a large number of evaluations of a typically expensive fitness function. For example, an evaluation could involve training a new machine learning model. An approximation (also known as meta-model or a surrogate) of the true function can be used in such applications to alleviate the computation cost. In this paper, we propose a two-stage surrogate-assisted evolutionary approach to address the computational issues arising from using Genetic Algorithm (GA) for feature selection in a wrapper setting for large datasets. We define “Approximation Usefulness” to capture the necessary conditions to ensure correctness of the EA computations when an approximation is used. Based on this definition, we propose a procedure to construct a lightweight qualitative meta-model by the active selection of data instances. We then use a meta-model to carry out the feature selection task. We apply this procedure to the GA-based algorithm CHC (Cross generational elitist selection, Heterogeneous recombination and Cataclysmic mutation) to create a Qualitative approXimations variant, CHCQX. We show that CHCQX converges faster to feature subset solutions of significantly higher accuracy (as compared to CHC), particularly for large datasets with over 100K instances. We also demonstrate the applicability of the thinking behind our approach more broadly to Swarm Intelligence (SI), another branch of the Evolutionary Computation (EC) paradigm with results of PSOQX, a qualitative approximation adaptation of the Particle Swarm Optimization (PSO) method. A GitHub repository with the complete implementation is available.22https://github.com/Ghaith81/Fast-Genetic-Algorithm-For-Feature-Selection.},
 authors = {['Mohammed Ghaith Altarabichi', 'Sławomir Nowaczyk', 'Sepideh Pashami', 'Peyman Sheikholharam Mashhadi']},
 journal = {Expert Systems with Applications},
 keywords = {['Feature selection', 'Evolutionary computation', 'Genetic Algorithm', 'Particle Swarm Intelligence', 'Fitness approximation', 'Meta-model', 'Optimization']},
 title = {Fast Genetic Algorithm for feature selection — A qualitative approximation approach},
 year = {2023}
}

@Filtered Article{239c821c-d8b8-4543-800e-a309eeab3dfb,
 abstract = {There is an intense debate whether risk-taking behavior is partially driven by cognitive abilities. The critical issue is whether choices arising from subjects with lower cognitive abilities are more likely driven by errors or lack of understanding than pure preferences for risk. The latter implies that the often-argued link between risk preferences and cognitive abilities (a common finding is that abilities relate negatively to risk aversion and positively to loss aversion) might be a spurious correlation. This experiment reports evidence from a sample of 556 participants who made choices in two risk-related tasks and completed three cognitive tasks, all with real monetary incentives: number-additions (including incentive-compatible expected number of correct additions), the Cognitive Reflection Test (to measure analytical/reflective thinking) and the Remote Associates Test (for convergent thinking). Results are unambiguous: none of our cognition measures plays any systematic role on risky decision making. Using structural equation modeling and factor analysis, we show that cognitive abilities are negatively associated with noisy, inconsistent choices and this effect may make higher ability individuals appear to be less risk averse and more loss averse. Yet we show that errors are more likely to appear when the two payoffs in a given decision exhibit similar probability. Therefore, our results suggest that failing to account for noisy decision making might have led to erroneously inferring a correlation between cognitive abilities and risk preferences in previous studies.},
 authors = {['Luis Amador-Hidalgo', 'Pablo Brañas-Garza', 'Antonio M. Espín', 'Teresa García-Muñoz', 'Ana Hernández-Román']},
 journal = {European Economic Review},
 keywords = {['Decision making under uncertainty', 'Cognitive abilities', 'Online experiment', 'Risk and loss aversion', 'Factor analysis']},
 title = {Cognitive abilities and risk-taking: Errors, not preferences},
 year = {2021}
}

@Filtered Article{23bf831e-91d5-41f5-9d23-960ebe5ad175,
 abstract = {The classical result by Dyer–Scott about fixed subgroups of finite order automorphisms of Fn being free factors of Fn is no longer true in Zm×Fn. Within this more general context, we prove a relaxed version in the spirit of Bestvina–Handel Theorem: the rank of fixed subgroups of finite order automorphisms is uniformly bounded in terms of m,n. We also study periodic points of endomorphisms of Zm×Fn, and give an algorithm to compute auto-fixed closures of finitely generated subgroups of Zm×Fn. On the way, we prove the analog of Day's Theorem for real elements in Zm×Fn, contributing a modest step into the project of doing so for any right angled Artin group (as McCool did with respect to Whitehead's Theorem in the free context).},
 authors = {['Mallika Roy', 'Enric Ventura']},
 journal = {Journal of Pure and Applied Algebra},
 keywords = {['Free-abelian times free', 'Automorphism', 'Fixed subgroup', 'Periodic subgroup', 'Auto-fixed closure']},
 title = {Fixed subgroups and computation of auto-fixed closures in free-abelian times free groups},
 year = {2020}
}

@Filtered Article{23e198c3-54ca-439c-809e-012750f49f94,
 authors = {['Erik D. Reichle']},
 journal = {Cognitive Systems Research},
 title = {Computational models of eye-movement control during reading: Theories of the “eye–mind” link},
 year = {2006}
}

@Filtered Article{24365cef-ddc0-4ac0-be85-bc0d3272db47,
 abstract = {This paper presents MR-LEAP (Mixed-Reality Learning Environment for Aspirational Programmers), a framework developed for learning programming through Mixed Reality and gamification mechanics. MR-LEAP’s architecture is designed to facilitate the understanding of basic programming concepts while allowing the gradual incorporation of more complex concepts. The framework provides a simple visual level editor. MR-LEAP is supported by the Mixed Reality Toolkit framework to promote portability to new Mixed Reality devices. Our goal is to facilitate programming education using Mixed Reality technology. MR-LEAP has already been used in both research and educational.},
 authors = {['Santiago Schez-Sobrino', 'Francisco M. García', 'Javier A. Albusac', 'Carlos Glez-Morcillo', 'Jose J. Castro-Schez', 'David Vallejo']},
 journal = {Software Impacts},
 keywords = {['Programming learning', 'Mixed reality', 'Gamification', 'Computational thinking', 'Problem solving']},
 title = {MR-LEAP: Mixed-Reality Learning Environment for Aspirational Programmers},
 year = {2024}
}

@Filtered Article{2499d132-8a41-42bf-8f54-dbe8d60aff37,
 abstract = {I don't believe that human-level intelligence is a well defined goal. As the cognitive-science community learns more about thinking and computation, the mileposts will keep changing in ways that we can't predict, as will the esteem we assign to past accomplishments. It would be fun to have a computer that could solve brain teasers as well as the average scientist, but focusing on such things, besides being parochial, overlooks the crucial role language plays in everything humans do, a role we understand hardly at all on a computational level. I am optimistic that we will eventually figure language out, but not without new ideas. Plus, when we can talk to machines, will we understand each other?},
 authors = {['Drew McDermott']},
 journal = {Artificial Intelligence},
 keywords = {['Speculation', 'Methodology', 'Natural language']},
 title = {Level-headed},
 year = {2007}
}

@Filtered Article{24e2592b-211a-448e-9f11-6b7e9347f6e2,
 abstract = {Ecosystem management in the face of uncertain disturbances has triggered increasing practices of resilience thinking. A multiscale probabilistic simulation-optimization framework is developed based on the nested nature of watersheds to inform decision robustness for Best Management Practices (BMPs). We presented a novel approach using hybrid Bayesian Networks (BNs) as interpretable and probabilistic emulators of process-based models. The hybrid BNs established at the scale of Hydrologic Response Units (HRUs) are embedded into simulation-optimization, whereby we analyze the cost-effectiveness-robustness of candidate BMP strategies at the subbasin scale. The optimal strategy is identified in compliance with water quality standards using watershed-scale integer programming. We apply the approaches in a typical intensively cultivated plateau watershed adjacent to Lake Dianchi, one of the three most eutrophic lakes in China. Our findings suggest that the hybrid BNs, incorporating both quantitative and qualitative information, are reliable emulators of the Soil and Water Assessment Tool (SWAT) in capturing critical pathways of diffuse phosphorus. Tradeoffs among cost, effectiveness, and robustness follow the law of diminishing marginal benefits. The optimum BMP strategies vary with policymakers’ preference toward robustness levels. Our findings indicate that robustness should be accounted for as an additional decision attribute besides costs and pollution mitigation. The benefits of the modeling framework are to (i) reduce over 99% computation complexity and support efficient decision-making under multifaceted uncertainties; (ii) improve interpretability and reliability of machine learning emulators; and (iii) inform policymakers of robustness with the probability of water quality restoration success.},
 authors = {['Feifei Dong', 'Jincheng Li', 'Chao Dai', 'Jie Niu', 'Yan Chen', 'Jiacong Huang', 'Yong Liu']},
 journal = {Journal of Cleaner Production},
 keywords = {['Diffuse nutrient', 'BMPs', 'Machine learning', 'Uncertainty', 'Simulation-optimization', 'Bayesian network']},
 title = {Understanding robustness in multiscale nutrient abatement: Probabilistic simulation-optimization using Bayesian network emulators},
 year = {2022}
}

@Filtered Article{24fe0cd6-a722-4593-9cb2-2e3d53e914ca,
 journal = {Chemometrics and Intelligent Laboratory Systems},
 title = {Computational Statistics and Data Analysis},
 year = {2004}
}

@Filtered Article{25570d6b-9099-4188-b295-f64e670361e2,
 abstract = {There has been an ongoing debate in research regarding the use of heuristics in decision-making. Advocators have succeeded in showing that applying heuristics not only reduces effort but can even be more accurate than analytical approaches under certain conditions. Others point out the biases and cognitive distortions inherent in disregarding information. Researchers have used both simulations and experiments to study how the use of heuristics affects the decision's outcome. However, a good decision is determined by the process and not a lucky outcome. It is a conscious reflection on the decision-maker's information and preferences. Therefore, a heuristic must be assessed by its ability to match a structured decision processing all available information. Thus, the question remains: how often does the reduction of information considered in heuristic decisions lead to a different recommended alternative? We applied different heuristics to a dataset of 945 real, personal decisions. We have found that by using heuristics instead of a fully developed decision structure, in 60.34% of cases, a different alternative would have been recommended to the decision-maker leading to a mean relative utility loss for the deviating decisions of 34.58%. This shows that a continuous effort to reflect on the weighing of objectives and alternatives leads to better decisions.},
 authors = {['Florian Methling', 'Sara J.M. Abdeen', 'Rüdiger {von Nitzsch}']},
 journal = {EURO Journal on Decision Processes},
 keywords = {['MCDM', 'Decision support', 'Heuristics', 'Utility theory', 'Value-focused thinking']},
 title = {Heuristics in multi-criteria decision-making: The cost of fast and frugal decisions},
 year = {2022}
}

@Filtered Article{255788d3-3f3d-4392-b36a-1cf95a4c893d,
 abstract = {In this paper, experimental, computational intelligence based and statistical investigations of warp tensions in different back-rest oscillations are presented. Firstly, in the experimental stage, springs having different stiffnesses are used to obtain different back-rest oscillations. For each spring, fabrics are woven in different weft densities and the warp tensions are measured and saved during weaving process. Secondly, in the statistical investigation, the experimental data are analyzed by using linear multiple and quadratic multiple-regression models. Later, in the computational intelligence based investigation, the data obtained from the experimental study are analyzed by using artificial neural networks that are universal approximators which provide a massively parallel processing and decentralized computing. Specially, radial basis function neural network structure is chosen. In this structure, cross-validation technique is used in order to determine the number of radial basis functions. Finally, the results of regression analysis, the computational intelligence based analysis and experimental measurements are compared by using the coefficient of determination. From the results, it is shown that the computational intelligence based analysis indicates a better agreement with the experimental measurement than the statistical analysis.},
 authors = {['Yıldıray Turhan', 'Sezai Tokat', 'Recep Eren']},
 journal = {Information Sciences},
 keywords = {['Neural networks', 'Radial basis function', 'Cross-validation', 'Data regression', 'Warp tension', 'Back-rest oscillation', 'Weft density']},
 title = {Statistical and computational intelligence tools for the analyses of warp tension in different back-rest oscillations},
 year = {2007}
}

@Filtered Article{2568bf19-454d-4eac-bc22-37df70823dcd,
 abstract = {The effect of background music (BGM) on cognitive task performance is a popular topic. However, the evidence is not converging: experimental studies show mixed results depending on the task, the type of music used and individual characteristics. Here, we explored how people use BGM while optimally performing various cognitive tasks in everyday life, such as reading, writing, memorizing, and critical thinking. Specifically, the frequency of BGM usage, preferred music types, beliefs about the scientific evidence on BGM, and individual characteristics, such as age, extraversion and musical background were investigated. Although the results confirmed highly diverse strategies among individuals regarding when, how often, why and what type of BGM is used, we found several general tendencies: people tend to use less BGM when engaged in more difficult tasks, they become less critical about the type of BGM when engaged in easier tasks, and there is a negative correlation between the frequency of BGM and age, indicating that younger generations tend to use more BGM than older adults. The current and previous evidence are discussed in light of existing theories. Altogether, this study identifies essential variables to consider in future research and further forwards a theory-driven perspective in the field.},
 authors = {['Franziska Goltz', 'Makiko Sadakata']},
 journal = {Acta Psychologica},
 keywords = {['Background music', 'Cognitive performance', 'Music perception']},
 title = {Do you listen to music while studying? A portrait of how people use music to optimize their cognitive performance},
 year = {2021}
}

@Filtered Article{25715ea9-0e5f-4fcf-8bc6-41109dfd7a4b,
 abstract = {Neurofeedback training is a form of brain training in which information about a neural measure is fed back to the trainee who is instructed to increase or decrease the value of that particular measure. This paper focuses on electroencephalography (EEG) neurofeedback in which the neural measures of interest are the brain oscillations. To date, the neural mechanisms that underlie successful neurofeedback training are still unexplained. Such an understanding would benefit researchers, funding agencies, clinicians, regulatory bodies, and insurance firms. Based on recent empirical work, an emerging theory couched firmly within computational neuroscience is proposed that advocates a critical role of the striatum in modulating EEG frequencies. The theory is implemented as a computer simulation of peak alpha upregulation, but in principle any frequency band at one or more electrode sites could be addressed. The simulation successfully learns to increase its peak alpha frequency and demonstrates the influence of threshold setting – the threshold that determines whether positive or negative feedback is provided. Analyses of the model suggest that neurofeedback can be likened to a search process that uses importance sampling to estimate the posterior probability distribution over striatal representational space, with each representation being associated with a distribution of values of the target EEG band. The model provides an important proof of concept to address pertinent methodological questions about how to understand and improve EEG neurofeedback success.},
 authors = {['Eddy J. Davelaar']},
 journal = {Neuroscience},
 keywords = {['neurofeedback', 'electroencephalography', 'computational neuroscience', 'computer model']},
 title = {Mechanisms of Neurofeedback: A Computation-theoretic Approach},
 year = {2018}
}

@Filtered Article{258aa555-70f0-416e-a92b-b96196d0cf96,
 abstract = {A claim is made that analysis will remain important and become a useful ally in helping computational materials science live up to its ultimate potential. Examples are given in the mechanical properties area where numerical simulations have been able to parameterize and mark out areas of validity for elasticity theory. The important role of developing asymptotic paths from one level or category of theory to another is commented on.},
 authors = {['Robb Thomson']},
 journal = {Computational Materials Science},
 title = {Whither computational materials science? Some thoughts from the mechanical properties front},
 year = {1994}
}

@Filtered Article{259e81ed-4229-411a-aadd-5a7425cb8ef1,
 abstract = {Publisher Summary
Block structured meshes not only provide the possibility to compute flows in complex geometries but also lend themselves in a natural way to coarse-grain parallel processing via the distribution of different blocks to different processors. Nevertheless, for some flow computations, a fine-grain data parallel implementation may be more appropriate. This chapter presents a study of such an implementation, which utilizes the simplicity of the data parallel approach. Particular attention is placed on a dynamic block management strategy that allows computations to be undertaken only in blocks where useful work is to be performed. The question of code portability among four different parallel computer systems is addressed in the chapter. This chapter concludes that the serial data-parallel multi-block method provides a number of advantages: (1) it retains the simplicity of the above-mentioned data parallel methods, because each block is treated individually in the same manner as for a single block computation; (2) it does not impose any parallelization constraints on the mesh generation procedure, in principle, any number of blocks of unequal size can be employed; the transfer of data between two blocks (block connectivity) is performed in a transparent manner via globally addressable memory contrasting with the explicit data transfer required by message passing implementations; (3) because individual blocks are treated sequentially, a simple dynamic block management algorithm can be applied to avoid performing unnecessary operations; and (4) the use of standard Fortran 90 facilitates code portability among different platforms supporting the data parallel programming method.},
 authors = {['M.L. Sawley', 'J.K. Tegnér', 'C.M. Bergman']},
 journal = {North-Holland},
 title = { - A serial data-parallel multi-block method for compressible flow computations},
 year = {1995}
}

@Filtered Article{25b1f3f1-7e05-4dd4-be16-1e798b3626f2,
 abstract = {Research in material behavior involves the study of relationships between material composition and capacities to negotiate internal and external pressures. Tuning material composition for performance allows for the integration of multifaceted functionality and embedded responsiveness within minimal material means. The relationships of material composition and system performance can be dissected into properties of topology (in count, type and association), forces (as the simulation of contextual pressures), and materiality (material properties and constraints of fabrication). When resourcing information about these aspects of material behavior from biological or technological systems, the physical precedents, as specimens and/or models, serve as the primary, and often sole, exemplar. While this is necessary to initiate the study of material make-up as it relates to specific morphological performance, there is an inherent limit when asking how and to what degree the knowledge resourced from that instance applies when alterations from the norm are generated. This research proposes the possibility for testing variants of a morphological system using physical models as the precedent while incorporating multiple means of computational analysis for extensive exploration. The framework begins with the initial stage of deducing principles, regarding material organization and behavior, through comparative physical and computational study. Subsequently, through methods of abduction, new vocabularies of form and potentials in performance are generated primarily through computational exploration. The framework is shaped by research into the design and materialization of complex pre-stressed form- and bending-active architectures. A novel aspect of this framework is the development of a software environment called springFORM. In this environment, material behavior is simulated using basic spring-based (particle system) methods. The novel contribution of this software is in providing means for both manual and algorithmic manipulations of mesh topologies and material properties during the form-finding process. A series of architectural prototypes, which range in scale, define rules for the relationship between topological-material complexity and the sequencing of particular exploratory methods. The studies define the value of the physical precedent as it engenders further material prototypes, spring-based explorations and simulations with finite element analysis. These rules and methods are further elaborated upon through studying the particularly fascinating structural capacity of banana leaf stalks, a material system which is stiff in bending yet highly flexible in torsion. Of interest is a functional robustness which allows for the negotiation of both self-weight and wind loading for a large and fully integrated leaf structure. Methods of simulation and meta-heuristics are developed to address the continual material and topological differentiation of the banana leaf stalk. Case studies are based upon examination of specimens from the species Musa acuminata and Ensete ventricosum. Mechanical properties and geometric descriptions of isolated moments within the stalk provide the basis for computational comparison. Fundamental properties and behaviors are extracted from the plant specimens, yet a full description is not possible because of the plant’s intricate spatial structure. In this case, the computational means serve to elucidate upon the behavior of the complete system as well as provide avenues for exploring its variants. This paper describes an extensible and calibrated framework which can foster enhanced biomimetic insights by explorations which are based upon but extend well beyond initial biological and/or technological precedents.},
 authors = {['Sean Ahlquist', 'Tim Kampowski', 'Omid {Oliyan Torghabehi}', 'Achim Menges', 'Thomas Speck']},
 journal = {Computer-Aided Design},
 keywords = {['Material behavior', 'Spring-based simulation', 'Computational design', 'Biomimetic research']},
 title = {Development of a digital framework for the computation of complex material and morphological behavior of biological and technological systems},
 year = {2015}
}

@Filtered Article{25cb6543-ec48-4442-a6c7-fa313f7b1f10,
 abstract = {This paper summarizes the state-of-the-art techniques used to simulate hurricane winds in atmospheric boundary-layer (ABL) for wind engineering testing. The wind tunnel simulation concept is presented along with its potential applications, advantages and challenges. ABL simulation at open-jet simulators is presented along with an application example followed by a discussion on the advantages and challenges of testing at these facilities. Some of the challenges and advantages of using computational fluid dynamics (CFD) are presented with an application example. The paper show that the way the wind can be simulated is complex and matching one parameter at full-scale may lead to a mismatch of other parameters. For instance, while large-scale testing is expected to improve Reynolds number and hence approach the full-scale scenario, it is challenging to generate large-scale turbulence in an artificially created wind. New testing protocols for low-rise structures and small-size architectural features are presented as an answer to challenging questions associated with both wind tunnel and open-jet testing. Results show that it is the testing protocol that can be adapted to enhance the prediction of full-scale physics in nature. Thinking out of the box and accepting non-traditional ABL is necessary to compensate for Reynolds effects and to allow for convenient experimentation. New research directions with focus on wind, rain and waves as well as other types of non-synoptic winds are needed, in addition to a more focus on the flow physics in the lower part of the ABL, where the major part of the infrastructure exists.},
 authors = {['Aly Mousaad Aly']},
 journal = {Building and Environment},
 keywords = {['Aerodynamics', 'Aeroelasticity', 'Atmospheric boundary-layer', 'Built environment', 'Experimental/computational wind engineering']},
 title = {Atmospheric boundary-layer simulation for the built environment: Past, present and future},
 year = {2014}
}

@Filtered Article{25fd46d7-e17e-40ea-901c-d07a82846768,
 abstract = {Background and objective
Music, the ubiquitous language across human cultures, is traditionally considered as a form of art but has been linked to biomolecules in recent years. However, previous efforts have only been addressed on sonification of nucleic acids and proteins to produce so-called life music, the soundscape from the basic building blocks of life. In this study, we attempted to, for the first time, conduct a reverse operation of this process, i.e. conversion of music to protein (CoMtP).
Methods
A novel notion termed musical protein (MP) –– the protein defined by music –– was proposed and, on this basis, we described a computational strategy to map the time sequence of music onto the spatial architecture of proteins, which considered that each note in the stave of a music (target) can be simply characterized by two acoustical quantities and that each residue in the primary sequence of a protein (hit) was represented by amino acid descriptors.
Results
A simulated annealing (SA) algorithm was applied to iteratively generate the best matched MP hit for a music target and structural bioinformatics was then used to model spatial advanced structure for the resulting MP. We also demonstrated that some small MPs derived from music segments may have potential biological functions, which, for example, can serve as antimicrobial peptides (AMPs) to inhibit clinical bacterial strains with moderate or high antibacterial potency.
Conclusions
This work may benefit many aspects; for example, it would open a door for the hearing-impaired persons to ‘listen’ music in a biological vision and could be a mean of exposing students to the concepts of biomolecules at an earlier age through the use of auditory characteristics. The CoMtP would also facilitate the rational design of proteins with biological and medicinal significance.},
 authors = {['Jun Su', 'Peng Zhou']},
 journal = {Computer Methods and Programs in Biomedicine},
 keywords = {['Musical protein', 'Life of music', 'Bioinformatics', 'Stave', 'Note', 'Piano', 'Conversion of music to protein']},
 title = {Musical protein: Mapping the time sequence of music onto the spatial architecture of proteins},
 year = {2024}
}

@Filtered Article{2609dc1b-1d04-4381-adb2-68f1d8544f67,
 abstract = {In this paper a computational model is presented for how a desire triggers responses and feelings. The model shows how these feelings can be biased, for example due to addicting experiences in the past. Both the strength of a response and of the associated feeling result from a converging dynamic pattern modeled by reciprocal causal interactions between the two. The model has been used to conduct a number of simulation experiments under varying circumstances. Moreover, it has been evaluated by formal analysis of emerging patterns entailed by the model. Furthermore, it has been pointed out how the computational model can be applied within an ambient agent system supporting a human in not being tempted. In a simple example scenario it is shown such an ambient agent system is able to predict and assess a human’s desire state, and use this assessment to suggest alternatives to avoid falling for certain temptations.},
 authors = {['Tibor Bosse', 'Mark Hoogendoorn', 'Zulfiqar A. Memon', 'Jan Treur', 'Muhammad Umair']},
 journal = {Cognitive Systems Research},
 keywords = {['Desire', 'Feeling', 'Computational model']},
 title = {A computational model for dynamics of desiring and feeling},
 year = {2012}
}

@Filtered Article{264f1f52-4587-4a20-960e-b4005aae14a0,
 abstract = { Abstract
This work reflects on the use of scenario- and problem-based learning as a way of conveying not only fundamental knowledge, but also to provide training in the use of computational Process Systems Engineering (PSE) tools applied to open-ended real world problems. The teaching framework also has a strong emphasis on the development of professional skills and to evaluate the recommended design solutions considering multiple perspectives such as economics, safety, environment and societal context. The framework is implemented through week-long group projects called Scenarios, taking place mainly in the first two years of study, and examples are given of different variations of Scenarios. This teaching approach has multiple benefits, including but not limited to, students’ understanding of PSE tools and the development of their critical engineering thinking.},
 authors = {['A. Tsatse', 'E. Sorensen']},
 journal = {Elsevier},
 keywords = {['problem-based learning', 'Scenarios', 'Process Systems Engineering']},
 title = {Reflections on the development of scenario and problem-based chemical engineering projects},
 year = {2021}
}

@Filtered Article{265f81f3-372b-4f63-91cc-db0d6d8c48cf,
 abstract = {This paper aims to explore mechanistic and teleological explanations of consciousness. In terms of mechanistic explanations, it critiques various existing views, especially those embodied by existing computational cognitive models. In this regard, the paper argues in favor of the explanation based on the distinction between localist (symbolic) representation and distributed representation (as formulated in the connectionist literature), which reduces the phenomenological difference to a mechanistic difference. Furthermore, to establish a teleological explanation of consciousness, the paper discusses the issue of the functional role of consciousness on the basis of the aforementioned mechanistic explanation. A proposal based on synergistic interaction between the conscious and the unconscious is advanced that encompasses various existing views concerning the functional role of consciousness. This two-step deepening explanation has some empirical support, in the form of a cognitive model and various cognitive data that it captures.},
 authors = {['Ron Sun']},
 journal = {Cognitive Systems Research},
 keywords = {['Consciousness', 'Cognition', 'Qualia', 'Implicit learning', 'Computation', 'Reduction', 'Teleology']},
 title = {Computation, reduction, and teleology of consciousness},
 year = {2001}
}

@Filtered Article{2681193b-e597-4d5f-b48f-922f14fb0fac,
 abstract = {The expectation-maximization maximum-likelihood (EM-ML) algorithm for image reconstruction in positron emission tomography (PET) essentially solves a large linear system of equations. In this paper, we study computational aspects of a recently developed preprocessing scheme for focusing the attention, and thus the computational resources, on a subset of the equations and unknowns in order to reduce the storage, computation, and communication requirements of the EM-ML algorithm. The approach is completely data-driven and uses no prior anatomic knowledge. The experimental results are obtained from runs on a small network of workstations using simulated phantom data as well as data obtained from a clinical ECAT 921 PET scanner.},
 authors = {['Jens Gregor', 'Dean A. Huff']},
 journal = {Parallel Computing},
 keywords = {['Distributed computing', 'Expectation-maximization', 'Image reconstruction', 'Positron emission tomography']},
 title = {A computational study of the focus-of-attention EM-ML algorithm for PET reconstruction1Research supported in part by the National Science Foundation under grant CDA-95-29459.1},
 year = {1998}
}

@Filtered Article{26823783-209b-41fd-b29c-c1224ba81dfe,
 abstract = {This study is aimed at investigating the contribution of the general intelligence factor if six PISA domains (reading, mathematical, scientific, financial literacies, global competence, and creative thinking) are combined in one measurement instrument. For achieving our goal, items based on the PISA frameworks are developed, students in grades 5–8 from three different Russian regions are assessed, and three IRT models (unidimensional, multidimensional, and bifactor) are applied to process the data. In addition, the correlations from the multidimensional model are estimated to examine the degree of cognitive specificity and mixture modeling is implemented to investigate ability differentiation across grades. Statistical analysis reveals that the bifactor model comprising one general and six specific factors, has a better fit in each grade. Based on this model, we compute the variance explained by the general factor, with the estimates varying between 60% and 70%. In general, the pure variance explained by specific factors does not exceed 10%. The correlations are above 0.40 in each grade and the averaged associations tend to increase from 6th to 8th grade, although they are smaller in years 6 and 7 compared to year 5. The general ability differentiation effect is observed in grades 6 to 8 and is not present in grade 5. Specific ability differentiation is more pronounced in reading literacy, especially in grade 5 to 7. The results obtained are discussed from the perspective of the ability and developmental differentiation/dedifferentiation problem.},
 authors = {['Nikita Kolachev', 'Galina Kovaleva']},
 journal = {Intelligence},
 keywords = {['Intelligence', 'G-factor', 'General cognitive ability', 'Bifactor model', 'PISA']},
 title = {General intelligence in middle school students from different Russian regions: Results of PISA-like tests},
 year = {2023}
}

@Filtered Article{26cd7e11-5bdf-4962-b165-686ba2a11c94,
 abstract = {RNA molecules are important cellular components involved in many fundamental biological processes. Understanding the mechanisms behind their functions requires RNA tertiary structure knowledge. Although modeling approaches for the study of RNA structures and dynamics lag behind efforts in protein folding, much progress has been achieved in the past two years. Here, we review recent advances in RNA folding algorithms, RNA tertiary motif discovery, applications of graph theory approaches to RNA structure and function, and in silico generation of RNA sequence pools for aptamer design. Advances within each area can be combined to impact many problems in RNA structure and function.},
 authors = {['Christian Laing', 'Tamar Schlick']},
 journal = {Current Opinion in Structural Biology},
 title = {Computational approaches to RNA structure prediction, analysis, and design},
 year = {2011}
}

@Filtered Article{2780f2eb-d2a3-4a2b-b742-f8a044daa165,
 abstract = {Problem:
Educational disparities in Mathematics performance are a persistent challenge. This study aims to unravel the complex factors contributing to these disparities among students internationally, with a focus on the interpretability of the contributing factors.
Methodology:
Utilizing data from the Programme for International Student Assessment (PISA), we conducted rigorous preprocessing and variable selection to prepare for applying binary classification interpretability models. These models were trained using the Stratified K-Fold technique to ensure balanced representation and assessed using six key metrics.
Solution:
By applying interpretability models such as Shapley Additive Explanations (SHAP) analysis, we identified critical factors impacting student performance, including reading accessibility, critical thinking skills, gender, and geographical location.
Results:
Our findings reveal significant disparities linked to resource availability, with students from lower socioeconomic backgrounds possessing fewer books and demonstrating lower performance in Mathematics. The geographical analysis highlighted regional educational disparities, with certain areas consistently underperforming in PISA assessments. Gender also emerged as a determinant, with females contributing differently to performance levels across the spectrum.
Conclusion:
The study provides insights into the multifaceted determinants of student Mathematics performance and suggests potential avenues for future research to explore global interpretability models and further investigate the socioeconomic, cultural, and educational factors at play.},
 authors = {['Ismael Gómez-Talal', 'Luis Bote-Curiel', 'José Luis Rojo-Álvarez']},
 journal = {Engineering Applications of Artificial Intelligence},
 keywords = {['Programme for International Student Assessment', 'Interpretable machine learning', 'Shapley additive explanations', 'Explainable black-box models']},
 title = {Understanding the disparities in Mathematics performance: An interpretability-based examination},
 year = {2024}
}

@Filtered Article{278859aa-7d2a-48b7-9b08-fe074c57acb1,
 abstract = {The image decomposition based method is one of the efficient and important face recognition solutions for the single sample per person problem. The low image decomposition performance and the unconvincing reconstruction of the approximation image are the two main limitations of the previous methods. In this paper, a new single sample face recognition method based on lower-upper (LU) decomposition algorithm is proposed. The procedure of the proposed method is as following. First, the single sample and its transpose are decomposed to two sets of basis images by using the LU decomposition algorithm, which is more efficient than the image decomposition algorithms of the previous works. Two approximation images are reconstructed from the two basis image sets by the reverse thinking approach based on experimental analysis. Then, the fisher linear discriminant analysis (FLDA) algorithm is used to evaluate the optimal projection space by using the new training set consisting of the single sample and its two approximation images for each person. Finally, the nearest neighbor classifier based on Euclidean distance is adopted as the final classification. We make two main contributions: one is that we propose to decompose the single sample and its transpose using the efficient LU decomposition algorithm, and reorder each basis image set according to the basis image energy; the other is that we present a reverse thinking approach based on experimental analysis to reconstruct the approximation image. The performance of the proposed method is verified using four public face databases, namely FERET, AR, ORL and Yale B. The experimental results indicate that the proposed method is efficient and outperforms several state-of-the-art approaches which are proposed to address the single sample per person problem.},
 authors = {['Changhui Hu', 'Mengjun Ye', 'Saiping Ji', 'Weili Zeng', 'Xiaobo Lu']},
 journal = {Neurocomputing},
 keywords = {['Face recognition', 'Single sample per person problem', 'Lower-upper decomposition algorithm', 'Reverse thinking approach based on experimental analysis']},
 title = {A new face recognition method based on image decomposition for single sample per person problem},
 year = {2015}
}

@Filtered Article{278b8a02-f40e-4368-9bd7-746b8b8b330a,
 abstract = {The crisis spurred by the pandemic of COVID-19 has revealed weaknesses in our epidemiologic methodologic corpus, which scientists are struggling to compensate. This article explores whether this phenomenon is characteristic of pandemics or not. Since the emergence of population-based sciences in the 17th century, we can observe close temporal correlations between the plague and the discovery of population thinking, cholera and population-based group comparisons, tuberculosis and the formalization of cohort studies, the 1918 Great Influenza and the creation of an academic epidemiologic counterpart to the public health service, the HIV/AIDS epidemic, and the formalization of causal inference concepts. The COVID-19 pandemic seems to have promoted the widespread understanding of population thinking both with respect to ways of flattening an epidemic curve and the societal bases of health inequities. If the latter proves true, it will support my hypothesis that pandemics did accelerate profound changes in epidemiologic methods and concepts.},
 authors = {['Alfredo Morabia']},
 journal = {Journal of Clinical Epidemiology},
 keywords = {['Epidemiology', 'History', 'Pandemics', 'Covid-19', 'Plague', 'Cholera', 'Tuberculosis', 'Influenza', 'HIV/AIDS']},
 title = {Pandemics and methodological developments in epidemiology history},
 year = {2020}
}

@Filtered Article{27b18674-15ea-4ed2-b368-99d2f3cba42e,
 abstract = {Background
Food retailers in community settings are gatekeepers to the crucial food systems changes needed to improve population nutrition. Evidence-based models of change are needed to enable shifts in these complex retail environments. Systems thinking offers unique insights by capturing potential unintended consequences and multiple pathways to success. This study sought to create a systems map for retailers, public health practitioners and other stakeholders seeking to implement healthy food retail policies. It aimed to identify (i) points of intervention through which community-based organisations can shift to healthier food provision, and (ii) key feedback loops that could drive potential unintended consequences of such policies in a complex system.
Methods
Semi-structured interviews (n = 26) were conducted, from 2015 to 2018, across four community food retail settings where healthy food retail policies had been implemented in Victoria, Australia. Interviews were coded by identifying causal relationships and their direction between factors. Vensim software was used to merge interview results and then reduce the map to the strongest and most frequent factors and relationships. Illustrative implementation stories and points of intervention were identified.
Findings
The resulting map is titled the Systems Thinking Approach for Retail Transformation (START) map. Five prominent implementation stories incorporating 17 factors highlighted that: 1) retailer resistance to change is strongest in the beginning but decreases with the demonstration of favourable initiative outcomes; 2) successive changes tend to be increasingly complex, and therefore harder for retailers to implement; 3) organisational resourcing can be influenced through multiple pathways; 4) customer acceptability of healthy changes and retailers' willingness to engage in changes influence each other; and 5) challenges in accessing healthy supply options make retailers more resistant to implementing healthy changes.
Conclusions
The application of systems thinking to the challenge of unhealthy food retail creates novel and practical insights for retailers and health promotion practitioners into what actions are most likely to promote healthy changes in complex retail environments.},
 authors = {['Tara Boelsen-Robinson', 'Miranda R. Blake', 'Andrew D. Brown', 'Oliver Huse', 'Claire Palermo', 'Neetu A. George', 'Anna Peeters']},
 journal = {Food Policy},
 keywords = {['Food retail', 'Systems mapping', 'Intervention', 'Community', 'Implementation', 'START map', 'Nutrition', 'Policy', 'Qualitative', 'Interviews']},
 title = {Mapping factors associated with a successful shift towards healthier food retail in community-based organisations: A systems approach},
 year = {2021}
}

@Filtered Article{27d92a29-664a-4b3f-9e91-6971c8ec4b73,
 abstract = {The central macro-evolutionary ratchet supports myriad further emergences as specializations, further developments, and various kinds of interactions among the differing processes. This chapter, in the section on interactive knowing, addresses, for example, situation knowledge, presupposition, higher order motivation and values, the important process characteristic of themes, central nervous system as a microgenetic dynamic system, thinking and concepts, the contact-content distinction, indexicality, attention, and will. Concerning learning, the function of topologies in heuristic learning is explored, multiple kinds of memory are addressed, such as “enactive”, “semantic”, episodic, and autobiographical, and some of the enabling and constraining developmental consequences of knowing levels are examined. In the section on emotions, basic principles of developmental differentiations of emotions are examined, and the evolution of emotion processes within organizations of microgenesis processes is framed. The discussion of reflective consciousness addresses enablings such as rehearsal and purpose, and extends the evolutionary analysis to the emergence of the possibility of reflection in the brain. Issues concerning experiencing, such as unities and boundaries, are addressed. Reflective experiencing of experiencing is presented as a resolution of a fundamental problem in the literature. Some further literature is addressed, such as higher order theory, Brentano, Searle, and folk psychology.},
 authors = {['Mark H. Bickhard']},
 journal = {Academic Press},
 keywords = {['interactive knowing', 'situation knowledge', 'presupposition', 'higher order motivation and values', 'themes', 'central nervous system as a microgenetic dynamic system', 'thinking and concepts', 'contact-content', 'indexicality', 'attention', 'will', 'learning', 'topologies in heuristic learning', '“enactive”', '“semantic”', 'episodic', 'and autobiographical memory', 'knowing levels', 'development', 'emotions', 'evolution of emotion processes', 'organizations of microgenesis processes', 'reflective consciousness', 'rehearsal', 'purpose', 'emergence of reflection in the brain', 'experiencing', 'experiencing of experiencing', 'higher order theory', 'Brentano', 'Searle', 'folk psychology']},
 title = {The mentality of Homo Sapiens},
 year = {2025}
}

@Filtered Article{27f688ec-9e07-4947-a9d3-df22f9111c1b,
 abstract = {Politics can seem home to the most calculating and yet least rational elements of humanity. How might we systematically characterize this spectrum of political cognition? Here, we propose reinforcement learning (RL) as a unified framework to dissect the political mind. RL describes how agents algorithmically navigate complex and uncertain domains like politics. Through this computational lens, we outline three routes to political differences, stemming from variability in agents’ conceptions of a problem, the cognitive operations applied to solve the problem, or the backdrop of information available from the environment. A computational vantage on maladies of the political mind offers enhanced precision in assessing their causes, consequences, and cures.},
 authors = {['Lion Schulz', 'Rahul Bhui']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['computational models', 'reinforcement learning', 'political psychology']},
 title = {Political reinforcement learners},
 year = {2024}
}

@Filtered Article{282c5b89-a60a-4fa3-af8b-4fcda44f3b82,
 abstract = {Solving and preventing environmental problems will continue to rely on systems thinking that translates and combines data, information, knowledge, and wisdom from numerous scientific and other perspectives. The chapter provides insights into possible directions for environmental systems science, especially ways to address complexities at every scale from cellular to planetary. Environmental scientists and engineers will engage in precision science and customized approaches to reduce risks, improve reliability and resilience, and ensure sustainability.},
 authors = {['Daniel A. Vallero']},
 journal = {Elsevier},
 keywords = {['Precautionary principle', 'Evidence-based risk assessment', 'Exposome', 'Translational science', 'Scientific workflow', 'Ontologies', 'Resilience', 'Data-driven decision-making', 'Precision science', 'Life cycle risk assessment (LCRA)']},
 title = {Chapter 14 - The Future},
 year = {2021}
}

@Filtered Article{289ab14a-dfff-496e-9255-436834da6f0f,
 abstract = {Objective:
Stimulation of axons within the dorsal columns of the human spinal cord has become a widely used therapy to treat refractory neuropathic pain. The mechanisms have yet to be fully elucidated and may even be contrary to standard “gate control theory.” Our hypothesis is that a computational model provides a plausible description of the mechanism by which dorsal column stimulation (DCS) inhibits wide dynamic range (WDR) cell output in a neuropathic model but not in a nociceptive pain model.
Materials and Methods:
We created a computational model of the human spinal cord involving approximately 360,000 individual neurons and dendritic processing of some 60 million synapses—the most elaborate dynamic computational model of the human spinal cord to date. Neuropathic and nociceptive “pain” signals were created by activating topographically isolated regions of excitatory interneurons and high-threshold nociceptive fiber inputs, driving analogous regions of WDR neurons. Dorsal column fiber activity was then added at clinically relevant levels (e.g., Aβ firing rate between 0 and 110 Hz by using a 210-μsec pulse width, 50–150 Hz frequency, at 1–3 V amplitude).
Results:
Analysis of the nociceptive pain, neuropathic pain, and modulated circuits shows that, in contradiction to gate control theory, 1) nociceptive and neuropathic pain signaling must be distinct, and 2) DCS neuromodulation predominantly affects the neuropathic signal only, inhibiting centrally sensitized pathological neuron groups and ultimately the WDR pain transmission cells.
Conclusion:
We offer a different set of necessary premises than gate control theory to explain neuropathic pain inhibition and the relative lack of nociceptive pain inhibition by using retrograde DCS. Hypotheses regarding not only the pain relief mechanisms of DCS were made but also regarding the circuitry of pain itself, both nociceptive and neuropathic. These hypotheses and further use of the model may lead to novel stimulation paradigms.},
 authors = {['Jeffrey E. Arle', 'Kristen W. Carlson', 'Longzhi Mei', 'Nicolae Iftimia', 'Jay L. Shils']},
 journal = {Neuromodulation: Technology at the Neural Interface},
 keywords = {['Chronic pain', 'dorsal column stimulation', 'gate control theory of pain', 'neural circuitry modeling', 'neuromodulation mechanism', 'neuropathic pain', 'spinal cord stimulation']},
 title = {Mechanism of Dorsal Column Stimulation to Treat Neuropathic but not Nociceptive Pain: Analysis With a Computational Model},
 year = {2014}
}

@Filtered Article{289cfb36-3215-4a1c-8d90-9400a0362360,
 abstract = {The processes of diagnosis and management involve clinical decision-making. However, decision-making is often affected by cognitive biases that can lead to medical errors. This statement presents a framework of clinical thinking and decision-making and shows how these processes can be bias-prone. We review examples of cognitive bias in obstetrics and introduce debiasing tools and strategies. When an adverse event or near miss is reviewed, the concept of a cognitive autopsy—a root cause analysis of medical decision-making and the potential influence of cognitive biases—is promoted as part of the review process. Finally, areas for future research on cognitive bias in obstetrics are suggested.},
 authors = {['Fouad Atallah', 'Rebecca F. Hamm', 'Christina M. Davidson', 'C. Andrew Combs']},
 journal = {American Journal of Obstetrics and Gynecology},
 keywords = {['decision-making', 'diagnostic error', 'disparities', 'implicit bias', 'inequity', 'medical error', 'racism']},
 title = {Society for Maternal-Fetal Medicine Special Statement: Cognitive bias and medical error in obstetrics—challenges and opportunities},
 year = {2022}
}

@Filtered Article{28c7cc4e-d1b9-4c0f-aaf4-0b13566df799,
 abstract = {This article highlights recurrent themes and research communities in Sustainability Assessment (SA), a rapidly growing trans-disciplinary area particularly relevant to the global evaluation community. This bibliometric analysis signals the emergence of a substantial research community based in Asia and the Middle East, whose production is distinct from North American and European-centric evaluation studies. While the latter primarily address methodological challenges related to sustainability issues in social policy, organizational capacity building, and public health, the broader SA literature centers on life-cycle assessments to integrate the analysis of environmental and socioeconomic effects in such domains as biodiversity, energy efficiency, urban planning, alternative agriculture, and supply chain management. This mapping exercise highlights the global distribution of research output and identifies existing gaps and potential future cross-fertilization. The transdisciplinary SA literature can draw from theory-based designs attuned to complexity and systems thinking. Policy analysts and evaluators can gain insights from diverse SA perspectives and policy approaches to tackle sustainability challenges more systematically.},
 authors = {['Mita Marra']},
 journal = {Evaluation and Program Planning},
 keywords = {['Sustainability Assessment (SA)', 'Sustainability', 'Bibliometric analysis', 'Evaluation Journals']},
 title = {Bridging the gaps in sustainability assessment: A systematic literature review, 2014–2023},
 year = {2025}
}

@Filtered Article{28f7f18f-6f65-4063-8d62-ca6c43332090,
 abstract = {Human creativity generates novel ideas to solve real-world problems. This thereby grants us the power to transform the surrounding world and extend our human attributes beyond what is currently possible. Creative ideas are not just new and unexpected, but are also successful in providing solutions that are useful, efficient and valuable. Thus, creativity optimizes the use of available resources and increases wealth. The origin of human creativity, however, is poorly understood, and semantic measures that could predict the success of generated ideas are currently unknown. Here, we analyze a dataset of design problem-solving conversations in real-world settings by using 49 semantic measures based on WordNet 3.1 and demonstrate that a divergence of semantic similarity, an increased information content, and a decreased polysemy predict the success of generated ideas. The first feedback from clients also enhances information content and leads to a divergence of successful ideas in creative problem solving. These results advance cognitive science by identifying real-world processes in human problem solving that are relevant to the success of produced solutions and provide tools for real-time monitoring of problem solving, student training and skill acquisition. A selected subset of information content (IC Sánchez–Batet) and semantic similarity (Lin/Sánchez–Batet) measures, which are both statistically powerful and computationally fast, could support the development of technologies for computer-assisted enhancements of human creativity or for the implementation of creativity in machines endowed with general artificial intelligence.},
 authors = {['Georgi V. Georgiev', 'Danko D. Georgiev']},
 journal = {Knowledge-Based Systems},
 keywords = {['Creativity', 'Divergence', 'Semantic networks', 'Similarity', 'WordNet']},
 title = {Enhancing user creativity: Semantic measures for idea generation},
 year = {2018}
}

@Filtered Article{292f9ef2-cb3a-4618-85aa-c9cb797f47e4,
 abstract = {This research investigates the effective incorporation of the human factor and user perception in text-based interactive media. In such contexts, the reliability of user texts is often compromised by behavioural and emotional dimensions. To this end, several attempts have been made in the state of the art, to introduce psychological approaches in such systems, including computational psycholinguistics, personality traits and cognitive psychology methods. In contrast, our method is fundamentally different since we employ a psychoanalysis-based approach; in particular, we use the notion of Lacanian discourse types, to capture and deeply understand real (possibly elusive) characteristics, qualities and contents of texts, and evaluate their reliability. As far as we know, this is the first time computational methods are systematically combined with psychoanalysis. We believe such psychoanalytic framework is fundamentally more effective than standard methods, since it addresses deeper, quite primitive elements of human personality, behaviour and expression which usually escape methods functioning at “higher”, conscious layers. In fact, this research is a first attempt to form a new paradigm of psychoanalysis-driven interactive technologies, with broader impact and diverse applications. To exemplify this generic approach, we apply it to the case-study of fake news detection; we first demonstrate certain limitations of the well-known Myers–Briggs Type Indicator (MBTI) personality type method, and then propose and evaluate our new method of analysing user texts and detecting fake news based on the Lacanian discourses psychoanalytic approach.},
 authors = {['Minas Gadalla', 'Sotiris Nikoletseas', 'José Roberto {de A. Amazonas}', 'José D.P. Rolim']},
 journal = {Intelligent Systems with Applications},
 keywords = {['Lacanian discourses', 'Psychoanalysis computing', 'GPT-3']},
 title = {Concepts and experiments on psychoanalysis driven computing},
 year = {2023}
}

@Filtered Article{29d1b952-9a93-4638-8c20-98dcbc5152cd,
 abstract = {Wishful thinking, defined as the tendency to over-estimate the probability of high-payoff outcomes, is a widely-documented phenomenon that can affect decision-making across numerous domains, including finance, management, and entrepreneurship. We design an experiment to distinguish and test the relationship between two easily-confounded biases, optimism and overconfidence, both of which can contribute to wishful thinking. We find that optimism and overconfidence are positively correlated at the individual level and that both help to explain wishful thinking. These findings suggest that ignoring optimism results in an upwardly biased estimate of the role of overconfidence in explaining wishful thinking. To illustrate this point, we show that 30% of our observations are misclassified as under- or overconfident if optimism is omitted from the analysis. Our findings have potential implications for the design of information interventions since how agents incorporate information depends on whether the bias is ego-related.},
 authors = {['Stephanie A. Heger', 'Nicholas W. Papageorge']},
 journal = {Journal of Economic Psychology},
 keywords = {['Subjective beliefs', 'Overconfidence', 'Optimism', 'Information', 'Experiments']},
 title = {We should totally open a restaurant: How optimism and overconfidence affect beliefs},
 year = {2018}
}

@Filtered Article{29dfea53-9486-4425-a348-f21b048985f8,
 abstract = {This study triangulates executive planning and visuo-spatial reasoning in the context of the Tower of London (TOL) task by using a variety of methodological approaches. These approaches include functional magnetic resonance imaging (fMRI), functional connectivity analysis, individual difference analysis, and computational modeling. A graded fMRI paradigm compared the brain activation during the solution of problems with varying path lengths: easy (1 and 2 moves), moderate (3 and 4 moves) and difficult (5 and 6 moves). There were three central findings regarding the prefrontal cortex: (1) while both the left and right prefrontal cortices were equally involved during the solution of moderate and difficult problems, the activation on the right was differentially attenuated during the solution of the easy problems; (2) the activation observed in the right prefrontal cortex was highly correlated with individual differences in working memory (measured independently by the reading span task); and (3) different patterns of functional connectivity were observed in the left and right prefrontal cortices. Results obtained from the superior parietal region also revealed left/right differences; only the left superior parietal region revealed an effect of difficulty. These fMRI results converged upon two hypotheses: (1) the right prefrontal area may be more involved in the generation of a plan, whereas the left prefrontal area may be more involved in plan execution; and (2) the right superior parietal region is more involved in attention processes while the left homologue is more of a visuo-spatial workspace. A 4CAPS computational model of the cognitive processes and brain activation in the TOL task integrated these hypothesized mechanisms, and provided a reasonably good fit to the observed behavioral and brain activation data. The multiple research approaches presented here converge on a deepening understanding of the combination of perceptual and conceptual processes in this type of visual problem solving.},
 authors = {['Sharlene D Newman', 'Patricia A Carpenter', 'Sashank Varma', 'Marcel Adam Just']},
 journal = {Neuropsychologia},
 keywords = {['Planning', 'fMRI', 'Spatial working memory', 'Problem solving', 'Tower of London', 'Computational modeling', '4CAPS']},
 title = {Frontal and parietal participation in problem solving in the Tower of London: fMRI and computational modeling of planning and high-level perception},
 year = {2003}
}

@Filtered Article{29fcce68-5234-41cd-a526-b4beba2142a3,
 abstract = {Three basic issues of granular computing are construction or definition of granules, measures of granules, and computation or reasoning with granules. This paper reviews the main theories of granular computing and introduces the definition of spatial granules. A granule is composed of one or more atomic granules. The rationality of this definition is explained from the four aspects: simplicity, applicability, measurability and visualization. A one-to-one correspondence is established between the granules and the points in the unit hypercube, and the coarsening and refining of the granules are the descending and ascending dimensions of the points, respectively. The weak fuzzy tolerance relation and weak fuzzy equivalence relation are defined so as to study on all fuzzy binary relations. The notion of layer granularity/fineness is introduced and each granule can be easily denoted by two numbers, which can be used to pre-process macro knowledge space and greatly improve the search speed. This paper also discusses the main properties of granules including the necessary and sufficient conditions of coarse-fine relation and the main principles of granular space.},
 authors = {['Liquan Zhao', 'Yiyu Yao']},
 journal = {Fuzzy Sets and Systems},
 keywords = {['Granularity', 'Fineness', 'Subsethood', 'Coarse-fine relation', 'Quotient join', 'Quotient meet', 'Granular space', 'Spatial granular computing']},
 title = {Towards the definition of spatial granules},
 year = {2024}
}

@Filtered Article{2abfa0fa-fb48-4512-8dec-6328701c1f8e,
 abstract = {Attribute reduction serves as a pivotal topic of rough set theory for data analysis. The ideas of tri-level thinking from three-way decision can shed new light on three-level attribute reduction. Existing classification-specific and class-specific attribute reducts consider only macro-top and meso-middle levels. This paper introduces a micro-bottom level of object-specific reducts. The existing two types of reducts apply to the global classification with all objects and a local class with partial objects, respectively. The new type applies to an individual object. These three types of reducts constitute tri-level attribute reducts. Their development and hierarchy are worthy of systematical explorations. Firstly, object-specific reducts are defined by object consistency from dependency, and they improve both classification-specific and class-specific reducts. Secondly, tri-level reducts are unified by tri-level consistency. Hierarchical relationships between object-specific reducts and class-specific, classification-specific reducts are analyzed, and relevant connections of three-way classifications of attributes are given. Finally, tri-level reducts are systematically analyzed, and two approaches, i.e., the direct calculation and hierarchical transition, are suggested for constructing a specific reduct. We build a framework of tri-level thinking and analysis of attribute reduction to enrich three-way granular computing. Tri-level reducts lead to the sequential development and hierarchical deepening of attribute reduction, and their results profit intelligence processing and system reasoning.},
 authors = {['Xianyong Zhang', 'Yiyu Yao']},
 journal = {Expert Systems with Applications},
 keywords = {['Attribute reduction', 'Three-way decision', 'Tri-level analysis', 'Object-specific attribute reducts', 'Tri-level attribute reducts', 'Granular computing']},
 title = {Tri-level attribute reduction in rough set theory},
 year = {2022}
}

@Filtered Article{2b42bde2-c57b-453a-868e-ecc4999fb2d9,
 abstract = {This paper presents a computational theory of developmental mental architectures for artificial and natural systems, motivated by neuroscience. The work is an attempt to approximately model biological mental architectures using mathematical tools. Six types of architecture are presented, beginning with the observation-driven Markov decision process as Type-1. From Type-1 to Type-6, the architecture progressively becomes more complete toward the necessary functions of autonomous mental development. Properties of each type are presented. Experiments are discussed with emphasis on their architectures.},
 authors = {['Juyang Weng']},
 journal = {Neurocomputing},
 keywords = {['Mental architecture', 'Agent architecture', 'Computational neural science', 'Cognitive development', 'Autonomous mental development', 'Developmental robots', 'Learning types', 'Developmental vision', 'Speech recognition', 'Language acquisition', 'Thinking', 'Reasoning', 'Autonomous planning']},
 title = {On developmental mental architectures},
 year = {2007}
}

@Filtered Article{2b72e0e0-599c-468a-aaca-c485928cdebd,
 abstract = {A quantitative research study gathers numerical data, which needs to be examined to make conclusions. Critical thinking regarding data analysis is facilitated primarily by quantitative data analysis. The purpose of data analysis is to identify the underlying trends, patterns, and connections in the context of a study. In quantitative data analysis, the focus is not just on applying statistical tests to existing data, but also on leveraging those tests as a means of deriving accurate insights from the data. This chapter aims to give a thorough theoretical review of current methodologies, related computational techniques, and dataset applications. A wide range of topics is covered, focusing on various statistical tests, interpolation, extrapolation, regression, modeling uncertainty analysis, machine learning methods, GIS techniques, data visualization tools etc. Each of these topics is extensively explored and discussed in terms of relevant literature and methods for air quality data and its applications.},
 authors = {['Pushp Raj Tiwari', 'Saurabh Kumar']},
 journal = {Elsevier},
 keywords = {['Machine learning', 'data science', 'statistics', 'statistical applications', 'artificial intelligence']},
 title = {Chapter 13 - Quantitative data analysis methods for air quality prediction},
 year = {2025}
}

@Filtered Article{2bfe9f77-f9ed-49b6-a6eb-637c8a77443a,
 abstract = {Although there is a deep historical relationship between hydrology and society, the relationship has considerably evolved in the last three decades. Hydrologists, in particular those involved in designing of decision-support tools, are experiencing a widening gap between an academic discipline which has progressively moved away from field-based applied natural science to computational hydrology, and the multiplication of stakeholders involved in the water-related issues addressed by research. The challenge for hydrology is now to negotiate this shift and to rethink its engagement in society. This paper provides a description of a planned process designed to improve hydrology-society interactions and to foster reflexivity in socio-hydrology. Based on an interdisciplinary reflexive process undertaken in Tunisia from 2016 to 2020, we identified three types of discomforts in the dialogue with society, inviting scientists to lucidly engage with these discomforts. We formulated four key reflexive propositions to achieve a better alignment of scientific stance, research practices and discourse. The first proposition concerns the need to explain more clearly the value systems scientists engage in and with society. The second concerns the need to position hydrology in society and not outside it, by reconsidering the functions that research fulfils in society. The third is an invitation to redefine the perimeter of the research interlocutors and the way to reach them. The fourth is to revisit scientific practices to build on the strengths of the dialogue between field-based natural science and computational hydrology. The paper concludes that adopting a reflexive posture towards these four dimensions of the dialogue between hydrology and society is an effective way to overcome discomforts and to refocus research stance, practices and discourse. It is a way to renew hydrology's place in society and to contribute to the current thinking in socio-hydrology initiated by hydrologists.},
 authors = {['Jeanne Riaux', 'Marcel Kuper', 'Sylvain Massuel', 'Insaf Mekki']},
 journal = {Journal of Hydrology},
 keywords = {['Socio-hydrology', 'Society', 'Hydrology', 'Interdisciplinarity', 'Reflexivity', 'Tunisia']},
 title = {Riding the waves of discomforts: Reflecting on the dialogue of hydrologists with society},
 year = {2023}
}

@Filtered Article{2c4212a8-0011-4388-86b0-53f16568f380,
 abstract = {The brain is thought to implement two decision-making systems: a goal-directed system in which decisions are made through planning on the basis of action–outcome relationships, and a habitual system in which behaviour reflects stimulus–response associations. A prominent theory of addiction sees it as arising due to an extreme dominance of habit over goal-directed action. The balance between these systems is thought to be arbitrated by the relative precision of their separate predictions of reward. In this paper, we argue that various factors in addiction create hyper-precise reward predictions in the habitual system and hypo-precise reward predictions in the goal-directed system, shifting the balance of behavioural control in favour of habit. Based on this, we offer a theoretical account of the utility of episodic future thinking in addiction, interpreting it as increasing the precision of reward estimates in the goal-directed system, thereby enhancing the control of this system over behaviour.},
 authors = {['Isaac Kinley', 'Michael Amlung', 'Suzanna Becker']},
 journal = {Brain and Cognition},
 keywords = {['Addiction', 'Goals', 'Habits', 'Free energy', 'Bayesian statistics', 'Episodic future thinking']},
 title = {Pathologies of precision: A Bayesian account of goals, habits, and episodic foresight in addiction},
 year = {2022}
}

@Filtered Article{2c543c65-cb9f-4928-b293-25ebc0a15096,
 abstract = {Anhedonia, a lack of pleasure in things an individual once enjoyed, and rumination, the process of perseverative and repetitive attention to specific thoughts, are hallmark features of depression. Though these both contribute to the same debilitating disorder, they have often been studied independently and through different theoretical lenses (e.g., biological vs. cognitive). Cognitive theories and research on rumination have largely focused on understanding negative affect in depression with much less focus on the etiology and maintenance of anhedonia. In this paper, we argue that by examining the relation between cognitive constructs and deficits in positive affect, we may better understand anhedonia in depression thereby improving prevention and intervention efforts. We review the extant literature on cognitive deficits in depression and discuss how these dysfunctions may not only lead to sustained negative affect but, importantly, interfere with an ability to attend to social and environmental cues that could restore positive affect. Specifically, we discuss how rumination is associated to deficits in working memory and propose that these deficits in working memory may contribute to anhedonia in depression. We further argue that analytical approaches such as computational modeling are needed to study these questions and, finally, discuss implications for treatment.},
 authors = {['Ashleigh V. Rutherford', 'Samuel D. McDougle', 'Jutta Joormann']},
 journal = {Clinical Psychology Review},
 keywords = {['Rumination', 'Emotion regulation', 'Working memory', 'Reinforcement learning', 'Depression']},
 title = {“Don't [ruminate], be happy”: A cognitive perspective linking depression and anhedonia},
 year = {2023}
}

@Filtered Article{2c71fdc2-d5e6-4a2f-b626-f5f915589b15,
 abstract = {Biomarkers remain the highest value proposition in cancer medicine today—especially protein biomarkers. Despite decades of evolving regulatory frameworks to facilitate the review of emerging technologies, biomarkers have been mostly about promise with very little to show for improvements in human health. Cancer is an emergent property of a complex system, and deconvoluting the integrative and dynamic nature of the overall system through biomarkers is a daunting proposition. The last 2 decades have seen an explosion of multiomics profiling and a range of advanced technologies for precision medicine, including the emergence of liquid biopsy, exciting advances in single-cell analysis, artificial intelligence (machine and deep learning) for data analysis, and many other advanced technologies that promise to transform biomarker discovery. Combining multiple omics modalities to acquire a more comprehensive landscape of the disease state, we are increasingly developing biomarkers to support therapy selection and patient monitoring. Furthering precision medicine, especially in oncology, necessitates moving away from the lens of reductionist thinking toward viewing and understanding that complex diseases are, in fact, complex adaptive systems. As such, we believe it is necessary to redefine biomarkers as representations of biological system states at different hierarchical levels of biological order. This definition could include traditional molecular, histologic, radiographic, or physiological characteristics, as well as emerging classes of digital markers and complex algorithms. To succeed in the future, we must move past purely observational individual studies and instead start building a mechanistic framework to enable integrative analysis of new studies within the context of prior studies. Identifying information in complex systems and applying theoretical constructs, such as information theory, to study cancer as a disease of dysregulated communication could prove to be “game changing” for the clinical outcome of cancer patients.},
 authors = {['Anna D. Barker', 'Mario M. Alba', 'Parag Mallick', 'David B. Agus', 'Jerry S.H. Lee']},
 journal = {Molecular & Cellular Proteomics},
 keywords = {['proteomics', 'cancer biomarkers', 'protein biomarkers', 'complex adaptive systems', 'clinical proteomics']},
 title = {An Inflection Point in Cancer Protein Biomarkers: What was and What's Next},
 year = {2023}
}

@Filtered Article{2d3cc81a-e841-4f30-80aa-d6a5a66fa00f,
 abstract = {Modeling and simulation skills are two core competences of computational science and thus should be a central part of any curriculum. While there is a well-founded methodology for the design of simulation algorithms today the teaching of modeling skills carries some intrinsic problems. The reason is that modeling is still partly an art and partly a science. As an important consequence for university education, the concepts for teaching modeling must be quite different from those for teaching simulation algorithms. Experiences made with the courses on ‘Modeling and Simulation’ at the University of Siegen are summarized and some general concepts for the teaching of modeling skills are presented. In particular, three practical approaches to modeling education are discussed with several examples.},
 authors = {['W. Wiechert']},
 journal = {Future Generation Computer Systems},
 keywords = {['Computational science education', 'Modeling and simulation', 'Modeling education']},
 title = {The role of modeling in computational science education},
 year = {2003}
}

@Filtered Article{2d45295f-9690-4911-8b29-32f38958f73f,
 abstract = {Reinforcement learning models provide an excellent example of how a computational process approach can help organize ideas and understanding of underlying neurobiology. In a strong sense, this is the assumption behind computational neuroscience. Computational psychiatry, as a translational arm of computational neuroscience, can also profit from the computational process approach but applied at many levels ranging from low-level neurobiology through characterization of mental states and even up to the level of multiple interacting humans. Here, we review some of the early evidence for why reinforcement learning in its modern versions moves well beyond behaviorist accounts and provides an excellent “computational paradigm” for framing value-dependent decision-making; something that goes awry in a number of psychiatry conditions. We focus in particular on how social exchange between humans can engage reward systems and can be used as a computational device good for parsing subjects into categories that relate in interesting ways to traditional depictions of psychopathology.},
 authors = {['P. {Read Montague}']},
 journal = {Academic Press},
 keywords = {['Approach and avoidance', 'Computational phenotyping', 'Computational psychiatry', 'Decision-making models', 'Economic games', 'Psychopathology', 'Reinforcement learning', 'Trust game']},
 title = {Chapter 11 - Computational Phenotypes Revealed by Interactive Economic Games},
 year = {2018}
}

@Filtered Article{2d7cb93f-d976-439f-a308-257b91a71b97,
 abstract = {Objective
Human voluntary movement is associated with two changes in electroencephalography (EEG) that can be observed as early as 1.5s prior to movement: slow DC potentials and frequency power shifts in the alpha and beta bands. Our goal was to determine whether and when we can reliably predict human natural movement BEFORE it occurs from EEG signals ONLINE IN REAL-TIME.
Methods
We developed a computational algorithm to support online prediction. Seven healthy volunteers participated in this study and performed wrist extensions at their own pace.
Results
The average online prediction time was 0.62±0.25s before actual movement monitored by EMG signals. There were also predictions that occurred without subsequent actual movements, where subjects often reported that they were thinking about making a movement.
Conclusion
Human voluntary movement can be predicted before movement occurs.
Significance
The successful prediction of human movement intention will provide further insight into how the brain prepares for movement, as well as the potential for direct cortical control of a device which may be faster than normal physical control.},
 authors = {['Ou Bai', 'Varun Rathi', 'Peter Lin', 'Dandan Huang', 'Harsha Battapady', 'Ding-Yu Fei', 'Logan Schneider', 'Elise Houdayer', 'Xuedong Chen', 'Mark Hallett']},
 journal = {Clinical Neurophysiology},
 keywords = {['Human intention', 'Voluntary movement', 'Prediction', 'Movement-related cortical potentials (MRCP)', 'Event-related desynchronization (ERD)', 'Electroencephalography (EEG)', 'Brain–computer interface (BCI)', 'Consciousness']},
 title = {Prediction of human voluntary movement before it occurs},
 year = {2011}
}

@Filtered Article{2d879b6c-a539-4bd3-929b-b9c66a1dc5bb,
 abstract = {Normal human observers compared either the width, height or area of two simultaneously-presented shapes (the standard and the test), with a cue to indicate which decision had to be made. On ‘area’ trials, test width was a random variable, ensuring that neither shape (aspect ratio), width nor height by themselves was a reliable signal. Weber fractions for width and height of both ellipses and rectangles were in the range 5–10%, but for area they were higher (10–20%) than predicted from the combination of noisy width and height decisions. With ellipses, observers were more likely to overestimate width or height when the other dimension differed from the standard in the same direction (e.g. both greater). We conclude that observers have no access to high-precision codes for 2-D area, and that they base their decisions on a variety of heuristics derived from 1-D codes. A second experiment measured acuity for changes in aspect ratio. For ellipses, accuracy for aspect ratio was higher than predicted by the combination of noisy width and height signals; for rectangles it was worse, suggesting that 2-D curvature is a potent cue to shape.},
 authors = {['M.J. Morgan']},
 journal = {Vision Research},
 keywords = {['Psychophysics', 'Shape', 'Weber fraction']},
 title = {The visual computation of 2-D area by human observers},
 year = {2005}
}

@Filtered Article{2da56ccb-0004-4dc9-8b7e-3396ba929817,
 abstract = {The ‘smart city’ is an oft-cited techno-urban imaginary promoted by businesses and governments alike. It thinks big, and is chiefly imagined in terms of large-scale information communications systems that hinge on the collection of real-time and so-called ‘big data’. Less talked about are the human-scale implications and user-experience of the smart city. Much of the current academic scholarship on smart cities offers synoptic and technical perspectives, leaving the users of smart systems curiously unaccounted for. While they purport to empower citizens, smart cities initiatives are rarely focused at the citizen-scale, nor do they necessarily attend to the ways initiatives can be user-led or co-designed. Drawing on the outcomes of a university studio, this article rethinks the smart city as a series of urban scales—metropolis, community, individual, and personal—and proposes an analytical model for classifying smart city initiatives in terms of engagement. Informed by the theory of proxemics, the model proposed analyses smart city initiatives in terms of the scope of their features and audience size; the actors accountable for their deployment and maintenance; their spatial reach; and the ability of design solutions to re-shape and adapt to different urban scenarios and precincts. We argue that the significance of this model lies in its potential to facilitate modes of thinking across and between scales in ways that can gauge the levels of involvement in the design of digitally mediated urban environments, and productively re-situate citizens as central to the design of smart city initiatives.},
 authors = {['Nicole Gardner', 'Luke Hespanhol']},
 journal = {City, Culture and Society},
 keywords = {['Smart cities', 'Architecture', 'Design', 'Physical computing', 'Proxemics', 'Computational design']},
 title = {SMLXL: Scaling the smart city, from metropolis to individual},
 year = {2018}
}

@Filtered Article{2db99664-e73f-4392-b57e-441b3b0b5d5f,
 abstract = {This chapter’s main aim is to interrogate how augmented reality (AR) content can change the relations of power within a place and transform the perception of place from being a material background for social interactions into a living agent that can have its own agency. We use the concept of sociotechnical imaginary and thematic analysis of AR-related media content to explore main narrations in the current discourse on AR and smart urban spaces. We identify two dominant themes: “smart information in place” and “subversion of meaning” that combine two ways of thinking about relation between place and AR. In the first one, AR acts not as an augmentation of place, but as a reducer of the experience of place down to seeing the world as information and data for the achievement of efficiencies in late capitalism. In the second one, AR can be seen as transcending the traditional constellations of power relations that shape places and spaces of modern cities, skewing them toward nonhuman actors, of which AR may be the most visible and influential. Through the visible differentiation of users, alteration of perception and the forcing of presence and behavior, AR is a technology that makes visible the systems and processes of control and mediation of space and place. As such, the smart space itself will become visible through the presence, use, and functioning of AR in a manner that has not been the case previously. We posit, therefore, that AR can be seen as a physical manifestation of the agency of place. This position can have consequences for the practical development of smart spaces and for theoretical consideration of the human-technology interaction in urban space in its material and digital dimensions.},
 authors = {['Michal Rzeszewski', 'Leighton Evans']},
 journal = {Academic Press},
 keywords = {['Augmented reality', 'Smart space', 'Agency of place', 'Power relations']},
 title = {Chapter 10 - Augmented reality content and relations of power in smart spaces},
 year = {2024}
}

@Filtered Article{2dc7e4d0-54d8-4cea-a62b-3f5a28169d02,
 abstract = {An ordinary differential equation (ODE) form of the radial return algorithm, which is essentially a Prandtl-Reuss material model, is combined with a strain-rate hardening model to produce an ODE that describes deviatoric stress through a prescribed density rise. An analytical solution is found to the resulting ODE for a specific choice of one of the hardening model’s parameters. That solution is used to prove that if the prescribed density rise is allowed to be infinitely thin, i.e., like a shock in the mathematical sense, the resulting deviatoric stress is still bounded. In other words, the singularity is integrable; integration of the radial return ODE regularizes the infinite strain rate and resulting yield stress in the presence of an ideal shock singularity. The analytical tools developed for this line of thinking are applied to study the variation of deviatoric stress through a nearly shock-like density rise using different density rise profiles, revealing the impact of the shape choice. The tools are also used to compute what rise times are needed to converge upon the correct value of deviatoric stress through a shock; the results indicate that most contemporary hydrocodes cannot be expected to achieve those rise times. A demonstration of connecting the analytical tools to a hydrocode, using surrogate numerical shock shapes, is provided thereby opening the door for using such surrogates to perform sub-grid computations of converged shock behavior for strain-rate hardening materials.},
 authors = {['Scott R. Runnels']},
 journal = {Mathematics and Computers in Simulation},
 keywords = {['Shocks', 'Plasticity', 'Hardening', 'Hydrodynamics', 'Radial return']},
 title = {Capturing plasticity effects in overdriven shocks on the finite scale},
 year = {2015}
}

@Filtered Article{2deb4a24-a412-464c-b713-313e1f387222,
 abstract = {A brief review is presented of modelling and simulation of HDR geothermal reservoirs both for hydraulic fracturing/stimulation to create artificial/engineered geothermal reservoirs and for long-term heat extraction operations. Firstly, modelling of the governing factors is surveyed and coupling among mechanical, thermal, hydraulic and chemical effects is discussed. Next the structure and modelling of reservoirs are discussed. Finally, the features of a variety of simulation codes are summarized.},
 authors = {['Kazuo Hayashi', 'Jonathan Willis-Richards', 'Robert J Hopkirk', 'Yuichi Niibori']},
 journal = {Geothermics},
 keywords = {['Reservoir', 'Hot dry rock', 'HDR', 'Modelling', 'Numerical simulation']},
 title = {Numerical models of HDR geothermal reservoirs—a review of current thinking and progress},
 year = {1999}
}

@Filtered Article{2e205c35-b6da-4c20-a441-0d4059a4bfee,
 abstract = {Motivated social behaviors such as mating are controlled by a complex network of limbic nuclei. Concepts of network organization derived from computational neuroscience may aid our understanding of the links between the neuroanatomical circuitry and what is represented by the anatomy. Research in my laboratory uses mating behavior in the male Syrian hamster as a model to elucidate how chemosensory and steroid cues are integrated in the brain. An interaction of odors and hormones is required for mating in this species. These two essential stimuli are transmitted through separate parallel pathways in the limbic system. The functional organization of the hamster mating behavior circuit is characterized by distributed representation, divergent and convergent neural pathways, and recurrent feedback. Odors and hormones have different modes of action on this neural network. While chemosensory cues stimulate the input units of the network, steroids facilitate behavior through the hidden units. In this manner, steroids appear to create a permissive environment for subsequent activation by odor cues.},
 authors = {['Ruth I. Wood']},
 journal = {Hormones and Behavior},
 title = {Thinking about Networks in the Control of Male Hamster Sexual Behavior},
 year = {1997}
}

@Filtered Article{2e271a74-3957-4465-9c23-92fe88cf4652,
 abstract = {Titanium-based semiconductors are wildly recognized as one of the most commonly used photocatalysts for photocatalysis. Energy band modulation is a key aspect of the catalytic activity of photocatalytic semiconductors, but the acquisition of semiconductor energy bands is still a complex and important task. In recent years, machine learning has played an important role in materials prediction, where the crystal structure of a material is usually used as input in energy band prediction. However, existing machine learning algorithms cannot accurately predict the energy bands of materials from structural components. Here, we convert the chemical formula into component descriptors after comparing first principles and ultraviolet-visible spectrophotometry (UV–vis) errors on bandgap values, and the component and bandgap values form a set of labeled data pairs. The chemical formula components are used as input to accurately predict the energy bands of the material. In our evaluation, the model outperforms existing machine learning methods in predicting energy bands, yielding mean absolute value errors of about 0.277 eV, and possesses a significant advantage in component prediction. In particular, this method of predicting the photocatalytic semiconductor energy band gap from chemical formula components provides a new way of thinking about photocatalyst selectivity.},
 authors = {['Jiawei Li', 'Zhengxin Chen', 'Jiang Wu', 'Jia Lin', 'Ping He', 'Rui Zhu', 'Cheng Peng', 'Hai Zhang', 'Wenhao Li', 'Xu Fang', 'Hongtao Shen']},
 journal = {Materials Today Communications},
 keywords = {['Random forest model', 'Chemical formula', 'Components', 'Bandgap', 'Machine learning']},
 title = {Constructing a link between multivariate titanium-based semiconductor band gaps and chemical formulae based on machine learning},
 year = {2023}
}

@Filtered Article{2e7c2cab-d566-48e8-9884-657a23320f57,
 abstract = {Previous attempts at modelling current observed endogenous financial variables in a macroeconomic model have concentrated on only one variable — the short-term rate of interest. This paper applies a general search algorithm to a macroeconomic model with an observed interest rate and exchange rate to solve the signal extraction problem. Firstly, the algorithm is tested against a linear model with a known analytical solution. Then, the algorithm is applied to all the observed current endogenous variables in a non-linear rational expectations model of the UK. The informational advantage of applying the signal extraction algorithm is evaluated in terms of the forecasting efficiency of the model.},
 authors = {['L. Lungu', 'K.G.P. Matthews', 'A.P.L. Minford']},
 journal = {Economic Modelling},
 keywords = {['Rational expectations', 'Partial current information', 'Signal extraction', 'Macroeconomic modelling']},
 title = {Partial current information and signal extraction in a rational expectations macroeconomic model: A computational solution},
 year = {2008}
}

@Filtered Article{2f284de7-ce31-474a-8e10-aade9983212a,
 abstract = {The issue of accounting fraud presents a significant challenge within the business sector, prompting an increase in scholarly investigations across various contexts. Despite this growing interest, research specifically addressing the Thai context has remained scarce. Thus, this quantitative study aimed to bridge this gap by assessing the proficiency of Thai Gen Z accountants in detecting accounting fraud, with a particular emphasis on their digital, data science, and diagnostic skills. The study collected data from 150 participants using a structured survey questionnaire distributed to licensed accountants affiliated with the Thailand accounting program. It adopted a theoretical framework inspired by social learning theory and information processing theory to examine both direct and mediated relationships among the key variables under investigation. The results were analyzed using Partial Least Squares Structural Equation Modeling (PLS-SEM) to examine these relationships. The results showed that digital competency have significant direct effects on the fraud detection skills, with diagnostic skills playing a key role in the process. The study revealed that digital competency not only furnishes accountants with necessary technological expertise but also bolsters their analytical skills, which are vital for identifying fraudulent activities. Likewise, data science literacy—encompassing skills in predictive analytics, big data management, and data insight communication—significantly enhances accountants' capacity to identify and understand fraudulent patterns. The emergent role of diagnostic skills as a key intermediary emphasizes the importance of comprehensive training programs that foster both technical prowess and critical analytical thinking.},
 authors = {['Narinthon Imjai', 'Watcharawat Promma', 'Nimnual Visedsun', 'Berto Usman', 'Somnuk Aujirapongpan']},
 journal = {International Journal of Information Management Data Insights},
 keywords = {['Fraud detection skills', 'Digital competency', 'Data science literacy', 'Diagnostic skills', 'Thai Gen Z accountants']},
 title = {Fraud detection skills of Thai Gen Z accountants: The roles of digital competency, data science literacy and diagnostic skills},
 year = {2025}
}

@Filtered Article{2f8ac118-59d3-4722-b1e6-bedf18afca0b,
 abstract = {Bone is probably the most frequently investigated biological material and finite element analysis (FEA) is the computational tool most commonly used for the analysis of bone biomechanical function. FEA has been used in bone research for more than 30 years and has had a substantial impact on our understanding of the complex behavior of bone. Bone is structured in a hierarchical way covering many length scales and this chapter reflects this hierarchical organization. In particular, the focus is on the applications of FEA for understanding the relationship between bone structure and its mechanical function at specific hierarchical levels. Depending on the hierarchical level, different issues have been investigated with FEA ranging from more clinically oriented topics related to bone quality (eg, predicting bone strength and fracture risk) to more fundamental problems dealing with the mechanical aspects of biological processes (eg, stress and strain around osteocyte lacunae) as well as with the micromechanical behavior of bone at its ultrastructure. A better understanding of the relationship between structure and mechanical function is expected to be important for the current trends in (bio)materials design, where the structure of biological materials is considered as a possible source of inspiration, as well as for more successful approaches in the prevention and treatment of age- and disease-related fractures.},
 authors = {['D. Ruffoni', 'G.H. {van Lenthe}']},
 journal = {Elsevier},
 keywords = {['Bone imaging', 'Bone research', 'Computational modeling', 'Femur', 'Finite element analysis', 'Fracture', 'Hierarchical structure', 'Micro-computed tomography', 'Osteoporosis', 'Radius', 'Strength', 'Vertebra']},
 title = {3.10 Finite Element Analysis in Bone Research: A Computational Method Relating Structure to Mechanical Function☆},
 year = {2017}
}

@Filtered Article{2fc45e7d-610f-4241-8b69-815150b0e870,
 abstract = {Learning can take place everywhere: in the home, the community, or at work. Learning outside school is often invisible because it is taken for granted, as common sense or cultural knowledge. It happens in the course of activities not designed for learning, so it can be described as thinking in action. The representational tools (number systems, graphs) and objects (crates, coins, bills) used outside school become part of our thinking as we act and think with them. A major process in learning outside school is guided participation, where learners take responsibility for accomplishing tasks guided by a more experienced person.},
 authors = {['T. Nunes']},
 journal = {Elsevier},
 keywords = {['Guided participation', 'Informal learning', 'Informal mathematics', 'Learning outside school', 'Nonformal learning', 'Oral arithmetic', 'Situated learning', 'Street mathematics', 'Thinking in action', 'Work-based learning']},
 title = {Learning Outside of School},
 year = {2010}
}

@Filtered Article{305074f9-7af4-44dd-a3e1-d3e20d317864,
 abstract = {Summary
A “technology lottery” describes a research idea or technology succeeding over others because it is suited to available software/hardware and not necessarily because it is superior. The nascent field of self-driving laboratories, particularly materials acceleration platforms (MAPs), is at risk: while it is logical and opportunistic to inject existing lab equipment and workflows with artificial intelligence (AI) and automation, such MAPs can constrain research by proliferating existing biases in science, mechatronics, and general-purpose computing. Rather than conformity, MAPs present opportunity to pursue new vectors of engineering physics with advances in cyber-physical learning and closed-loop, self-optimizing systems. We outline a simulation-based MAP program to design computers that use physics to solve optimization problems: the physical computing (PC)-MAP can mitigate hardware-software-substrate-user information losses present in all other MAP classes and eliminate lotteries by perfecting alignment between computing problems and media. We describe early PC advances and research pursuits toward optimal design of new materials and computing media.},
 authors = {['Erik Peterson', 'Alexander Lavin']},
 journal = {Matter},
 keywords = {['materials acceleration platforms', 'AI-driven science', 'simulation intelligence', 'physical computing', 'self-driving labs', 'inverse design', 'computational metamaterials']},
 title = {Physical computing for materials acceleration platforms},
 year = {2022}
}

@Filtered Article{30caeb7b-fa2f-4cfe-be36-5df6f0f230b5,
 abstract = {Early identification and adequate treatment can help prevent lung disorders from becoming chronic, severe, and life-threatening. X-ray images are commonly used and an automated and effective method involving deep learning techniques can potentially contribute to quick and accurate diagnosis of lung disorders. However, in the study of medical imaging using deep learning, two obstacles limit interpretability. One is an insufficient and imbalanced number of training samples in most medical datasets. The other is excessive training time. Although training time can be reduced by decreasing the number of pixels in the images, training with low resolution images tends to result in poor performance. This study represents a solution to overcome these impediments by balancing the number of images and reducing overall processing time while preserving accuracy. The dataset used in this research contains an unequal number of images in the different classes. The quantity of data in the classes is balanced by creating synthetic images based on the patterns and characteristics of the original images, using a Deep Convolutional Generative Adversarial Network (DCGAN). Unwanted regions are removed from the X-ray images, the brightness and contrast of the images are enhanced, and the abnormalities are highlighted by using different artifact removal, noise reduction, and enhancement techniques. We propose a Modified Compact Convolutional Transformer (MCCT) model using 32 × 32 sized images for the categorization of lung disorders into four classes. An ablation study of eleven cases is employed to adjust several hyper parameters and layer topologies. This reduces training time while preserving accuracy. Six transfer learning models, VGG19, VGG16, ResNet152, ResNet50, ResNet50V2, and MobileNet are applied with the same image size the performance is compared with the proposed MCCT model. Our MCCT model records the greatest test accuracy of 95.37%, requiring a short training time, 10-12 s/epoch, whereas the other models only reach near-moderate performance with accuracies ranging from 43% to 79% and training times of 80-90 s/epoch. The robustness of the model with regards to the number of training samples is validated by training the model multiple times reducing the number of training images gradually from 49621 images to 6204 images. Results suggest that even with a smaller dataset, the performance is sustained. Our proposed approach may contribute to an effective CAD based diagnostic system by addressing the issues of insufficient and imbalanced numbers of medical images, excessive training times and low-resolution images.},
 authors = {['Inam Ullah Khan', 'Sami Azam', 'Sidratul Montaha', 'Abdullah Al Mahmud', 'A.K.M. Rakibul Haque Rafid', 'Md. Zahid Hasan', 'Mirjam Jonkman']},
 journal = {Intelligent Systems with Applications},
 keywords = {['COVID-19', 'Chest X-rays', 'Image Preprocessing', 'Modified compact convolutional transformer', 'Deep convolutional GAN', 'Hyper-parameter Tuning']},
 title = {An effective approach to address processing time and computational complexity employing modified CCT for lung disease classification},
 year = {2022}
}

@Filtered Article{30dbf6d4-f36e-4263-ad30-b7f8d761bd7c,
 abstract = {Examining who (and what) stands to gain and lose in the smart city, and what kind of urban life smart cities create, are questions of a philosophical and ethical import. Since the early 2010s scholars and journalists have critiqued the smart city paradigm and examined its various projects and experimental initiatives to surface its ethical dimensions and significance. The smart city's rhetorical swing from tech-centric to human-centric and from smart city to smart citizen is construed here as a further effort to create the image of a more ethical smart city. Consequently, the topic of ethics has also intersected with the smart city in particular ways, and predominantly through the lens of data governance and the protection of privacy rights. This chapter argues for an expanded approach to smart city ethics. It proposes a focus on urban technology design to bridge the gap between a micro-ethical focus on data ethics and macro-level political-ethical critique. Bringing philosophical thinking on technology together with a design-led approach to urban technology is argued to provide a further way to draw out a potentially different set of ethical concerns and to explore how urban life can be lived with technology.},
 authors = {['Nicole Gardner']},
 journal = {Elsevier},
 keywords = {['Design', 'process', 'Ethics', 'Ethics by design', 'Material ethics', 'Philosophy of technology', 'Postphenomenology', 'Practical ethics', 'Smart city', 'Urban technology', 'Value-sensitive design']},
 title = {Chapter 2 - Ethics and the smart city},
 year = {2024}
}

@Filtered Article{3138c4be-ead5-4b35-b8d9-3416b7f86135,
 abstract = {The discussion of enhancing entrepreneurial competence in Higher Education Institution (HEI), especially in engineering and computing major, has increased for the recent years. This study aims to propose and test a structural model of relationship of Indonesian HEI entrepreneurship education with entrepreneurship competence to assess student entrepreneurship competences especially in undergraduate level. Thus, this study provides the contribution in this stream by creating a subject specialized that fit with specific study program to enhance entrepreneurial competence for freshmen student called Integrative Survival Experience especially in engineering and computing major. We measure its output by using EntreComp questionnaires framework from European Commission. A combination of Structural Equation Modelling (SEM) and neural network was implemented as analytic approach in this study. The results show that the freshmen engineering and computing students develop entrepreneurial competence by enhancing the specific sets of ideas and opportunities as well as the capability to manage resources for taking the action afterwards. Apparently, the entrepreneurial competence development process of engineering and computing students differs with that of business and management students.},
 authors = {['Rendika Nugraha', 'Nanang Ali Sutisna', 'Adhi Setyo Santoso', 'Ihsan Hadiansah', 'Johan Krisnanto Runtuk']},
 journal = {Procedia Computer Science},
 keywords = {['Entrepreneurial competence', 'techno-entrepreneurship', 'creativity', 'ethical', 'sustainable thinking', 'motivation', 'perseverance', 'mobilizing others', 'learning through experience taking initiative', 'cope with uncertainty']},
 title = {A SEM-neural network approach for understanding the entrepreneurial competence development of freshmen engineering and computing students},
 year = {2023}
}

@Filtered Article{31694782-5dea-49e6-8cf3-5c60e2de1b4f,
 abstract = {Surface defect detection in industrial production is critical for quality control. Traditional manual design of detection models is time-consuming, inefficient, and lacks adaptability to diverse defect scenarios. To address these limitations, we propose TMNAS (Template-Module Neural Architecture Search), a bi-level optimization framework that automates the design of high-performance defect detection models. TMNAS uniquely integrates predefined template-modules into a flexible search space, enabling simultaneous exploration of architectural components and parameters. By incorporating a single-objective genetic algorithm with a computational complexity penalty term, our approach effectively avoids local optima and significantly reduces search resource consumption. Extensive experiments on industrial defect datasets demonstrate that TMNAS surpasses state-of-the-art models, while on the COCO benchmark, it achieves a competitive mean average precision (mAP) of 58.4%, all with lower computational overhead.},
 authors = {['Wanrong Tan', 'Lingling Huang', 'Hong Li', 'Menghao Tan', 'Jin Xie', 'Weifeng Gao']},
 journal = {Expert Systems with Applications},
 keywords = {['Neural architecture search', 'Defect detection', 'Bi-level optimization', 'Genetic algorithm', 'Penalty term']},
 title = {Neural architecture search with integrated template-modules for efficient defect detection},
 year = {2025}
}

@Filtered Article{319c45ec-817e-4e43-82e6-ef230dea9f7f,
 abstract = {Overview of traditional approaches based on self-consistent approximations in composite materials is presented. Their restrictions are underlined. Neoclassical approach previously introduced in the Preface, is illustrated and compared to methods applied in statistical mechanics. The structural sums, the key construction of the neoclassical approach, are outlined. Method of series and asymptotic method of approximants, Padé approximants, DLog Padé approximants, Factor, Root, Additive approximants are briefly discussed. Notion of Self-Similarity and renormalization-group is introduced. Minimal difference and minimal derivative methods of calculation for short series are discussed in detail. Critical Index is calculated from various short series. DLog root approximants are introduced and illustrate by several examples, where the DLog Padé approximants fail. DLog additive approximants are introduced and presented iteratively. Multiple examples are presented in the chapter and in the appendix. Method of Log Padé approximants is suggested.},
 authors = {['Piotr Drygaś', 'Simon Gluzman', 'Vladimir Mityushev', 'Wojciech Nawalaniec']},
 journal = {Woodhead Publishing},
 keywords = {['Self-consistent approximation', 'structural sum', 'statistical mechanics methods', 'self-Similarity and renormalization-group']},
 title = {1 - Introduction to computational methods and theory of composites},
 year = {2020}
}

@Filtered Article{31bb9f7f-096c-4b8b-9ffd-7c83744c06f6,
 abstract = {Generative Artificial Intelligence foundation models (for example Generative Pre-trained Transformer – GPT – models) can generate the next token given a sequence of tokens. How can this ‘generative AI’ be compared with the ‘real’ intelligence of the human brain, when for example a human generates a whole memory in response to an incomplete retrieval cue, and then generates further prospective thoughts? Here these two types of generative intelligence, artificial in machines and real in the human brain are compared, and it is shown how when whole memories are generated by hippocampal recall in response to an incomplete retrieval cue, what the human brain computes, and how it computes it, are very different from generative AI. Key differences are the use of local associative learning rules in the hippocampal memory system, and of non-local backpropagation of error learning in AI. Indeed, it is argued that the whole operation of the human brain is performed computationally very differently to what is implemented in generative AI. Moreover, it is emphasized that the primate including human hippocampal system includes computations about spatial view and where objects and people are in scenes, whereas in rodents the emphasis is on place cells and path integration by movements between places. This comparison with generative memory and processing in the human brain has interesting implications for the further development of generative AI and for neuroscience research.},
 authors = {['Edmund T. Rolls']},
 journal = {Heliyon},
 keywords = {['The brain and AI', 'Generative Pre-trained Transformer', 'Generative artificial intelligence', 'Episodic memory', 'Semantic memory', 'Hippocampal memory system', 'Chat-GPT']},
 title = {The memory systems of the human brain and generative artificial intelligence},
 year = {2024}
}

@Filtered Article{31c67a42-9f1f-494c-8a77-6e6830afd9cb,
 abstract = {This research explores how deductive engineering thinking, as opposed to an abductive design rationale, can influence how robotic methods of fabricating building components are developed. The goal of this research is to demonstrate how creative thinking can introduce alternative robotic fabrication techniques targeted for the architectural mass-customization process. For this purpose, we chose robotic dieless sheet metal folding as the main fabrication technique, due to its wide range of applications in both the architectural construction and manufacturing industries. Two robotic sheet metal folding projects were developed. The first, an example of tool programming, took advantage of an engineering approach and was focused on the affordances of the tool (an industrial robotic arm). The second project, one of material programming, employed a design methodology and was directed towards the affordances of the material (i.e., stainless steel sheet metal). By discussing the advantages and disadvantages of each approach, this research argues that both engineering and design should be considered required and complementary processes in the development of new creative fabrication solutions, allowing them to and make the overall production process more efficient.},
 authors = {['Shani Sharif', 'Russell Gentry']},
 journal = {Automation in Construction},
 keywords = {['Robotic fabrication', 'Mass-customization', 'Dieless sheet metal folding']},
 title = {Robotic sheet metal folding: Tool vs. material programming},
 year = {2022}
}

@Filtered Article{31d8cf1a-9aca-49aa-a1fc-fcb90863308b,
 abstract = {Major Depressive Disorder (MDD) is a leading health burden worldwide. Previous research has demonstrated that linguistic analysis of depressed individuals’ written and oral speech has potential as a diagnostic and monitoring biomarker. We sought to determine if the semantic content of speech differs between individuals with current MDD, past MDD, and controls. We recruited 53 volunteers for a simulated telehealth psychiatric intake interview. The sample included 14 individuals with current MDD, 21 with past MDD, and 18controls, all confirmed using a semi-structured diagnostic interview. The manually-transcribed interview transcripts were analyzed utilizing the LIWC-22 dictionary and statistical tests were applied to identify differences in the linguistic patterns between each clinical categorization. When comparing depressed subjects (either current or past) versus controls, significant differences were found for emotional tone, total function words, auxiliary verbs, negative tone, negative emotion, anxiety, sadness, attention, and visual. Individuals with past MDD only differed from those with current MDD in use of analytical thinking and auxiliary verbs. These results indicate that LIWC categories could differentiate current or past depressed subjects from controls, but fewer differences emerged when comparing current and past MDD. Further prospective studies with larger sample sizes are needed to confirm these findings.},
 authors = {['Lisette Corbin', 'Emily Griner', 'Salman Seyedi', 'Zifan Jiang', 'Kailey Roberts', 'Mina Boazak', 'Ali {Bahrami Rad}', 'Gari D. Clifford', 'Robert O. Cotes']},
 journal = {Journal of Affective Disorders Reports},
 keywords = {['Depression', 'LIWC', 'Psychiatric interview', 'Computational linguistics']},
 title = {A comparison of linguistic patterns between individuals with current major depressive disorder, past major depressive disorder, and controls in a virtual, psychiatric research interview},
 year = {2023}
}

@Filtered Article{31fe3031-9d9a-4b0a-b90b-de909d548228,
 abstract = {In 2021, Universidad de la República in Uruguay approved a new Chemical Engineering undergraduate program that incorporates novel conceptual definitions such as competency-based education. This paper describes the process of defining the new curriculum plan and presents the program's structure, as well as specific and cross-disciplinary competencies. These competencies are then compared to the learning outcomes established in the guide for programs accreditation of the Institution of Chemical Engineers. To provide practical examples of how the competency-based approach was incorporated into the program, three specific cases are presented. The first case focuses on the implementation of the internship and industry project. The second case illustrates the incorporation of computational tools as an essential part of different courses throughout the degree program. Finally, the third case describes a new design for the fluid mechanics laboratory that emphasizes hands-on learning and helps students develop several competencies.},
 authors = {['E. Castelló', 'C. Santiviago', 'J. Ferreira', 'R. Coniglio', 'E. Budelli', 'V. Larnaudie', 'M. Passeggi', 'I. López']},
 journal = {Education for Chemical Engineers},
 keywords = {['Engineering education', 'Professional skills', 'Unconventional laboratory practice', 'Industrial Internships', 'Autonomous learning']},
 title = {Towards competency-based education in the chemical engineering undergraduate program in Uruguay: Three examples of integrating essential skills},
 year = {2023}
}

@Filtered Article{3200c833-2b24-44a2-a312-59a4733052bf,
 abstract = {Sensorimotor integration is an active domain of speech research and is characterized by two main ideas, that the auditory system is critically involved in speech production and that the motor system is critically involved in speech perception. Despite the complementarity of these ideas, there is little crosstalk between these literatures. We propose an integrative model of the speech-related “dorsal stream” in which sensorimotor interaction primarily supports speech production, in the form of a state feedback control architecture. A critical component of this control system is forward sensory prediction, which affords a natural mechanism for limited motor influence on perception, as recent perceptual research has suggested. Evidence shows that this influence is modulatory but not necessary for speech perception. The neuroanatomy of the proposed circuit is discussed as well as some probable clinical correlates including conduction aphasia, stuttering, and aspects of schizophrenia.},
 authors = {['Gregory Hickok', 'John Houde', 'Feng Rong']},
 journal = {Neuron},
 title = {Sensorimotor Integration in Speech Processing: Computational Basis and Neural Organization},
 year = {2011}
}

@Filtered Article{32a34904-bd43-422d-925f-fc61161bba5a,
 abstract = {Conscious or unbidden, thoughts fill our heads from morning to night. But what are they, and what exactly is thinking? Join philosopher Tim Bayne on a journey into the fantastic, elusive and ceaseless world our minds create},
 authors = {['Tim Bayne']},
 journal = {New Scientist},
 title = {Thought},
 year = {2013}
}

@Filtered Article{337b3645-1cca-4f6c-a598-fb44e8d6930a,
 abstract = {Electroencephalogram (EEG) signals contain various information about the cognitive thinking, emotion, and thoughts of a person. Verbal communication is the normal form of interaction method used, but various kinds of physically disabled people who are not in the condition to express themselves can be assisted using the EEG signal rehabilitation technique. EEG signals can be used effectively in rehabilitation by using brain–computer interfaces (BCIs). BCI is a technology that allows interaction between the brain and a computer. This kind of technique can be used to treat patients with paralyzed muscles and locked in syndromes by helping them interact with others using their EEG signals. The application of BCI can be in medical field, education, and security. In this chapter, all aspects of BCIs are discussed in great detail and also have worked on motor imaginary-based dataset and have used linear discriminant analysis (LDA) algorithm as the classifier, which showed 91% accuracy.},
 authors = {['Anand Mohan', 'R.S. Anand']},
 journal = {Academic Press},
 keywords = {['Brain–computer interface (BCI)', 'EEG', 'Machine learning', 'Motor imagery', 'PSD']},
 title = {Chapter 51 - Exploring the exciting potential and challenges of brain computer interfaces},
 year = {2025}
}

@Filtered Article{33dfa7e0-8943-40d5-904e-7dc6ce8a195b,
 abstract = {Building circular economic systems is crucial to address ecological challenges like climate change. The twin transition suggests that, to maximize the impact of sustainable solutions, humans and (disruptive) technologies need to be effectively integrated. Methods to conceptually build such (eco)systems integrating these and assess their ecological impact before implementation are lacking. This paper addresses this gap by proposing the Circulise framework, a model-driven method designed to build circular systems and evaluate their environmental performance. The approach promotes design-thinking to create socio-technical ecosystems that can be evaluated at the light of their alignment with circular economy and/or sustainability principles and be used to generate operational software behavior. The Circulise framework was developed following the methodological guidance of design science research. It is applied in this paper to the case of Fanyatu, a non-profit organization focused on reforestation in the Congo Basin, showing its ability to create a circular ecosystem not only supporting the creation of regenerative CO2-absorbing forests but also empowering and improving the quality of life of the local communities involved in the planting of trees. In Fanyatu’s case, Circulise’s strategic planning and technology integration lead to virtuous cycles, enabling a snowball effect in forest creation and the promotion of sustainable projects. The framework’s scalability and versatility allow it to be applied across various contexts, enabling the creation of customized circular ecosystems for sustainability tailored to specific human and technological needs.},
 authors = {['Yves Wautelet', 'Xavier Rouget']},
 journal = {Expert Systems with Applications},
 keywords = {['Circulise', 'Circular economy development', 'Cryptocurrency', 'Reforestation', 'Sustainability', 'Sustainability engineering', 'Twin transition']},
 title = {Circulise, a model-driven framework to build and align socio-technical systems for the twin transition: Fanyatu’s case of sustainability in reforestation},
 year = {2025}
}

@Filtered Article{344072d4-8a1a-47b1-9878-3eb97bdf8a19,
 abstract = {Graph convolutional networks (GCN) have become the mainstream in skeleton-based action recognition. For further performance improvement, existing methods propose to utilize self-attention to model long-range features of joints. However, these methods cannot balance accuracy with computational efficiency. In this paper, we propose the Switching Partition Strategy (SPAR) Network that uses the self-attention mechanism for the simultaneous and efficient extraction of spatial–temporal long-range information from the skeleton. We design two partition strategies that reduce the computational cost and improve the efficiency of the computation of self-attention. Extensive experiments are conducted on two large-scale datasets, i.e. NTU RGB+D 60 and NTU RGB+D 120, to evaluate the performance of the proposed SPAR network. The results demonstrate that our method outperforms the state-of-the-art on accuracy as well as computational cost.},
 authors = {['ZiJie Zhu', 'RenDong Ying', 'Fei Wen', 'PeiLin Liu']},
 journal = {Neurocomputing},
 keywords = {['Action recognition', 'Self-attention', '3D-skeleton', 'Graph convolutional networks']},
 title = {SPAR: An efficient self-attention network using Switching Partition Strategy for skeleton-based action recognition},
 year = {2023}
}

@Filtered Article{34a79cef-3488-40c3-bae0-01c3d99363ee,
 abstract = {This paper describes the development of EoS Simulator, a cubic equations of state simulator created in the MATLAB R2022b App Designer platform, which aims to be a practical digital tool for chemical engineering students that facilitates the solution, analysis, and critical thinking about thermodynamic problems. In the simulator, numerical algorithms were implemented based on a theoretical framework, such as fugacity test, bracketing methods, and the calculation of residual properties. EoS Simulator can estimate two-phase envelopes, isobars, isotherms, and surfaces related to PTVHS properties. MATLAB Grader courses were proposed to test student learning using the software in two different workshops. The evaluation was based on the achievement of tasks related to intended learning outcomes. Survey responses about the simulator and learning environment were collected, concluding that most students improved their skills in understanding thermodynamics phenomena, but some improvements are necessary for future versions of the software and online courses.},
 authors = {['Mariola Camacho-Lie', 'Rodrigo Alberto Hernández-Ochoa', 'Adriana Palacios']},
 journal = {Education for Chemical Engineers},
 keywords = {['Teaching of thermodynamics', 'Digital tools in education', 'Deep learning', 'Constructive Alignment', 'Preparation for Future Learning', 'Productive Failure']},
 title = {Development of basic thermodynamics workshops integrating a cubic equations of state simulator and MATLAB Grader courses},
 year = {2024}
}

@Filtered Article{34c6bd4f-38dc-4be6-90e7-5fed10e91d9b,
 abstract = {Computing the topology of an algebraic plane curve C means computing a combinatorial graph that is isotopic to C and thus represents its topology in R2. We prove that, for a polynomial of degree n with integer coefficients bounded by 2ρ, the topology of the induced curve can be computed with Õ(n8ρ(n+ρ)) bit operations (Õ indicates that we omit logarithmic factors). Our analysis improves the previous best known complexity bounds by a factor of n2. The improvement is based on new techniques to compute and refine isolating intervals for the real roots of polynomials, and on the consequent amortized analysis of the critical fibers of the algebraic curve.},
 authors = {['Michael Kerber', 'Michael Sagraloff']},
 journal = {Journal of Symbolic Computation},
 keywords = {['Topology computation', 'Algebraic curve', 'Amortized analysis', 'Complexity analysis']},
 title = {A worst-case bound for topology computation of algebraic curves},
 year = {2012}
}

@Filtered Article{356f7ea4-16ae-464f-ac37-66e51314cadd,
 abstract = {Recent studies from sociocultural perspectives have explored the effects of Concept-based Language Instruction (C-BLI) on L2 development through the explicit teaching of scientific concepts. However, there has been little research into the effects of C-BLI on the development of L2 academic literacy. This article reports on a case study of how C-BLI mediated a Chinese doctoral student's development of conceptual knowledge of context and subsequent context-specific performance in academic writing. Drawing on data from writing tutorials and interviews, the learner's drafts and invited comments, and think-aloud protocols, the study revealed that the C-BLI interventions that integrated symbolic and dialogic mediation helped the learner attain and enhance awareness of contextual components. The learner appropriated the concept as a tool for thinking in judging appropriateness of rules of thumb and choices of exclusive discourse features in specific contexts of use, which consequently mediated his planning for writing and resulted in the development of performance. The study demonstrates the potential of C-BLI as a driving force for the development of conceptual knowledge and context-specific performance in the academic literacy of L2 learners. It has pedagogical implications for curriculum design, C-BLI-informed literacy and concept-based materials, and teacher development to stimulate teacher awareness in C-BLI.},
 authors = {['Jing Chen', 'Danli Li']},
 journal = {Journal of English for Academic Purposes},
 keywords = {['Concept-based language instruction', '', 'Mediation', 'Concept appropriation', 'Writing activity', 'Academic literacy']},
 title = {Instructed concept appropriation for developing knowledge of second language academic discourse context},
 year = {2021}
}

@Filtered Article{3594d953-048b-4358-9b16-dbfc8ed310c4,
 abstract = {Work-integrated learning (WIL) is a pedagogical activity designed to enhance the integration of theoretical knowledge and practical skills in an authentic context. WIL is typically accomplished through work placement, but a non-placement WIL is potentially promising. In this study, a non-placement WIL programme was incorporated into chemical engineering final year projects. The students worked on industrial problems without a work placement. The purpose of the study was to investigate work-related skills learned in a non-placement WIL programme. A qualitative dominant mixed-methods research approach was adopted. Data was collected using a quantitative questionnaire (n = 69) and a qualitative interview (n = 15). Quantitative findings revealed no significant difference between students working on non-placement WIL and academic projects. However, qualitative findings revealed seven insightful work-related skills in the non-placement WIL: (1) professional relationship with industrial experts and academic supervisors, (2) virtual communication and collaboration, (3) technology skills in the latest industrial software and tools, (4) motivation to undertake novel and challenging industrial problems, (5) creative and innovative strategies, (6) application of higher order thinking skills to model authentic problems, (7) inductive and deductive reasoning. The COVID-19 pandemic has changed how engineers work. Today, it is a necessity to embrace creative problem-solving skills and adopt various types of modern technologies to work effectively and remotely.},
 authors = {['Su Ting Yong', 'Nishanth G. Chemmangattuvalappil', 'Dominic C.Y. Foo']},
 journal = {South African Journal of Chemical Engineering},
 keywords = {['Non-placement', 'Work-integrated learning', 'Virtual communication', 'Technology usage', 'Critical thinking', 'Problem solving']},
 title = {Students’ perception of non-placement work-integrated learning in chemical engineering: Work-related skills towards the post-pandemic future},
 year = {2024}
}

@Filtered Article{359cc99d-4aba-4061-9ae7-2b52ef0ab19e,
 abstract = {Despite the well known benefits of physical units, matrices, and matrix algebra in engineering computations, most engineering analysis packages are essentially dimensionless. This paper describes the design and implementation of matrix and finite element stack machines for Aladdin, a new computational environment that embeds units inside matrix and finite element calculations. Functionality of the Aladdin stack machine is illustrated by working step by step through the setup and execution of three examples: (1) Parsing and stack machine execution for x=2in; (2) Deflection analysis of a cantilever beam, and (3) Rollup maneuver for a long cantilever beam.},
 authors = {['Mark A. Austin']},
 journal = {Advances in Engineering Software},
 keywords = {['Stack machine', 'Matrix computations', 'Physical units', 'Scripting language design', 'Finite element analysis']},
 title = {Matrix and finite element stack machines for structural engineering computations with units},
 year = {2006}
}

@Filtered Article{362fe661-7dbe-4b7e-a951-23d68d6f63ef,
 abstract = {The experience of the COVID-19 pandemic, which has accelerated many chaotic processes in modern society, has highlighted in a very serious and urgent way the need to understand complex processes in order to achieve the common well-being. Modern high performance computing technologies, quantum computing, computational intelligence are shown to be extremely efficient and useful in safeguarding the fate of mankind. These technologies are the state-of-the-art of IT evolution and are fundamental to be competitive and efficient today. If a company is familiar with these techniques and technologies, it will be able to deal with any unexpected and complicated scenarios more efficiently and effectively. The main contribution of our work is a set of best practices and case studies that can help the researcher address computationally complex problems. We offer a range of software technologies, from high performance computing to machine learning and quantum computing, which represent today the state-of-the-art to deal with extremely complex computational issues, driven by chaotic events and not easily predictable. In this chapter we analyze the different technologies and applications that will lead mankind to overcome this difficult moment as well as to understand more and more deeply the profound aspects of very complex phenomena. In this environment of rising complexity, in terms of technology, algorithms, and changing lifestyles, it is critical to emphasize the importance of achieving maximum efficiency and outcomes while protecting the integrity of everyone's personal data and respecting the human being as a whole.},
 authors = {['Damiano Perri', 'Marco Simonetti', 'Osvaldo Gervasi', 'Sergio Tasso']},
 journal = {Academic Press},
 keywords = {['Cloud computing', 'Computational intelligence', 'Container', 'High performance computing', 'Machine learning', 'Multi-chaos', 'Neural networks', 'Privacy', 'Quantum computing']},
 title = {Chapter 4 - High-performance computing and computational intelligence applications with a multi-chaos perspective},
 year = {2022}
}

@Filtered Article{3669098a-6cde-4224-99e8-669fc21b451d,
 abstract = {To achieve better classification performance using case-based reasoning classifiers, we propose a retrieval-based revision method with trustworthiness evaluation for problem solving. An improved case evaluation method is employed to evaluate the trustworthiness of the suggested solution after the reuse step, which will divide the target cases and its suggested solutions into a trustworthy set and an untrustworthy set in accordance with a threshold value of trustworthiness. The attribute weights are adjusted by running a genetic algorithm and are used in the second round of retrieval of the untrustworthy set to obtain the classification results. Experimental results demonstrate that our proposed method performs favorably compared with other methods. Also, the proposed method has less computation complexity for the trustworthiness evaluation, and enhances understanding on thinking and inference for case-based reasoning classifiers.},
 authors = {['Aijun Yan', 'Dianhui Wang']},
 journal = {Expert Systems with Applications},
 keywords = {['Case-based reasoning classifiers', 'Classification accuracy', 'Case evaluation', 'Case revision']},
 title = {Trustworthiness evaluation and retrieval-based revision method for case-based reasoning classifiers},
 year = {2015}
}

@Filtered Article{37c6db58-6db8-4672-a844-414bb0021f56,
 abstract = {The demand to satisfy environmental and economic performance requirements of buildings highlights the application of the responsive skin facades in offering superior performance, as compared to conventional façades. With this respect, responsive skins have become a growing field of research during the recent decade while a thorough review of studies investigating their design and technology aspects is still missing. To fill the identified gap, this study aims to present a systematic literature review and state of the art in an untouched research area of the responsive skins, integrated with their geometric and mechanism design approaches. To this end, a total of 89 studies, collected from two major bibliographic databases of Scopus and Google Scholar from the first of 2010 to the mid of 2021, were reviewed and several classifications and analyses on the associated design thinking, skin systems and responsive mechanisms were presented. The gap analysis of the findings indicates that the lack of controllable substitution design for mechanical skins is one of the reasons preventing the application of responsive skins in construction industry. Furthermore, the gap between simulation and constructability and the relationship between the designed skin geometry with climatic analysis and performance provide basis for future studies.},
 authors = {['Saba {Fattahi Tabasi}', 'Saeed Banihashemi']},
 journal = {Frontiers of Architectural Research},
 keywords = {['Responsive skin', 'Architectural design', 'Mechanism design']},
 title = {Design and mechanism of building responsive skins: State-of-the-art and systematic analysis},
 year = {2022}
}

@Filtered Article{37f9b973-ed5f-4f4c-b6e4-fc5e39638f34,
 abstract = {Novel computational methods such as artificial neural networks, adaptive neuro-fuzzy inference systems and genetic programming are used in this chapter for the modeling of the nonlinear behavior of composite laminates subjected to constant amplitude loading. The examined computational methods are stochastic nonlinear regression tools, and can therefore be used to model the fatigue behavior of any material, provided that sufficient data are available for training. They are material independent methods that simply follow the trend of the available data, in each case giving the best estimate of their behavior. Application on a wide range of experimental data gathered after fatigue testing glass/epoxy and glass/polyester laminates proved that their modeling ability compares favorably with, and is to some extent superior to, other modeling techniques.},
 authors = {['Anastasios P. Vassilopoulos', 'Efstratios F. Georgopoulos']},
 journal = {Woodhead Publishing},
 keywords = {['Fatigue', 'Composites', 'Artificial neural network', 'Genetic programming', 'ANFIS', 'S-N curves']},
 title = {10 - Computational intelligence methods for the fatigue life modeling of composite materials},
 year = {2020}
}

@Filtered Article{37f9d760-e569-4715-bdfd-d41da4e55ebb,
 abstract = {Publisher Summary
This chapter discusses creative thinking in design. Design is a creative occupation, and good designers are creative people. One of the most vexing and perennial questions in design education concerns the balance between the free, open-ended, and expressive work demanded of the student and attention to the acquisition of knowledge, discipline, and experience. The effects of experience on problem solving are not always beneficial. In industry, the need to improve already successful products provides the ultimate test of creative thinking. When using personal analogy, the problem solver identifies personally with some part of the problem or solution, thus acting out the situation. Fantasy analogy allows the designer to suspend the sense of credulity and to explore the seemingly fantastic or impossible. Creativity is not only skill or talent but is also related to context—the situation within which the person perceives the problem and performs the process.},
 authors = {['Bryan Lawson']},
 journal = {Butterworth-Heinemann},
 title = {9 - Creative thinking},
 year = {1990}
}

@Filtered Article{383241cc-3617-412d-8b47-6cdb9012c0be,
 abstract = {The market for carbon fibers is forecast to experience a double-digit growth over the next years. The reason for this development can be found in the special characteristics of Carbon Fiber Reinforced Plastics (CFRP) like high stiffness and strength at very low weight which make this composite an ideal material for lightweight design. However, the design of parts made of CFRP is a tightrope walk between costs, mechanical characteristics and manufacturability for product developers. On the one hand, the mechanical properties are highly dependent on the ideal fiber orientation within the part and the unique material characteristics can only be exploited with a suitable fiber orientation, but on the other hand, the ideal fiber orientation is often not manufacturable or the required manufacturing technique is too expensive. Therefore, a novel algorithm to support product developers in finding a manufacturable fiber orientation or patch layout which is as close as possible to the ideal fiber orientation is introduced. This algorithm computes and highlights areas with constant fiber orientation (=cluster) based upon the ideal fiber alignment from the CAIO method. With the help of the visualization of the clusters, product developers can be supported in the decision for the best patch placement and geometry as well as in choosing the best manufacturing technique. It is important to point out that the algorithm is intended for endless fiber reinforced parts only.},
 authors = {['Daniel Klein', 'Kaja Scheler', 'Sandro Wartzack']},
 journal = {Procedia CIRP},
 keywords = {['Lightweight design', 'Early design stages', 'Endless fibre reinforced composites']},
 title = {Computation and Visualization of Patch Geometries for the Design of Carbon Fiber Reinforced Parts at Early Design Stages},
 year = {2014}
}

@Filtered Article{3839e6aa-b14c-49b3-9593-46b804f84d95,
 abstract = {The last decade consolidated the cyberspace as fifth domain of military operations, which extends its preliminarily intelligence and information exchange purposes towards enabling complex offensive and defensive operations supported/supportively of parallel kinetic domain actuations. Although there is a plethora of well documented cases on strategic and operational interventions of cyber commands, the cyber tactical military edge is still a challenge, where cyber fires barely integrate to the traditional joint targeting cycle due to, among others, long planning/development times, asymmetric effects, strict target reachability requirements, or the fast propagation of collateral damage; the latter rapidly deriving on hybrid impacts (political, economic, social, etc.) and evidencing significant socio-technical gaps. In this context, it is expected that Tactical Clouds disruptively facilitate cyber operations at the edge while exposing the rest of the digital assets of the operation to them. On these grounds, the main purpose of the conducted research is to review and in depth analyze the risks and opportunities of jeopardizing the sustainability of the military Tactical Clouds at their cyber edge. Along with a 1) comprehensively formulation of the researched problematic, the study 2) formalizes the Tactical Denial of Sustainability (TDoS) concept; 3) introduces the phasing, potential attack surfaces, terrains and impact of TDoS attacks; 4) emphasizes the related human and socio-technical aspects; 5) analyzes the threats/opportunities inherent to their impact on the cloud energy efficiency; 6) reviews their implications at the military cyber thinking for tactical operations; 7) illustrates five extensive CONOPS that facilitate the understanding of the TDoS concept; and given the high novelty of the discussed topics, this paper 8) paves the way for further research and development actions.},
 authors = {['Marco Antonio {Sotelo Monge}', 'Jorge {Maestre Vidal}']},
 journal = {Future Generation Computer Systems},
 keywords = {['Cyber defense', 'Economical Denial of Sustainability', 'Military operations', 'Situational Awareness', 'Tactical Denial of Sustainability']},
 title = {Conceptualization and cases of study on cyber operations against the sustainability of the tactical edge},
 year = {2021}
}

@Filtered Article{38b4ee50-a474-477f-9d8f-a0277395e787,
 authors = {['Mauro Gallegati', 'Antonio Palestrini', 'Alberto Russo']},
 journal = {Academic Press},
 title = {Chapter 1 - An Introduction to Agent-Based Computational Macroeconomics},
 year = {2017}
}

@Filtered Article{38f3dbe1-b78b-4865-be1f-7695b505016b,
 abstract = {A defining aspect of being human is an ability to reason about the world by generating and adapting ideas and hypotheses. Here we explore how this ability develops by comparing children’s and adults’ active search and explicit hypothesis generation patterns in a task that mimics the open-ended process of scientific induction. In our experiment, 54 children (aged 8.97±1.11) and 50 adults performed inductive inferences about a series of causal rules through active testing. Children were more elaborate in their testing behavior and generated substantially more complex guesses about the hidden rules. We take a ‘computational constructivist’ perspective to explaining these patterns, arguing that these inferences are driven by a combination of thinking (generating and modifying symbolic concepts) and exploring (discovering and investigating patterns in the physical world). We show how this framework and rich new dataset speak to questions about developmental differences in hypothesis generation, active learning and inductive generalization. In particular, we find children’s learning is driven by less fine-tuned construction mechanisms than adults’, resulting in a greater diversity of ideas but less reliable discovery of simple explanations.},
 authors = {['Neil R. Bramley', 'Fei Xu']},
 journal = {Cognition},
 keywords = {['Hypothesis generation', 'Active learning', 'Inductive inference', 'Developmental change', 'Concept learning', 'Program induction']},
 title = {Active inductive inference in children and adults: A constructivist perspective},
 year = {2023}
}

@Filtered Article{39090096-7a5d-4d53-ac29-b297e76bc239,
 abstract = {Blocks-based environments have been used to promote computational thinking (CT) and programming learning mostly in elementary and middle schools. In many countries, like Brazil and Portugal, isolated initiatives have been launched to promote CT learning, but until now there is no evidence of a widespread use of this type of environments. Consequently, it is not common that students that reach higher education nowadays are familiar with CT and programming. This paper presents the development of a serious game to support the learning of basic computer programming. It is a blocks-based environment including also resources that allow the teacher to follow the student’s progress and customize in-game tasks. Four cycles of experiments were conducted, improving both the game and how it was used. Based on the results of these experiences, the key contribution of this paper is a set of fourteen findings and recommendations to the creation and use of a game-based approach to support introductory computer programming learning for novices.},
 authors = {['Adilson Vahldick', 'Paulo Roberto Farah', 'Maria José Marcelino', 'António José Mendes']},
 journal = {Computers in Human Behavior Reports},
 keywords = {['Computer programming learning', 'Blocks-based approach', 'Serious games']},
 title = {A blocks-based serious game to support introductory computer programming in undergraduate education},
 year = {2020}
}

@Filtered Article{39122fb0-a0b8-447b-986d-6653245fbd55,
 abstract = {Appraisal theories of emotion argue that emotions arise from a process of comparing individual needs and concerns to external demands. That is, emotions cannot be explained by solely focusing on the environment or by solely focusing on the individual. Rather, they reflect an individual’s subjective assessment of their relation to the environment. Appraisal theories further posit this relationship is characterized by the individual (appraised) in terms of a set of criteria, variously called appraisal variables, checks, or dimensions; for example, Is an event congruent with a person’s goals or concerns?; Who or what caused it?; Was it unexpected?; and What control does the person have over its unfolding? The results of these appraisal checks are in turn mapped to emotion. In this chapter, we do not explore appraisal as a theory of emotion elicitation. Rather, we suggest appraisal theories, specifically the criteria that appraisal theories posit, provide a useful framework for characterizing how situations are perceived by a person and influence their behavior. We go on to suggest that computational models of appraisal can provide a clear specification of how these criteria are assessed. Furthermore, such models can help us explore the dynamics of the person-environment relation, specifically how changes in the environment as well as changes in the person’s perceptions and behavior arise and induce those dynamics.},
 authors = {['Nutchanon Yongsatianchot', 'Stacy Marsella']},
 journal = {Academic Press},
 title = {Chapter 19 - Computational models of appraisal to understand the person-situation relation},
 year = {2021}
}

@Filtered Article{39ced81b-d75d-4522-9cf2-b483b3f953f0,
 abstract = {Dielectric properties of heterogeneous materials for various condensed-matter systems have been gaining world-wide attention over the past 50 or so years in the design (or engineering) of materials structures for desired properties and functional purposes. These applications range from cable and current limiters to sensors. These multiscale systems lead to challenging problems of connecting micro- or meso-structural features to macroscopic materials response, i.e. permittivity, conductivity. This article first reviews progress made at that time of the underlying physics of dielectric heterostructures and points out the missing elements that have led to a resurgence of interest in these and related materials. Recent advances in computational electromagnetics provide unparalleled control over morphology in this class of materials to produce a seemingly unlimited number of exquisitely structured materials endowed with tailored electromagnetic, and other physical properties. In the text to follow, we illustrate how an ab initio computational technique can be used to accurately characterize structure–dielectric property relationships of periodic heterostructures in the quasistatic limit. More specifically, we have carried out two-dimensional (2D) and three-dimensional (3D) numerical studies of two-component materials in which equal-sized inclusions, with shape and orientation and possibly fused together, are fixed in a periodic square (2D) or cubic (3D) array. Boundary-integral equations (BIE) are derived from Green's theorem and are solved for the local field with appropriate periodicity conditions on a unit cell of the structures using the field calculation package PHI3D. A number of illustrative examples shows how this computational technique can provide very accurate predictions for the complex effective permittivity of translationally-invariant heterostructures. The performance of the method is also compared with those of other computational and analytical techniques. We comment on how this computational method helps identify some important characteristics for rationalizing and predicting the structure of composite materials in terms of the nature, size, shape and orientation of their constituents.},
 authors = {['C. Brosseau', 'A. Beroual']},
 journal = {Progress in Materials Science},
 title = {Computational electromagnetics and the rational design of new dielectric heterostructures},
 year = {2003}
}

@Filtered Article{3a158771-a2ac-4f48-9b2d-fce4da9d7dbe,
 abstract = {We address the need to develop efficient algorithms for numerical simulation of models, based in part or entirely on adaptive resonance theory. We introduce modifications that speed up the computation of the gated dipole field (GDF) in the Exact ART neural network. The speed increase of our solution amounts to at least an order of magnitude for fields with more than 100 gated dipoles. We adopt a ‘divide and rule’ approach towards the original GDF differential equations by grouping them into three categories, and modify each category in a separate way. We decouple the slow-dynamics part — the neurotransmitters from the rest of system, solve their equations analytically, and adapt the solution to the remaining fast-dynamics processes. Part of the node activations are integrated by an unsophisticated numerical procedure switched on and off according to rules. The remaining activations are calculated at equilibrium. We implement this logic in a Generalized Net (GN) — a tool for parallel processes simulation which enables a fresh look at developing efficient models. Our software implementation of generalized nets appears to add little computational overhead.},
 authors = {['George Mengov', 'Kalin Georgiev', 'Stefan Pulov', 'Trifon Trifonov', 'Krassimir Atanassov']},
 journal = {Neural Networks},
 keywords = {['Gated dipole field', 'Adaptive resonance theory', 'Generalized net']},
 title = {Fast computation of a gated dipole field},
 year = {2006}
}

@Filtered Article{3a18b34f-8bdc-4b27-ba8b-0246380a51b8,
 abstract = {Online Food Delivery Platforms (OFDPs) has witnessed phenomenal growth in the past few years, especially this year due to the COVID-19 pandemic. This Pandemic has forced many governments across the world to give momentum to OFD services and make their presence among the customers. The Presence of several multinational and national companies in this sector has enhanced the competition and companies are trying to adapt various marketing strategies and exploring the brand experience (BEX) dimension that helps in enhancing the brand equity (BE) of OFDPs. BEXs are critical for building brand loyalty (BL) and making companies profitable. Customers can experience different kinds of brand experiences through feeling, emotions, affection, behavior, and intellect. The present research work is taken up to analyze the factors affecting BEX and its impact on BL and BE of the OFDPs and analyze the mediating role of BL in the relationship between BEX and BE of the OFDPs in the Indian context. A survey of 457 Indian customers was carried out. A questionnaire was used for data collection and a mediation study was used to test hypothesized relationships. Our computational analysis reveals that BEX influences the BL and BE of OFDPs. The study further indicates that BL mediates the relationship between BEX and BE of OFDPs. The effective marketing and relationship management practices will help company to enhance BEX that will enable in enhancing BL and raising BE of their product. It therefore provides a more thorough analysis of BEX constructs and their consequences than previous research. Some of the managerial implication, limitations, and scope of future research are also presented in the study.},
 authors = {['Sufyan Habib', 'Nawaf N. Hamadneh', 'S. Al wadi', 'Ra’ed Masa’deh']},
 journal = {Computers, Materials and Continua},
 keywords = {['Hypothesis tests', 'brand experience', 'online food delivery platform', 'statistical tests', 'COVID-19']},
 title = {Computation Analysis of Brand Experience Dimensions: Indian Online Food Delivery Platforms},
 year = {2020}
}

@Filtered Article{3a236a6b-9e2a-48a0-93a3-bb0d4bb707d9,
 abstract = {Applied artificial intelligence, particularly large language models, in biomedical research is accelerating, but effective discovery and validation requires a toolset without limitations or bias. On January 30, 2023, the National Academies of Sciences, Engineering, and Medicine (NAS) appointed an ad hoc committee to identify the needs and opportunities to advance the mathematical, statistical, and computational foundations of digital twins in applications across science, medicine, engineering, and society. On December 15, 2023, the NAS released a 164-page report, “Foundational Research Gaps and Future Directions for Digital Twins.” This report described the importance of using digital twins in biomedical research. The current study was designed to develop an innovative method that incorporated phenotype-ranking algorithms with knowledge engineering via a biomimetic digital twin ecosystem. This ecosystem applied real-world reasoning principles to nonnormalized, raw data to identify hidden or "dark" data. Clinical exome sequencing study on patients with endometriosis indicated four variants of unknown clinical significance potentially associated with endometriosis-related disorders in nearly all patients analyzed. One variant of unknown clinical significance was identified in all patient samples and could be a biomarker for diagnostics. To the best of our knowledge, this is the first study to incorporate the recommendations of the NAS to biomedical research. This method can be used to understand the mechanisms of any disease, for virtual clinical trials, and to identify effective new therapies.},
 authors = {['William G. Kearns', 'Georgios Stamoulis', 'Joseph Glick', 'Lawrence Baisch', 'Andrew Benner', 'Dalton Brough', 'Luke Du', 'Bradford Wilson', 'Laura Kearns', 'Nicholas Ng', 'Maya Seshan', 'Raymond Anchan']},
 journal = {The Journal of Molecular Diagnostics},
 title = {The Application of Knowledge Engineering via the Use of a Biomimetic Digital Twin Ecosystem, Phenotype-Driven Variant Analysis, and Exome Sequencing to Understand the Molecular Mechanisms of Disease},
 year = {2024}
}

@Filtered Article{3a2f36c2-8f50-4126-ad87-b48433f82485,
 abstract = {Evolutionary computation is a field of study of computational systems which uses ideas and gets inspirations from natural evolution and adaptation. Although the history of evolutionary computation can be traced back to 1950s, it was only in the last decade or so that the field started to grow rapidly. In recent years, there have been many successful applications of various evolutionary computation techniques in artificial intelligence, machine learning, numerical optimization, combinatorial optimization, etc. The theory of evolutionary computation has also been enriched greatly. There is a much better understanding of why and how evolutionary computation techniques work (or do not work) than five or six years ago. This article reports some of the latest developments presented at the recent 1999 Congress on Evolutionary Computation (CEC '99).},
 authors = {['Xin Yao']},
 journal = {Cognitive Systems Research},
 title = {Evolutionary computation comes of age},
 year = {1999}
}

@Filtered Article{3a628f46-cc27-4544-bf1a-a2cd6f1111d0,
 abstract = {Improving the flexible and deep peak shaving capability of supercritical (SC) unit under full operating conditions to adapt a larger-scale renewable energy integrated into the power grid is the main choice of novel power system. However, it is particularly challenging to establish an accurate SC unit model under large-scale variable loads and deep peak shaving. To this end, a data-driven modeling strategy combining Transformer-Extra Long (Transformer-XL) and quantum chaotic nutcracker optimization algorithm is proposed. Firstly, three models of the SC unit under once-through/recirculation/shut-down are built via analyzing its mechanism of the operation process, respectively. Secondly, the superior performance of Transformer-XL in obtaining global feature information is employed to effectively solve the problem of high information dependence caused by the strong coupling and nonlinearity of SC unit. Then, the improved quantum chaotic nutcracker optimization algorithm with higher search accuracy is proposed to obtain the optimal parameters of Transformer-XL based on the logistic chaotic mapping and quantum thinking. Feature information dependencies and optimal parameter settings are fully considered in the proposed modeling scheme, which results in an accurate model of SC unit under full operating conditions. Finally, various simulations and comparisons are conducted based on the on-site data of 600 MW SC unit to demonstrate the superiority of the proposed data-driven modeling strategy. According to the improved Transformer-XL, the mean square errors of the proposed SC unit model under once-through/recirculation/shut-down modes are less than 2.500E-03, which verifies the high accuracy of the model. Consequently, the developed model is suitable for application in the controller designing and the operating efficiency and flexibility improvement of SC unit.},
 authors = {['Guolian Hou', 'Tianhao Zhang', 'Ting Huang']},
 journal = {ISA Transactions},
 keywords = {['Supercritical unit', 'Transformer-XL', 'Once-through/recirculation/shut-down mode', 'Quantum chaotic nutcracker optimization algorithm']},
 title = {Data-driven modeling of 600 MW supercritical unit under full operating conditions based on Transformer-XL},
 year = {2025}
}

@Filtered Article{3aac3088-3b56-48aa-a0c5-ca09a5561fb2,
 abstract = {The present work is undertaken to investigate the molecular interaction between Memantine (Alzheimer's drug) and DL-Alanine (amino acid) using DFT and vibrational spectroscopic methods, in particular, Fourier Transform Infrared Spectroscopy (FTIR) and Raman techniques. The DFT calculations of these molecules are carried out using the B3LYP/6–311 ++ G (d, p) level of theory. The experimental FTIR and Raman spectra of the molecules are compared to the respective DFT computed wavenumbers. A satisfactory agreement is obtained between experimental and computed wavenumbers. Further, HOMO-LUMO energy gap, Natural Bond Orbital (NBO) analysis, total energy, zero-point vibrational energy, Molecular Electrostatic Potential (MEP), chemical potential, hardness, ionization energy, global electrophilicity index, dipole moments, and first-order hyperpolarizabilities of the interacting state are reported and compared to the respective parameters of the individual states. The NBO analysis of the molecules indicates the transfer of charge between DL-Alanine and Memantine through NH•••O intermolecular hydrogen bonds. The molecular docking studies of the molecules are  performed to investigate the binding affinity of the ligand with the 6DG7 receptor.},
 authors = {['Jyotshna Saikia', 'Bhargab Borah', 'Th. Gomti Devi']},
 journal = {Journal of Molecular Structure},
 keywords = {['DL-Alanine', 'Memantine', 'Raman', 'FTIR', 'DFT']},
 title = {Study of interacting mechanism of amino acid and Alzheimer's drug using vibrational techniques and computational method},
 year = {2021}
}

@Filtered Article{3ac061e7-c873-4de9-8a00-e9d787243eb9,
 abstract = {Increasing evidence suggests that altered states of consciousness (ASC) are associated with both positive and negative effects on components of creative performance, and convergent and divergent thinking in particular. We provide a metacontrol framework that allows characterizing factors that induce ASC in terms of their general impact on the information processing style of problem solvers. We discuss behavioral and neuronal findings from three areas that reflect strong connections between ASC and the underlying effects on metacontrol on the one hand and components of creativity on the other hand: drug-induced ASC, meditation-induced ASC, and hallucinations. While more, and especially more systematic research is needed, we identify a general trend, suggesting that factors that induce ASC are likely to alter the metacontrol state by biasing it toward either persistence, which is beneficial for convergent thinking and other persistence-heavy operations, or flexibility, which is beneficial for divergent thinking and other flexibility-heavy operations.},
 authors = {['Luisa Prochazkova', 'Bernhard Hommel']},
 journal = {Academic Press},
 keywords = {['Altered states of consciousness (ASC)', 'Cannabis', 'Convergent thinking', 'Creativity', 'Divergent thinking', 'Hallucinations', 'Meditation', 'Metactontrol', 'Psychedelics']},
 title = {Chapter 6 - Altered states of consciousness and creativity},
 year = {2020}
}

@Filtered Article{3b54c5d3-6250-477a-a942-0b536cf640af,
 abstract = {Coupling of life-cycle thinking with urban metabolism (UM) has the potential to improve sustainable urban planning. Current urban metabolism models are largely ‘black-box’ methods which do not reveal the non-linearity of feedback loops and complex internal dynamics of urban systems. The integration of system dynamics (SD) with UM based on a life-cycle thinking approach can provide built environment professionals (e.g. town planners, civil engineers, architects) with a ‘transparent-box’ solution for assessing the potential of urban projects, plans, and their implementation. This paper describes the development of a method that integrates input-output (IO) table flows with SD modelling to improve the completeness of UM assessments. This modelling framework can also allow for a ‘nested’ multi-region assessment which takes into account sustainability burdens consequent to urban system changes occurring elsewhere in the national and/or global economy. Pros and cons of this proposal are showcased by the illustration of a model for Lisbon.},
 authors = {['Thomas Elliot', 'Benedetto Rugani', 'Javier Babí Almenar', 'Samuel Niza']},
 journal = {Procedia CIRP},
 keywords = {['Life-cycle thinking', 'system dynamics', 'urban metabolism', 'urban planning']},
 title = {A Proposal to Integrate System Dynamics and Carbon Metabolism for Urban Planning},
 year = {2018}
}

@Filtered Article{3b85257b-1053-4216-b2f0-dbf8b3be01ec,
 abstract = {Achieving the objectives of sustainable development in water and agri-food systems requires the utilisation of decision-support tools in stakeholder-driven processes to construct and simulate various scenarios and evaluate the outcomes of associated policy interventions. While it is common practice to involve stakeholders in participatory modelling processes, their comprehensive documentation and the lessons learned remain scarce. In this paper, we share our experience of engaging stakeholders throughout the entire system dynamics modelling process. We draw on two projects implemented in the Volta River Basin, West Africa, to understand the dynamics of water and agri-food systems under changing environmental and socioeconomic conditions. We outline eight key insights and lessons as practical guides derived from each stage of the participatory modelling process, including the pre-workshop stage, problem definition, model conceptualization, simulation model formulation, model testing and verification, and policy design and evaluation. Our findings demonstrate that stakeholders can actively contribute to all phases of the system dynamics modelling process, including parameter estimation, sensitivity analysis, and numerical simulation experiments. However, we encountered notable challenges, including the time-intensive nature of the process, the struggle to reach a consensus on the modelled problem, and the difficulty of translating the conceptual model into a simulation model using stock and flow diagrams – all of which were addressed through a structured facilitation process. While the projects were anchored in the specific context of West Africa, the key lessons and insights highlighted have broader significance, particularly for researchers employing PSDM in regions characterised by multifaceted human-environmental systems and where stakeholder involvement is crucial for holistic understanding and effective policy interventions. This paper contributes practical guidance for future efforts with participatory modelling, particularly in regions worldwide grappling with sustainable development challenges in water and agri-food systems, and where stakeholder involvement is crucial for holistic understanding of the multiple challenges and for designing effective policy interventions.},
 authors = {['Julius H. Kotir', 'Renata Jagustovic', 'George Papachristos', 'Robert B. Zougmore', 'Aad Kessler', 'Martin Reynolds', 'Mathieu Ouedraogo', 'Coen J. Ritsema', 'Ammar Abdul Aziz', 'Ron Johnstone']},
 journal = {Journal of Cleaner Production},
 keywords = {['Africa', 'Group model building', 'Systems thinking', 'Stakeholder engagement', 'System modelling', 'Sustainable development']},
 title = {Field experiences and lessons learned from applying participatory system dynamics modelling to sustainable water and agri-food systems},
 year = {2024}
}

@Filtered Article{3bb74e83-684c-4246-9317-4946a80b140a,
 abstract = {The prediction of surface ozone is essential attributing to its impact on human and environmental health. Volatile organic compounds (VOCs) are crucial in driving ozone concentration; particularly in urban areas where VOC limited regimes are prominent. The limited measurements of VOCs, however, hinder assessing the VOC-ozone relationship. This work applies machine learning (ML) algorithms for temporal forecasting of surface ozone over a metropolitan city in India. The availability of continuous VOCs measurement data along with meteorology and other pollutants during 2014–2016 makes it possible to deduce the influence of various input parameters on surface ozone prediction. After evaluating the best ML model for ozone prediction, simulations were carried out using varied input combinations. The combination with isoprene, meteorology, NOx, and CO (Isop + MNC) was the best with RMSE 4.41 ppbv and MAPE 6.77%. A season-wise comparison of simulations having all data, only meteorological data and Isop + MNC as input showed that Isop + MNC simulation gives the best results during the summer season (RMSE: 5.86 ppbv, MAPE: 7.05%). This shows the increased ability of the model to capture ozone peaks (high ozone during summer) relatively better when isoprene data is used. The overall results highlight that using all available data doesn't necessarily give best prediction results; also critical thinking is essential when evaluating the model results.},
 authors = {['Ritesh Kalbande', 'Bipin Kumar', 'Sujit Maji', 'Ravi Yadav', 'Kaustubh Atey', 'Devendra Singh Rathore', 'Gufran Beig']},
 journal = {Chemosphere},
 keywords = {['Ozone', 'VOCs', 'Machine learning', 'Meteorology', 'Isoprene']},
 title = {Machine learning based quantification of VOC contribution in surface ozone prediction},
 year = {2023}
}

@Filtered Article{3bd7bce5-2937-42b7-b702-a737120bb68e,
 abstract = {This research methods article introduces the open source PERSUADE 2.0 corpus. The PERSUADE 2.0 corpus comprises over 25,000 argumentative essays produced by 6th-12th grade students in the United States for 15 prompts on two writing tasks: independent and source-based writing. The PERSUADE 2.0 corpus also provides detailed individual and demographic information for each writer. The goal of the PERSUADE 2.0 corpus is to advance research into relationships between discourse elements, their effectiveness, writing quality, writing tasks and prompts, and demographic and individual differences.},
 authors = {['S.A. Crossley', 'Y. Tian', 'P. Baffour', 'A. Franklin', 'M. Benner', 'U. Boser']},
 journal = {Assessing Writing},
 keywords = {['Corpus linguistics', 'Writing assessment', 'Argumentation', 'Individual differences']},
 title = {A large-scale corpus for assessing written argumentation: PERSUADE 2.0},
 year = {2024}
}

@Filtered Article{3bf75730-f23e-46b7-83e6-447088974c0b,
 abstract = {Using diffusion to define distances between points on a manifold (or a sampled data set) has been successfully employed in various applications such as data organization and approximately isometric embedding of high dimensional data in low dimensional Euclidean space. Recently, P. Jones has proposed a diffusion distance which is both intuitively appealing and scales appropriately with increasing time. In the first part of our paper, we present an efficient tree-based approach to computing an approximation to Jonesʼs diffusion distance. We also show our approximation is comparable to Jonesʼs distance. Neither Jonesʼs distance, nor our approximation, satisfies the triangle inequality; in particular, in the case of heat flow on Rn, Jonesʼs separation distance gives a scaled square of the Euclidean distance. In the second part of our paper, we present a general construction to obtain an “almost” metric from a general distance. We also discuss a numerical procedure to implement our construction. Additionally, we show that in the case of heat flow on Rn, we recover (scaled) Euclidean distance from Jonesʼs distance.},
 authors = {['Maxim J. Goldberg', 'Seonja Kim']},
 journal = {Applied and Computational Harmonic Analysis},
 keywords = {['Tree', 'Diffusion', 'Distance', 'Metric']},
 title = {An efficient tree-based computation of a metric comparable to a natural diffusion distance},
 year = {2012}
}

@Filtered Article{3c75d383-af78-4723-9d30-3fca6e95e608,
 abstract = {Software piracy represents a major damage to the moral fabric associated with the respect of intellectual property. The rate of software piracy appears to be increasing globally, suggesting that additional research that uses new approaches is necessary to evaluate the problem. The study remedies previous econometric and methodological shortcomings by applying Bayesian, robust and evolutionary computation robust regression algorithms to formally test empirical literature on software piracy. To gain further insights into software piracy at the global level, the study also uses five neuro-computational intelligence methodologies: multi-layer perceptron neural network (MLP), probabilistic neural network (PNN), radial basis function neural network (RBF), generalized regression neural network (GRNN) and Kohonen’s self-organizing maps (SOM) to classify, predict and cluster software piracy rates among 102 nations. At the empirical level, this research shows that software piracy is significantly affected by the wealth of nation as measured by gross domestic product (GDP), the nation’s expenditure on research and development and the nation’s judicial efficiency. At the methodological level, this research shows that neuro-computational models outperform traditional statistical techniques such as regression analysis, discriminant analysis and cluster analysis in predicting, classifying and clustering software piracy rates due to their robustness and flexibility of modeling algorithms.},
 authors = {['Mohamed M. Mostafa']},
 journal = {Expert Systems with Applications},
 keywords = {['Global software piracy', 'Ethical behavior', 'Neural networks', 'Bayesian regression', 'Evolutionary computation models']},
 title = {A neuro-computational intelligence analysis of the global consumer software piracy rates},
 year = {2011}
}

@Filtered Article{3cba3893-591d-4c09-87b6-9d2ce24eb6d8,
 abstract = {Theoretical proposals suggest that emotional intelligence (EI) may favor creativity. In the present paper, two studies are reported with French adults to examine the degree to which the ability to identify emotion is related to creative performance. This component of ability EI was hypothesized to be positively associated with a divergent thinking task involving emotional information. Contrary to our expectations, the first study (n=95) indicated that ability to identify emotions in faces and images was negatively related to idea generation ability. The second study (n=100) including a measure of alexithymia confirmed this relation. Moreover, evaluating emotional creativity, we observed a significant negative link between the ability to identify emotions and the tendency to experience emotions differently from those of others. We discuss these results suggesting an opposition between consensual/convergent thinking concerning emotions (ability EI) and divergent thinking.},
 authors = {['F. Zenasni', 'T.I. Lubart']},
 journal = {Personality and Individual Differences},
 keywords = {['Creativity', 'Ability EI', 'Alexythimia', 'Emotional creativity', 'Divergent thinking']},
 title = {Perception of emotion, alexithymia and creative potential},
 year = {2009}
}

@Filtered Article{3cd33350-ce49-4aed-a219-e3bfa912ae23,
 abstract = {Improving flood resilience of communities requires a holistic understanding of risks and resilience options as well as the preferences and priorities of different stakeholders. Innovations in risk and resilience assessment have helped communities to identify gaps in their flood risk management strategy but selecting and implementing resilience solutions remains a big challenge for many decision-makers. In addition to traditional appraisals and cost-benefit assessments this also calls for a participatory process in which various stakeholders are encouraged to adopt a system-level approach in identifying interventions that can maximise a range of benefits and co-benefits. In this study, we investigate how a combination of modelling and measurement methods can help decision-makers with their flood resilience strategies. We apply a participatory system thinking approach combining Fuzzy Cognitive Mapping (FCM) with a flood resilience measurement framework called Flood Resilience Measurement for Communities (FRMC). We first investigate stakeholders' biases on flood resilience interventions, and then lead them through a system thinking exercise using FCM and FRMC to elicit mental models representing important aspects of flood resilience and their interrelation. These are then aggregated, representing the collective perceptions and knowledge of stakeholders, and used to identify the most beneficial resilience actions in terms of direct and indirect impacts on flood resilience. We apply this approach to the case of Lowestoft, a coastal town in England exposed to significant flood risk. Developed in close collaboration with the local authorities, the ambition is to support decision-making on flood resilience interventions. We find that this combination of methods enables system-level thinking and inclusive decision-making about flood resilience which can ultimately encourage transformative decisions on prioritization of actions and investments.},
 authors = {['Sara Mehryar', 'Swenja Surminski']},
 journal = {Science of The Total Environment},
 keywords = {['Flood resilience', 'Participatory decision making', 'Resilience measurement tool', 'Mind mapping', 'Fuzzy cognitive mapping']},
 title = {Investigating flood resilience perceptions and supporting collective decision-making through fuzzy cognitive mapping},
 year = {2022}
}

@Filtered Article{3d2d7c94-e5f5-402f-aa12-38a3faf2ca68,
 abstract = {The retina confers upon us the gift of vision, enabling us to perceive the world in a manner unparalleled by any other tissue. Experimental and clinical studies have provided great insight into the physiology and biochemistry of the retina; however, there are questions which cannot be answered using these methods alone. Mathematical and computational techniques can provide complementary insight into this inherently complex and nonlinear system. They allow us to characterise and predict the behaviour of the retina, as well as to test hypotheses which are experimentally intractable. In this review, we survey some of the key theoretical models of the retina in the healthy, developmental and diseased states. The main insights derived from each of these modelling studies are highlighted, as are model predictions which have yet to be tested, and data which need to be gathered to inform future modelling work. Possible directions for future research are also discussed. Whilst the present modelling studies have achieved great success in unravelling the workings of the retina, they have yet to achieve their full potential. For this to happen, greater involvement with the modelling community is required, and stronger collaborations forged between experimentalists, clinicians and theoreticians. It is hoped that, in addition to bringing the fruits of current modelling studies to the attention of the ophthalmological community, this review will encourage many such future collaborations.},
 authors = {['Paul A. Roberts', 'Eamonn A. Gaffney', 'Philip J. Luthert', 'Alexander J.E. Foss', 'Helen M. Byrne']},
 journal = {Progress in Retinal and Eye Research},
 keywords = {['Oxygen', 'Neuroglobin', 'Photoreceptors', 'Angiogenesis', 'Retinitis pigmentosa', 'Choroidal neovascularisation']},
 title = {Mathematical and computational models of the retina in health, development and disease},
 year = {2016}
}

@Filtered Article{3d8868d8-3b51-4f61-a264-23bf73e3f641,
 abstract = {Publisher Summary
This chapter introduces some of the vocabulary and concepts used in designing computer systems. It also introduces the “systems perspective,” a way of thinking about systems that is global and encompassing rather than focused on particular issues. The usual course of study of computer science and engineering begins with linguistic constructs for describing computations (software) and physical constructs for realizing computations (hardware). To develop applications that have these requirements, the designer must look beyond the software and hardware and view the computer system as a whole. In doing so, the designer encounters many new problems—so many that the limit on the scope of computer systems generally arises neither from laws of physics nor from theoretical impossibility, but rather from limitations of human understanding.},
 authors = {['Jerome H. Saltzer', 'M. Frans Kaashoek']},
 journal = {Morgan Kaufmann},
 title = {Chapter 1 - Systems},
 year = {2009}
}

@Filtered Article{3deb62c3-7588-4374-85a9-9ebb8bed96d9,
 abstract = {Coacervates show promise in drug delivery systems due to their biocompatibility, versatility, and outstanding ability to penetrate cells. With the advent of all-atom (AA) and coarse-grained (CG) models, these computational tools function as ‘computational microscopes’, providing valuable insights to complement experimental research. This review covers the latest innovations in coacervate-based drug delivery systems. It summarizes the molecular properties and phase behavior of coacervates composed of polyelectrolytes, intrinsically disordered proteins (IDPs), and other biomolecules, along with their interactions with carried drugs and cell membranes. Additionally, this review highlights current challenges and limitations in this fast-moving field and proposes potential avenues for future research.},
 authors = {['Yang Liu', 'Rongrong Zou', 'Yiwei Wang', 'Minghao Wang', 'Fan Fan', 'Yeqiang Zhou', 'Huixu Xie', 'Mingming Ding']},
 journal = {Precision Medicine and Engineering},
 keywords = {['Drug delivery', 'Coacervate', 'Molecular dynamics', 'Machine learning', 'Protein encapsulation']},
 title = {Investigating coacervates as drug carriers using molecular dynamics},
 year = {2024}
}

@Filtered Article{3e0a2aa3-7a04-41dc-98d3-1d8792a49dc1,
 abstract = {MXenes are a newly emerging family of two-dimensional (2D) materials that include carbonitrides, nitrides, and carbides of transition metals. They have attracted much interest from scientists and researchers due to their potential use in electrocatalysts, where a two-electron transfer process is applied. Their remarkable properties, such as strong chemical and structural stability, high electrical conductivity, and large active surface area, make them effective for their potential in advanced hydrogen evolution reactions (HER). This thorough analysis starts by carefully outlining the forward-thinking advances in MXene synthesis and development. It then explores the theoretical and empirical aspects of MXene-based HER electrocatalysts. This review paper presents methods for improving the HER catalytic activity of MXene, including terminal modification, metal-atom doping, and the creation of various nanostructures to increase the density of active sites. The study clarifies current issues and new opportunities and provides a valuable framework for the future development of effective MXene-based electrocatalysts for HERs.},
 authors = {['Kunjal Soni', 'Rakesh Kumar Ameta']},
 journal = {Journal of Alloys and Compounds Communications},
 keywords = {['MXenes', '2D materials', 'Two-electron transfer process', 'Hydrogen evolution process', 'Electrocatalysts']},
 title = {Advancements in MXene-based electrocatalysts for hydrogen evolution reaction processes: A comprehensive review},
 year = {2024}
}

@Filtered Article{3e3df886-ae71-4944-8a0a-c6824345d9ac,
 abstract = {Current Mathematics Curriculum concerns are focused on students’ needs to think mathematically rather than just mathematical computation. Students should be able to develop more complex, abstract, and powerful mathematical structures. This can dramatically enable them to solve a broad variety of meaningful problems. Furthermore, students ought to become autonomous and self-motivated in their mathematical activities such as acquiring mathematical concepts, skills and problem solving; meta-cognitively aware of their mathematical thinking; highly motivated in mathematics learning and develop positive attitudes towards mathematical task. To achieve this learning goal, an investigation into efficient learning mode, the problem-based learning (PBL) was undertaken. PBL has been successfully applied in medical, engineering, economics, and accounting field but lack of evidence in mathematics field. This study examined possible outcomes of PBL among postgraduate students who were taking Educational Statistic course. Three statistic tests were employed to assess the students’ performance in statistic learning. The Meta-cognitive Awareness Inventory (MAI), which comprises of 52 items was used to assess the students’ meta-cognitive strategy in solving Educational Statistics problems. Students’ motivation towards the PBL learning was measured by Keller's Motivational Design Questionnaire with 36 items. Comparison of students’ performance based on three tests showed that there is significant diffrence between mean performance (F [2,28]=5.571, p<0.05). In addition, results indicated that there is significant positive effects on students meta-cognitive awareness (t [30]=3.358, p<0.05) and on students motivation level (t [30]=2.484, p<0.05) after undergoing PBL intervention.},
 authors = {['Rohani Ahmad Tarmizi', 'Sahar Bayat']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Problem-based learning', 'E-learning', 'Statistic learning performance', 'Mental load']},
 title = {Effects of Problem-based Learning Approach in Learning of Statistics among University Students},
 year = {2010}
}

@Filtered Article{3e8b0779-86fd-466f-a67d-42cc1a35aea0,
 abstract = {Where ordinary experiments are impossible and observational data scarce and indirect—particularly in paleoecosystems—computational experiments are often our only means to learn about reality. There are good arguments to count such model-based predictions as evidence, testing hypotheses and updating our beliefs about the world. However, the epistemic weight of computational experiments depends on an adequate model representation of the target system, transparency about predictive uncertainty, and the avoidance of confirmation bias. I argue that mechanistic models are particularly suited for paleoecological predictions but that iterative uncertainty analyses should guide their development. Using a Bayesian framework I propose preregistration and blinded analysis as tools to strengthen the epistemic value of computational experiments. Here, a preregistration marks the boundary between exploratory model development, which establishes credence in the model, and predictive model application, which tests hypotheses. As good modeling practice I suggest clarifying epistemic goals at the outset of a project and accordingly choose methods to maximize the epistemic weight of the computational experiment.},
 authors = {['Wolfgang Traylor']},
 journal = {Ecological Modelling},
 keywords = {['Epistemology', 'Bayesian', 'Preregistration', 'Blinding', 'Uncertainty analysis']},
 title = {Model-based experiments as epistemic evidence in paleoecology},
 year = {2024}
}

@Filtered Article{3f56b4ee-6aee-4a8c-b01c-889d03bd7923,
 abstract = {This paper presents a novel evolutionary design approach in a multi-agent design environment. Multi-agent system architecture offers a promising framework for dynamically managing cooperative agents in a distributed environment while the tree structure based generic algorithm provides a foundation for supporting evolutionary and innovative design abilities. Design is a complex knowledge discovery process. Creative design is a human trait that is not easily converted into a computational tool. Rather than to implement the innovative design by computers, this environment is used to stimulate the imagination of designers and extend their thinking space. It wants to explore a feasible and useful evolutionary approach in a distributed environment that will give the designers concrete help for the creative designs. This approach is illustrated by a mobile phone design example, which used binary algebraic expression tree to form sketch shapes and a feature based product tree to produce component combination choices. Because evolution is guided by human selectors, the evolutionary algorithm is not complex. It shows that approach is able to generate some creative solutions, demonstrating the power of explorative evolution.},
 authors = {['Hong Liu', 'Mingxi Tang']},
 journal = {Applied Soft Computing},
 keywords = {['Multi-agent system', 'Evolutionary computing', 'Generic algorithm', 'Computer-aided design', 'Creative design']},
 title = {Evolutionary design in a multi-agent design environment},
 year = {2006}
}

@Filtered Article{3f78962f-2e35-48fa-b73a-13627ee36dca,
 abstract = {This paper explores how models can support productive thinking. For us a model is a thing, a tool to help make sense of something. We restrict attention to specific models for whole-number multiplication, hence the wording of the title. They support evolving thinking in large measure through the ways their users redesign them. They assume new forms, come to be seen and understood in different ways. We show how work that learners do with models can help them to transform, not simply their understanding of key concepts, but also how they come to view themselves as thinkers and learners, as collaborators in a social process that their work and thinking help to constitute. We draw on recent research on core knowledge, especially by Carey, Spelke, and Tomasello, to clarify how models, as we view them here, can underpin specific actions that support emerging understanding.},
 authors = {['Bob Speiser', 'Chuck Walter']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Model', 'Representation', 'Presentation', 'Operator product', 'Controlled variable', 'Frame', 'Core knowledge', 'Analog magnitude', 'Parallel individuation', 'Shared intentionality']},
 title = {Models for products},
 year = {2011}
}

@Filtered Article{3fc5ad39-6319-4737-a892-fa5e20f23f2c,
 abstract = {In this paper, we discuss the importance of information systems in modeling interactive computations performed on (complex) granules and we propose a formal approach to interactive computations based on generalized information systems and rough sets which can be combined with other soft computing paradigms such as fuzzy sets or evolutionary computing, but also with machine learning and data mining techniques. Information systems are treated as dynamic granules used for representing the results of the interaction of attributes with the environment. Two kinds of attributes are distinguished, namely, the perception attributes, including sensory attributes, and the action attributes. Sensory attributes are the basic perception attributes, other perception attributes are constructed on the basis of the sensory ones. Actions are activated when their guards, being often complex and vague concepts, are satisfied to a satisfactory degree. The guards can be approximated on the basis of measurements performed by sensory attributes rather than defined exactly. Satisfiability degrees for guards are results of reasoning called the adaptive judgment. The approximations are induced using hierarchical modeling. We show that information systems can be used for modeling more advanced forms of interactions in hierarchical modeling. The role of hierarchical interactions is emphasized in the modeling of interactive computations. Some illustrative examples of interactions used in the ACT-R 6.0 system are reported. ACT-R 6.0 is based on a cognitive architecture and can be treated as an example of a highly interactive complex granule which can be involved in hierarchical interactions. For modeling of interactive computations, we propose much more general information systems than the studied dynamic information systems (see, e.g., Ciucci (2010) [8] and Pałasiński and Pancerz (2010) [32]). For example, the dynamic information systems are making it possible to consider incremental changes in information systems. However, they do not contain the perception and action attributes necessary for modeling interactive computations, in particular for modeling intrastep interactions.},
 authors = {['Andrzej Skowron', 'Piotr Wasilewski']},
 journal = {Theoretical Computer Science},
 keywords = {['Interactive computing', 'Interactive systems', 'Multi-agent systems', 'Rough sets', 'Granular computing', 'Wisdom technology']},
 title = {Information systems in modeling interactive computations on granules},
 year = {2011}
}

@Filtered Article{3ffab066-7cc2-4f66-b559-d4aea53c7074,
 authors = {['G. Post', 'D. Bonazzi']},
 journal = {Engineering Geology},
 title = {Latest thinking on the Malpasset accident},
 year = {1987}
}

@Filtered Article{40153726-3c42-45c6-b56e-f48c96e34009,
 authors = {['Daniel Sui']},
 journal = {Computers, Environment and Urban Systems},
 title = {Terrae Incognitae and Limits of Computation: Whither GIScience?},
 year = {2001}
}

@Filtered Article{40a1c821-d65e-4eac-9b16-819cd057b2c5,
 abstract = {Abstract The methodological characteristics of the chronotopologic data analysis chain are discussed. Various kinds of knowledge are considered and properly classified, and several illustrative examples in applied sciences are presented. Big data and data-driven analyses are critically reviewed, and their implementation carefully assessed. Data scale types are classifications considered in property- and attribute-oriented settings. Classical statistics inadequacies are pointed out and the need of a chronotopology-dependent statistics is outlined. The chronotopologic visualization thinking mode and techniques are briefly reviewed.},
 authors = {['Jiaping Wu', 'Junyu He', 'George Christakos']},
 journal = {Elsevier},
 keywords = {['Methodological chain', 'Knowledge bases', 'Big data', 'Scales', 'Visualization', 'Chronotopologic statistics']},
 title = {Chapter 3 - CTDA methodology},
 year = {2022}
}

@Filtered Article{40e5ee95-04b3-4608-9228-c6c14223ba9f,
 abstract = {This position paper is an outcome of discussions that took place at the third FIPSE Symposium in Rhodes, Greece, between June 20–22, 2016 (http://fi-in-pse.org). The FIPSE objective is to discuss open research challenges in topics of Process Systems Engineering (PSE). Here, we discuss the societal and industrial context in which systems thinking and Process Systems Engineering provide indispensable skills and tools for generating innovative solutions to complex problems. We further highlight the present and future challenges that require systems approaches and tools to address not only ‘grand’ challenges but any complex socio-technical challenge. The current state of Process Systems Engineering (PSE) education in the area of chemical and biochemical engineering is considered. We discuss approaches and content at both the unit learning level and at the curriculum level that will enhance the graduates’ capabilities to meet the future challenges they will be facing. PSE principles are important in their own right, but importantly they provide significant opportunities to aid the integration of learning in the basic and engineering sciences across the whole curriculum. This fact is crucial in curriculum design and implementation, such that our graduates benefit to the maximum extent from their learning.},
 authors = {['Ian T. Cameron', 'Sebastian Engell', 'Christos Georgakis', 'Norbert Asprion', 'Dominique Bonvin', 'Furong Gao', 'Dimitrios I. Gerogiorgis', 'Ignacio E. Grossmann', 'Sandro Macchietto', 'Heinz A. Preisig', 'Brent R. Young']},
 journal = {Computers & Chemical Engineering},
 title = {Education in Process Systems Engineering: Why it matters more than ever and how it can be structured},
 year = {2019}
}

@Filtered Article{414d95ac-c74d-4565-bc6a-0df34511827d,
 abstract = {Steady state and transient characteristics of a natural circulation loop working with water are obtained. For this purpose, 3D steady state and transient CFD simulations are performed. The CFD model includes pipe thickness as well as secondary side coolant passage apart from primary side. Steady state and transient characteristics are computed for various configurations i.e. Vertical Heater Vertical Cooler (VHVC), Horizontal Heater Horizontal Cooler (HHHC), etc. Steady state data was compared with available correlations. Flow initiation transients were compared with experimental data. Both the steady state and transient results are found to be in good agreement with previously published data. The reason for formation of unidirectional and bi-directional pulsing in HHHC configuration at different powers is explained with the help of temperature fields at different instants of time. Effect of sudden power rise/power step back on instability in HHHC configuration is estimated using CFD simulations.},
 authors = {['Jayaraj Yallappa Kudariyawar', 'Abhijeet Mohan Vaidya', 'Naresh Kumar Maheshwari', 'Polepalle Satyamurthy']},
 journal = {International Journal of Thermal Sciences},
 keywords = {['Natural circulation loop', '3D CFD simulation', 'Instability']},
 title = {Computational study of instabilities in a rectangular natural circulation loop using 3D CFD simulation},
 year = {2016}
}

@Filtered Article{41c14bf7-f12c-4a66-901c-23f1e9a1d713,
 abstract = {Publisher Summary
One of the central challenges of computer science is to get a computer to solve a problem without programming it explicitly. The challenge is to create an automatic system whose input is a high-level statement of a problem's requirements and whose output is a satisfactory solution to the given problem. This challenge is the common goal of such fields of research as artificial intelligence and machine learning. Paraphrasing Arthur Samuel, this challenge addresses the question: How can computers are made to do what needs to be done, without being told exactly how to do it? As Samuel further explained: “The aim is to get machines to exhibit behavior, which if done by humans, would be assumed to involve the use of intelligence.” This chapter provides an affirmative answer to the following two questions: Starting only with a high-level statement of the problem's requirements, can computers automatically discover the solution to nontrivial problems? And, can automatically created solutions be competitive with the products of human creativity and inventiveness? In answering these questions, this chapter focuses on a biologically inspired domain-independent problem-solving technique of evolutionary computation, called genetic programming. For each problem, genetic programming automatically creates entities that improve on previously patented inventions, or duplicate the functionality of previously patented inventions or duplicate the functionality of previously patented inventions. The chapter also discusses the importance of illogic in achieving creativity and inventiveness.},
 authors = {['John R. Koza', 'Forrest H. Bennett', 'David Andre', 'Martin A. Keane']},
 journal = {Morgan Kaufmann},
 title = {Chapter 10 - Genetic Programming: Biologically Inspired Computation That Exhibits Creativity in Producing Human-Competitive Results},
 year = {2002}
}

@Filtered Article{41c2dabe-ebf1-4c3f-979d-adc19dcb6fa1,
 authors = {['Jennifer Hallinan']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['explicit models of memory', 'stochastic generative approach', 'evolution of the neural modularity', 'metarepresentation']},
 title = {Thinking Beyond the Fringe},
 year = {2001}
}

@Filtered Article{41d38b93-5781-4125-85a1-9f6eaaa11290,
 abstract = {Neuropsychological data suggest that being overweight or obese is associated with a tendency to perseverate behavior despite negative feedback. This deficit might be observed due to other cognitive factors, such as working memory (WM) deficits or decreased ability to deduce model-based strategies when learning by trial-and-error. In the present study, a group of subjects with overweight or obesity (Ow/Ob, n = 30) was compared to normal-weight individuals (n = 42) in a modified Reinforcement Learning (RL) task. The task was designed to control WM effects on learning by manipulating cognitive load and to foster model-based learning via deductive reasoning. Computational modelling and analysis were conducted to isolate parameters related to RL mechanisms, WM use, and model-based learning (deduction parameter). Results showed that subjects with Ow/Ob had a higher number of perseverative errors and used a weaker deduction mechanism in their performance than control individuals, indicating impairments in negative reinforcement and model-based learning, whereas WM impairments were not responsible for deficits in RL. The present data suggests that obesity is associated with impairments in negative reinforcement and model-based learning.},
 authors = {['Gibson Weydmann', 'Igor Palmieri', 'Reinaldo A.G. Simões', 'Samara Buchmann', 'Eduardo Schmidt', 'Paulina Alves', 'Lisiane Bizarro']},
 journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
 keywords = {['Overweight', 'Reinforcement Learning', 'Working Memory', 'Computational Modelling']},
 title = {Disentangling negative reinforcement, working memory, and deductive reasoning deficits in elevated BMI},
 year = {2025}
}

@Filtered Article{41f366ee-2996-4f2f-b258-c6d271cc7660,
 abstract = {The past decade has witnessed the emergence of a new scientific discipline––computational social and organizational science. Within organization science in particular, and social science more generally, scientists and practitioners are turning to computational analysis to address fundamental socio-technical problems that are so complex and dynamic that they cannot be fully addressed by traditional techniques. Consequently, there is an explosion of computational models, computationally generated findings, interest in doing simulation, and a dearth of support for this enterprise. This paper contains discussions of the underlying fundamental perspective, the relation of models to empirical data and characteristics of necessary infrastructure.},
 authors = {['Kathleen M Carley']},
 journal = {Simulation Modelling Practice and Theory},
 keywords = {['Computational modeling', 'Simulation', 'Organization science', 'Organizational design', 'Organizational learning']},
 title = {Computational organizational science and organizational engineering},
 year = {2002}
}

@Filtered Article{4208cc90-102e-493d-a1ff-1b5a17e5c0e5,
 abstract = {Publisher Summary
This chapter analyzes the four methods of capital allocation for operational risk by the Basel Committee on Banking Supervision: (1) the basic indicator approach, (2) standard approach, (3) internal measurement approach, and (4) loss distribution approach. The chapter also explains why the internal measurement approach and loss distribution approach require leadership in databasing and datamining. The chapter depicts these four methods for computation of operational risk charges on a double scale: expected amount of capital allocation and complexity. Valid solutions take account of different perspectives and definitions of operational risk. The way operational risk is managed is affected by the manner in it is viewed, added with how the board and CEO come to grips with operational risk, the skills and tools at the regulators' disposal, and the resolve to put the risk under lock and key. This is the reason why Basel II wants banks to put aside capital for operational risk control.},
 authors = {['Dimitris N. Chorafas']},
 journal = {Butterworth-Heinemann},
 title = {7 - Five models by the Basel Committee for computation of operational risk},
 year = {2004}
}

@Filtered Article{4218add5-24d4-4908-bf6a-e4c23c18273f,
 authors = {['Michaël J.P Selby']},
 journal = {Journal of Economic Dynamics and Control},
 title = {Computational Aspects of Complex Securities},
 year = {2000}
}

@Filtered Article{42199cca-9ac5-4fb2-8d88-fd8486ad9319,
 abstract = {A mechanism underlying the computational properties of the cognitive architecture is construed based on a minimal list of operational clusters. This general processing mechanism constitutes the dynamic infrastructure of mind (DIM). DIM consists in categories of mental operations foundational for learning that contain inborn components called inner operations, which are self-developing in the interaction mind-environment. Within the DIM paradigm, the input cognitive systems are not domain specific or core-knowledge specific, they are operational specific and capable of further developments that become domain specific while experiencing the environment. Arguments for this construal come from three sources: literature review, data collected through classroom observations, and a four-year experimental study of teaching and learning mathematics in primary grades. The outcomes of that experiment led to a methodology of learning based on activating the operational infrastructure of mind, which enhances students' flexibility of thinking and predicts the capacity to solve creatively a variety of problems.},
 authors = {['Florence Mihaela Singer']},
 journal = {New Ideas in Psychology},
 keywords = {['Cognitive architecture', 'Dynamic infrastructure of mind', 'Learning', 'Operational categories']},
 title = {The dynamic infrastructure of mind—A hypothesis and some of its applications},
 year = {2009}
}

@Filtered Article{4225d924-6ce6-416b-9100-da763bec52d0,
 abstract = {Identifying influential nodes in social networks has significant applications in terms of social analysis and information dissemination. How to capture the crucial features of influential nodes without increasing the computational complexity is an urgent issue to be solved in the context of big data. Laplacian centrality (LC) measures nodal influence by computing nodes' degree, making it extremely low complexity. However, there is still significant room for improvement. Consequently, we propose the improved Laplacian centrality (ILC) to identify influential nodes based on the concept of self-consistent. Identifying results on 9 real networks prove that ILC is superior to LC and other 6 classical measures in terms of ranking accuracy, top-k nodes identification and discrimination capability. Moreover, the computational complexity of ILC has not significantly increased compared to LC, and remains the linear order of magnitude O(m). Additionally, ILC has excellent robustness and universality such that there is no need to adjust parameters according to different network structures.},
 authors = {['Xiaoyu Zhu', 'Rongxia Hao']},
 journal = {Chaos, Solitons & Fractals},
 keywords = {['Social network', 'Influential nodes', 'Centrality measure', 'Improved Laplacian centrality']},
 title = {Identifying influential nodes in social networks via improved Laplacian centrality},
 year = {2024}
}

@Filtered Article{422a3141-7b4e-4008-a952-65ebf77dec1c,
 abstract = {Energy conservation and emissions reduction in the construction industry are important steps in achieving China's goals of peak carbon emissions by 2030 and carbon neutrality by 2060. The premise for building carbon emission (CE) reduction is to produce accurate CE calculations. Existing calculation methods for building CEs have many problems, such as complicated calculations, large data demands, time-consuming and laborious processes, weak design orientation of results, and poor feedback on emission reduction. At the same time, the calculation of CEs during the process of architectural design faces obstacles such as uncertainty of information, incomplete data, and difficulty in obtaining a bill of quantities based on design information. To resolve these obstacles, this study, based on a designer's vocabulary and thinking mode, describes the construction of a “design-oriented” calculation methods for building-embodied carbon emissions (ECEs). The prediction and assessment of the impact on the building environment during the architectural design process were helpful for identifying the key areas for carbon reduction, exploring potential emission reduction hotspots, and providing timely feedback for design optimization, which can have important theoretical value and practical significance in promoting the construction of low-carbon buildings.},
 authors = {['Mei Lu', 'Zhixing Luo', 'Yujie Cang', 'Nan Zhang', 'Liu Yang']},
 journal = {Fundamental Research},
 keywords = {['Design process', 'embodied carbon emissions', 'calculation methods', 'conceptual design', 'scheme design', 'construction drawing design']},
 title = {Methods for Calculating Building-Embodied Carbon Emissions for the Whole Design Process},
 year = {2024}
}

@Filtered Article{42a8c9eb-f362-4eee-b51f-68d4cd464eee,
 abstract = {In recent years, the bandwidth constraints in satellite Internet of Things (IoT) applications have spurred the development of novel methods for compressing transmitted speech. For satellite voice communications, it is essential to achieve high-quality codecs with a bit rate below 1 kbps, particularly for channels such as Beidou-3, which often operate under such limitations. Neural network-based vocoders have emerged as a promising solution within the AI community, offering high-fidelity audio compression. In this paper, we propose LMCodec2, a causal speech codec designed to operate across a range of bit rates while delivering high-quality audio at extremely low bit rates, specifically tailored for satellite voice transmission. LMCodec2 utilizes a Transformer-based language model to predict tokens frame by frame, achieving a 25 % reduction in bit rate without compromising decoded audio quality. Our experimental evaluations demonstrate that LMCodec2 produces high-quality decoded audio at 0.76 kbps and 1.15 kbps. Notably, at 0.76 kbps, LMCodec2 achieves a MUSHRA (Multi-Stimulus Test with Hidden Reference and Anchor) score that surpasses Encodec's performance at 1.5 kbps. Audio demonstrations, including real-world self-recorded speech datasets, are available at https://dingweipeng.github.io/JACK.github.io. LMCodec2 provides a new way of thinking to addressing the challenges of bandwidth-limited satellite voice communications.},
 authors = {['Dingwei Peng', 'Qizhen Weng', 'Ningze Zhong', 'Ting Xie', 'Can Gong', 'Xiangwei Zhu', 'Xuelin Yuan', 'Mingjun Ouyang']},
 journal = {Computers and Electrical Engineering},
 keywords = {['End-to-end codec', 'VQ-VAE', 'GAN', 'Transformer model', 'Huffman coding']},
 title = {LMCodec2: Ultra-low bit rate codec with causal multiple transformers},
 year = {2025}
}

@Filtered Article{42ef1dcd-d629-4b0f-ae9a-8d18c9707f35,
 abstract = {Mindwandering (MW) is associated with both positive and negative outcomes. Among the latter, negative mood and negative cognitions have been reported. However, the underlying mechanisms linking mindwandering to negative mood and cognition are still unclear. We hypothesized that MW could either directly enhance negative thinking or indirectly heighten the accessibility of negative thoughts. In an undergraduate sample (n=79) we measured emotional thoughts during the Sustained Attention on Response Task (SART) which induces MW, and accessibility of negative cognitions by means of the Scrambled Sentences Task (SST) after the task. We also measured depressive symptoms and rumination. Results show that in individuals with elevated levels of depressive symptoms MW during SART predicts higher accessibility of negative thoughts after the task, rather than negative thinking during the task. These findings contribute to our understanding of the underlying mechanisms of MW and provide insight into the relationship between task-involvement and affect.},
 authors = {['Igor Marchetti', 'Ernst H.W. Koster', 'Rudi {De Raedt}']},
 journal = {Consciousness and Cognition},
 keywords = {['Mindwandering', 'Negative cognitions', 'Mood', 'Depression', 'Individual differences']},
 title = {Mindwandering heightens the accessibility of negative relative to positive thought},
 year = {2012}
}

@Filtered Article{42f92d7d-f39f-489b-8b0c-123fdf0e26cc,
 abstract = {A computational model with three interacting components for context sensitive reinforcement learning, context processing and automation can autonomously learn a focus attention and a shift attention task. The performance of the model is similar to that of normal children, and when a single parameter is changed, the performance on the two tasks approaches that of autistic children.},
 authors = {['Petra Björne', 'Christian Balkenius']},
 journal = {Cognitive Systems Research},
 keywords = {['Autism', 'Attention', 'Computational model']},
 title = {A model of attentional impairments in autism: first steps toward a computational theory},
 year = {2005}
}

@Filtered Article{43c9002d-f638-484e-b4fa-922cd3989f6e,
 abstract = {Metaphor has had a prolific presence in sociology: as a subject of empirical study, as a tool for theorizing, and as ideas that live public lives beyond the ivory tower. Despite all this work, tracing the core dynamics of metaphors over time remains a persistent challenge. To address this problem, I propose a systematic analysis of metaphor anchored around four core questions. What do metaphors describe? How do metaphors change over time? What do metaphors do? And how do conventional metaphors emerge? I address these questions in the context of one persistent and prolific metaphor: the network metaphor. To do so, I offer a formal model of the career of metaphor by drawing on cognitive linguistics and sociological theories of public ideas and professional careers. Specifically, I integrate the Google Books Ngram corpus with computational techniques that leverage co-occurrence to visualize and interpret the career of the network metaphor with respect to four areas: its jurisdiction, temporality, ecology, and conventionality. I leverage the network metaphor as both a revelatory case for evaluating my model and a critical case for deepening our historical understanding of the network. My analysis lends validation to the argument that the network metaphor has achieved the status of a broad category for thinking about contemporary life. However, it also demonstrates that the network metaphor had a lively history prior to the twentieth century. This includes significant changes in how it was used in the first half of the nineteenth century, a career standing in for anatomical structures of the body in the mid-nineteenth century, and a predominant expression in simile form prior to the start of the twentieth century.},
 authors = {['Vincent Yung']},
 journal = {Poetics},
 keywords = {['Metaphor', 'Network', 'Formal model', 'Computational hermeneutics', 'Text analysis']},
 title = {A visual approach to interpreting the career of the network metaphor},
 year = {2021}
}

@Filtered Article{43c9e616-116a-4d9e-95ff-1e3864825391,
 abstract = {The level of moral development and moral intensity in cognitive psychology will not only affect the ethical behavior of accountants, but also have a direct impact on the quality and level of accounting work. Therefore, in this paper, the ethical behavior of accountants was analyzed from the perspective of cognitive psychology. Computer-aided data mining techniques were introduced, and government accounting risk assessment management of financial accountants was studied. In this paper, the principle of cognitive psychology to measure the ethical level of accountants was first described. The predicament of moral judgments was analyzed and an optimization plan to improve the ethical intention of accountants was proposed. Support Vector Machine classification technology in data mining was studied to explore how to conduct effective and reliable evaluation, so as to provide a scientific basis for decision-making in improving accounting management. After the simulation experiment, it is proved that continuously improving the ethical standards of accountants and strengthening the forecast of accounting risks can continue to optimize the accounting office management.},
 authors = {['Jiyou Li']},
 journal = {Cognitive Systems Research},
 keywords = {['Computational linguistics', 'Accounting optimization', 'Research']},
 title = {Government accounting optimization based on computational linguistics},
 year = {2019}
}

@Filtered Article{43e28076-3816-441d-8523-3c00bd894424,
 abstract = {Feature production norms are collected by asking participants to name the defining features of a concept, such as tail, fur, and meow for the concept cat. Collection and analysis of features should include special considerations for methodology and data processing including the task instructions, segmentation, word removal, spell checking and more. The processed data can be used to define concept similarity, control stimuli for new experimental studies, and in computational models of memory and knowledge representation.},
 authors = {['Erin M. Buchanan']},
 journal = {Elsevier},
 keywords = {['Datasets', 'Features', 'Knowledge', 'Memory', 'Norms', 'Production task', 'Semantics']},
 title = {Semantic Feature Production Norms},
 year = {2024}
}

@Filtered Article{43f2a721-8445-48bd-b14a-5813990505ba,
 abstract = {This research was conducted to confirm the feasibility of a STEAM education program in which the mathematics, physics, and Korean traditional arts underlying the hanok roofline are investigated using educational tools of GeoGebra and 4Dframe. This paper contends that this program has the potential to engage students in knowledge restructuring regarding the perception of the hanok’s architectural beauty, Newton's concept of gravity, and mathematical functions. The Octagonal Pavilion in Tapgol Park in Seoul, South Korea, a representative hanok, was used as an educational resource. GeoGebra is used to determine that the roofline of the Octagonal Pavilion generally follows the formula of the catenary curve and then the roofline is modelled using 4Dframe. The catenary form of the roofline of the hanok is linked to the Korean sense of beauty in the pursuit of naturalness under the influence of gravity and organically harmonizes with the environment. The class described in this study, in which the curve of the roofline of the Octagonal Pavilion is explored using GeoGebra and 4Dframe, can help develop creative and critical thinking in students in the context of STEAM education. The findings of this study have the potential to expand the scope of STEAM education to include content for creative education.},
 authors = {['Hyunshik Ju', 'Hogul Park', 'Eun Young Jung', 'Seoung-Hey Paik']},
 journal = {Thinking Skills and Creativity},
 keywords = {['STEAM education program', 'Creativity', 'Catenary curve', 'Korean traditional architecture', 'Modelling', 'GeoGebra', '4Dframe']},
 title = {Proposal for a STEAM education program for creativity exploring the roofline of a hanok using GeoGebra and 4Dframe},
 year = {2022}
}

@Filtered Article{441feaa2-df8e-442f-a261-51d27121e138,
 abstract = {An intricate relation exists between gene trees and species phylogenies, due to evolutionary processes that act on the genes within and across the branches of the species phylogeny. From an analytical perspective, gene trees serve as character states for inferring accurate species phylogenies, and species phylogenies serve as a backdrop against which gene trees are contrasted for elucidating evolutionary processes and parameters. In a 1997 paper, Maddison discussed this relation, reviewed the signatures left by three major evolutionary processes on the gene trees, and surveyed parsimony and likelihood criteria for utilizing these signatures to elucidate computationally this relation. Here, I review progress that has been made in developing computational methods for analyses under these two criteria, and survey remaining challenges.},
 authors = {['Luay Nakhleh']},
 journal = {Trends in Ecology & Evolution},
 title = {Computational approaches to species phylogeny inference and gene tree reconciliation},
 year = {2013}
}

@Filtered Article{442cdf88-8813-4b39-bf84-234b94a937e3,
 abstract = {The continuous integration of young neurons into the adult brain represents a novel form of structural plasticity and has inspired the creation of numerous computational models to understand the functional role of adult neurogenesis. These computational models consist of abstract models that focus on the utility of new neurons in simple neural networks and biologically based models constrained by anatomical data that explore the role of new neurons in specific neural circuits such as the hippocampus. Simulation results from both classes of models have suggested a number of theoretical roles for neurogenesis such as increasing the capacity to learn novel information, promoting temporal context encoding, and influencing pattern separation. In this review, we discuss strategies and findings of past computational modeling efforts, current challenges and limitations, and new computational approaches pertinent to modeling adult neurogenesis.},
 authors = {['Kristofor D. Carlson', 'Fred Rothganger', 'James B. Aimone']},
 journal = {Academic Press},
 keywords = {['Adult neurogenesis', 'structural plasticity', 'computational neural model', 'hippocampus', 'dentate gyrus']},
 title = {Chapter 20 - Computational Perspectives on Adult Neurogenesis},
 year = {2017}
}

@Filtered Article{45273861-aa87-4b60-b349-daf9487f8487,
 abstract = {This study uses actor-oriented transfer perspective to investigate different ways in which students make connections across the domains of mathematics and computing. We interview first-year students at the University of Oslo as they work with a set of tutorials that we designed to integrate knowledge from both domains. The cases we present here demonstrate four different types of cross-domain connections: (a) mathematically reproducing the work of a computer program, (b) cyclically improving a program to produce better output, (c) coupling math to output to justify program improvements and (d) coupling math to code to justify program design. We provide rich examples of the ways in which students make these connections and discuss affordances for mathematical learning in this context.},
 authors = {['Odd Petter Sand', 'Elise Lockwood', 'Marcos D. Caballero', 'Knut Mørken']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Computing', 'Modeling', 'Programming', 'Thinking and learning', 'Connections', 'Undergraduate students']},
 title = {Three cases that demonstrate how students connect the domains of mathematics and computing},
 year = {2022}
}

@Filtered Article{456edaeb-814f-4f7a-9d3a-1c42041adf19,
 abstract = {Misophonia is a disorder in which specific common sounds such as another person breathing or chewing, or the ticking of a clock, cause an atypical negative emotional response. Affected individuals may experience anger, irritability, annoyance, disgust, and anxiety, as well as physiological autonomic responses, and may find everyday environments and contexts to be unbearable in which their ‘misophonic stimuli’ (often called ‘trigger sounds’) are present. Misophonia is gradually being recognized as a genuine problem that causes significant distress and has negative consequences for individuals and their families. It has only recently come under scientific scrutiny, as researchers and clinicians are establishing its prevalence, distinguishing it from other disorders of sensory sensitivity such as hyperacusis, establishing its neurobiological bases, and evaluating the effectiveness of potential treatments. While ideas abound as to the mechanisms involved in misophonia, few have coalesced into models. The aim of the present work is to summarize and extend recent thinking on the mechanistic basis of misophonia, with a focus on moving towards neurologically-informed cognitive models that can (a) account for extant findings, and (b) generate testable predictions. We hope this work will facilitate future refinements in our understanding of misophonia, and ultimately inform treatments.},
 authors = {['Marie-Anick Savard', 'Emily B.J. Coffey']},
 journal = {Hearing Research},
 keywords = {['Misophonia', 'Cognitive science', 'Cognitive neuroscience', 'Sound sensitivity', 'Models']},
 title = {Toward cognitive models of misophonia},
 year = {2025}
}

@Filtered Article{45b8a13e-c4b7-4d0d-9ba3-a222238a87df,
 abstract = {This mixed-method case study examined the potential of computer-assisted, math game making activities in facilitating design-based math learning for school children. Sixty-four middle school children participated in Scratch-based, math game making activities. Data were collected via activity and conversation observation, artifact analysis, interviewing, and survey. The study findings indicated that participants developed significantly more positive dispositions toward mathematics after computer game making. The study also found that experience-driven game design processes helped to activate children's reflection on everyday mathematical experiences. Mathematical thinking and content experience were intertwined within the process of computer game authoring. On the other hand, children designers were involved in game-world and story crafting more than mathematical representation. And it was still challenging for them to perform computer game coding with abstract reasoning.},
 authors = {['Fengfeng Ke']},
 journal = {Computers & Education},
 keywords = {['Learning by design', 'Game-based learning', 'Mathematical disposition', 'Thinking mathematically', 'Computer game making']},
 title = {An implementation of design-based learning through creating educational computer games: A case study on mathematics learning during design and computing},
 year = {2014}
}

@Filtered Article{4654dea5-11c0-4a05-9e0e-cdbffa6c0e44,
 abstract = {CONTEXT
Big data applications in agriculture evolve fast, as more experience, applications, good practices and computational power become available. Actual solutions to real-life problems are scarce. What characterizes the adoption of big data problems to solutions and to what extent is there a match between them?
OBJECTIVE
We aim to assess the conditions of the adoption of big data technologies in agricultural applications, based on the investigation of twelve real-life practical use cases in the precision agriculture and livestock domain.
METHODS
We use a mixed method approach: a case study research around the twelve use cases of Horizon 2020 project CYBELE, varying from precision arable and livestock farming to fishing and food security, and a stakeholder survey (n = 56). Our analysis focuses on four perspectives: (1) the drivers of change that initiated the use cases; (2) the big data characteristics of the problem; (3) the technological maturity level of the solution both at start and end of the project; (4) the stakeholder perspective.
RESULTS AND CONCLUSIONS
Results show that the use cases’ drivers of change are a combination of data-, technology, research- and commercial interests; most have at least a research drive. The big data characteristics (volume, velocity, variety, veracity) are well-represented, with most emphasis on velocity and variety. Technology readiness levels show that the majority of use cases started at experimental or lab environment stage and aims at a technical maturity of real-world small-scale deployment. Stakeholders’ main concern is cost, user friendliness and to embed the solution within their current work practice. The adoption of better-matching big data solutions is modest. Big data solutions do not work out-of-the-box when changing application domains. Additional technology development is needed for addressing the idiosyncrasies of agricultural applications.
SIGNIFICANCE
We add a practical, empirical assessment of the current status of big data problems and solutions to the existing body of mainly theoretical knowledge. We considered the CYBELE research project as our laboratory for this. Our strength is that we interviewed the use case representatives in person, and that we included the stakeholders’ perspective in our results. Large-scale deployments need effective interdisciplinary approaches and long-term project horizons to address issues emerging from big data characteristics, and to avoid compartmentalization of agricultural sciences. We need both an engineering perspective – to make things work in practice – and a systems thinking perspective – to offer holistic, integrated solutions.},
 authors = {['Sjoukje A. Osinga', 'Dilli Paudel', 'Spiros A. Mouzakitis', 'Ioannis N. Athanasiadis']},
 journal = {Agricultural Systems},
 keywords = {['Big data solutions', 'Precision Agriculture', 'Case study', 'Stakeholders', 'Technological maturity level', 'Mixed-method approach']},
 title = {Big data in agriculture: Between opportunity and solution},
 year = {2022}
}

@Filtered Article{4692065b-2d0c-4be3-836e-ca2aa057407e,
 abstract = {This study delves into the intricate psycholinguistic mechanisms that underpin image formation within the educational network discourse, with a specific focus on the dynamics of suggestion and manipulation. In an era where digital communication reigns supreme, understanding how language shapes perceptions and influences behavior is paramount. This research seeks to unravel the complex interplay between suggestion, manipulation, and the formation of images within educational networks. Drawing from insights across disciplines such as psychology, linguistics, and communication studies, this study examines how linguistic cues and contextual factors interact to shape individuals' perceptions and responses within educational settings. Acknowledging the transformative power of language in shaping attitudes, beliefs, and actions, this study aims to shed light on the subtle yet profound ways in which educators employ linguistic strategies to influence discourse within educational networks. By employing a multifaceted approach that integrates theoretical frameworks with empirical analysis, this research endeavors to uncover the underlying mechanisms driving suggestion and manipulation within educational discourse. Through a meticulous examination of textual elements, discourse patterns, and communicative strategies employed by educators in digital environments, this study seeks to elucidate the intricate processes involved in image formation. By exploring the role of suggestion and manipulation in shaping perceptions, attitudes, and behaviors, this research contributes to a deeper understanding of the psycholinguistic underpinnings of educational network discourse. Furthermore, this study not only offers theoretical insights but also practical implications for educators, policymakers, and practitioners involved in educational communication. By highlighting the ethical considerations and implications of linguistic manipulation within educational networks, this research aims to empower stakeholders to navigate digital discourse with greater awareness and discernment. In conclusion, this study represents a significant contribution to the field of thinking skills and creativity by offering new insights into the psycholinguistic dynamics of image formation within educational networks. By unraveling the complexities of suggestion and manipulation, this research opens avenues for further inquiry and underscores the importance of critical thinking and creativity in navigating contemporary digital landscapes.},
 authors = {['Hanna Truba', 'Sergii Khrapatyi', 'Kyrylo Harashchuk', 'Dmytro Shvets', 'Alina Proskurnia']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Psycholinguistics', 'Image', 'Suggestion', 'Manipulation', 'Attraction', 'Fascination']},
 title = {Psycholinguistic underpinnings of image formation: Suggestion and manipulation in the educational network discourse},
 year = {2024}
}

@Filtered Article{46f2632a-afbd-4bd9-8cde-a62a7ad2cf6e,
 abstract = {In the context of architectural education, design studio projects generally begin with the research of design themes and contexts; however, few attempts have been made to appreciate site analysis as reliable architectural design research. This study aims to explore strategies that link site analysis and design application to bridge the gap between research and representation as a framework for applying architectural design education. To bridge this knowledge gap, this study describes four phases of site analysis—(1) site selection, (2) site survey, (3) problem identification, and (4) suggestion for the design approach —in which visual expression of the representation technique was explored to develop a creative design approach through observation and analysis of students’ work. A curriculum that adopts the four phases of site analysis was developed based on the SPC and expanded for second-year architecture students in the university. The results showed that there are differences between architecture students in schematizing specific ideas and analysis methods, and that a substantial change in the process occurs when including visual expression in site analysis. In addition, the combination of group-based work and site analysis led to problem-solving that showed co-evolution. Finally, the study describes how research shapes site analysis and explains how representation can contribute to understanding the research. Site analysis consists of an initial attempt to explore research related to creative approaches, and may benefit both architecture educators and students.},
 authors = {['Eun Joo Park', 'Keunhye Lee', 'Eunki Kang']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Architectural design studio', 'Creative thinking', 'Design education', 'Design methodology', 'Site analysis']},
 title = {The impact of research and representation of site analysis for creative design approach in architectural design studio},
 year = {2023}
}

@Filtered Article{470d3a6d-1587-4e55-8791-4b0bc3e93dc4,
 abstract = {Nature is capable of astonishing feats of computation. Now, we are re-engineering molecules, cells and even whole organisms into living processors, says Edd Gent},
 authors = {['Edd Gent']},
 journal = {New Scientist},
 title = {Computing comes to life},
 year = {2023}
}

@Filtered Article{47ea6fcf-dbeb-46c3-9a47-126ea63bec1f,
 abstract = {Examination of numerical cognition encompasses multiple facets (eg, discrete vs. continuous properties, subitizing, estimation, counting, etc.). Many models have been suggested to explain these features. By looking into the basic ability to perceive size, against the complex one of counting, we hypothesize that counting system evolved on the basis of a primitive size perception system rather than the two systems evolved separately. In this chapter, we present a novel way of using evolutionary computation techniques to evolve artificial neural networks (ANNs) first to perceive size and then to count, and compare their counting skills to a different group of ANNs who evolved to count from scratch. The results revealed better counting skills when evolving first to perceive size (or other classification task) and then to count over those who evolved just to count. In addition, ANNs who evolved with continuous stimuli presented better counting skills than those evolved with discrete stimuli.},
 authors = {['Gali Barabash Katz', 'Amit Benbassat', 'Moshe Sipper']},
 journal = {Academic Press},
 keywords = {['numerical cognition', 'size perception', 'counting', 'evolutionary algorithms', 'genetic algorithms', 'artificial neural networks']},
 title = {Chapter 6 - Development of Counting Ability: An Evolutionary Computation Point of View},
 year = {2016}
}

@Filtered Article{48d78778-0780-4bdc-9e09-e31aa1ba5b6c,
 authors = {['G.B. Fiore', 'G. Guadagni', 'M. Soncini', 'S. Vesentini', 'A. Redaelli']},
 journal = {Journal of Biomechanics},
 title = {Multi-scale computational analysis of fluid dynamics in the Toraymyxin adsorption cartridge},
 year = {2006}
}

@Filtered Article{492a271d-b2b6-400a-9344-945c884aa5fb,
 abstract = {AI technology is reshaping language classrooms, prompting students to adopt flexible roles exhibiting linguistic competence and self-regulated learning (SRL) skills. Considerable studies explore the necessary integrated learning perspectives, emphasizing AI's adaptive role as a mind tool. In AI-mediated language learning, the technology's metacognitive importance enables students to learn with AI as a partner, encouraging independent critical thinking. Within Zimmerman's SRL model, AI as a mind tool is integrated for improving language students' strategic employment in a cyclical process. A systematic review, following PRISMA protocols, examines the intersection of AI and self-regulated language learning (SRLL) over 2000–2022. Findings highlight AI's evolving role, predominantly through algorithms and systems, aiming for micro and macro integration. Interactive AI has not fully engaged in two-way directions, despite a familiar process approach in reviewed studies. In the favored ESL/EFL research context, task-specific AI is utilized to encourage cyclical improvement with learner autonomy enhancement mainly among higher education students at intermediate level or above. Pedagogical values are possible when major SRL phases are fully practiced, even without highly autonomous AI. Future research is directed toward adaptive personalized technology by exploring the dynamic interplay between AI technologies and SRLL as educational practices under Education 4.0 principles.},
 authors = {['Wenli-Li Chang', 'Jerry Chih-Yuan Sun']},
 journal = {System},
 keywords = {['Self-regulated language learning', 'AI mediation', 'Learning partner', 'Systematic review']},
 title = {Evaluating AI's impact on self-regulated language learning: A systematic review},
 year = {2024}
}

@Filtered Article{493f1ac6-af1e-4584-87c7-14d685e8cb36,
 abstract = {The production of food that is environmentally friendly and presents a high economic return is one of the current concerns for the food industry. Eco-efficiency links the environmental performance of a product to its economic value. In this context, this study combines Life Cycle Assessment (LCA) and Life Cycle Costing (LCC) to propose a two-step eco-efficiency methodology assessment for the fish canning industry. An eco-label rating system based on a descriptive weighting of environmental (Global Warming Potential, Acidification Potential, Eutrophication Potential and the ReCiPe Single Score Endpoint) and economic (Value Added) indicators was applied to the canned anchovy. Secondly, LCA-LCC results were coupled to linear programming (LP) tools in order to define a composite eco-efficiency index. This approach enables translation into economic terms of the environmental damage caused when a given alternative is chosen. In particular, different origins for anchovy species (South American vs. Cantabrian) and related waste management alternatives (landfill, incineration and valorization) were evaluated under this cradle to gate approach. Results indicated that substantial differences can be observed depending on the origin of the fish. Anchovies landed in Cantabria show a higher value added score at the expense of larger environmental impacts, mainly due to fuel use intensity. Moreover, its environmental scores are lowered when fish residues are valorized into marketable products, while increasing the value added. This study demonstrates the environmental and economic benefits of applying circular economy. According to this, it is possible to introduce the cradle-to-cradle concept in the fish canned industry. The methodology proposed is intended to be useful to decision-makers in the anchovy canning sector and can be applied to other regions and industrial sectors.},
 authors = {['Jara Laso', 'Isabel García-Herrero', 'María Margallo', 'Ian Vázquez-Rowe', 'Pére Fullana', 'Alba Bala', 'Cristina Gazulla', 'Ángel Irabien', 'Rubén Aldaco']},
 journal = {Resources, Conservation and Recycling},
 keywords = {['Life cycle assessment', 'Life cycle costing', 'Eco-efficiency', 'Engraulis encrasicolus', 'Linear programming']},
 title = {Finding an economic and environmental balance in value chains based on circular economy thinking: An eco-efficiency methodology applied to the fish canning industry},
 year = {2018}
}

@Filtered Article{4998d82d-bbf2-45c7-b105-ff6a41a906f1,
 abstract = {In cheap talk games, equilibrium analysis predicts extreme limits on the information that can be transmitted when senders and receivers have different goals. Yet experimental evidence suggests that senders overcommunicate relative to this baseline, revealing more information than predicted in equilibrium. We propose that overcommunication may be due in part to limited cognitive engagement by subjects, captured by level-k thinking. To test this conjecture, we compare two elicitation methods, direct response and the strategy method, holding other elements of the game fixed. Existing experimental studies of cheap talk games use the standard direct response method, while the strategy method—in which subjects make selections for all contingent choices—is believed to encourage more thoughtful decisionmaking. We therefore expect senders to transmit less information with the strategy method than with direct response. In contrast, we find the reverse: the strategy method increased overcommunication. Further examination suggests that this occurred because senders played more naïvely with the strategy method than with direct response. Our findings suggest that the strategy method and direct response do not elicit the same choices in cheap talk games.},
 authors = {['William Minozzi', 'Jonathan Woon']},
 journal = {Journal of Behavioral and Experimental Economics},
 keywords = {['Strategic information transmission', 'Sender-receiver games', 'Strategy method', 'Laboratory experiment']},
 title = {Direct response and the strategy method in an experimental cheap talk game},
 year = {2020}
}

@Filtered Article{49aef5ce-9a08-4f38-bc27-5d36bca87056,
 abstract = {In this paper we report the work that jeKnowledge (Júnior Empresa da Faculdade de Ciências e Tecnologias da Universidade de Coimbra), a student-led initiative, has done in the ‘jeKnowledge academy’ courses to actively engage Portuguese high-school students in STEM education through hands-on projects based on the low-cost Arduino platform. F2F activities, based on a peer-assisted learning strategy, were complemented with tutorials and more advanced project suggestions in a blog. Pre and post surveys on students' attitudes towards programming and peer-coaching were administered to pre-university and first year college participants, finding an overall increase in the Likert scale for all the programming-related constructs under study (confidence, interest, gender, usefulness and professional) after the introductory course. As regards the peer-based learning approach, younger students seemed to be more eager to be taught in a less formal way than their older counterparts. The course resulted in high degrees of satisfaction for both the student tutors and their tutees.},
 authors = {['Pablo Martín-Ramos', 'Maria João Lopes', 'M. Margarida {Lima da Silva}', 'Pedro E.B. Gomes', 'Pedro S. {Pereira da Silva}', 'José P.P. Domingues', 'Manuela {Ramos Silva}']},
 journal = {Computers in Human Behavior},
 keywords = {['Attitudes survey', 'Arduino', 'High school', 'Programming', 'Peer coaching']},
 title = {Reprint of ‘First exposure to Arduino through peer-coaching: Impact on students' attitudes towards programming’},
 year = {2018}
}

@Filtered Article{49b908d7-1a2a-4712-a8c8-bdb285341795,
 abstract = {The debate about the relationship between economic growth and environmental sustainability triggers a range of associations. Here we analyze open-ended textual responses of citizens and scientists concerning their associations with the terms “economic growth” and “green growth”. We derive from the responses a number of topics and examine how associations differ across distinct opinion segments of people, namely supporters of Green growth, Agrowth and Degrowth. The results indicate that the general public is more critical of the notion of economic growth than academic researchers. Citizens stress problems of corruption, social inequality, unemployment and poverty, with less variation among the three opinion segments compared to scientists. The latter more strongly emphasize the environmental consequences of economic growth. Concerning associations of scientists with the term “green growth”, we find topics questioning its feasibility to be more likely expressed by Degrowth supporters, while topics stressing the possibility of sustainable economic growth by Green growth supporters. We find that topic polarization is stronger for scientists than citizens. Our results provide further validation for opinion clusters identified in previous studies and uncover additional insights about related views on growth and sustainability.},
 authors = {['Ivan Savin', 'Stefan Drews', 'Jeroen {van den Bergh}']},
 journal = {Ecological Economics},
 keywords = {['Structural topic modelling', 'Growth-vs-environment debate', 'Public opinion', 'Scientific opinion', 'Green growth']},
 title = {Free associations of citizens and scientists with economic and green growth: A computational-linguistics analysis},
 year = {2021}
}

@Filtered Article{49c1c891-346f-458d-9099-f95e85c50b54,
 abstract = {Publisher Summary
This chapter provides a chronological account of the steps which describes the present theoretical framework for thinking about depiction. The experimental results provides good evidence for the following assertions: (1) knowledge is a necessary part of all acts of depiction done by people of all ages and of all levels of skill, (2) knowledge is a main determinant of looking strategies, (3) the role of knowledge in the organization of looking strategies is one of determining the level of description to be used as the basis of analytic processes, and (4) "good" copying performance (i.e., "accurate" in terms of scene-specific and view-specific relations) can be equated with level of description accessed. The chapter emphasizes on: (1) each descending level of description implies an increasing disintegration of the analytic task. (2) Analysis for depiction is concerned with variance. It is concerned with relations that change according to viewing circumstances. In effect, they can be considered as novel relations. There is much evidence that people's ability to maintain "novel" relations in memory is severely limited. (3) The model consisting of a group of straight lines is only capable of being analyzed at the lowest levels of description.},
 authors = {['Francis Pratt']},
 journal = {North-Holland},
 title = {A Theoretical Framework for Thinking About Depiction},
 year = {1984}
}

@Filtered Article{49f0255c-1470-4ba6-9996-2a19460bdbd6,
 authors = {['Alan Dix']},
 journal = {Morgan Kaufmann},
 title = {CHAPTER 14 - Upside-Down ∀s and Algorithms—Computational Formalisms and Theory},
 year = {2003}
}

@Filtered Article{4a0941bc-7afd-4d07-95aa-52406aae1ff6,
 abstract = {There are a number of numerical methods for solving three-dimensional hydrodynamical models. An important aspect of any method is its efficient use of parallel computer architectures in an effort to minimize the clock time requirements in certain simulations such as oil spill modeling which uses three-dimensional hydrodynamics. The vertical-horizontal splitting (VHS) algorithm, using the method of characteristics for the two-dimensional horizontal plane and a generalization of the Crank-Nicholson method for vertical integration, is well-suited for the parallel architecture of the CM-2 machine.},
 authors = {['H.M. Cekirge', 'J. Berlin', 'R.A. Bernatz', 'M. Koch']},
 journal = {Mathematical and Computer Modelling},
 keywords = {['Methods of characteristics', 'Tidal currents', 'Parallel computations', 'Three-dimensional hydrodynamics', 'Tidal currents in the Arabian Gulf']},
 title = {An appropriate algorithm in parallel computations for three-dimensional hydrodynamics},
 year = {1994}
}

@Filtered Article{4a3de28f-fe87-4122-bc9c-d56b9b779f69,
 abstract = {To solve the problem of fire investigation caused by lack of exacting logical reasoning, it is of significance in helping that abduction, an important logical thinking should be introduced to the field of fire investigation. This paper first analyzes the fundamental reasoning forms of abduction as well as its general situation of application. Combined with practical work experience, the mode of application of abduction to fire investigation is put forward. The author shows it in detail by analyzing a real fire case. It is north noting that some matters needing attention in application are presented in the end. This paper will be conductive to constructing the right logical reasoning model in fire investigation.},
 authors = {['Shi Wang', 'Zhong-jun Shu']},
 journal = {Procedia Engineering},
 keywords = {['Fire investigation', 'Abduction', 'Logical thinking']},
 title = {Research on Application of Abduction to Fire Investigation},
 year = {2016}
}

@Filtered Article{4a474e30-36ad-4e56-8e58-8199cd1e02c6,
 abstract = {Drawing on research on graphic contextualization cues in punctuation and typography, this paper describes a three-stage, mixed-methods approach to digital discourse analysis. It introduces the terms ‘scale’ and ‘scaling’ as methodological metaphors for a researcher’s planned, yet contingent movement through formations of digital textual data that differ in terms of volume, method of collection, processing, and analysis. ‘Scaling-as-method’ aims to replace static binaries (such as ‘micro’ and ‘macro’, ‘small’ and ‘big’ data, ‘manual’ and ‘automated’ processing) by the vision of a researcher who shifts their degree of abstraction, or ‘distance’, towards digital data, while moving from close to distant reading and back again. The paper exemplifies this three-stage process on the example of the indignation mark, aka <!!1>, a twist on the iterated exclamation mark that is attested in digital discourse in various languages as a cue of double-voicing. The explorative examination of a small dataset (Stage 1) leads to the computational collection and distributional analysis of a much larger dataset (‘scaling up’, Stage 2), followed by the manual annotation of a selected subset of this data (‘scaling down’, Stage 3). Each stage draws on a different amount of data, which enables different techniques of processing and analysis, and relies on a specific combination of abductive, deductive, and inductive reasoning. Yet all three stages complement one another in a kaleidoscopic way towards understanding connections between punctuation practices and participatory political discourse online. Scaling as method is not a closed recipe, but an adaptable procedure that can be applied to a variety of discrete digital features. It does not aim to replace established methods of computational social media analysis, but to boost research that is predominantly based on the manual collection and annotation of social media data, and to enables a dialogue between multiple understandings of context.},
 authors = {['Jannis Androutsopoulos']},
 journal = {Discourse, Context & Media},
 keywords = {['Scaling', 'Mixed-methods', 'Digital discourse', 'Abduction', 'Indignation mark', 'Reddit']},
 title = {Scaling as method: A three-stage, mixed-methods approach to digital discourse analysis},
 year = {2024}
}

@Filtered Article{4a86a782-64b9-446c-b184-63c19f7aad72,
 abstract = {The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [M.L. Campagnolo, C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002) 977–100]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [O. Bournez and E. Hainry, Elementarily computable functions over the real numbers and R-sub-recursive functions, Theoretical Computer Science 348 (2005) 130–147], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.},
 authors = {['Manuel L. Campagnolo', 'Kerry Ojakian']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['Computable Analysis', 'Real Recursive Functions', 'Elementary Computable']},
 title = {The Methods of Approximation and Lifting in Real Computation},
 year = {2007}
}

@Filtered Article{4b4055a8-f8e6-4a78-bce0-c2b77afa62ef,
 abstract = {Standard economic theory uses mathematics as its main means of understanding, and this brings clarity of reasoning and logical power. But there is a drawback: algebraic mathematics restricts economic modeling to what can be expressed only in quantitative nouns, and this forces theory to leave out matters to do with process, formation, adjustment, and creation—matters to do with nonequilibrium. For these we need a different means of understanding, one that allows verbs as well as nouns. Algorithmic expression is such a means. It allows verbs—processes—as well as nouns—objects and quantities. It allows fuller description in economics, and can include heterogeneity of agents, actions as well as objects, and realistic models of behavior in ill-defined situations. The world that algorithms reveal is action-based as well as object-based, organic, possibly ever-changing, and not fully knowable. But it is strangely and wonderfully alive.},
 authors = {['W. Brian Arthur']},
 journal = {Journal of Economic Behavior & Organization},
 keywords = {['Economic theory', 'Mathematics in economics', 'Algorithms', 'Complexity economics', 'Computational economics']},
 title = {Economics in nouns and verbs},
 year = {2023}
}

@Filtered Article{4b56b588-5dd9-4370-96ec-bd714d3e6c4a,
 abstract = {Artificial intelligence (AI) in its various forms finds more and more its way into complex distributed systems. For instance, it is used locally, as part of a sensor system, on the edge for low-latency high-performance inference, or in the cloud, e.g. for data mining. Modern complex systems, such as connected vehicles, are often part of an Internet of Things (IoT). This poses additional architectural challenges. To manage complexity, architectures are described with architecture frameworks, which are composed of a number of architectural views connected through correspondence rules. Despite some attempts, the definition of a mathematical foundation for architecture frameworks that are suitable for the development of distributed AI systems still requires investigation and study. In this paper, we propose to extend the state of the art on architecture framework by providing a mathematical model for system architectures, which is scalable and supports co-evolution of different aspects for example of an AI system. Based on Design Science Research, this study starts by identifying the challenges with architectural frameworks in a use case of distributed AI systems. Then, we derive from the identified challenges four rules, and we formulate them by exploiting concepts from category theory. We show how compositional thinking can provide rules for the creation and management of architectural frameworks for complex systems, for example distributed systems with AI. The aim of the paper is not to provide viewpoints or architecture models specific to AI systems, but instead to provide guidelines based on a mathematical formulation on how a consistent framework can be built up with existing, or newly created, viewpoints. To put in practice and test the approach, the identified and formulated rules are applied to derive an architectural framework for the EU Horizon 2020 project “Very efficient deep learning in the IoT” (VEDLIoT) in the form of a case study.},
 authors = {['Hans-Martin Heyn', 'Eric Knauss', 'Patrizio Pelliccione']},
 journal = {Journal of Systems and Software},
 keywords = {['AI systems', 'Architectural frameworks', 'Compositional thinking', 'Requirements engineering', 'Systems engineering']},
 title = {A compositional approach to creating architecture frameworks with an application to distributed AI systems},
 year = {2023}
}

@Filtered Article{4b7c2b02-2cbd-4de3-8c13-b398f49381c4,
 abstract = {Aircraft aerodynamics have been predicted using computational fluid dynamics for a number of years. While viscous flow computations for cruise conditions have become commonplace, the non-linear effects that take place at high angles of attack are much more difficult to predict. A variety of difficulties arise when performing these computations, including challenges in properly modeling turbulence and transition for vortical and massively separated flows, the need to use appropriate numerical algorithms if flow asymmetry is possible, and the difficulties in creating grids that allow for accurate simulation of the flowfield. These issues are addressed and recommendations are made for further improvements in high angle of attack flow prediction. Current predictive capabilities for high angle of attack flows are reviewed, and solutions based on hybrid turbulence models are presented.},
 authors = {['Russell M. Cummings', 'James R. Forsythe', 'Scott A. Morton', 'Kyle D. Squires']},
 journal = {Progress in Aerospace Sciences},
 title = {Computational challenges in high angle of attack flow prediction},
 year = {2003}
}

@Filtered Article{4bb14901-a91b-4bcb-8334-90ebed67f5d2,
 abstract = {Some of the results on the criteria for the existence of an analytic continuation into a domain of a function given on a part of its boundary obtained by one of the authors are applied to the Riemann Hypothesis on the zeta-function zeroes. We include all of the basic structural information needed on the previous results on analytic continuation. Some comprehensive numerical experiments have been performed. We have found two important trends in the associated numerical results. The first one is that these findings favor the view that the Riemann Hypothesis is valid. The second one corresponds to a new conjecture on monotonic behavior of some sequences of integrals. The computational experiments have been performed with the Mathematica V3.0.},
 authors = {['L. Aizenberg', 'V. Adamchik', 'V.E. Levit']},
 journal = {Computers & Mathematics with Applications},
 keywords = {['ς-function', 'Riemann Hypothesis', 'Analytic continuation of a function given on a part of its boundary', 'Holomorphic functions', 'Conformal mappings', 'Unit disk', 'Computational experiments']},
 title = {One computational approach in support of the Riemann hypothesis},
 year = {1999}
}

@Filtered Article{4bdcb8fa-0523-4c11-abf2-47802f24604a,
 abstract = {Computational models of human collective behavior offer promise in providing quantitative and empirically verifiable accounts of how individual decisions lead to the emergence of group-level organizations. Agent-based models (ABMs) describe interactions among individual agents and their environment, and provide a process-oriented alternative to descriptive mathematical models. Recent ABMs provide compelling accounts of group pattern formation, contagion and cooperation, and can be used to predict, manipulate and improve upon collective behavior. ABMs overcome an assumption that underlies much of cognitive science – that the individual is the crucial unit of cognition. The alternative advocated here is that individuals participate in collective organizations that they might not understand or even perceive, and that these organizations affect and are affected by individual behavior.},
 authors = {['Robert L. Goldstone', 'Marco A. Janssen']},
 journal = {Trends in Cognitive Sciences},
 title = {Computational models of collective behavior},
 year = {2005}
}

@Filtered Article{4c14cec4-21f1-41e2-a43e-9389e26ae786,
 abstract = {This paper analyses six years of tutor feedback produced after inquiries made by students in a second linear algebra course at a university mathematics support center (MSC). We utilized Mason’s (2002) pedagogical tactics to build a model to analyze MSC tutors' feedback responding to these students’ queries. The aim of this research was to investigate the nature of students’ difficulties with concepts in a second linear algebra course that emphasizes theories and proof, in addition to examining the tactics employed by tutors to resolve student difficulties. We analyzed 227 feedback comments from 44 tutors based on their interactions with 82 students over six years. Our findings indicated that the most common areas of difficulty were basis, vector space, subspace, span, and proof. Tutor tactics deployed included ‘being mathematical’, ‘simplifying and complexifying’, and ‘worked examples’. We also discuss some implications for linear algebra tutor training.},
 authors = {['Anthony Cronin', 'Sepideh Stewart']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Mathematics tutors', 'Second courses in linear algebra', 'Mathematics support center', 'Feedback', 'Tutors’ tactics', 'Advanced mathematical thinking']},
 title = {Analysis of tutors’ responses to students’ queries in a second linear algebra course at a mathematics support center},
 year = {2022}
}

@Filtered Article{4c4b0d7f-f290-4746-b7f0-e542c59badeb,
 authors = {['AndrzejK. Konopka']},
 journal = {Computers & Chemistry},
 title = {Computational experiments in molecular biology: Searching for the ‘big picture’},
 year = {1994}
}

@Filtered Article{4c71228c-527e-4359-bc0c-7b4200b335b5,
 abstract = {What happens when artificial sensors are coupled with the human senses? Using technology to extend the senses is an old human dream, on which sensory substitution and other augmentation technologies have already delivered. Laser tactile canes, corneal implants and magnetic belts can correct or extend what individuals could otherwise perceive. Here we show why accommodating intelligent sensory augmentation devices not just improves but also changes the way of thinking and classifying former sensory augmentation devices. We review the benefits in terms of signal processing and show why non-linear transformation is more than a mere improvement compared to classical linear transformation.},
 authors = {['Louis Longin', 'Ophelia Deroy']},
 journal = {Consciousness and Cognition},
 keywords = {['Sensory substitution', 'Sensory extension', 'Intelligent sensory augmentation', 'Information quality', 'Senses', 'Artificial intelligence']},
 title = {Augmenting perception: How artificial intelligence transforms sensory substitution},
 year = {2022}
}

@Filtered Article{4cb76f47-a7d2-407d-8df5-f3ab21a82390,
 abstract = {Computer game programming has been touted as a promising strategy for engaging children in the kinds of thinking that will prepare them to be producers, not just users of technology. But little is known about what they learn when programming a game. In this article, we present a strategy for coding student games, and summarize the results of an analysis of 108 games created by middle school girls using Stagecast Creator in an after school class. The findings show that students engaged in moderate levels of complex programming activity, created games with moderate levels of usability, and that the games were characterized by low levels of code organization and documentation. These results provide evidence that game construction involving both design and programming activities can support the learning of computer science concepts.},
 authors = {['Jill Denner', 'Linda Werner', 'Eloy Ortiz']},
 journal = {Computers & Education},
 keywords = {['Construction of computer games', 'Secondary education', 'Programming', 'After-school']},
 title = {Computer games created by middle school girls: Can they be used to measure understanding of computer science concepts?},
 year = {2012}
}

@Filtered Article{4cbcf9de-d8e1-4372-8bc4-bf8c8c6a3f88,
 abstract = {It is acknowledged that technological innovation is needed in all sectors to cope with new demands. From social innovations, it introduces novel ideas, whether products, services, or models, to fulfil societal needs and foster new partnerships or collaborations. The aim is to enhance social interactions and elevate human well-being. Development of cutting-edge digital technologies is reality. The challenge is on human resource side, how quickly we can prepare employees to adapt to the requirements of Industry 4.0 and Society 5.0. In the paper, the new competencies were identified with the business stakeholders by conducting a survey among industry partners to recognize the requirements for the future labor market. The stakeholders in the construction field act as target groups for monitoring and development of the competencies. The focus of the result part is on the competencies companies mostly miss from their employees from digital perspectives, e.g. reason to hire highly educated people, training possibilities for digitally upskilling employees, lacking appropriate competencies (critical thinking, systems and analytical thinking, information management, advanced computer/IT skills (AI), ensuring security). Digital skills Advanced data/IT skills were the competencies in that companies defined as key competencies expected to be developed in 21st-century higher education employees. It is highly important to consider the job market needs for development and to adopt the engineering education system to be responsive to the needs of labor market.},
 authors = {['Alenka Temeljotov Salaj', 'Olav Torp', 'Elham Andalib']},
 journal = {IFAC-PapersOnLine},
 keywords = {['smart solutions', 'competencies', 'AI', 'education']},
 title = {Competencies for Smart City Challenges},
 year = {2024}
}

@Filtered Article{4cde5e00-3597-4424-9a0b-b91e6f3e214e,
 abstract = {Is there a neural system dedicated to generic magnitude judgments? In this issue of Neuron, Pinel et al. report qualitative spatial overlap of fMRI responses during judgments of luminance, size, and numerical magnitude but also quantitative response differences in intraparietal cortex that mirror behavioral interference between perceptual and symbolic magnitude.},
 authors = {['Andreas Kleinschmidt']},
 journal = {Neuron},
 title = {Thinking Big: Many Modules or Much Cortex?},
 year = {2004}
}

@Filtered Article{4d1f032b-5161-431b-90a7-f951eb54ab19,
 abstract = {It is now rare to find biological, or genetic investigations that do not rely on the tools, data, and thinking drawn from the genomic sciences. Much of this revolution is powered by contemporary sequencing approaches that readily deliver large, genome-wide data sets that not only provide genetic insights but also uniquely report molecular outcomes from experiments that biophysicists are increasingly using for potentiating structural and mechanistic investigations. In this perspective, I describe a path of how biophysical thinking greatly contributed to this revolution in ways that parallel advancements in computer science through discussion of several key inventions, described as “foundational devices.” These discussions also point at the future of how biophysics and the genomic sciences may become more finely integrated for empowering new measurement paradigms for biological investigations.},
 authors = {['David C. Schwartz']},
 journal = {Biophysical Journal},
 title = {Biophysics and the Genomic Sciences},
 year = {2019}
}

@Filtered Article{4d96ff41-ba2b-4b57-a21c-4413bf2a8232,
 abstract = {Forecasting economic choice is hard because today we still do not know enough about human motivation. A fundamental problem is the lack of knowledge about how the neural networks in the brain give rise to thinking and decision making. One way to address the issue has been to develop simplified economic experiments, in which participants need skills of little complexity and their minds employ cognitive mechanisms, already well understood by mathematical psychology and neuroscience. Here we take a neural model for rudimentary emotion generation and memorizing and use it as a guiding theory to understand decision making in an experimental oligopoly market. For the first time in that line of research, participants are put in a lab virtual social network serving to exchange opinions about deals with companies. On average, choices become significantly more predictable when people participate in the network, in contrast to working alone with expert information. Calibrating the model for each person, we find that some people are predicted with startling precision.},
 authors = {['George Mengov', 'Nikolay Georgiev', 'Irina Zinovieva', 'Anton Gerunov']},
 journal = {Journal of Behavioral and Experimental Economics},
 keywords = {['Decision making', 'Virtual social network', 'Emotional economic choice', 'Neural model']},
 title = {Virtual social networking increases the individual's economic predictability},
 year = {2022}
}

@Filtered Article{4d9cd589-94d7-4aae-b506-65de0ef1c118,
 abstract = {The distinction between convergent and divergent cognitive processes given by Guilford (1956) had a strong influence on the empirical research on creative thinking. Neuroscientific studies typically find higher event-related synchronization in the EEG alpha rhythm for individuals engaged in creative ideation tasks compared to intelligence-related tasks. This study examined, whether these neurophysiological effects can also be found when both cognitive processing modes (convergent vs. divergent) are assessed by means of the same task employing a simple variation of instruction. A sample of 55 participants performed the alternate uses task as well as a more basic word association task while EEG was recorded. On a trial-by-trial basis, participants were either instructed to find a most common solution (convergent condition) or a most uncommon solution (divergent condition). The answers given in the divergent condition were in both tasks significantly more original than those in the convergent condition. Moreover, divergent processing was found to involve higher task-related EEG alpha power than convergent processing in both the alternate uses task and the word association task. EEG alpha synchronization can hence explicitly be associated with divergent cognitive processing rather than with general task characteristics of creative ideation tasks. Further results point to a differential involvement of frontal and parietal cortical areas by individuals of lower versus higher trait creativity.},
 authors = {['Emanuel Jauk', 'Mathias Benedek', 'Aljoscha C. Neubauer']},
 journal = {International Journal of Psychophysiology},
 keywords = {['EEG', 'Creativity', 'Alpha synchronization', 'Divergent thinking', 'Convergent thinking']},
 title = {Tackling creativity at its roots: Evidence for different patterns of EEG alpha activity related to convergent and divergent modes of task processing},
 year = {2012}
}

@Filtered Article{4db25dd6-48d3-41ec-8d7f-d4cf8003d735,
 abstract = {This study explores the enhancement of creativity in undergraduate students studying computer science. We assume that everybody has creative potential. As a teacher, we can explicitly encourage creative thinking, providing space to let students collaboratively discover and explore their creativity. This paper presents a dialogical framework to help the teacher fostering creativity among students of computer science in programming and interaction design. The framework presented here involves underlying dialogic processes from seven collaborative and creative dimensions that allow students to develop creativity. The use of the pedagogical framework makes it possible to teachers create significant interaction design and computer programming experiences to students, motivating them to activate mental processes underlying creativity. Students can simultaneously activate two or more ideas, images, or thoughts and have them interact, prompt thought experiments, change cognitive perspectives, raise new points of view, and risk category mistakes.},
 authors = {['Deller James Ferreira']},
 journal = {Procedia Computer Science},
 keywords = {['Creativity', 'Programming', 'Interaction design']},
 title = {Fostering the Creative Development of Computer Science Students in Programming and Interaction Design},
 year = {2013}
}

@Filtered Article{4de72f30-99e2-4047-bcc6-367305bfe148,
 abstract = {We present a new double-degree graduate (Master’s) programme developed together by the ITMO University, Russia and University of Amsterdam, The Netherlands. First, we look into the global aspects of integration of different educational systems and list some funding opportunities. Then, we describe our double-degree program curriculum, suggest the timeline of enrollment and studies, and give some examples of student research topics. Finally, we discuss the issues of joint programs with Russia and suggest possible solutions, analyze the results of the first three student intakes and reflect on the lessons learnt, and share our thoughts and experiences that could be of interest to the international community expanding the educational markets to the vast countries like Russia, China or India. The paper is written for education professionals and contains useful information for potential students. This is an extended version of a conference paper (http://dx.doi.org/10.1016/j.procs.2014.05.130) invited to this special issue of the Journal of Computational Science.},
 authors = {['Valeria V. Krzhizhanovskaya', 'Alexey V. Dukhanov', 'Anna Bilyatdinova', 'Alexander V. Boukhanovsky', 'Peter M.A. Sloot']},
 journal = {Journal of Computational Science},
 keywords = {['Computational science', 'Master’s programme', 'Graduate program', 'Double degree', 'Curriculum', 'Enrollment', 'Student research', 'Funding opportunities']},
 title = {Russian-Dutch double-degree Master’s programme in computational science in the age of global education},
 year = {2015}
}

@Filtered Article{4e391ec9-f3fb-40ad-ac88-64186d710be5,
 abstract = {The Matlab environment has become widely used among the computational mechanics community, not only for research purposes but also to teach either undergraduate or graduate classes. This paper aims to present a new toolbox devoted to computational mechanics and in particular to solid mechanics. Both recent and well-established numerical formulations have been implemented in it. One of its strengths resides in the fact that it was developed within an object-oriented framework. This key feature makes the CastLab toolbox easy-to-use and with extensive capabilities for customized user developments. After a brief description of the theoretical background related to the problems that can be solved by means of the toolbox, several representative case-studies are presented. These examples have been selected to illustrate not only the numerical efficiency of the toolbox, which is of primary importance for research purposes, but also its strong educational and pedagogic potential.},
 authors = {['Benjamin Richard', 'Giuseppe Rastiello', 'Cédric Giry', 'Francesco Riccardi', 'Romili Paredes', 'Eliass Zafati', 'Santosh Kakarla', 'Chaymaa Lejouad']},
 journal = {Advances in Engineering Software},
 keywords = {['Matlab toolbox', 'Nonlinear solid mechanics', 'Computational mechanics', 'Educational tools', 'Finite elements']},
 title = {CastLab: an object-oriented finite element toolbox within the Matlab environment for educational and research purposes in computational solid mechanics},
 year = {2019}
}

@Filtered Article{4e49fbdd-630c-49cb-9536-78fc40ab4b98,
 authors = {['George Sarton']},
 journal = {Vistas in Astronomy},
 title = {The astral religion of antiquity and the “thinking machines” of to-day},
 year = {1955}
}

@Filtered Article{4e8a33db-63a3-4af1-b224-d592893c8af1,
 abstract = {The new E.U. proposal for a general data protection regulation has been introduced to give an answer to the challenges of the evolving digital environment. In some cases, these expectations could be disappointed, since the proposal is still based on the traditional main pillars of the last generation of data protection laws. In the field of consumer data protection, these pillars are the purpose specification principle, the use limitation principle and the “notice and consent” model. Nevertheless, the complexity of data processing, the power of modern analytics and the “transformative” use of personal information drastically limit the awareness of consumers, their capability to evaluate the various consequences of their choices and to give a free and informed consent. To respond to the above, it is necessary to clarify the rationale of the “notice and consent” paradigm, looking back to its origins and assessing its effectiveness in a world of predictive analytics. From this perspective, the paper considers the historical evolution of data protection and how the fundamental issues coming from the technological and socio-economic contexts have been addressed by regulations. On the basis of this analysis, the author suggests a revision of the “notice and consent” model focused on the opt-in and proposes the adoption of a different approach when, such as in Big Data collection, the data subject cannot be totally aware of the tools of analysis and their potential output. For this reason, the author sustains the provision of a subset of rules for Big Data analytics, which is based on a multiple impact assessment of data processing, on a deeper level of control by data protection authorities, and on the different opt-out model.},
 authors = {['Alessandro Mantelero']},
 journal = {Computer Law & Security Review},
 keywords = {['Data protection', 'Consent', 'Data protection impact assessment', 'Big Data', 'Data protection authorities']},
 title = {The future of consumer data protection in the E.U. Re-thinking the “notice and consent” paradigm in the new era of predictive analytics},
 year = {2014}
}

@Filtered Article{4ea87df6-becc-432d-a605-427ace69934e,
 abstract = {Sustainable land use is crucial for achieving Carbon Neutrality goals, which requires a scientific identification of optimized pathways for land use patterns across multiple scales. Yet, current land use studies predominantly focus on single scales but lack system thinking and fail to establish complementary cross-regional carbon neutrality collaboration schemes. Applying life-cycle thinking to analyze land use sustainability and carbon neutrality potential at multiple scales could address this challenge. This study aims to present China's first multi-scale spatiotemporal optimization pathway for sustainable land use to improve carbon neutrality potential. It systematically integrates the complex spatial coupling relationships between land use intensity and efficiency. We integrate multi-scale sustainable land use pathways, spanning grid, basin, and administrative levels, and unveil significant variations in land use sustainability and carbon neutrality potential across China. Sixty-three percent of China's land is in low sustainability, and the overall carbon neutrality potential in China is relatively low, with regions accounting for <30 % facing more carbon neutrality missions. Implementing sequential and partitioned governance modes can effectively support China in achieving sustainable land use and advancing Carbon Neutrality goals. Our sustainable land use pathways for China provide valuable insights for systematically undertaking carbon neutrality actions across different scales.},
 authors = {['Zhenci Xu']},
 journal = {Sustainable Production and Consumption},
 keywords = {['Carbon neutrality', 'Land use', 'Multi-scales', 'System thinking', 'China']},
 title = {Towards carbon neutrality in China: A systematic identification of China's sustainable land-use pathways across multiple scales},
 year = {2024}
}

@Filtered Article{4f21e26f-d4b6-4625-879f-10d38148807a,
 abstract = {The under-reporting of public safety incidents is a long-standing issue. In this paper, we propose a computational cognitive modeling approach to understand and design a mobile crowdsourcing system for improving campus safety reporting. In particular, we adopt drift-diffusion models (DDMs) from cognitive psychology to investigate the effect of various factors on users’ reporting tendency for public safety. Our lab experiment and online study show consistent results on how location context impacts people's reporting decisions. This finding informs the design of a novel location-based nudge mechanism, which is tested in another lab experiment with 84 participants and proved to be effective in changing users’ reporting decisions. Our follow-up interview study further suggests that the influence of people's mobility patterns (e.g., expected walking distance) could explain why the nudge design is effective. Our work not only informs the design of mobile crowdsourcing for public safety reporting but also demonstrates the value of applying a computational cognitive modeling approach to address HCI research questions more broadly.},
 authors = {['Yun Huang', 'Corey White', 'Huichuan Xia', 'Yang Wang']},
 journal = {International Journal of Human-Computer Studies},
 keywords = {['Mobile crowdsourcing', 'Cognitive computational method', 'Public safety', 'User contribution', 'Drift-diffusion decision model', 'Nudge mechanism']},
 title = {A computational cognitive modeling approach to understand and design mobile crowdsourcing for campus safety reporting},
 year = {2017}
}

@Filtered Article{4f92505d-1598-4c48-b17c-cff732dd55cf,
 abstract = {Despite detailed psychophysical, neurophysiological and electrophysiological investigation, the number and nature of independent and parallel motion processing mechanisms in the visual cortex remains controversial. Here we use computational modelling to evaluate evidence from two psychophysical studies collectively thought to demonstrate the existence of three separate and independent motion processing channels. We show that the pattern of psychophysical results can largely be accounted for by a single mechanism. The results demonstrate that a low-level luminance based approach can potentially provide a wider account of human motion processing than generally thought possible.},
 authors = {['Christopher P. Benton', 'Alan Johnston', 'Peter W. McOwan']},
 journal = {Vision Research},
 keywords = {['Motion perception', 'Computational modelling', 'Second-order', 'Feature tracking']},
 title = {Computational modelling of interleaved first- and second-order motion sequences and translating 3f+4f beat patterns},
 year = {2000}
}

@Filtered Article{5002701a-f84d-4f7b-b804-9884f4a7e88d,
 abstract = {Verification and validation (V&V) are the primary means to assess accuracy and reliability in computational simulations. This paper presents an extensive review of the literature in V&V in computational fluid dynamics (CFD), discusses methods and procedures for assessing V&V, and develops a number of extensions to existing ideas. The review of the development of V&V terminology and methodology points out the contributions from members of the operations research, statistics, and CFD communities. Fundamental issues in V&V are addressed, such as code verification versus solution verification, model validation versus solution validation, the distinction between error and uncertainty, conceptual sources of error and uncertainty, and the relationship between validation and prediction. The fundamental strategy of verification is the identification and quantification of errors in the computational model and its solution. In verification activities, the accuracy of a computational solution is primarily measured relative to two types of highly accurate solutions: analytical solutions and highly accurate numerical solutions. Methods for determining the accuracy of numerical solutions are presented and the importance of software testing during verification activities is emphasized. The fundamental strategy of validation is to assess how accurately the computational results compare with the experimental data, with quantified error and uncertainty estimates for both. This strategy employs a hierarchical methodology that segregates and simplifies the physical and coupling phenomena involved in the complex engineering system of interest. A hypersonic cruise missile is used as an example of how this hierarchical structure is formulated. The discussion of validation assessment also encompasses a number of other important topics. A set of guidelines is proposed for designing and conducting validation experiments, supported by an explanation of how validation experiments are different from traditional experiments and testing. A description is given of a relatively new procedure for estimating experimental uncertainty that has proven more effective at estimating random and correlated bias errors in wind-tunnel experiments than traditional methods. Consistent with the authors’ contention that nondeterministic simulations are needed in many validation comparisons, a three-step statistical approach is offered for incorporating experimental uncertainties into the computational analysis. The discussion of validation assessment ends with the topic of validation metrics, where two sample problems are used to demonstrate how such metrics should be constructed. In the spirit of advancing the state of the art in V&V, the paper concludes with recommendations of topics for future research and with suggestions for needed changes in the implementation of V&V in production and commercial software.},
 authors = {['William L. Oberkampf', 'Timothy G. Trucano']},
 journal = {Progress in Aerospace Sciences},
 title = {Verification and validation in computational fluid dynamics},
 year = {2002}
}

@Filtered Article{501ca595-ceb4-441a-8199-0ebd2bb735cc,
 authors = {['Glyn W. Humphreys']},
 journal = {Trends in Neurosciences},
 title = {Vol. 3: Thinking: edited by Daniel N. Osherson and Edward E. Smith (x + 308 pages) ISBN 0 262 65035 5},
 year = {1991}
}

@Filtered Article{50b3db9e-92b2-4dae-8c85-37f171fcb5b4,
 abstract = {A computer simulation of the transient three-dimensional heat transfer process occurring during the manufacturing of seamless tubes carried out by TAMSA, Tubos de Acero de Mexico, is reported. The work was performed by a team which combines Canadian and Mexican researchers and comprises both experimental and computational aspects. The Mexican team concentrated its efforts on experimentally investigating the metallurgical pattern of the mandrel, while the Canadian team devoted its time to the computer simulation and analysis of heat transfer and flow processes. In this paper, only the latter part is presented. The numerical simulation uses the Star-CD commercial CFD software package which is based on the finite volume methodology. The results show the importance of the cooling water channel configuration in relation to the mandrel temperature distribution and resulting metallurgical structure.},
 authors = {['M. Reggio', 'F. McKenty', 'Luc Gravel', 'J. Cortes', 'G. Morales', 'M.-A. {Ladron de Guevara}']},
 journal = {Applied Thermal Engineering},
 keywords = {['Seamless tube', 'Heat transfer', 'Computational simulation', 'CFD']},
 title = {Computational analysis of the process for manufacturing seamless tubes},
 year = {2002}
}

@Filtered Article{5147fe26-e588-447d-807a-34495982ee2e,
 abstract = {The aim of this paper is to investigate the ethical and responsible use of AI chatbots in education in support of critical thinking, cognitive flexibility and self-regulation in terms of their potential to enhance and motivate teaching and learning in contemporary education environments. AI chatbots such as ChatGPT by OpenAI appear to be improving in conversational and other capabilities and this paper explores such advances using version 4. Based on a review of the research literature, a conceptual framework is formulated for responsible use of AI chatbots in education supporting cognitive flexibility in AI-rich learning environments. The framework is then operationalized for use in this paper through the development of exemplars for math, english language arts (ELA), and studying with ChatGPT to close learning gaps in an effort to foster more ethical and responsible approaches to the design and development of AI chatbots for application and use in teaching and learning environments. This paper extends earlier foundational work on cognitive flexibility and AI chatbots as well as work on cognitive flexibility in support of creativity and innovation with AI chatbots in urban civic spaces.},
 authors = {['Sarah A. Chauncey', 'H. Patricia McKenna']},
 journal = {Computers and Education: Artificial Intelligence},
 keywords = {['AI ethics', 'AI responsibility', 'AI-Rich learning environments', 'Cognitive flexibility', 'Critical thinking', 'Self-regulation']},
 title = {A framework and exemplars for ethical and responsible use of AI Chatbot technology to support teaching and learning},
 year = {2023}
}

@Filtered Article{517b7cb2-3781-4c6f-b935-49e923d191a8,
 abstract = {This study presents the structure of a strategic crisis management model, considering the context and characteristics of a Brazilian community college. Strategic crisis management associated with coordination and control mechanisms theoretically underpinned the problem under study. It is a single case study, with grounded theory as the strategy for data treatment and content analysis to interpret and present the findings. In the model, the strategic and tactical stages systematized the dynamics of crisis management in a coordinated manner. Strategic crisis management was considered a continuous process rather than a strictly punctual one. The dynamism of the model's operationalization considered some premises that guided the behavior of the leadership and the crisis management team: i) pragmatic strategic thinking shaped by rationality; ii) quick responses in facing the crisis; iii) simplicity in actions; iv) reversible decisions susceptible to flexibilization; v) creativity and boldness to innovate and set new standards; v) collaboration for common causes.},
 authors = {['Gisele Silveira {Coelho Lopes}', 'Carlos Ricardo Rossetto', 'Micheline {Ramos de Oliveira}', 'Jorge Oneide Sausen', 'Rudimar {Antunes da Rocha}']},
 journal = {International Journal of Disaster Risk Reduction},
 keywords = {['Crisis management', 'Organizational strategy', 'Community university']},
 title = {The structure of a strategic crisis management model: The context and characteristics of a brazilian community college},
 year = {2023}
}

@Filtered Article{51bbc40f-84a2-4cb7-9167-9b1f5cef0111,
 abstract = {This paper develops the implication of neurofinance with respect to the efficient markets hypothesis. Neurofinance informs us that thinking imposes strain on the mind, in the sense that thinking is a comparatively laborious, biologically costly, and neurologically expensive cognitive process. The paper shows that people balance the costs and benefits of thinking and demonstrates mathematically that such balancing makes financial markets inefficient.},
 authors = {['Kavous Ardalan']},
 journal = {Global Finance Journal},
 keywords = {['Neurofinance', 'Behavioral finance', 'Costly thinking', 'Efficient markets hypothesis']},
 title = {Neurofinance versus the efficient markets hypothesis},
 year = {2018}
}

@Filtered Article{51ca4de2-da96-46c7-b73d-0806901e72fc,
 abstract = {One of the most debated questions in psychology and cognitive science is the nature and the functioning of the mental processes involved in deductive reasoning. However, all existing theories refer to a specific deductive domain, like syllogistic, propositional or relational reasoning. Our goal is to unify the main types of deductive reasoning into a single set of basic procedures. In particular, we bring together the microtheories developed from a mental models perspective in a single theory, for which we provide a formal foundation. We validate the theory through a computational model (UNICORE) which allows fine-grained predictions of subjects’ performance in different reasoning domains. The performance of the model is tested against the performance of experimental subjects—as reported in the relevant literature—in the three areas of syllogistic, relational and propositional reasoning. The computational model proves to be a satisfactory artificial subject, reproducing both correct and erroneous performance of the human subjects. Moreover, we introduce a developmental trend in the program, in order to simulate the performance of subjects of different ages, ranging from children (3–6) to adolescents (8–12) to adults (>21). The simulation model performs similarly to the subjects of different ages. Our conclusion is that the validity of the mental model approach is confirmed for the deductive reasoning domain, and that it is possible to devise a unique mechanism able to deal with the specific subareas. The proposed computational model (UNICORE) represents such a unifying structure.},
 authors = {['Bruno G. Bara', 'Monica Bucciarelli', 'Vincenzo Lombardo']},
 journal = {Cognitive Science},
 keywords = {['Mental models', 'Deduction', 'Computational model', 'Development']},
 title = {Model theory of deduction: a unified computational approach},
 year = {2001}
}

@Filtered Article{5259b700-1797-43ab-9a76-937e3e776ed8,
 abstract = {This paper transfers a randomized algorithm, originally used in geometric optimization, to computational problems in commutative algebra. We show that Clarkson's sampling algorithm can be applied to two problems in computational algebra: solving large-scale polynomial systems and finding small generating sets of graded ideals. The cornerstone of our work is showing that the theory of violator spaces of Gärtner et al. applies to polynomial ideal problems. To show this, one utilizes a Helly-type result for algebraic varieties. The resulting algorithms have expected runtime linear in the number of input polynomials, making the ideas interesting for handling systems with very large numbers of polynomials, but whose rank in the vector space of polynomials is small (e.g., when the number of variables and degree is constant).},
 authors = {['Jesús A. {De Loera}', 'Sonja Petrović', 'Despina Stasi']},
 journal = {Journal of Symbolic Computation},
 keywords = {['Violator spaces', 'Ideal generators', 'Solving polynomial systems', 'Randomized algorithm in algebra', 'Large sparse systems of equations']},
 title = {Random sampling in computational algebra: Helly numbers and violator spaces},
 year = {2016}
}

@Filtered Article{52c2bced-dd92-4937-a69e-ba2254130748,
 abstract = {ABSTRACT
Background and objective
Diagnosis of breast intraductal proliferative lesions (BIDPLs) in hematoxylin-eosin (HE) images remains a time-consuming and intractable topic because of subjective processes and subtle morphological differences. Convolutional neural networks (CNNs) show great potential for providing objective analysis strategies for HE images. In this study, we proposed a novel knowledge distillation (KD) framework using CNN-based nuclei segmentation-assisted classification (KDCNN-NSAC).
Methods
The diagnosis of BIDPLs is treated as multiple class classification tasks in the BReAst Carcinoma Subtyping dataset. The KDCNN-NSAC fully leveraged the epithelial and stromal nuclei-level features in training phases and performed region-of-interest (ROI)-level classifications in predicting phases. Then, the whole slide image (WSI) was diagnosed based on the risk ratings of the ROIs within it, instead of processing a WSI.
Results
The principal results showed that in ROI-level classifications, KDCNN-NSAC outperformed the state-of-the-art methods for 7-class classification with an average F1 score of 63.26% and achieves F1 score of 98.36% and 94.21%, respectively, in distinguishing BIDPLs from invasive cancer and normal tissue. The WSI-level predictions obtained a high degree of consistency with the pathologists’ annotation (kappa value of 0.88). Ablation experiments showed that nuclei segmentation and classification components improve the performance of the baseline model in KDCNN-NSAC by 3%.
Conclusions
The KDCNN-NSAC makes the model focus on important cellular information and predicts the WSI in accordance with the pathologists’ diagnostic thinking, thus improving model explainability. Moreover, the introduce of KDCNN-NSAC will help achieve superior performance in diagnosing BIDPLs.},
 authors = {['Xiangmin Li', 'Jiamei Chen', 'Bo Luo', 'Minyan Xia', 'Xu Zhang', 'Hangjia Zhu', 'Yutian Zhang-Cai', 'Yongshun Chen', 'Yang Yang', 'Yaofeng Wen']},
 journal = {Heliyon},
 keywords = {['Breast intraductal proliferative lesions', 'Breast cancer', 'Knowledge distillation', 'Convolutional neural network', 'Nuclei segmentation', 'Classification']},
 title = {Classifying breast intraductal proliferative lesions via a knowledge distillation framework using convolutional neural network-based nuclei-segmentation-assisted classification (KDCNN-NSAC)},
 year = {2025}
}

@Filtered Article{52e094c9-8e14-4469-a64e-29399a06ca41,
 abstract = {Human systems need to be adaptive to the consequences of natural hazards. Public policy decisions on natural hazard mitigation can benefit from computational models that embody a comprehensive view of the system. Such models need to be transparent and integrate both expert and lay expert knowledge and experience in an efficient manner. By integrating hard and soft sciences within an overall systems framework, scientists, policy makers and communities can better understand how to improve adaptive capacity. We present a fuzzy cognitive map based Auto-Associative Neural Networks framework generated from a development mixed method integration (triangulation) for adaptive policy formulations. The specific policies relate to preparation for, response to, and recovery from earthquakes in mountainous ski-field environments – a case study chosen to highlight the framework. Three different data collection techniques – expert geomorphic assessments, semi-structured qualitative interviews with three stakeholder groups (experts and lay experts), and fuzzy cognitive maps (FCM) (node and arc maps of stakeholder perceptions) were employed. FCM were first analysed using Graph theory indices to determine map structure. Special attention was paid to subsequent processing of fuzzy cognitive maps (e.g., condensation and aggregation) with qualitative followed by quantitative means to simplify the FCM from the original total of 300 variables to 5 high-level themes to improve the efficacy of subsequent policy simulations. Specifically, the use of Self Organising Maps (SOM) to group concepts (condensation) and individual stakeholders (aggregation) into social group FCMs is a novel contribution to advancing FCM. In the process, SOM also enabled the embedment of nonlinear relationships inherent in the system in the simplified FCM allowing a platform for realistic and meaningful policy simulations based on collective perceptions. Specifically, each of the three simplified stakeholder group FCM and a total social group FCM was represented by Auto-Associative Neural Networks (AANN) which converts an FCM into a dynamical system that allows policy scenario simulations based on input from both expert and lay expert stakeholders. A policy scenario is the level of importance given to a set of concepts and their effects on the system behaviour as revealed by the simulations. We present the results from one of several policy simulations to highlight the effectiveness of the mixed-method integration leading to simplified-FCM based ANNN simulations. Results revealed the similarities and differences between stakeholder group responses in relation to the scenario analysed and how these formed collective responses in the total social group map. Furthermore, outcomes of group and total social group simulations could be interpreted from individual and group stakeholder FCMs giving credibility to the mixed-method approach.},
 authors = {['Sandhya Samarasinghe', 'Graham Strickert']},
 journal = {Environmental Modelling & Software},
 keywords = {['Fuzzy cognitive maps', 'Auto-Associative Neural Networks', 'Self-organizing maps', 'Natural hazard mitigation', 'Earthquakes', 'Mixed-method triangulation', 'Policy simulation']},
 title = {Mixed-method integration and advances in fuzzy cognitive maps for computational policy simulations for natural hazard mitigation},
 year = {2013}
}

@Filtered Article{5304c5a8-59d9-449d-a808-9ff0e2178f99,
 abstract = {Rationale and Objectives
Previous studies of aneurysm flow dynamics based on three-dimensional (3D) rotational angiography (RA) images were limited to aneurysms with a single route of blood inflow. However, aneurysms of the circle of Willis frequently involve locations with more than one source of inflow, such as aneurysms of the anterior communicating artery. The highest resolution images of cerebral vessels are from RA images, but this technique is limited to visualizing only one route of inflow at a time, leaving a significant limitation in the application of 3DRA image sets for clinical studies of patient-specific computational fluid dynamics (CFD) simulations. In this report, subject-specific models of cerebral aneurysms with multiple avenues of flow are constructed from RA images by using a novel combination of image coregistration and surface merging techniques.
Materials and Methods
RA images are obtained by means of contrast injection in each vessel that provides inflow to the aneurysm. Anatomic models are constructed independently of each of these vascular trees and fused together into a single model. The model is used to construct a finite element grid for CFD simulations of hemodynamics.
Results
Three examples of patient-specific models are presented: an anterior communicating artery aneurysm, a basilar tip aneurysm, and a model of an entire circle of Willis with five coincident aneurysms. The method is evaluated with a numeric phantom of an aneurysm in the anterior communicating artery.
Conclusion
These examples show that this new technique can be used to create merged network numeric models for CFD modeling. Furthermore, intra-aneurysmal flow patterns are influenced strongly by merging of the two inflow streams. This effect decreases as distance from the merging streams increases.},
 authors = {['Marcelo A. Castro', 'Christopher M. Putman', 'Juan R. Cebral']},
 journal = {Academic Radiology},
 title = {Patient-Specific Computational Modeling of Cerebral Aneurysms With Multiple Avenues of Flow From 3D Rotational Angiography Images},
 year = {2006}
}

@Filtered Article{534543b4-cc94-49ba-9360-52130b7e036f,
 abstract = {As one of the machine learning (ML) models, the multi-head self-attention mechanism (MSM) is competent in encoding high-level feature representations, providing computing superiorities, and systematically processing sequences bypassing the recurrent neural networks (RNN) models. However, the model performance and computational results are affected by head number, and the lack of impact interpretability has become a primary obstacle due to the complex internal working mechanisms. Therefore, the effects of the head number of the MSM on the accuracy of the result, the robustness of the model, and computation efficiency are investigated in the remaining useful life (RUL) prediction of rolling bearings. The results show that the accuracy of prediction results will be reduced caused by large or few head numbers. In addition, the more heads are selected, the more robust and higher the predictive efficiency of the model is achieved. The above effects are explained relying on the visualization of the attention weight distribution and functional networks, which are constructed and solved by the equivalent fully connected layer and graph theory analysis, respectively. The model's attention coefficient distribution during training and prediction shows that the representative information will be captured inadequately if fewer heads are selected, which causes MSM to neglect to assign large attention coefficients to degraded information. On the contrary, representational degradation information and redundant information will be acquired by models with too many heads. MSM will be disturbed by this redundant information in the attention weight distribution, resulting in incorrect allocation of attention. Both of these cases will reduce the accuracy of the prediction results. In addition, the selection rules of the head number are established based on the feature complexity that is measured by the sample entropy (SamEn). The local range for head selection is also found based on the relationship between head number and feature complexity; The effects of the head number of the MSM on the robustness of the model and computation efficiency are explained by the changes in the three parameters (average of the clustering coefficients, global efficiency, and of the average shortest path length) of the graph, which is constructed after solving the function network. The research provides a reference for rolling bearing prediction with high computational accuracy, calculation efficiency, and strong robustness using MSM.},
 authors = {['Qiwu Zhao', 'Xiaoli Zhang', 'Fangzhen Wang', 'Panfeng Fan', 'Erick Mbeka']},
 journal = {Neurocomputing},
 keywords = {['Remaining useful life prediction', 'Machine learning', 'Multi-head self-attention mechanism', 'Interpretability', 'Graph theory', 'Functional networks']},
 title = {The effect of the head number for multi-head self-attention in remaining useful life prediction of rolling bearing and interpretability},
 year = {2025}
}

@Filtered Article{535cbd28-f05d-4d78-9459-cb93a314dad5,
 abstract = {Artistic creativity is studied through the construction of computational models of a number of well-known modern artists. In particular, the work of Piet Mondrian, M.C. Escher and Paul Klee are suitable vehicles for investigation since their work is accompanied by extensive writings describing the ideas and motivation behind their compositions. In particular, we have tried to abstract from their theories, rules that describe the construction process or the properties that their finished artefacts posses in order to create software programs that can articulate these rules. In this way, we are able to simulate either automatically or with user interaction, the process of creating works of art of a similar genre and satisfying the properties desired by the artist. Since the rules are bound to be considerably more complex than those currently exposed, we are looking to use machine-learning techniques to develop more sophisticated agents, which may behave more closely like the actual artist.},
 authors = {['Mike Holcombe', 'Samantha Smith', 'Rowan Merewood', 'Andy Swingeford']},
 journal = {Elsevier},
 title = {30 Computational modelling of creativity in abstract art},
 year = {2005}
}

@Filtered Article{53f79bd0-d430-4881-8520-5283bb7f4d86,
 abstract = {Resilience Engineering principles are becoming increasingly popular in healthcare to improve patient safety. FRAM is the best-known Resilience Engineering method with several examples of its application in healthcare available. However, the guidance on how to apply FRAM leaves gaps, and this can be a potential barrier to its adoption and potentially lead to misuse and disappointing results. The article provides a self-reflective analysis of FRAM use cases to provide further methodological guidance for successful application of FRAM to improve patient safety. Five FRAM use cases in a range of healthcare settings are described in a structured way including critical reflection by the original authors of those studies. Individual reflections are synthesised through group discussion to identify lessons for the operationalisation of FRAM in healthcare. Four themes are developed: (1) core characteristics of a FRAM study, (2) flexibility regarding the underlying epistemological paradigm, (3) diversity with respect to the development of interventions, and (4) model complexity. FRAM is a systems analysis method that offers considerable flexibility to accommodate different epistemological positions, ranging from realism to phenomenology. We refer to these as computational FRAM and reflexive FRAM, respectively. Practitioners need to be clear about their analysis aims and their analysis position. Further guidance is needed to support practitioners to tell a convincing and meaningful “system story” through the lens of FRAM.},
 authors = {['M. Sujan', 'L. Pickup', 'M.S. {de Vos}', 'R. Patriarca', 'L. Konwinski', 'A. Ross', 'P. McCulloch']},
 journal = {Safety Science},
 keywords = {['Patient Safety', 'FRAM', 'Resilience Engineering', 'System Safety']},
 title = {Operationalising FRAM in Healthcare: A critical reflection on practice},
 year = {2023}
}

@Filtered Article{548e25d8-51e4-47a2-951e-bc30c3a665f0,
 abstract = {We consider a class of Kuramoto models, with an array of N individual k-dimensional clocks (k>1), coupled within a directed, range dependent, network. For each directed connection, a signal triggered at the sending clock incurs a (real valued) time delay before arriving at the receiving clock, where it induces an instantaneous phase reset affecting all k-phases. Instantaneous phase resetting maps (PRMs) for k-dimensional clocks have received little attention. The system may be treated as open and subject to periodic, or other types of, PRM forcing at any individual clock, as a result of external forcing stimuli. We show how the full system, with Nk phase variables, responds to such stimuli, as the impact spreads across the entire network. Beyond simulations, we employ methods to reverse engineer the dynamical behaviour of the whole: estimating the intrinsic dimensions of the responses to different experiments; and by analysing pairwise comparisons between those responses. This shows that the system’s responses are governed by a hierarchy of internal dynamical modes, existing across both the Nk phases and over time. We argue that this Kuramoto system is a model for the human cortex, where each k-dimensional clock models the dynamics of a single neural column, which contains 10,000 densely inter-connected neurons. The Kuramoto model exploits the natural network of networks architecture of the human cortex. An array of N=1M such columns/clocks is at the 10B neuron scale of the human cortex. However its simulation is far more accessible than very large scale (VLS) simulations of neuron-to-neuron systems on supercomputers. The latent modes may have important implications for cognition (information processing) and for consciousness (the existence of internal phenomenological experiences). We argue that the existence of the latter plays a key role in preconditioning the former, reducing the decision sets and the cognitive load, and thus enabling a fast-thinking evolutionary advantage. This is the first time that systems of k-dimensional clocks (k> 1), coupled via time-lagged PRMs, within range dependent networks, have been deployed to demonstrate the basic internal phenomenological elements (of consciousness) and their potential role within immediate cognition. Statement of Significance: We argue that this Kuramoto system is a model for the human cortex, where each of 1M k-dimensional clocks models the dynamics of a single neural column (containing 10,000 densely inter-connected neurons). This Kuramoto model exploits the natural network of networks architecture of the human cortex. Large scale human cortex simulations, with 10B neutrons, usually require a super computer. We show that similar results, using this model, can be obtained on a laptop. In particular we show that such dynamical can support internal phenomenological elements (of conscious experience) and we discuss their potential role in preconditioning immediate cognition, furnishing a “fast thinking” evolutionary advantage to the human brain.},
 authors = {['Martin Brennan', 'Peter Grindrod CBE']},
 journal = {Brain Multiphysics},
 keywords = {['Kuramoto models', 'Range-dependent networks', 'High dimensional clocks', 'Phase-resetting maps', 'The human cortex', 'Consciousness']},
 title = {Generalised Kuramoto models with time-delayed phase-resetting for k-dimensional clocks},
 year = {2023}
}

@Filtered Article{548f603e-f605-4ec7-bfed-aebc96b811c3,
 abstract = {This chapter describes the Bioconductor project and details of its open source facilities for analysis of microarray and other high‐throughput biological experiments. Particular attention is paid to concepts of container and workflow design, connections of biological metadata to statistical analysis products, support for statistical quality assessment, and calibration of inference uncertainty measures when tens of thousands of simultaneous statistical tests are performed.},
 authors = {['Mark Reimers', 'Vincent J. Carey']},
 journal = {Academic Press},
 title = {[8] Bioconductor: An Open Source Framework for Bioinformatics and Computational Biology},
 year = {2006}
}

@Filtered Article{5497dbfc-9589-4a04-a2eb-165316ceac11,
 abstract = {Autism Spectrum Conditions are typified by a divergence in cognitive style from that of the non-autistic population. Cognitive differences in autism may underlie significant strengths, but also increase vulnerability to psychopathology such as anxiety, which is a major problem for many autistic people. Many autistic people also do not respond to typical psychotherapeutic interventions, suggesting that autism-specific models and interventions are needed. We advance a theoretical model explaining how three constructs, attenuated predictions, intolerance of uncertainty, and ‘black and white thinking’, may interact to lead to anxiety in autism. We hope to start a dialogue surrounding how we can best address specific autistic cognitive differences that may lead to distress by developing appropriate models, measurements, and psychotherapeutic interventions.},
 authors = {['Eloise Stark', 'James Stacey', 'Will Mandy', 'Morten L. Kringelbach', 'Francesca Happé']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['autism', 'cognition', 'anxiety', 'predictive processing', 'intolerance of uncertainty', 'black and white thinking']},
 title = {Autistic Cognition: Charting Routes to Anxiety},
 year = {2021}
}

@Filtered Article{54e41b5b-c679-4749-befe-e24ccf443ad6,
 abstract = {In the past, a range of computational models have been developed for analysing the social implications of spatial patterns and types. While such models are typically focussed on macro-patterns, often in cellular or linearly-organised spaces, few models exist for predicting where people will cluster within complex environments. One reason for this relates to the inherent uncertainty associated with spatial attributes and consequently of human spatial behaviours. The present paper draws on the concept of fuzzy spatial objects to develop an approach to handle such uncertainty in architecture. Focussing on large, open plan spaces, where the configuration of space does not define strict patterns of usage, the paper proposes a computational model for predicting patterns of spatial inhabitation. This new model relies on the theory of fuzzy sets to propose the existence of a “fuzzy architectural spatial object, (FASO)” which is comprised of spatial units with degrees of membership that reflect the possibility of a person being present in a sub-space or involved in a sub-function within a larger space. This model calculates and visualises the FASOs using a fuzzy inference engine and represents the space as distributed possibilities of presence according to the given data. After describing the model the paper demonstrates its application in the prediction of patterns of usage within a major exhibition space, and then presents a check of the efficacy of this prediction against the actual inhabitation of the space.},
 authors = {['Aslı Çekmiş', 'Işıl Hacıhasanoğlu', 'Michael J. Ostwald']},
 journal = {Building and Environment},
 keywords = {['Fuzzy logic', 'Fuzzy set', 'Spatial uncertainty', 'Occupancy prediction', 'Open-planned spaces']},
 title = {A computational model for accommodating spatial uncertainty: Predicting inhabitation patterns in open-planned spaces},
 year = {2014}
}

@Filtered Article{5599ee8a-ef21-4b94-8af2-84222a29b2df,
 abstract = {This paper examines the recent emergence of AI-powered chatbots such as ChatGPT through the lens of the Cognitive Mediation Networks Theory (CMNT), deducing that the introduction of this radically new technology will likely create a new stage of collective cognitive functioning, called “Sophotechnic Mediation”, with characteristics that can be extrapolated from the way these new tools work and the dynamics of the sociocultural structures being created around them. From that description, the Sophotechnic Mediation Scale is proposed as a means to assess the extent of an individual's internalization of the new form of thinking. A preliminary empirical investigation with 132 higher education professors and students found the instrument to be statistically consistent, yielding a unidimensional and Gaussian score that behaves as a developmental trait emerging from the interaction with generative AIs, mediated by age and mastery of previous digital technologies and their cultural elements. It is concluded that the results are suggestive of the validity of the new scale and warrant further research.},
 authors = {['Bruno Campello {de Souza}', 'Agostinho {Serrano de Andrade Neto}', 'Antonio Roazzi']},
 journal = {Computers in Human Behavior: Artificial Humans},
 keywords = {['ChatGPT', 'Chatbots', 'Artificial intelligence', 'Cognitive mediation networks theory', 'Hyperculture', 'Sophotechnic']},
 title = {The generative AI revolution, cognitive mediation networks theory and the emergence of a new mode of mental functioning: Introducing the Sophotechnic Mediation scale},
 year = {2024}
}

@Filtered Article{55a66727-5460-4655-abb4-fce554941708,
 abstract = {In the XIX century and earlier physicists such as Newton, Mayer, Hooke, Helmholtz and Mach were actively engaged in the research on psychophysics, trying to relate psychological sensations to intensities of physical stimuli. Computational physics allows to simulate complex neural processes giving a chance to answer not only the original psychophysical questions but also to create models of the mind. In this paper several approaches relevant to modeling of the mind are outlined. Since direct modeling of the brain functions is rather limited due to the complexity of such models a number of approximations is introduced. The path from the brain, or computational neurosciences, to the mind, or cognitive sciences, is sketched, with emphasis on higher cognitive functions such as memory and consciousness. No fundamental problems in understanding of the mind seem to arise. From a computational point of view realistic models require massively parallel architectures.},
 authors = {['Włodzisław Duch']},
 journal = {Computer Physics Communications},
 title = {Computational physics of the mind},
 year = {1996}
}

@Filtered Article{55e93721-1182-46fa-b479-6f3181d010e4,
 abstract = {Dynamical games model interactions between agents that take place in ever-shifting environments. Due to the increasing penetration of autonomous systems to society, understanding and predicting the outcomes of these games has become crucial. In this work, we highlight the importance of nonequilibrium solutions to dynamical games through the lens of bounded rationality. We describe the principles of level-k thinking and cognitive hierarchy – concepts developed in the field of economics – via mathematical tools and formulation of control theory. We describe the main principles of bounded rationality for nonequilibrium differential games in both nonlinear non-zero-sum and linear zero-sum settings. The importance of those approaches is highlighted in problems of pursuit evasion between Unmanned Aerial Vehicles, while the core of the bounded rationality principles that we employ are extended to discrete stochastic dynamical games. The versatility of the proposed approach is complemented by rigorous mathematical guarantees that enable predictability of the games’ outcomes.},
 authors = {['Kyriakos G. Vamvoudakis', 'Filippos Fotiadis', 'Aris Kanellopoulos', 'Nick-Marios T. Kokolakis']},
 journal = {Annual Reviews in Control},
 title = {Nonequilibrium dynamical games: A control systems perspective},
 year = {2022}
}

@Filtered Article{560d4e1b-335a-441a-99db-8cc84c88a7cd,
 abstract = {Creative thought relies on the reorganisation of existing knowledge. Sleep is known to be important for creative thinking, but there is a debate about which sleep stage is most relevant, and why. We address this issue by proposing that rapid eye movement sleep, or ‘REM’, and non-REM sleep facilitate creativity in different ways. Memory replay mechanisms in non-REM can abstract rules from corpuses of learned information, while replay in REM may promote novel associations. We propose that the iterative interleaving of REM and non-REM across a night boosts the formation of complex knowledge frameworks, and allows these frameworks to be restructured, thus facilitating creative thought. We outline a hypothetical computational model which will allow explicit testing of these hypotheses.},
 authors = {['Penelope A. Lewis', 'Günther Knoblich', 'Gina Poe']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['sleep', 'memory', 'creativity', 'reactivation', 'replay', 'consolidation']},
 title = {How Memory Replay in Sleep Boosts Creative Problem-Solving},
 year = {2018}
}

@Filtered Article{5626b9d5-a3bc-43b5-8423-8cc627330573,
 abstract = {This paper proposes a project-based learning course utilising multiple quadrotors, aiming to solidify and amplify technical knowledge and develop critical thinking capabilities. An engineering problem is provided as a surveillance mission with multiple quadrotors autonomously searching, detecting, and tracking ground vehicles. The course covers all stages of multi-agent autonomy development, from identiflying system requirements, designing software and hardware, to conducting demonstrations in a drone flying arena. During the course, students improve their ability to critically formulate, solve and evaluate engineering problems as well as gain and apply technical knowledge in all aspects of autonomy such as fight dynamics, control, navigation, guidance, task allocation, situational awareness and communication. The paper details the problem design, course timeline, outcomes, and key lessons learnt from the course.},
 authors = {['Hae-In Lee', 'Dmitry Ignatyev', 'Hyo-Sang Shin', 'Antonios Tsourdos']},
 journal = {IFAC-PapersOnLine},
 keywords = {['Project-Based Learning', 'Autonomous Multi-Agent System', 'Unmanned Aerial Vehicles', 'Quadrotor', 'Surveillance']},
 title = {Project-Based Learning Course Design for Multi-Agent Autonomy Using Quadrotors⁎⁎This group design project of academic year 2023/24 has been sponsored by Leonardo S.p.A.},
 year = {2024}
}

@Filtered Article{566c56b1-693d-459c-bc6e-e50a19aa35b9,
 abstract = {A new form of e-governance is proposed based on systems seen in biological life at all scales. This model of e-governance offers the performance of collective superintelligence, equally high ethical quality, and a substantial reduction in resource requirements for government functions. In addition, the problems seen in modern forms of government such as misrepresentation, corruption, lack of expertise, short-term thinking, political squabbling, and popularity contests may be rendered virtually obsolete by this approach. Lastly, this model of government generates a digital ecosystem of intelligent life which mirrors physical citizens, serving to bridge the emotional divide between physical and digital life, while also producing the first form of government able to keep pace with accelerating technological progress.},
 authors = {['Kyrtin Atreides']},
 journal = {Procedia Computer Science},
 keywords = {['mASI', 'AGI', 'e-governance', 'mediated artificial superintelligence', 'collective superintelligence', 'direct digital democracy', 'liquid democracy', 'EAP', 'Effective Altruistic Principles', 'Ethical Living Democracy', 'ELD']},
 title = {E-governance with ethical living democracy},
 year = {2021}
}

@Filtered Article{56b9b377-b64e-48a1-ae9f-f43d15c43bfe,
 abstract = {Recognized as the crown jewel of system software, Operating System (OS) is notoriously challenging to teach and learn, given its abstract concepts, broad scope, and the imperative for hands-on experience. How to cater to the keen interest in OS among students across all universities while providing a rich system software engineering experience? Engaging the OS community on campus plays a pivotal role in enhancing undergraduate education. Our institution has implemented a novel approach over the past nine years through the miniOS pilot class initiative. This program allows students enrolled in the OS course to opt into an additional, practical component where they engage in developing the OS kernel using miniOS, our teaching platform. Participants dedicate their spare time to a series of labs aimed at incrementally introducing them to miniOS, collaborate on a project to add a new functional module to miniOS, and ultimately present their work in a formal defense. This hands-on experience supplements the theoretical coursework, with the notable advantage that pilot class participants are exempt from the traditional final exam. Instead, their grade is determined by their contributions to the pilot class, with distinguished projects being integrated into the evolving miniOS kernel—a witness to the collective effort of each class. Since its inception in 2015, the miniOS pilot class has nurtured 182 undergraduates and 22 graduate students, contributing significantly to the OS community engagement on campus. Through this initiative, we have gleaned six key insights and six lessons, which we are eager to share with the broader educational community.},
 authors = {['Jianhua Gu', 'Mingxuan Liu', 'Tianhai Zhao']},
 journal = {Future Generation Computer Systems},
 keywords = {['Community engagement', 'Software engineering education', 'Operating system', 'Software engineering management']},
 title = {Enhancing campus OS community engagement through the miniOS pilot class: A nine-year journey},
 year = {2024}
}

@Filtered Article{570cff70-6891-4493-81d8-19638e718780,
 abstract = {Pair programming (PP) is a usefulness approach to fostering computational thinking (CT) for young students. However, there are many factors to impact the effectiveness of PP. Among all factors, the social factors are often ignored by researchers. Therefore, this study aimed to explore the impact of two social factors (gender and partnership) on PP in a primary school setting. To that end, we conducted PP experiments in four classes from the sixth grade in a Chinese primary school. The research results indicated: (a) there was no significant difference on compatibility among the gender pairs, but a significant difference among partnership pairs; (b) there was no significant difference on programming achievement and confidence among different pairs, and girls became more productive and confidence in PP; and (c) PP tightened up the partnership within pairs. These findings suggest that teachers should take partnership into account as an important factor in PP or other collaborative learning, and adopt PP as an effective approach to decrease the gender gap in programming courses, and make students socialize.},
 authors = {['Baichang Zhong', 'Qiyun Wang', 'Jie Chen']},
 journal = {Computers in Human Behavior},
 keywords = {['Primary education', 'Pair programming', 'Social factors', 'Gender', 'Partnership']},
 title = {The impact of social factors on pair programming in a primary school},
 year = {2016}
}

@Filtered Article{570fceec-ba3d-4e89-b3ac-c7896cc94d67,
 abstract = {Objective
This work aims to create an automatic detection process of cardiac structures in both short-axis and long-axis views. A workflow inspired by human thinking process, for better explainability.
Methods
we began by separating the images into two classes: long axis and short axis, using a Residual Network model. Then, we used Particle Swarm Optimization for general segmentation. After segmentation, a characterization step based on shape descriptors calculated from bounding box and ANOVA for features selection were applied on the binary images to detect the location of each region of interest: lung, left and right ventricle in the short-axis view, the aorta, the left heart (left atrium and ventricle), and the right heart (right atrium and ventricle) in the long axis view.
Results
we achieved a 90% accuracy on view separation. We have selected: Elongation, Compactness, Circularity, Type Factor, for short axis identification; and:Area, Centre of Mass Y, Moment of Inertia XY, Moment of Inertia YY, for long axis identification.
Conclusion
a successful separation of long axis and short axis views allows for a better characterization and detection of segmented cardiac structures. After that, any method can be applied for segmentation, attribute selection, and classification.
Significance
an attempt to introduce explainability into cardiac image segmentation, we tried to mimic the human workflow while computerizing each step. The process seems to be valid and added clarity and interpretability to the detection.},
 authors = {['Laidi Amel', 'Mohammed Ammar', 'Mostafa {El Habib Daho}', 'Said Mahmoudi']},
 journal = {Biomedical Signal Processing and Control},
 keywords = {['Cardiac MRI Segmentation', 'Shape Descriptors', 'Particle Swarm Optimization', 'Residual Network', 'Interpretability']},
 title = {Toward an automatic detection of cardiac structures in short and long axis views},
 year = {2023}
}

@Filtered Article{571933cc-cf21-4407-82fb-1508988ec42c,
 abstract = {Optimization problems of modern day power system are very challenging to resolve because of its design complexity, wide geographical dispersion and influence from many unpredictable factors. For that reason, it is essential to apply most effective optimization techniques by taking full benefits of simplified formulation and execution of a particular problem. This study presents a summary of significant hybrid bio-inspired computational intelligence (CI) techniques utilized for power system optimization. Authors have reviewed an extensive range of hybrid CI techniques and examined the motivations behind their improvements. Various applications of hybrid bio-inspired CI algorithms have been highlighted in this paper. In addition, few drawbacks regarding the hybrid CI algorithms are explained. Current trends in CI techniques from the past researches have also been discussed in the domain of power system optimization. Lastly, some future research directions are suggested for further advancement of hybrid techniques.},
 authors = {['Imran Rahman', 'Junita Mohamad-Saleh']},
 journal = {Applied Soft Computing},
 keywords = {['Computational intelligence', 'Hybrid optimization', 'Optimization', 'Bio-inspired computation', 'Power system']},
 title = {Hybrid bio-Inspired computational intelligence techniques for solving power system optimization problems: A comprehensive survey},
 year = {2018}
}

@Filtered Article{5754136d-c7c2-42c0-90d5-4b3b46ffe261,
 abstract = {Alzheimer’s disease (AD) is an irreversible neurological illness identified by deficits in thinking, behavior, and memory. Early detection and prevention of AD is a crucial and difficult task. DL (Deep Learning) has gained significant attention recently as a potential tool for early AD detection. However, traditional diagnostic methods such as cognitive tests and manual analysis of brain imaging are time consuming and prone to error. Hence, there is a need for an automated model which shows better classification performance. To tackle these issues, this study presents a system to improve AD recognition performance. Initially, the pre-processing and skull stripping processes are performed. Then, for segmenting the grey, whiter matters and Cerebrospinal Fluid (CSF), the optimal clustering process is carried out. Here, the optimal center of clusters is selected by the metaheuristic optimization Adaptive Reptile Search Algorithm-Clustering Approach (ARSA-CA) is utilized. The proposed ARSA is the integration of the optimization RSA and simulated annealing (SA). Finally, for classifying the different classes of AD, the DL approach pre-trained SqueezeNet is utilized. The experimentation is carried out on the ADNI and OASIS datasets and achieved better accuracies of 98.3% and 98.2% respectively. Thus, it is proved that the proposed model is suitable for identifying AD.},
 authors = {['K. {Emily Esther Rani}', 'S. Baulkani']},
 journal = {Biomedical Signal Processing and Control},
 keywords = {['Alzheimer disease', 'Optimal center', 'Clustering', 'Adaptive reptile search algorithm', 'Pre-trained squeezenet']},
 title = {Alzheimer disease classification using optimal clustering based pre-trained SqueezeNet model},
 year = {2025}
}

@Filtered Article{58880a55-acd0-493c-8620-6a34bf28e46f,
 abstract = {Artificial intelligence (AI) has led to a shift in modern dance education. Immersive technologies have become increasingly common worldwide, helping educators to improve the quality of dance pedagogy and increase the effectiveness of dance training. The article investigates the ways of using immersive technologies powered by artificial intelligence in dance education. The research explores the theoretical literature on dance education and the use of artificial intelligence in dance education and dance choreography. The scholars examine the impact of innovative technology solutions used in dance pedagogical practice on the development of dance skills in students. The study also discusses the functionality of interactive and multimedia dance teaching systems, including AI-powered virtual mentoring and cognitive simulations of human mind operations. The research analysed the use of virtual reality (VR), augmented reality (AR), and mixed reality (MR) in dance education. This research also focuses on mobile applications used for teaching modern dance. The proposed framework for dance education is based on digital technologies, which help to develop dance skills and improve teaching practices. The scholars conclude that the development and improvement of dance skills are possible only if a teacher combines virtual and real environments in everyday practices. The findings can be used by dance teachers, professional dancers, software developers, and researchers who examine the innovative practices and the application of artificial intelligence in dance education. The ecosystem model reframes thinking about approaches to dance education and can serve as the basis for further development of dance courses and dance style teaching modes.},
 authors = {['Zheng Wang']},
 journal = {Technology in Society},
 keywords = {['Artificial intelligence', 'Dance education', 'Dance mobile applications', 'Educational ecosystem', 'Immersive technologies', 'Intelligent action recognition system', 'Interactive dance training', 'Virtual', 'Augmented and mixed reality', 'Virtual mentoring']},
 title = {Artificial intelligence in dance education: Using immersive technologies for teaching dance skills},
 year = {2024}
}

@Filtered Article{590bcf60-02fb-4865-82bd-f4f81ca25740,
 abstract = {Brain-computer interface (BCI) technologies are progressing rapidly and may eventually be implemented widely within society, yet their risks have arguably not yet been comprehensively identified, nor understood. This study analysed an anticipated invasive BCI system lifecycle to identify the individual, organisational, and societal risks associated with BCIs, and controls that could be used to mitigate or eliminate these risks. A BCI system lifecycle work domain analysis model was developed and validated with 10 subject matter experts. The model was subsequently used to undertake a systems thinking-based risk assessment approach to identify risks that could emerge when functions are either undertaken sub-optimally or not undertaken at all. Eighteen broad risk themes were identified that could negatively impact the BCI system lifecycle in a variety of unique ways, while a larger number of controls for these risks were also identified. The most concerning risks included inadequate regulation of BCI technologies and inadequate training of BCI stakeholders, such as users and clinicians. In addition to specifying a practical set of risk controls to inform BCI device design, manufacture, adoption, and utilisation, the results demonstrate the complexity involved in managing BCI risks and suggests that a system-wide coordinated response is required. Future research is required to evaluate the comprehensiveness of the identified risks and the practicality of implementing the risk controls.},
 authors = {['Brandon J. King', 'Gemma J.M. Read', 'Paul M. Salmon']},
 journal = {Applied Ergonomics},
 keywords = {['Brain-computer interfaces', 'Risk assessment', 'System modelling']},
 title = {Identifying risk controls for future advanced brain-computer interfaces: A prospective risk assessment approach using work domain analysis},
 year = {2023}
}

@Filtered Article{59237627-9f07-43a4-8bdc-d265ea330fef,
 abstract = {ABSTRACT
Online food delivery has transformed the last-mile of food and grocery delivery, with unnoticed yet often significant impacts upon the transport and logistics network. This new model of food delivery is not just increasing congestion in urban centers though, it is also changing the contours and qualities of those doing delivery—namely through gig economy work. This new system of food consumption and provision is rapidly gaining traction, but assessments around its current and future sustainability tend to hold separate the notions of social, environmental and economic sustainability—with few to date working to understand how these can interact, influence and be in conflict with one another. This paper seeks to work with this broader understanding of sustainability, whilst also foregrounding the perspectives of gig economy couriers who are often marginalized in such assessments of the online food delivery system. We make use of systems thinking and Campbell’s conflict model of sustainability to do this. In assessing the online food delivery in this way, we seek to not only provide a counternarrative to some of these previous assessments, but to also challenge those proposing the use of gig economy couriers as an environmentally sustainable logistics intervention in other areas of last-mile logistics to consider how this might impact the broader sustainability of their system, now and in the future.},
 authors = {['Carolynne Lord', 'Oliver Bates', 'Adrian Friday', 'Fraser McLeod', 'Tom Cherrett', 'Antonio Martinez-Sykora', 'Andy Oakey']},
 journal = {International Journal of Sustainable Transportation},
 keywords = {['Gig economy couriers', 'path dependence', 'rebounds', 'sustainability', 'systems thinking']},
 title = {The sustainability of the gig economy food delivery system (Deliveroo, UberEATS and Just-Eat): Histories and futures of rebound, lock-in and path dependency},
 year = {2023}
}

@Filtered Article{593b1db8-ec51-4200-8d00-44f525a1b10e,
 abstract = {In the recent years, engineering new-to-nature CO2- and C1-fixing metabolic pathways made a leap forward. New, artificial pathways promise higher yields and activity than natural ones like the Calvin-Benson-Bassham (CBB) cycle. The question remains how to best predict their in vivo performance and what actually makes one pathway “better” than another. In this context, we explore aerobic carbon fixation pathways by a computational approach and compare them based on their specific activity and yield on methanol, formate, and CO2/H2 considering the kinetics and thermodynamics of the reactions. Besides pathways found in nature or implemented in the laboratory, this included two completely new cycles with favorable features: the reductive citramalyl-CoA cycle and the 2-hydroxyglutarate-reverse tricarboxylic acid cycle. A comprehensive kinetic data set was collected for all enzymes of all pathways, and missing kinetic data were sampled with the Parameter Balancing algorithm. Kinetic and thermodynamic data were fed to the Enzyme Cost Minimization algorithm to check for respective inconsistencies and calculate pathway-specific activities. The specific activities of the reductive glycine pathway, the CETCH cycle, and the new reductive citramalyl-CoA cycle were predicted to match the best natural cycles with superior product-substrate yield. However, the CBB cycle performed better in terms of activity compared to the alternative pathways than previously thought. We make an argument that stoichiometric yield is likely not the most important design criterion of the CBB cycle. Still, alternative carbon fixation pathways were paretooptimal for specific activity and product-substrate yield in simulations with C1 substrates and CO2/H2 and therefore hold great potential for future applications in Industrial Biotechnology and Synthetic Biology.},
 authors = {['Hannes Löwe', 'Andreas Kremling']},
 journal = {BioDesign Research},
 title = {In-Depth Computational Analysis of Natural and Artificial Carbon Fixation Pathways},
 year = {2021}
}

@Filtered Article{59581ce2-f40e-454b-a973-984b5a8ac76e,
 abstract = {Power consumption in computational clusters has reached critical levels. High-end cluster performance improves exponentially while the power consumed and heat dissipated increase operational costs and failure rates. Yet, the demand for more powerful machines continues to grow. In this chapter, we motivate the need to reconsider the traditional performance-at-any-cost cluster design approach. We propose designs where power and performance are considered critical constraints. We describe power-aware and low power techniques to reduce the power profiles of parallel applications and mitigate the impact on performance.},
 authors = {['Kirk W. Cameron', 'Rong Ge', 'Xizhou Feng']},
 journal = {Elsevier},
 title = {Designing Computational Clusters for Performance and Power},
 year = {2007}
}

@Filtered Article{595dc38d-0f9e-4aeb-ae92-1fdec902e783,
 abstract = {Analog integrated circuit (IC) design highly depends on reasoning, which distinguishes itself from other areas of IC design. Most of its innovation arises from qualitative reasoning by a pencil and paper. Innovation on the circuit structure needs quick analytical justification. Circuit-level reduced-scale modeling is a popular reasoning means. Circuit simulation tools can only serve partial justification on a design, while design insight still has to be acquired via manual analysis. A basic question has been in existence for many decades: how can we automate the analog IC design process? Many analog synthesis tools proposed decades ago could not make it to this date in the design practice. In this survey the major reason is attributed to the black-box style of the tool design. Human designer's creativity is shielded away from the tool operation while the formal design knowledge hardcoded in those tools remains at a very primitive level. By analyzing the defects of those existing tools, this survey advocates an open tool development philosophy whose major goal is to support human-machine interaction. On the one side a design automation tool is mainly aimed at providing aid for tasks that require analytical deduction while on the other side designers are expected to exercise their creativity based on the machine-generated results. Such human-machine co-working style is believed to be a more feasible solution to analog IC design automation based on the currently available computation technology. In this survey the art of symbolic computation is promoted to be the enabling technology for computer-aided analytical generation. The symbolic computation technology today can support topological and analytical reasoning that is the most demanding need in the analog IC design practice. This survey further calls for more research on the formal methods that are applicable to design knowledge representation, human-machine interaction, and design inference. Some preliminary research results are reviewed and future research directions are pointed out.},
 authors = {['Guoyong Shi']},
 journal = {Integration},
 keywords = {['Analog integrated circuit (IC)', 'Computer-aided reasoning (CAR)', 'Formal information processing', 'Graph-pair decision diagram (GPDD)', 'Knowledge representation', 'Operational amplifier (opamp)', 'Symbolic computation']},
 title = {Toward automated reasoning for analog IC design by symbolic computation – A survey},
 year = {2018}
}

@Filtered Article{5981688f-260b-4366-ba3e-8ebb07265058,
 abstract = {Almost all design problems in the sciences and engineering can be formulated as optimization problems, and many image processing problems can also be related to or formulated as optimization problems. These optimization problems can be solved by optimization techniques. However, these problems are often highly nonlinear and are subject to multiple nonlinear constraints, which makes them very challenging to solve. The further complication to these challenges is the stringent time requirements and high dimensionality, which means that traditional optimization techniques, such as gradient-based methods cannot deal with such kinds of problems well. Recent trends tend to use bio-inspired optimization techniques as a promising alternative, and it is usually combined with traditional methods, especially in the area of image processing. These bio-inspired computational methods are usually based on swarm intelligence and can be very effective in coping with nonlinearity in real-world problems. This chapter presents an overview of bio-inspired computation and its application in image processing, including some current trends and important issues, such as efficiency and time constraints.},
 authors = {['X.-S. Yang', 'J.P. Papa']},
 journal = {Academic Press},
 keywords = {['algorithm', 'ant algorithm', 'artificial neural networks', 'bee algorithm', 'bat algorithm', 'bio-inspired computation', 'cuckoo search', 'firefly algorithm', 'harmony search', 'particle swarm optimization', 'metaheuristics', 'swarm intelligence', 'support vector machine', 'signal and image processing']},
 title = {Chapter 1 - Bio-inspired computation and its applications in image processing: an overview},
 year = {2016}
}

@Filtered Article{59f51b56-8198-4d93-9d75-25f5c49936b8,
 abstract = {Protocol analysis is the name for the methodology for eliciting, transcribing, and encoding verbal reports of thoughts into objective data for evaluating and testing theories of thinking. Philosophers since Aristotle have introspected on their own thinking as a means to analyze the structure of their thought processes. However, introspective analysis of one's thoughts and behavior was found to be reactive. In response to these criticisms a general theoretical framework was developed for how participants could verbalize their thinking without influencing the course of their thinking. Instructions to elicit such immediate reports were developed and shown to uncover thinking without the reactive effects due to explanations and descriptions of their thinking. Rigorous methods for analyzing verbal reports have been developed based on a formal analysis of the tasks. Short segments of verbal reports are coded into formal categories and the resulting data show similar reliability and validity as other forms of data on cognitive processes, such as reaction times and eye fixations. This general framework for collecting and analyzing verbal reports of thinking has been applied to laboratory studies of memory, problem solving, and decision making, and to everyday life in the study of expert performance and text comprehension.},
 authors = {['K.A. Ericsson']},
 journal = {Pergamon},
 title = {Protocol Analysis in Psychology},
 year = {2001}
}

@Filtered Article{5ab4ae02-851f-460c-8d48-9180cb250dce,
 authors = {['Margaret A. Boden']},
 journal = {North-Holland},
 title = {INFORMATION, COMPUTATION, AND COGNITIVE SCIENCE},
 year = {2008}
}

@Filtered Article{5b61e593-b6ae-4fe5-bfe3-be7fc9aac12d,
 abstract = {One of the greatest challenges facing the cognitive sciences is to explain what it means to know a language, and how the knowledge of language is acquired. The dominant approach to this challenge within linguistics has been to seek an efficient characterization of the wealth of documented structural properties of language in terms of a compact generative grammar—ideally, the minimal necessary set of innate, universal, exception-less, highly abstract rules that jointly generate all and only the observed phenomena and are common to all human languages. We review developmental, behavioral, and computational evidence that seems to favor an alternative view of language, according to which linguistic structures are generated by a large, open set of constructions of varying degrees of abstraction and complexity, which embody both form and meaning and are acquired through socially situated experience in a given language community, by probabilistic learning algorithms that resemble those at work in other cognitive modalities.},
 authors = {['Shimon Edelman', 'Heidi Waterfall']},
 journal = {Physics of Life Reviews},
 keywords = {['Computational cognitive linguistics', 'Psycholinguistics', 'Machine learning', 'Language acquisition']},
 title = {Behavioral and computational aspects of language and its acquisition},
 year = {2007}
}

@Filtered Article{5b954f7e-9f8c-4a64-b01f-a6531ac33150,
 abstract = {Cognitive computing seeks to build applications which model and mimic human thinking. One approach toward achieving this goal is to develop brain-inspired computational models. A prime example of such a model is the class of deep convolutional networks which is currently used in pattern recognition, machine vision, and machine learning. We offer a brief review of the mammalian neocortex, the minicolumn, and the ventral pathway. We provide descriptions of abstract neural circuits that have been used to model these areas of the brain. This include Poisson spiking networks, liquid computing networks, spiking models of feature discovery in the ventral pathway, spike-timing-dependent plasticity learning, restricted Boltzmann machines, deep belief networks, and deep convolutional networks. In summary, this chapter explores abstractions of neural networks found within the mammalian neocortex that support cognition and the beginnings of cognitive computation.},
 authors = {['A.S. Maida']},
 journal = {Elsevier},
 keywords = {['Brain simulation', 'Deep belief networks', 'Convolutional networks', 'Liquid computing', 'Biological neural networks', 'Neocortex']},
 title = {Chapter 2 - Cognitive Computing and Neural Networks: Reverse Engineering the Brain},
 year = {2016}
}

@Filtered Article{5bc069d0-d7b8-4098-a8c5-484ba9e53daf,
 authors = {['Hoon Hong', 'Deepak Kapur', 'Peter Paule', 'Franz Winkler', '{Faculty of RISC-Linz}']},
 journal = {Journal of Symbolic Computation},
 title = {Bruno Buchberger — A life devoted to symbolic computation},
 year = {2006}
}

@Filtered Article{5bd32783-f2c7-4a14-849e-40b15048721c,
 abstract = {Business processes are a key driver of organizational success, which is why business process improvement (BPI) is a central activity of business process management. Despite an abundance of approaches, BPI as a creative task is time-consuming and labour-intensive. Most importantly, its level of computational support is low. The few computational BPI approaches hardly leverage the opportunities brought about by computational creativity, neglect process data, and rely on rather rigid improvement patterns. Given the increasing amount of process data in the form of event logs and the uptake of generative machine learning for automating creative tasks in various domains, there is huge potential for BPI. Hence, following the design science research paradigm, we specified, implemented, and evaluated ProcessGAN, a novel computational BPI approach based on generative adversarial networks that supports the creation of BPI ideas. Our evaluation shows that ProcessGAN improves the creativity of process designers, particularly the originality of BPI ideas, and shapes up useful in real-world settings. Moreover, ProcessGAN is the first approach to combine BPI and computational creativity.},
 authors = {['Christopher {van Dun}', 'Linda Moder', 'Wolfgang Kratsch', 'Maximilian Röglinger']},
 journal = {Decision Support Systems},
 keywords = {['Business process improvement', 'Business process redesign', 'Generative adversarial networks', 'Generative machine learning', 'Process mining']},
 title = {ProcessGAN: Supporting the creation of business process improvement ideas through generative machine learning},
 year = {2023}
}

@Filtered Article{5bdcff72-0e65-4f95-b373-e3d49ee7124d,
 abstract = {Background
Self-guided, Web-based interventions for depression show promising results but suffer from high attrition and low user engagement. Online peer support networks can be highly engaging, but they show mixed results and lack evidence-based content.
Objective
Our aim was to introduce and evaluate a novel Web-based, peer-to-peer cognitive reappraisal platform designed to promote evidence-based techniques, with the hypotheses that (1) repeated use of the platform increases reappraisal and reduces depression and (2) that the social, crowdsourced interactions enhance engagement.
Methods
Participants aged 18-35 were recruited online and were randomly assigned to the treatment group, “Panoply” (n=84), or an active control group, online expressive writing (n=82). Both are fully automated Web-based platforms. Participants were asked to use their assigned platform for a minimum of 25 minutes per week for 3 weeks. Both platforms involved posting descriptions of stressful thoughts and situations. Participants on the Panoply platform additionally received crowdsourced reappraisal support immediately after submitting a post (median response time=9 minutes). Panoply participants could also practice reappraising stressful situations submitted by other users. Online questionnaires administered at baseline and 3 weeks assessed depression symptoms, reappraisal, and perseverative thinking. Engagement was assessed through self-report measures, session data, and activity levels.
Results
The Panoply platform produced significant improvements from pre to post for depression (P=.001), reappraisal (P<.001), and perseverative thinking (P<.001). The expressive writing platform yielded significant pre to post improvements for depression (P=.02) and perseverative thinking (P<.001), but not reappraisal (P=.45). The two groups did not diverge significantly at post-test on measures of depression or perseverative thinking, though Panoply users had significantly higher reappraisal scores (P=.02) than expressive writing. We also found significant group by treatment interactions. Individuals with elevated depression symptoms showed greater comparative benefit from Panoply for depression (P=.02) and perseverative thinking (P=.008). Individuals with baseline reappraisal deficits showed greater comparative benefit from Panoply for depression (P=.002) and perseverative thinking (P=.002). Changes in reappraisal mediated the effects of Panoply, but not the expressive writing platform, for both outcomes of depression (ab=-1.04, SE 0.58, 95% CI -2.67 to -.12) and perseverative thinking (ab=-1.02, SE 0.61, 95% CI -2.88 to -.20). Dropout rates were similar for the two platforms; however, Panoply yielded significantly more usage activity (P<.001) and significantly greater user experience scores (P<.001).
Conclusions
Panoply engaged its users and was especially helpful for depressed individuals and for those who might ordinarily underutilize reappraisal techniques. Further investigation is needed to examine the long-term effects of such a platform and whether the benefits generalize to a more diverse population of users.
Trial Registration
ClinicalTrials.gov NCT02302248; https://clinicaltrials.gov/ct2/show/NCT02302248 (Archived by WebCite at http://www.webcitation.org/6Wtkj6CXU).},
 authors = {['Robert R Morris', 'Stephen M Schueller', 'Rosalind W Picard']},
 journal = {Journal of Medical Internet Research},
 keywords = {['Web-based intervention', 'crowdsourcing', 'randomized controlled trial', 'depression', 'cognitive behavioral therapy', 'mental health', 'social networks']},
 title = {Efficacy of a Web-Based, Crowdsourced Peer-To-Peer Cognitive Reappraisal Platform for Depression: Randomized Controlled Trial},
 year = {2015}
}

@Filtered Article{5be0e334-54c5-48c2-a59a-0a0853d5ba60,
 abstract = {In this paper we propose the use of neural interference as the origin of quantum-like effects in the brain. We do so by using a neural oscillator model consistent with neurophysiological data. The model used was shown elsewhere to reproduce well the predictions of behavioral stimulus-response theory. The quantum-like effects are brought about by the spreading activation of incompatible oscillators, leading to an interference-like effect mediated by inhibitory and excitatory synapses.},
 authors = {['J. Acacio {de Barros}']},
 journal = {Biosystems},
 keywords = {['Disjunction effect', 'Quantum cognition', 'Quantum-like model', 'Neural oscillators', 'Stimulus-response theory']},
 title = {Quantum-like model of behavioral response computation using neural oscillators},
 year = {2012}
}

@Filtered Article{5c7525af-8116-4eb6-928f-d9cc4a849937,
 abstract = {This paper discusses the creation of an artificial labor market (ALM) as an agent-based simulation model. We trace the development of the ALM by adapting the traditional simulation life cycle into two main parts: the model phase and the simulation phase. In the modeling phase of the life cycle, we focus upon agent representation and specification within the virtual world. In the simulation phase, we discuss the use of scenario planning as the experimentation vehicle. Throughout, we use military recruit market as an example to illustrate the methodology. The benefits of the ALM are (1) it provides a virtual world for continuous computational experimentation, (2) it supports market segmentation by allowing “drilldowns” to finer and finer levels of granularity, and (3) when connected via a common OLAP interface to a “real world” counterpart, it facilitates a tightly integrated, persistent, “sense and respond” decision support functionality.},
 authors = {['Alok Chaturvedi', 'Shailendra Mehta', 'Daniel Dolk', 'Rick Ayer']},
 journal = {European Journal of Operational Research},
 keywords = {['Artificial intelligence', 'Decision support systems', 'Simulation', 'Modelling systems and languages', 'Economics']},
 title = {Agent-based simulation for computational experimentation: Developing an artificial labor market},
 year = {2005}
}

@Filtered Article{5c8aa745-12a5-4af8-9fc9-f35b11607a28,
 abstract = {Human ratings of conceptual disorganization, poverty of content, referential cohesion and illogical thinking have been shown to predict psychosis onset in prospective clinical high risk (CHR) cohort studies. The potential value of linguistic biomarkers has been significantly magnified, however, by recent advances in natural language processing (NLP) and machine learning (ML). Such methodologies allow for the rapid and objective measurement of language features, many of which are not easily recognized by human raters. Here we review the key findings on language production disturbance in psychosis. We also describe recent advances in the computational methods used to analyze language data, including methods for the automatic measurement of discourse coherence, syntactic complexity, poverty of content, referential coherence, and metaphorical language. Linguistic biomarkers of psychosis risk are now undergoing cross-validation, with attention to harmonization of methods. Future directions in extended CHR networks include studies of sources of variance, and combination with other promising biomarkers of psychosis risk, such as cognitive and sensory processing impairments likely to be related to language. Implications for the broader study of social communication, including reciprocal prosody, face expression and gesture, are discussed.},
 authors = {['Cheryl M. Corcoran', 'Vijay A. Mittal', 'Carrie E. Bearden', 'Raquel {E. Gur}', 'Kasia Hitczenko', 'Zarina Bilgrami', 'Aleksandar Savic', 'Guillermo A. Cecchi', 'Phillip Wolff']},
 journal = {Schizophrenia Research},
 keywords = {['Psychosis', 'Automated language analysis', 'Natural language processing', 'Machine learning', 'Semantic coherence', 'Discourse coherence', 'Referential coherence', 'Semantic density', 'Latent semantic analysis', 'Digital phenotyping', 'Psychosis risk', 'Clinical high risk', 'Ultra high risk', 'Schizophrenia']},
 title = {Language as a biomarker for psychosis: A natural language processing approach},
 year = {2020}
}

@Filtered Article{5cd4066a-ba90-40da-b550-307ece6070f4,
 abstract = {Land degradation and early forms of desertification in both advanced economies and emerging countries reflect complex socio-environmental processes driven by multiple interactions between biophysical and socioeconomic forces across different spatial scales. The present study investigates desertification risk, land degradation, and socio-demographic dynamics through the lens of “resilience,” adopting complex adaptive systems (CAS) thinking. The resilience of socio-environmental systems exposed to land degradation is defined as the capacity of a regional economy to respond to crises and reorganize by making changes to preserve functions, structure, and feedback, and to promote future development options. By reviewing the socioeconomic resilience of local socio-ecological systems exposed to land degradation, this study achieves a better comprehension of the multifaceted processes that lead to a higher risk of desertification and the intimate relationship with underlying population trends and demographic dynamics. A comprehensive approach based on resilience thinking was formulated to review both environmental and socio-demographic issues at the landscape scale, and provide a suitable foundation for sustainability science and regional development policies.},
 authors = {['Gianluca Egidi', 'Luca Salvati']},
 journal = {Chinese Journal of Population, Resources and Environment},
 keywords = {['Population dynamics', 'Ecosystem functioning', 'Socio-ecological resilience', 'Complex adaptive systems', 'Interpretative framework']},
 title = {Desertification risk, economic resilience and social issues: From theory to practice},
 year = {2020}
}

@Filtered Article{5d2d18eb-75f7-4104-917a-4615685b32cf,
 abstract = {Background
Nurses are essential for assessing and managing acute pain in hospitalized patients, especially those who are unable to self-report pain. Given their role and subject matter expertise (SME), nurses are also essential for the design and development of a supervised machine learning (ML) model for pain detection and clinical decision support software (CDSS) in a pain recognition automated monitoring system (PRAMS). Our first step for developing PRAMS with nurses was to create SME-friendly data labeling software.
Purpose
To develop an intuitive and efficient data labeling software solution, Human-to-Artificial Intelligence (H2AI).
Method
The Human-centered Design for Embedded Machine Learning Solutions (HCDe-MLS) model was used to engage nurses. In this paper, HCDe-MLS will be explained using H2AI and PRAMS as illustrative cases.
Findings
Using HCDe-MLS, H2AI was developed and facilitated labeling of 139 videos (mean = 29.83 min) with 3189 images labeled (mean = 75 s) by 6 nurses. OpenCV was used for video-to-image pre-processing; and MobileFaceNet was used for default landmark placement on images. H2AI randomly assigned videos to nurses for data labeling, tracked labelers’ inter-rater reliability, and stored labeled data to train ML models.
Conclusions
Nurses’ engagement in CDSS development was critical for ensuring the end-product addressed nurses’ priorities, reflected nurses’ cognitive and decision-making processes, and garnered nurses’ trust for technology adoption.},
 authors = {['Naomi A. Kaduwela', 'Susan Horner', 'Priyansh Dadar', 'Renee C.B. Manworren']},
 journal = {International Journal of Medical Informatics},
 keywords = {['Clinical decision support software', 'Data labeling', 'Human-centered Design for Embedded Machine Learning Solutions Machine Learning', 'Machine learning models']},
 title = {Application of a human-centered design for embedded machine learning model to develop data labeling software with nurses: Human-to-Artificial Intelligence (H2AI)},
 year = {2024}
}

@Filtered Article{5d6f2b30-5ff3-4713-b667-55331b3a1a1f,
 abstract = {The concepts and principles of (computational) verb logic, (computational) verb set, (computational) verb number, (computational) verb prediction and (computational) verb control are presented.},
 authors = {['Tao Yang']},
 journal = {Information Sciences},
 title = {Computational verb systems: computing with perceptions of dynamics},
 year = {2001}
}

@Filtered Article{5d7f9d3c-9f63-4181-8a48-e7f59d85d43e,
 abstract = {The induction problem of inferring a predictive function (i.e., model) from finite data is a central component of the scientific enterprise in cognitive science, computer science and statistics, and yet the problem is fundamentally ill posed. Many models can often provide equally good fits to a given observed data set but they may differ considerably in their ability to generalize to new, as yet unseen, data sets generated from the same underlying process. To make this inductive inference problem well posed one needs to define a justifiable measure of generalizability and then use the measure to choose among a set of competing models. Many such measures have been proposed in the past, notably by scientists in the fields of machine learning and algorithmic coding theory. A representative list of such approaches includes the structural risk minimization method and Vapnik-Chervonenkis dimension, the regularization theory, and the minimum description length principle. This article presents a review of these computational approaches to model evaluation. Also discussed are the interesting connections between the computational approaches and some of the statistical approaches to model evaluation such as the Akaike information criterion, the Bayesian information criterion and Bayesian model selection.},
 authors = {['I.J. Myung']},
 journal = {Pergamon},
 title = {Computational Approaches to Model Evaluation},
 year = {2001}
}

@Filtered Article{5db380e5-c037-4d17-8cf6-e84b0a9221d3,
 abstract = {The generation of alternative policies is essential in complex decision tasks with multiple interests and stakeholders. A diverse set of policies is typically desirable to cover the range of options and objectives. Decision modelling literature has often assumed that clearly defined decision alternatives are readily available. This is not a realistic assumption in practice. We present a structured process model for the generation of policy alternatives in settings that include non-quantifiable elements and where portfolio optimisation approaches are not applicable. Behavioural issues and path dependence as well as heuristics and biases which can occur during the process are discussed. The behavioural experiment compares policy alternatives obtained by using two different portfolio generation techniques. The results of the experiment demonstrate that path dependence can occur in policy generation. We report thinking patterns of subjects which relate to biases and heuristics.},
 authors = {['Raimo P. Hämäläinen', 'Tuomas J. Lahtinen', 'Kai Virtanen']},
 journal = {EURO Journal on Decision Processes},
 keywords = {['Policy decision', 'Generation of policy alternatives', 'Portfolio decision analysis', 'Path dependence', 'Cognitive biases and heuristics']},
 title = {Generating policy alternatives for decision making: A process model, behavioural issues, and an experiment},
 year = {2024}
}

@Filtered Article{5de3d352-aac5-4c82-93da-54e22f117f3c,
 abstract = {This paper discusses a parallel algorithm and data structures for implementing multimaterial, two-step Eulerian finite difference solution schemes on hypercube architectures. Selected problems in impact dynamics have been modeled on the Connection Machine model CM5, and the results are compared with computational results reported in the literature, as well as direct comparison with experimental data.},
 authors = {['K.D. Kimsey', 'M.A. Olson']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 title = {Parallel computation of impact dynamics},
 year = {1994}
}

@Filtered Article{5de4a0ef-66cd-4d4e-b1cd-6e5896dd4501,
 abstract = {Computational models of cells, tissues and organisms are necessary for increased understanding of biological systems. In particular, modeling approaches will be crucial for moving biology from a descriptive to a predictive science. Pharmaceutical companies identify molecular interventions that they predict will lead to therapies at the organism level, suggesting that computational biology can play a key role in the pharmaceutical industry. We discuss pharmaceutically-relevant computational modeling approaches currently used as predictive tools. Specific examples demonstrate how companies can employ these computational models to improve the efficiency of transforming targets into therapies.},
 authors = {['Neil Kumar', 'Bart S. Hendriks', 'Kevin A. Janes', 'David {de Graaf}', 'Douglas A. Lauffenburger']},
 journal = {Drug Discovery Today},
 title = {Applying computational modeling to drug discovery and development},
 year = {2006}
}

@Filtered Article{5e1823cc-6aeb-4e5e-9edd-eecf951fb326,
 abstract = {European Union has introduced the European Trading System (ETS) as a tool for developing and implementing international treaties related to climate changes and to identify the most cost-effective methods for reducing greenhouse gas emissions, in particular carbon dioxide (CO2), which is the most substantial. Companies producing carbon emissions must effectively manage associated costs by buying or selling carbon emission futures. Viewed from this perspective, this paper provides a model for managing the risk by buying and selling carbon emission futures by implementing techniques that leverage computational intelligence. Three computational intelligence techniques are proposed to provide accurate and timely forecasts for changes in the price of carbon: a novel hybrid neuro-fuzzy controller that forms a closed-loop feedback mechanism called PATSOS; an artificial neural network (ANN) based system; an adaptive neuro-fuzzy inference system (ANFIS). Results are based on 1074 daily carbon price observations collected to comprise a useful time-series dataset and for evaluation of the proposed techniques. The extra-sample performance of the proposed techniques is calculated. Analysis results are compared with those produced by other models. Comparison studies reveal that PATSOS is the most accurate and promising methodology for predicting the price of carbon. It is stated that this paper registers a first attempt to apply a hybrid neuro-fuzzy controller to forecasting carbon prices.},
 authors = {['George S. Atsalakis']},
 journal = {Applied Soft Computing},
 keywords = {['ANFIS forecasting', 'Carbon allowance', 'Carbon price forecasting', 'Computational intelligent forecasting', 'Neuro-fuzzy forecasting', 'PATSOS forecasting']},
 title = {Using computational intelligence to forecast carbon prices},
 year = {2016}
}

@Filtered Article{5e4bf35b-3c62-411a-9dce-77825f890bf6,
 abstract = {With the introduction of the European Commission's “Safe and Sustainable-by-Design” (SSbD) framework, the interest in understanding the implications of safety and sustainability assessments of chemicals, materials, and processes at early-innovation stages has skyrocketed. Our study focuses on the “Safe-by-Design” (SbD) approach from the nanomaterials sector, which predates the SSbD framework. In this assessment, SbD studies have been compiled and categorized into reviews, case studies, and frameworks. Reviews of SbD tools have been further classified as quantitative, qualitative, or toolboxes and repositories. We assessed the SbD case studies and classified them into three categories: safe(r)-by-modeling, safe(r)-by-selection, or safe(r)-by-redesign. This classification enabled us to understand past SbD work and subsequently use it to define future SSbD work so as to avoid confusion and possibilities of “SSbD-washing” (similar to greenwashing). Finally, the preexisting SbD frameworks have been studied and contextualized against the SSbD framework. Several key recommendations for SSbD based on our analysis can be made. Knowledge gained from existing approaches such as SbD, green and sustainable chemistry, and benign-by-design approaches needs to be preserved and effectively transferred to SSbD. Better incorporation of chemical and material functionality into the SSbD framework is required. The concept of lifecycle thinking and the stage-gate innovation model need to be reconciled for SSbD. The development of high-throughput screening models is critical for the operationalization of SSbD. We conclude that the rapid pace of both SbD and SSbD development necessitates a regular mapping of the newly published literature that is relevant to this field.},
 authors = {['Akshat Sudheshwar', 'Christina Apel', 'Klaus Kümmerer', 'Zhanyun Wang', 'Lya G. Soeteman-Hernández', 'Eugenia Valsami-Jones', 'Claudia Som', 'Bernd Nowack']},
 journal = {Environment International},
 keywords = {['Safe-by-Design (SbD)', 'Safe and Sustainable-by-Design (SSbD)', 'Literature mapping', 'SSbD implementation']},
 title = {Learning from Safe-by-Design for Safe-and-Sustainable-by-Design: Mapping the current landscape of Safe-by-Design reviews, case studies, and frameworks},
 year = {2024}
}

@Filtered Article{5e77209c-800a-4be3-bc5d-343f1de7f5a7,
 abstract = {Developing students’ creative thinking abilities while learning mathematics has been recently emphasized by many scholars, with many nations including creative thinking in mathematics as one of their overarching curriculum goals. The first purpose of the present study is to develop a framework to identify what type of mathematical tasks promote the mathematical creativity of students. The second purpose is to analyze to what degree the most commonly used three middle school curricula (i.e., Eureka, The Go Math!, and CPM) in the U.S. include creativity-directed tasks in their textbooks using this framework. Analyzing 1,500 mathematical tasks in each curriculum revealed that different curricula emphasize different dimensions of the creativity-directed tasks categories (i.e., open-ended tasks, problem-posing, connections, extensions, visualizations, and communication) presented in the framework. The result also revealed that open-ended problems are more common in the 6th grade textbooks than 7th and 8th grade textbooks regardless of the three selected middle school mathematics curricula. The implication of this study is to guide teachers with the strength and weakness of textbooks in terms of their inclusiveness of creativity-directed tasks to inform their teaching. Additionally, it is critical for curriculum developers to pay particular attention in including tasks that supporting each category and subcategory proportionately across the three years of middle school rather than emphasizing a few of them in one grade and almost completely ignoring them in previous or later years.},
 authors = {['Ali Bicer', 'Aylin Marquez', 'Karla Valesca Matute Colindres', 'Angela Ann Schanke', 'Libni Berenice Castellon', 'Luke M. Audette', 'Celal Perihan', 'Yujin Lee']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Creativity-directed tasks', 'Creativity in mathematics textbooks', 'Creativity in mathematics curricula', 'Creative thinking in mathematics']},
 title = {Investigating creativity-directed tasks in middle school mathematics curricula},
 year = {2021}
}

@Filtered Article{5fbd5724-2db8-40eb-ae13-564d65c5a15b,
 abstract = {The Electronic Cocktail Napkin is an experimental computer-based environment for sketching and diagramming in conceptual design. The project's goal is to develop a computational drawing environment to support conceptual designing in a way that leads smoothly from diagrams to more formal and structured representations of schematic design. With computational representations for conceptual designs, computer-supported editing, critquing, analysis, and simulation can be employed earlier in the design process, where it can have a greater impact on outcomes. The paper describes the Electronic Cocktail Napkin program-its recognition and parsing of diagrams and management of spatial constraints, its drawing environment, and two experimental query-by-diagram schemes for retrieving information from architectural databases.},
 authors = {['Mark D. Gross']},
 journal = {Design Studies},
 keywords = {['conceptual design', 'computer-based environment', 'diagrams', 'sketching']},
 title = {The Electronic Cocktail Napkin—a computational environment for working with design diagrams},
 year = {1996}
}

@Filtered Article{5fc4c41f-d84d-4ec1-841e-68b7e715610d,
 abstract = {Publisher Summary
This chapter focuses on the logical, historical and computational approaches to the philosophy of science. It discusses how the logical approach to philosophy of science was introduced by the Vienna Circle, and developed by them and their followers and associates. The logical approach to philosophy of science remained the dominant subject throughout the 1950s; but, from the early 1960s, it was challenged by a striking development of the historical approach. The historical approach was not introduced for the ﬁrst time in the 1960s. On the contrary, it had been developed by Mach and Duhem much earlier. Although, Mach and Duhem are cited by the Vienna Circle as important inﬂuences on their philosophy, the Vienna Circle did not adopt the historical features of these two thinkers. In the excitement generated by the new logic of Frege and Russell, history of science seems to have been temporarily forgotten. The general idea of the historical approach is not new in the 1960s, however, that decade saw striking developments in this approach. After Kuhn, the analysis of scientiﬁc revolutions became a major problem for philosophy of science, while Lakatos applied the historical approach to mathematics for the ﬁrst time.},
 authors = {['Atocha Aliseda', 'Donald Gillies']},
 journal = {North-Holland},
 title = { - Logical, Historical and Computational Approaches},
 year = {2007}
}

@Filtered Article{5ff8ab56-a623-464b-b1eb-19ffd9ce0b45,
 abstract = {Abstract
This chapter presents a review and criticism of the actual innovation approaches, highlighting how the social and economic scenario imposes the necessity of rethinking innovation and consumption models. Specifically, we discuss how the recent crises – which together affect finance, food and climate change and their implications for human development – are forcing organisations to find new solutions and models for responding to emerging needs and expectations. In this regard, we elaborate on the important role that may be played by traditional knowledge as a source of inspiration for innovation, since creativity can find a reliable support in what society has found to be suitable in the past for its development needs.},
 authors = {['ANTONIO MESSENI PETRUZZELLI', 'VITO ALBINO']},
 journal = {Chandos Publishing},
 keywords = {['triple crisis', 'innovation', 'tradition']},
 title = {1 - Re-thinking the innovation approach},
 year = {2012}
}

@Filtered Article{6009456c-bea3-4b64-a45c-2616f8a66e95,
 abstract = {Alzheimer’s disease (AD) affects millions of individuals worldwide and is considered a serious global health issue due to its gradual neuro-degenerative effects on cognitive abilities such as memory, thinking, and behavior. There is no cure for this disease but early detection along with a supportive care plan may aid in improving the quality of life for patients. Automated detection of AD is challenging because its symptoms vary in patients due to genetic, environmental, or other co-existing health conditions. In recent years, multiple researchers have proposed automated detection methods for AD using MRI and fMRI. These approaches are expensive, have poor temporal resolution, do not offer real-time insights, and have not proven to be very accurate. In contrast, only a limited number of studies have explored the potential of Electroencephalogram (EEG) signals for AD detection. In contrast, Electroencephalogram (EEG) signals present a cost-effective, non-invasive, and high-temporal-resolution alternative for AD detection. Despite their potential, the application of EEG signals in AD research remains under-explored. This study reviews publicly available EEG datasets, the variety of machine learning models developed for automated AD detection, and the performance metrics achieved by these methods. It provides a critical analysis of existing approaches, highlights challenges, and identifies key areas requiring further investigation. Key findings include a detailed evaluation of current methodologies, prevailing trends, and potential gaps in the field. What sets this work apart is its in-depth analysis of EEG signals for Alzheimer’s Disease detection, providing a stronger and more reliable foundation for understanding the potential role of EEG in this area.},
 authors = {['Frnaz Akbar', 'Imran Taj', 'Syed Muhammad Usman', 'Ali Shariq Imran', 'Shehzad Khalid', 'Imran Ihsan', 'Ammara Ali', 'Amanullah Yasin']},
 journal = {Brain Research Bulletin},
 keywords = {['Electroencephalogram', 'Alzheimer’s disease', 'EEG', 'Mild cognitive impairment', 'Frontal temporal dementia', 'Neuro-degenerative']},
 title = {Unlocking the potential of EEG in Alzheimer's disease research: Current status and pathways to precision detection},
 year = {2025}
}

@Filtered Article{6042c83f-198a-4443-9099-8decd66d98d5,
 abstract = {Emotion recognition using Artificial Intelligence (AI) is a fundamental prerequisite to improve Human-Computer Interaction (HCI). Recognizing emotion from Electroencephalogram (EEG) has been globally accepted in many applications such as intelligent thinking, decision-making, social communication, feeling detection, affective computing, etc. Nevertheless, due to having too low amplitude variation related to time on EEG signal, the proper recognition of emotion from this signal has become too challenging. Usually, considerable effort is required to identify the proper feature or feature set for an effective feature-based emotion recognition system. To extenuate the manual human effort of feature extraction, we proposed a deep machine-learning-based model with Convolutional Neural Network (CNN). At first, the one-dimensional EEG data were converted to Pearson's Correlation Coefficient (PCC) featured images of channel correlation of EEG sub-bands. Then the images were fed into the CNN model to recognize emotion. Two protocols were conducted, namely, protocol-1 to identify two levels and protocol-2 to recognize three levels of valence and arousal that demonstrate emotion. We investigated that only the upper triangular portion of the PCC featured images reduced the computational complexity and size of memory without hampering the model accuracy. The maximum accuracy of 78.22% on valence and 74.92% on arousal were obtained using the internationally authorized DEAP dataset.},
 authors = {['Md. Rabiul Islam', 'Md. Milon Islam', 'Md. Mustafizur Rahman', 'Chayan Mondal', 'Suvojit Kumar Singha', 'Mohiuddin Ahmad', 'Abdul Awal', 'Md. Saiful Islam', 'Mohammad Ali Moni']},
 journal = {Computers in Biology and Medicine},
 keywords = {['Emotion', 'Convolutional neural network', 'Feature extraction', 'EEG', "Pearson's correlation coefficient", 'Complexity']},
 title = {EEG Channel Correlation Based Model for Emotion Recognition},
 year = {2021}
}

@Filtered Article{60693d05-e96d-40a0-9cc4-8135383829ad,
 abstract = {Although rating scales to assess formal thought disorder exist, there are no objective, high-reliability instruments that can quantify and track it. This proof-of-concept study shows that CoVec, a new automated tool, is able to differentiate between controls and patients with schizophrenia with derailment and tangentiality. According to ratings from the derailment and tangentiality items of the Scale for the Assessment of Positive Symptoms, we divided the sample into three groups: controls, patients without formal thought disorder, and patients with derailment/tangentiality. Their lists of animals produced during a one-minute semantic fluency task were processed using CoVec, a newly developed software that measures the semantic similarity of words based on vector semantic analysis. CoVec outputs were Mean Similarity, Coherence, Coherence-5, and Coherence-10. Patients with schizophrenia produced fewer words than controls. Patients with derailment had a significantly lower mean number of words and lower Coherence-5 than controls and patients without derailment. Patients with tangentiality had significantly lower Coherence-5 and Coherence-10 than controls and patients without tangentiality. Despite the small samples of patients with clinically apparent thought disorder, CoVec was able to detect subtle differences between controls and patients with either or both of the two forms of disorganization.},
 authors = {['Luca Pauselli', 'Brooke Halpern', 'Sean D. Cleary', 'Benson S. Ku', 'Michael A. Covington', 'Michael T. Compton']},
 journal = {Psychiatry Research},
 keywords = {['Automatic Data Processing', 'Formal Thought Disorder', 'Psychosis', 'Schizophrenia', 'Semantics', 'Semantic Fluency Tasks']},
 title = {Computational linguistic analysis applied to a semantic fluency task to measure derailment and tangentiality in schizophrenia},
 year = {2018}
}

@Filtered Article{607e7a4a-a857-4966-944b-66807873dc59,
 abstract = {Summary
The TNM system of staging cancers is a simple and effective method to map the extent of tumours. It had traditionally strived to maintain a balance between being simple and user-friendly on one hand and comprehensive on the other. A number of revisions have taken place over the years with the goal of improving utility. However, numerous controversies surround the TNM system. There is a school of thought that contends that patient co-morbidity and specific tumour-related factors should be incorporated to add further prognostic capabilities in the TNM system, but this raises concerns that such an approach may unnecessarily complicate the system. This review highlights some controversies that surround the TNM system and suggests prognostic indicators that may be added to make it more useful in guiding treatment decisions and predicting outcomes.},
 authors = {['Kapila Manikantan', 'Suhail I. Sayed', 'Konstantinos N. Syrigos', 'Peter Rhys-Evans', 'Chris M. Nutting', 'Kevin J. Harrington', 'Rehan Kazi']},
 journal = {Cancer Treatment Reviews},
 keywords = {['TNM stage', 'Head and neck cancer', 'Co-morbidity']},
 title = {Challenges for the future modifications of the TNM staging system for head and neck cancer: Case for a new computational model?},
 year = {2009}
}

@Filtered Article{60815d56-c484-45a0-98db-9900c7a27246,
 abstract = {A fast algorithm for Linear Quadratic(LQ) control with linear equality constraints is derived and exploited for stabilizing predictive control synthesis. The algorithm requires only O(Nn) computations for an nth order plant and N-steps prediction horizon, and possesses a remarkable numerical accuracy.},
 authors = {['L. Chisci', 'A. Garulli', 'G. Zappa']},
 journal = {IFAC Proceedings Volumes},
 keywords = {['Predictive control', 'linear quadratic regulators', 'control algorithms', 'fast parallel algorithms', 'fast Kalman algorithms', 'computational methods', 'adaptive control']},
 title = {Fast Computation of Stabilizing Predictive Control Laws},
 year = {1995}
}

@Filtered Article{60afea39-d340-4218-b66d-75494eef0bfa,
 abstract = {Abstract:
Knowledge complexity poses a problem to the modeller of representation and, in turn, of reasoning. We seek to overcome this problem by using our ‘privileged’ sense of vision. This means that we render dynamic, multi-modal phenomena as graphic depictions, be they technical visualizations, thought experiments, logic constructions or network graphs. The ‘graphic act’ has been described a being a fundamental activity in human perception, and both scientists and artists have undertaken advanced analyses of the human perception of nature based on visual experiments. Analysis based on reasoning is similarly a graphic act, in the sense that it seeks out patterns of connectivity among socio-spatial agents and entities. Reasoning also depends upon symbolism, which serves to overcome the problem of infinity in nature (a matter that lies at the heart of machine computation). Hence, this chapter introduces some elements in logical reasoning, set theory and computation, and outlines the particular importance of working with symbols. It also provides an introduction to data modelling with graphs, including current advances in graph databases. It provides some ‘tools for thinking’ about knowledge complexity and suggests the potential power in adapting these technologies to organize knowledge of dynamic, complex domains. It also introduces some standard methods for spatial data modelling, including powerful surface network models, which borrow from physical landscape analysis, to support reasoning about knowledge-driven domains.},
 authors = {['Jamie O’Brien']},
 journal = {Chandos Publishing},
 keywords = {['graph databases', 'logic and computing', 'reasoning', 'spatial data structures', 'visualization']},
 title = {7 - Reasoning with graphs},
 year = {2014}
}

@Filtered Article{61b332c9-fb88-484b-a6d2-a13a320dd566,
 abstract = {While agent-based modeling (ABM) has become one of the most powerful tools in quantitative social sciences, it remains difficult to explain their structure and performance. We propose to use artificial intelligence both to build the models from data, and to improve the way we communicate models to stakeholders. Although machine learning is actively employed for pre-processing data, here for the first time, we used it to facilitate model development of a simulation model directly from data. Our suggested framework, ML-ABM accounts for causality and feedback loops in a complex nonlinear system and at the same time keeps it transparent for stakeholders. As a result, beside the development of a behavioral ABM, we open the ‘blackbox’ of purely empirical models. With our approach, artificial intelligence in the simulation field can open a new stream in modeling practices and provide insights for future applications.},
 authors = {['Firouzeh Taghikhah', 'Alexey Voinov', 'Tatiana Filatova', 'J. Gareth Polhill']},
 journal = {Journal of Computational Science},
 keywords = {['Behavioral analytics', 'Social communications', 'Interpretable artificial intelligence', 'Conceptual modeling', 'Systems thinking']},
 title = {Machine-assisted agent-based modeling: Opening the black box},
 year = {2022}
}

@Filtered Article{61c1bb07-bcd7-4488-b8e6-041aa1d68c13,
 abstract = {We present the stabilized-finite-element/interface-capturing (SFE/IC) method developed for parallel computation of unsteady flow problems with two-fluid interfaces and free surfaces. The SFE/IC method involves stabilized formulations, an interface-sharpening technique, and the enforcement of global mass conservation for each fluid. The SFE/IC method has been efficiently implemented on the CRAY T3E parallel supercomputer. A number of 2D test problems are presented to demonstrate how the SFE/IC method works and the accuracy it attains. We also show how the SFE/IC method can be very effectively applied to 3D simulation of challenging flow problems, such as two-fluid interfaces in a centrifuge tube and operational stability of a partially filled tanker truck driving over a bump.},
 authors = {['Shahrouz Aliabadi', 'Tayfun E. Tezduyar']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 title = {Stabilized-finite-element/interface-capturing technique for parallel computation of unsteady flows with interfaces},
 year = {2000}
}

@Filtered Article{61e558a1-12e1-4f97-87f9-216d16997c10,
 abstract = {The design-build studio (DBS) is an emergent paradigm in architecture education. Recently, researchers have addressed the success of integrating the design-build concept into the conventional studio in the advanced years of education to improve several issues related to the design process: social, environmental, technological, …etc. However, its efficiency in terms of contribution to the learning experience has not yet been addressed. This paper examines the implementation of the DBS concept as a new teaching model to improve the problem-solving skills (PSS) of first-year students in engineering education. It also discusses the effectson learning experiences and pedagogical outcomes in both quantitative and qualitative terms. The research’s results identify the significance of applying this model as a real-simulated method-based learning experience. It can help students to improve their learning experiences and to enhance students self-confidence regarding PSS.},
 authors = {['Mohammed F. Elaby', 'Hesham M. Elwishy', 'Saeed F. Moatamed', 'Mahmoud A. Abdelwahed', 'Ahmed E. Rashiedy']},
 journal = {Ain Shams Engineering Journal},
 keywords = {['Problem-solving', 'Critical thinking', 'Design-build studio', 'Instructional design']},
 title = {Does design-build concept improve problem-solving skills? An analysis of first-year engineering students},
 year = {2022}
}

@Filtered Article{6224624f-6e1e-46a9-abda-00ecb0c162a0,
 abstract = {Teaching computer programming is an important topic. Due to Science and Technology initiatives, these topics are considered in different training cycles. For higher education, students must cultivate fundamental concepts for the development of software applications, which not only contribute to the knowledge of programming languages but also to opening guidelines for computational thinking. However, selecting a proper tool can be complex. Especially for the diversity of alternatives on the web. Further, not all of them meet basic usability requirements. In this study, we present a set of platforms that seek to develop programming skills based on video games. The search consisted of 4 stages: (i) definition of the research questions, (ii) scope review, (iii) execution of search and (iv) platform selection. Finally, we employ a usability heuristic evaluation for a novice programming system to determine best practices.},
 authors = {['Jaime Díaz', 'Jeferson Arango López', 'Samuel Sepúlveda', 'Gabriel Mauricio {Ramírez Villegas}', 'Danay Ahumada', 'Fernando Moreira']},
 journal = {Procedia Computer Science},
 keywords = {['Human-Computer Interaction', 'Usability', 'Video-games', 'Training', 'Computer Programming']},
 title = {Evaluating Aspects of Usability in Video Game-Based Programming Learning Platforms},
 year = {2021}
}

@Filtered Article{62383883-6d5e-430d-bc4f-10bc884a963e,
 abstract = {We introduce UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a novel software infrastructure tailored for mesoscale complex fluid simulations on GPUs. The UAMMD library encompasses a comprehensive range of computational schemes optimized for the GPU, spanning from molecular dynamics to immersed boundary fluctuating-hydrodynamics. Developed in CUDA/C++14, this header-only open-source software serves both as a simulation engine and as a library with a modular architecture, offering a vast array of independent modules, categorized as interactors (neighbor search, bonded, non-bonded and electrostatic interactions, etc.) and integrators (molecular dynamics, dissipative particle dynamics, smooth particle hydrodynamics, Brownian hydrodynamics and a rather complete array of Immersed Boundary -IB- schemes). UAMMD excels in schemes that couple particle-based elastic structures with continuum fields in different regions of the mesoscale. To that end, thermal fluctuations can be added in physically consistent ways, and fast modes can be eliminated to adapt UAMMD to different regimes (compressible or incompressible flow, inertial or Stokesian dynamics, etc.). Thus, UAMMD is extremely useful for coarse-grained simulations of nanoparticles, and soft and biological matter (from proteins to viruses and micro-swimmers). Importantly, all UAMMD developments are hand-to-hand validated against experimental techniques, and it has proven to quantitatively reproduce experimental signals from quartz-crystal microbalance, atomic force microscopy, magnetic sensors, optic-matter interaction and ultrasound.
Program summary
Program Title: UAMMD CPC Library link to program files: https://doi.org/10.17632/srrt2y5s4m.1 Developer's repository link: https://github.com/RaulPPelaez/UAMMD/ Licensing provisions: GPLv3 Programming language: C++/CUDA Nature of problem: The key problem addressed in computational physics is simulating the behavior of matter at various scales, encompassing both discrete (particle-based) and continuum (field-based) approaches. The challenge lies in accurately and efficiently modeling interactions at different spatio-temporal scales, ranging from atomic (microscopic) to fluid dynamics (macroscopic). This complexity is further amplified in mesoscale regions, where different physics domains intersect, necessitating advanced computational techniques to capture the nuanced dynamics of systems such as colloids, polymers, and biological structures. Solution method: The present solution consists in the creation of UAMMD (Universally Adaptable Multiscale Molecular Dynamics), a CUDA/C++14 library designed for GPU-accelerated complex fluid simulations. UAMMD offers a flexible platform that integrates discrete particle dynamics with continuum fluid dynamics. It supports a variety of computational schemes, each tailored for specific spatio-temporal regimes. The library's modular architecture allows for the seamless introduction of new algorithms and easy integration into existing codebases. Additional comments including restrictions and unusual features: UAMMD's design emphasizes modularity and GPU-native architecture, optimizing computational efficiency and flexibility. However, its focus on GPU acceleration and low level nature means it requires compatible hardware and familiarity with CUDA programming. While UAMMD is versatile in handling various physical regimes, it currently lacks certain standard force field potentials and multi-GPU support. Nonetheless, its ongoing development and open-source nature promise continual enhancements.},
 authors = {['Raúl P. Peláez', 'Pablo Ibáñez-Freire', 'Pablo Palacios-Alonso', 'Aleksandar Donev', 'Rafael Delgado-Buscalioni']},
 journal = {Computer Physics Communications},
 keywords = {['Molecular dynamics', 'Hydrodynamics', 'C++', 'CUDA', 'Soft matter']},
 title = {Universally Adaptable Multiscale Molecular Dynamics (UAMMD). A native-GPU software ecosystem for complex fluids, soft matter, and beyond},
 year = {2025}
}

@Filtered Article{62edd508-04ff-498e-8529-ffba441bee6e,
 abstract = {The current status of oil spill modeling is presented. The physical and chemical processes taking place in oil spills are explained for the design of an ideal oil spill model (IOSM). The requirements of an IOSM for forecasting are rapid response, contingency planning and training. The use of parallel computation techniques in oil spill modeling is introduced and delineated.},
 authors = {['H.M. Cekirge', 'C.P. Giammona', 'J. Berlin', 'C. Long', 'M. Koch', 'R. Jamail']},
 journal = {Spill Science & Technology Bulletin},
 title = {Oil spill modeling using parallel computations},
 year = {1994}
}

@Filtered Article{63092dc0-6b60-40c9-a1f5-75c2d6cba1b6,
 abstract = {We study players interacting under the veil of ignorance, who have—coarse—beliefs represented as subsets of opponents' actions. We analyze when these players follow max⁡min or max⁡max decision criteria, which we identify with pessimistic or optimistic attitudes, respectively. Explicitly formalizing these attitudes and how players reason interactively under ignorance, we characterize the behavioral implications related to common belief in these events: while optimism is related to Point Rationalizability, a new algorithm—Wald Rationalizability—captures pessimism. Our characterizations allow us to uncover novel results: (i) regarding optimism, we relate it to wishful thinking á la Yildiz (2007) and we prove that dropping the (implicit) “belief-implies-truth” assumption reverses an existence failure described therein; (ii) we shed light on the notion of rationality in ordinal games; (iii) we clarify the conceptual underpinnings behind a discontinuity in Rationalizability hinted in the analysis of Weinstein (2016).},
 authors = {['Pierfrancesco Guarino', 'Gabriel Ziegler']},
 journal = {Games and Economic Behavior},
 keywords = {['Ignorance', 'Optimism/pessimism', 'Point/Wald Rationalizability', 'Interactive epistemology', 'Wishful thinking', 'Börgers dominance']},
 title = {Optimism and pessimism in strategic interactions under ignorance},
 year = {2022}
}

@Filtered Article{6345d4b9-4ff8-4522-8c69-eb75fc37ed35,
 abstract = {The expression big data is often used in a manner which implies that immediate insight is readily available. Unfortunately, this raises unrealistic expectations. A model which encapsulates the powerful concepts of statistical thinking remains an invaluable component of good analysis.},
 authors = {['Adrian W. Bowman']},
 journal = {Statistics & Probability Letters},
 keywords = {['Big data', 'Statistical models']},
 title = {Big questions, informative data, excellent science},
 year = {2018}
}

@Filtered Article{634abb2f-33fb-4264-b83a-994698261da4,
 abstract = {We propose that human social cognition is structured around a basic understanding of ourselves and others as intuitive utility maximizers: from a young age, humans implicitly assume that agents choose goals and actions to maximize the rewards they expect to obtain relative to the costs they expect to incur. This ‘naïve utility calculus’ allows both children and adults observe the behavior of others and infer their beliefs and desires, their longer-term knowledge and preferences, and even their character: who is knowledgeable or competent, who is praiseworthy or blameworthy, who is friendly, indifferent, or an enemy. We review studies providing support for the naïve utility calculus, and we show how it captures much of the rich social reasoning humans engage in from infancy.},
 authors = {['Julian Jara-Ettinger', 'Hyowon Gweon', 'Laura E. Schulz', 'Joshua B. Tenenbaum']},
 journal = {Trends in Cognitive Sciences},
 title = {The Naïve Utility Calculus: Computational Principles Underlying Commonsense Psychology},
 year = {2016}
}

@Filtered Article{63a3362f-c836-46fc-9717-edd8377400ea,
 abstract = {When the pressure fluctuations caused by turbulence vorticity in the boundary layer are scattered by a sharp trailing edge, acoustic energy is generated and propagated to the far field. This trailing edge noise is emitted from aircraft wings, turbomachinery blades, wind turbine blades, helicopter blades, etc. Being dominant at high frequencies, this trailing-edge noise is a key element that annoys human hearing. This article covers virtually the entire landscape of modern research into trailing-edge noise including theoretical developments, numerical simulations, wind tunnel experiments, and applications of trailing-edge noise. The theoretical approach includes Green’s function formulations, Wiener–Hopf methods that solve the mixed boundary-value problem, Howe’s and Amiet’s models that relate the wall pressure spectrum to acoustic radiation. Recent analytical developments for poroelasticity and serrations are also included. We discuss a hierarchy of numerical approaches that range from semi-empirical schemes that estimate the wall pressure spectrum using mean-flow and turbulence statistics to high-fidelity unsteady flow simulations such as Large Eddy Simulation (LES) or Direct Numerical Simulation (DNS) that resolve the sound generation and scattering process based on the first-principles flow physics. Wind tunnel experimental research that provided benchmark data for numerical simulations and unravel flow physics is reviewed. In each theoretical, numerical, and experimental approach, noise control methods for mitigating trailing-edge noise are discussed. Finally, highlights of practical applications of trailing-edge noise prediction and reduction to wind turbine noise, fan noise, and rotorcraft noise are given. The current challenges in each approach are summarized with a look toward the future developments. The review could be useful as a primer for new researchers or as a reference point to the state of the art for experienced professionals.},
 authors = {['Seongkyu Lee', 'Lorna Ayton', 'Franck Bertagnolio', 'Stephane Moreau', 'Tze Pei Chong', 'Phillip Joseph']},
 journal = {Progress in Aerospace Sciences},
 keywords = {['Trailing-edge noise', 'Aeroacoustics', 'Turbulent boundary layer']},
 title = {Turbulent boundary layer trailing-edge noise: Theory, computation, experiment, and application},
 year = {2021}
}

@Filtered Article{64014893-dd71-4fb8-868e-015078f084a0,
 abstract = {This paper proposes the concept of the probabilistic unbalanced linguistic term set which considers not only the probability of linguistic variables but also the non-uniform and non-symmetric distribution of linguistic labels. A new computational model on basis of Archimedean copula and corresponding co-copula is developed to deal with probabilistic unbalanced linguistic information. The most advantage of the model is that it can keep the closure of the operation. Some operational properties and particular cases are further investigated. We present the concepts of Archimedean copula weighted probabilistic unbalanced linguistic arithmetic average aggregation operator and Archimedean copula weighted probabilistic unbalanced linguistic geometric average aggregation operator, some properties are also discussed. Finally, the effectiveness and universality of the developed approach are illustrated by a hospital selection and comparison analysis. A sensitivity analysis is also performed to test the robustness of proposed methods.},
 authors = {['Bing Han', 'Zhifu Tao', 'Huayou Chen', 'Ligang Zhou', 'Jinpei Liu']},
 journal = {Computers & Industrial Engineering},
 keywords = {['Multiple attribute group decision making', 'Probabilistic unbalanced linguistic term set', 'Archimedean copula', 'Weighted average aggregation operator']},
 title = {A new computational model based on Archimedean copula for probabilistic unbalanced linguistic term set and its application to multiple attribute group decision making},
 year = {2020}
}

@Filtered Article{646c39fc-a709-4d15-8568-f89d37d25e7b,
 abstract = {We describe experimental results to demonstrate the wide-ranging computational ability of quasiperiodic oscillators built from rings of differentiating Schmitt triggers. We describe a theoretical model based on necklace functions to compute the number of states supportable by a ring circuit of a given size. Experimental results are presented to demonstrate that probabilistic state machines can be built from these ring circuits. Other experimental results are given to demonstrate that the rings can model spiking neural network circuits.},
 authors = {['Edward A. Rietman', 'Mark W. Tilden', 'Manor Askenazi']},
 journal = {Robotics and Autonomous Systems},
 keywords = {['Quasiperiodic oscillators', 'Microdynamics', 'Schmitt trigger']},
 title = {Analog computation with rings of quasiperiodic oscillators: the microdynamics of cognition in living machines},
 year = {2003}
}

@Filtered Article{64883441-b628-40df-aa1d-c696608cb4f1,
 abstract = {In this paper we consider the problem of numerical computation of the mean time to failure (MTTF) in Markovian dependability and/or performance models. The problem can be cast as a system of linear equations which is solved using an iterative method preserving sparsity of the Markov chain matrix. For highly dependable systems, system failure is a rare event and the above system solution can take an extremely large number of iterations. We propose to solve the problem by dividing the computation in two parts. First, by making some of the high probability states absorbing, we compute the MTTF of the modified Markov chain. In a subsequent step, by solving another system of linear equations, we are able to compute the MTTF of the original model. We prove that for a class of highly dependable systems, the resulting method can speed up computation of the MTTF by orders of magnitude. Experimental results supporting this claim are presented. We also obtain bounds on the convergence rate for computing the mean entrance time of a rare set of states in a class of queueing models.},
 authors = {['Philip Heidelberger', 'Jogesh K. Muppala', 'Kishor S. Trivedi']},
 journal = {Performance Evaluation},
 keywords = {['Markov chains', 'Mean time to failure', 'Numerical methods']},
 title = {Accelerating mean time to failure computations},
 year = {1996}
}

@Filtered Article{64bd353c-40f9-4d10-807f-a81819ef0c3b,
 authors = {['Stig W Omholt']},
 journal = {Mathematical Biosciences},
 title = {Eberhard O. Voit, Computational Analysis of Biochemical Systems. A Practical Guide for Biochemists and Molecular Biologists, Cambridge University Press, 2000, 531 pages (ISBN 0-521-78579-0; paperback)},
 year = {2003}
}

@Filtered Article{653e2925-7684-4130-abaf-e8ee2e45fda8,
 abstract = {The mechanisms and processes of visual thinking are introduced together with how this knowledge can help us make design decisions. We begin with a review of the evidence that we actually take in very little information with each glance and the implication that seeing is a process exquisitely tuned to our cognitive task of the moment. As a key part of this process, our brains execute visual queries using eye movements; visual features are detected in parallel to pick out just what is needed to resolve part of a cognitive problem and move on the next step. We begin to understand how seeing can be a distributed cognitive process executed partly in the brain and partly using a visualization as a tool. In particular, when the visualization is part of an interactive computer application, it provides the primary interface between cognitive operations in the human brain and computational operations. The theory of predictive cognition is introduced as a basis for how we should design presentations.},
 authors = {['Colin Ware']},
 journal = {Morgan Kaufmann},
 keywords = {['Visual queries', 'visual search', 'distributed cognition', 'predictive cognition', 'visual system', 'visual thinking']},
 title = {Chapter 1 - Visual Queries},
 year = {2022}
}

@Filtered Article{65461126-7e50-4c39-a1d7-1413d7f15622,
 abstract = {Research on resident participation in neighborhood regeneration provides valuable insights for urban policymakers in environmental governance. While previous studies have extensively examined various influencing factors, they often neglect the impact of behavioral inertia. To address this gap, this study conducts a behavioral experiment to quantitatively assess the presence and impact of behavioral inertia on residents' governance and financial participation behaviors. A total of 576 valid survey questionnaires were collected, and conditional logit model and ordered logit model were utilized for analysis. The study reveals that behavioral inertia is indeed observable in residents' governance participation and financial participation behaviors. Furthermore, the findings underscore distinct drivers of behavioral inertia for these two types of participation behaviors, with emotional reactions predominantly influencing governance participation, while short-term thinking largely shapes financial participation. Theoretically, this study uses the innovative concept of “behavioral inertia” to offer a new explanatory framework for aspects of behavior that cannot be solely explained by the attributes of regeneration plans. Furthermore, the behavioral experiments utilized in this study exemplify how the research framework of behavioral science can be applied to the study of urban governance in a broad context internationally. Practically, the research findings provide valuable insights for urban policymakers to tailor measures aimed at promoting resident participation and fostering sustainable urban development.},
 authors = {['Xinyue Fu', 'Guiwen Liu', 'Hongjuan Wu', 'Taozhi Zhuang', 'Ruopeng Huang', 'Fanning Yuan', 'Yuhang Zhang']},
 journal = {Environmental Impact Assessment Review},
 keywords = {['Neighborhood regeneration', 'Resident participation', 'Behavioral inertia', 'Behavioral experiment']},
 title = {Dissecting behavioral inertia in shaping different resident participation behaviors in neighborhood regeneration: A quantitative behavioral experiment},
 year = {2024}
}

@Filtered Article{659eb906-ab32-463b-914c-5ee39869d9ca,
 abstract = {Studying large complex problems that often arise in computational stochastic dynamics (CSD) demands significant computer power and data storage. Parallel processing can help meet these requirements by exploiting the computational and storage capabilities of multiprocessing computational environments. The challenge is to develop parallel algorithms and computational strategies that can take full advantage of parallel machines. This paper reviews some of the characteristics of parallel computing and the techniques used to parallelize computational algorithms in CSD. The characteristics of parallel processor environments are discussed, including parallelization through the use of message passing and parallelizing compilers. Several applications of parallel processing in CSD are then developed: solutions of the Fokker–Planck equation, Monte Carlo simulation of dynamical systems, and random eigenvector problems. In these examples, parallel processing is seen to be a promising approach through which to resolve some of the computational issues pertinent to CSD.},
 authors = {['E.A. Johnson', 'C. Proppe', 'B.F. Spencer', 'L.A. Bergman', 'G.S. Székely', 'G.I. Schuëller']},
 journal = {Probabilistic Engineering Mechanics},
 keywords = {['Computational stochastic dynamics', 'Parallel computing', 'Monte Carlo simulation', 'Random eigenvalue', 'Fokker–Planck equation']},
 title = {Parallel processing in computational stochastic dynamics},
 year = {2003}
}

@Filtered Article{65e00470-8f33-4398-a0e8-56376291c8b4,
 abstract = {Accurate modeling for forecasting of stock market volatility is a widely interesting research area both in academia as well as financial markets. This paper proposes an innovative Fuzzy Computationally Efficient EGARCH model to forecast the volatility of three stock market indexes. The proposed model represents a joint estimation of the membership function parameters of a TSK-type fuzzy inference system along with the leverage effect, asymmetric shock by leverage effect of EGARCH model in forecasting highly nonlinear and complicated financial time series model more accurately. Further unlike the conventional TSK type fuzzy neural network the proposed model uses a functional link neural network (FLANN) in the consequent part of the fuzzy rules to provide an improved mapping. Moreover, a differential evolution (DE) algorithm is suggested to solve the parameters estimation problem of Fuzzy Computationally Efficient EGARCH model. Being a parallel direct search algorithm, DE has the strength of finding global optimal solutions regardless of the initial values of its few control parameters. Furthermore, the DE based algorithm aims to achieve an optimal solution with a rapid convergence rate. The proposed model has been compared with some GARCH family models and hybrid fuzzy systems and GARCH models based on three performance metrics: MSFE, RMSFE, and MAFE. The results indicate that the proposed method offers significant improvements in volatility forecasting performance in comparison with all other specified models.},
 authors = {['Rajashree Dash', 'P.K. Dash']},
 journal = {Applied Soft Computing},
 keywords = {['Volatility prediction', 'Stock markets', 'GARCH model variants', 'Fuzzy logic based hybrids', 'Fuzzy inference with nonlinear functions', 'Multistep prediction', 'Differential evolution', 'Super predictive ability test']},
 title = {An evolutionary hybrid Fuzzy Computationally Efficient EGARCH model for volatility prediction},
 year = {2016}
}

@Filtered Article{666af239-55fa-4020-9606-9242188fbbf4,
 abstract = {Product concept generation and evaluation are two major activities for obtaining an optimal concept in conceptual design. In this paper, an integrated computational intelligence approach is proposed for dealing with these two aspects. A group of satisfactory concepts are generated first by using genetic algorithm and incorporating the information from knowledge base. Then concept evaluation and decision making are implemented using fuzzy neural network to obtain an optimal concept. Our procedure of using computational intelligence in conceptual design is described. The key issues in implementing the proposed approach are discussed, and finally the applicability of the proposed method is illustrated with an engineering example.},
 authors = {['Hong-Zhong Huang', 'Ruifeng Bo', 'Wei Chen']},
 journal = {Mechanism and Machine Theory},
 keywords = {['Conceptual design', 'Computational intelligence', 'Optimal concept', 'Genetic algorithm', 'Fuzzy neural network']},
 title = {An integrated computational intelligence approach to product concept generation and evaluation},
 year = {2006}
}

@Filtered Article{6696aa34-ce07-4ce0-ba6f-119852e65445,
 abstract = {The crucial role that the supersonic parachute plays in space exploration missions has been widely recognized, as it directly impacts the safe landing of probes. However, parachute models with optimization on different aerodynamic performances often involve design conflicts with each other. Additionally, the parachute design focusing on a single point cannot fully adapt to different speed ranges during stable descent. This complexity makes it challenging to use traditional shape design methods, which rely on empirical knowledge, to address these coupled design issues. Faced with the design challenges of Mars parachutes, this study, inspired by aircraft aerodynamic optimization principles, establishes a shape design method specifically for the stable descent phase of Mars parachutes. The method combines numerical simulation and surrogate-based optimization strategies, aiming to enhance the overall performance during stable descent and meet various demands of different exploration missions. Meanwhile, by providing a rapid estimate of the shape during the design phase, the method significantly improves computational efficiency. The optimal models effectively balance comprehensive performance in the supersonic-transonic-subsonic speed domain by conducting shape optimization research on the disk-gap-band parachute using the surrogate-based optimization strategy. Also, it exhibits better deceleration and stability across the entire speed range compared to the base model, even when deviating from the design Mach number. Importantly, the advantages of canopy-only optimization for drag performance extend to the capsule-canopy two-body system, enhancing the drag performance of the canopy in the two-body system. This strategic approach reduces the transient calculation time for the two-body system, further improving computational efficiency. The method provides a practical and forward-thinking solution for the design of Mars parachutes.},
 authors = {['Lulu Jiang', 'Guanhua Chen', 'Xiaopeng Xue', 'Xin Pan', 'Gang Chen']},
 journal = {Aerospace Science and Technology},
 keywords = {['Supersonic Parachute', 'Mars atmosphere', 'Aerodynamic Optimization', 'Shape design', 'Wide Speed Range']},
 title = {Surrogate-based Shape Optimization Design for the Stable Descent of Mars Parachutes},
 year = {2024}
}

@Filtered Article{6714c00d-f307-4dfa-9249-f7098deaf07f,
 abstract = {Purpose
In this paper, the author has tried to outline the main ideas in connection with what the author conceives to be the university of the future, a university that should not only educate people within the university system but also prepare them to fill specific job positions at both local and global levels, apart from necessarily providing them with the critical thinking and competences in autonomous learning that will make them flexible and capable of adapting to the job market and to a fast-changing world in general.
Design/methodology/approach
The author has revised some of the major issues that are going to determine the direction of the university of the future, i.e. the employment opportunities of tomorrow; the role of new technologies, especially the impact of artificial intelligence (AI); quality in higher education; and internationalization.
Findings
The author has also pointed out the importance of the technologies and the great role they indisputably play in present and future education at all levels, a fact that has been particularly and hugely enhanced and promoted by the COVID-19 pandemic situation, thereby facilitating and fostering distance learning. This is very much connected to the application of AI to higher education, another unavoidable issue of utmost importance for the university of the future. While these technological advances present a challenge to universities, which must determine which are necessary and desirable and how to implement them, it is, ultimately, our responsibility to use them, in an ethical way, to the benefit of our students. The university of the future also has to be of high quality, and this involves carrying out important and decisive action having to do with matters of inclusion, hiring policies and the expansion of international opportunities for all parties involved.
Originality/value
This paper outlines the main ideas in connection with what the author conceives to be the university of the future, a university that should not only educate people within the university system but also prepare them to fill specific job positions at both local and global levels, apart from necessarily providing them with the critical thinking and competences in autonomous learning that will make them flexible and capable of adapting to the job market and to a fast-changing world in general. Moreover, the role of new technologies (especially the impact of AI), quality and internationalization are also discussed as relevant factors in this view of the university of the future.},
 authors = {['Ricardo Mairal']},
 journal = {On the Horizon},
 keywords = {['Employment', 'Internationalization', 'Higher education', 'Quality', 'Artificial intelligence', 'Online and distance education']},
 title = {What should the university of the future look like?},
 year = {2022}
}

@Filtered Article{6730e245-63b8-4163-886c-7c4d706b9dba,
 abstract = {After a brief review of the evolution of thinking about systems, consisting of an ensemble of components, the chapter analyzes the nondeterminism, nonlinearity, and phase transitions in complex systems. A range of topics pertinent to complexity, such as self-organization, self-organized criticality, power law distributions, computational irreducibility, and quantitative characterization of complexity are then covered. Cybernetics and the interdisciplinary nature of complexity conclude the chapter.},
 authors = {['Dan C. Marinescu']},
 journal = {Elsevier},
 keywords = {['Complexity', 'Emergence', 'Phase transitions', 'Open systems', 'Nondeterminism', 'Self-similarity', 'Fractal geometry', 'Power Law distribution']},
 title = {Chapter 1 - Complex Systems},
 year = {2017}
}

@Filtered Article{676b0203-e840-4deb-9ff4-5c2ec6627025,
 abstract = {Computational biology and bioinformatics (CBB), the terms often used interchangeably, represent a rapidly evolving biological discipline. With the clear potential for discovery and innovation, and the need to deal with the deluge of biological data, many academic institutions are committing significant resources to develop CBB research and training programs. Yale formally established an interdepartmental Ph.D. program in CBB in May 2003. This paper describes Yale’s program, discussing the scope of the field, the program’s goals and curriculum, as well as a number of issues that arose in implementing the program. (Further updated information is available from the program’s website, www.cbb.yale.edu.)},
 authors = {['Mark Gerstein', 'Dov Greenbaum', 'Kei Cheung', 'Perry L. Miller']},
 journal = {Journal of Biomedical Informatics},
 keywords = {['Bioinformatics', 'Computational biology', 'Educational programs', 'Curriculum']},
 title = {An interdepartmental Ph.D. program in computational biology and bioinformatics: The Yale perspective},
 year = {2007}
}

@Filtered Article{67b288b0-d8db-47a7-84b7-74ee810b3cdc,
 abstract = {The aim of the study is to describe a methodological approach to represent, interpret, model and manage pluristratified archaeological contexts. The proposed methodology envisages a digital workflow, a BIM-thinking strategy, which integrates geometric and texture data obtained from laser and photogrammetric scans with information about construction techniques and materials, archaeological reports and documentation. The integration is based on a balanced combination of open-source and proprietary solutions, allowing professionals to work with their “comfort software” and assuring interoperability through the adoption of Open Standards. Experimentations are being conducted exploring the potential of connecting semantic 3D modelling and virtual reconstructions based on archaeological data made with Blender and the Extended Matrix Tool, with BIM software and capabilities thanks to the BlenderBIM addon. The proposed workflow, in combination with the described data-sharing-oriented process, adopts a new approach towards 3D models in order to promote a more sustainable mindset towards 3D dataset life-cycle by optimizing their usage and reducing waste on different levels, such as re-documenting the same structure twice. The expected overall result is the ability to generate semantic models that can enhance our understanding of the context as much as foster multidisciplinary BIM (Building Information Modelling) collaboration thus improving archaeological research, documentation and conservation practices.},
 authors = {['Matteo Lombardi', 'Dario Rizzi']},
 journal = {Digital Applications in Archaeology and Cultural Heritage},
 keywords = {['Digital archaeology', 'Semantic modelling', 'HBIM', 'Blender', 'BlenderBIM', 'Extended matrix']},
 title = {Semantic modelling and HBIM: A new multidisciplinary workflow for archaeological heritage},
 year = {2024}
}

@Filtered Article{67e00147-66e4-4f75-a54a-6a89782085e1,
 abstract = {Parkinson’s disease (PD) is a prevalent chronic neurodegenerative disorder characterized by both motor and non-motor symptoms. The significant heterogeneity among PD patients poses a major challenge for treatment interventions. Current clinical interventions for PD primarily target motor symptoms, often neglecting non-motor symptoms, which can lead to unnecessary complications in non-motor symptoms while treating motor symptoms. Therefore, it is crucial to provide comprehensive and precise intervention strategies that encompass both symptom types. To address this issue, we develop a deep learning framework of clinical intervention strategies for PD (CISL-PD) based on counterfactual thinking. This framework introduces Directional Counterfactual Dual Generative Adversarial Networks (DCD-GANs), which apply various counterfactual constraints to longitudinal data to generate practical and plausible counterfactual instances aligned with clinical reality. By analyzing these counterfactual instances and their differences from the original instances, we explore PD intervention strategies with duration-specific fine regulation of multidimensional features. Experiments conducted on 374 PD patients from the Parkinson’s Progression Markers Initiative (PPMI) demonstrate that the counterfactual instances generated by DCD-GANs surpass other state-of-the-art models in terms of similarity (0.307 ± 0.246), sparsity (0.513 ± 0.161), smoothness (0.238 ± 0.135), and trend consistency (0.100 ± 0.089). From these generated counterfactual instances, we develop three clinically feasible intervention strategies that address both motor and non-motor symptoms and identify corresponding patterns of PD with distinct progression differences. Validation on an independent cohort of 351 patients from the National Institute of Neurological Disorders and Stroke Parkinson’s Disease Biomarkers Program (PDBP) confirmed the framework’s robustness and generalizability. By offering precise, multidimensional intervention strategies that can address both motor and non-motor symptoms, the CISL-PD framework has the potential to enhance patient outcomes, reduce complications, improve overall quality of life, and guide clinical decision-making.},
 authors = {['Changrong Pan', 'Yu Tian', 'Lingyan Ma', 'Tianshu Zhou', 'Shuyu Ouyang', 'Jingsong Li']},
 journal = {Expert Systems with Applications},
 keywords = {['Parkinson’s disease', 'Intervention strategies', 'Counterfactual generation', 'Generative Adversarial Network']},
 title = {CISL-PD: A deep learning framework of clinical intervention strategies for Parkinson’s disease based on directional counterfactual Dual GANs},
 year = {2025}
}

@Filtered Article{6808d985-1572-4ba7-a2d3-5758efa91afe,
 abstract = {Broiler represents approximately 1.5% of the Brazilian Gross Domestic Product (GDP). Brazil is one of the world's largest producers and exporters of chicken. Aiming to improve and sustain a competitive advantage, producers have invested in improvements in production systems in general, in particular aviary heating systems. However, producers need to choose the best among several alternatives of energy sources for heating. This decision impacts the environment to a greater or a lesser extent depending on the energy source chosen. The aim of this study is to develop a computational model to understand systemically and dynamically the environmental externalities based on the choice of energy source for aviary heating. The identification of criteria that influence the choice for heating systems was possible through a multiple case-study in the southern region of Brazil. By designing a computational model of system dynamics, it was possible to visualize scenarios using different energy sources and their respective negative environmental externalities. From the analysis of four scenarios, we sought to identify the one with the best relation to environmental and economic performance. It was evidenced that the scenario with the best relation was that using pellets as an energy source for aviary heating. The developed model may be applied to solve similar decision-making problems.},
 authors = {['Ricardo Brandão Mansilha', 'Dalila Cisco Collatto', 'Daniel Pacheco Lacerda', 'Maria Isabel {Wolf Motta Morandi}', 'Fabio Sartori Piran']},
 journal = {Journal of Cleaner Production},
 keywords = {['Broiler', 'Environmental externalities', 'Energy sources', 'Systems thinking', 'System dynamics']},
 title = {Environmental externalities in broiler production: An analysis based on system dynamics},
 year = {2019}
}

@Filtered Article{6910e257-c0e4-45e9-919b-77ee95d5a31a,
 abstract = {This study aims at exploring a possibility of developing a creativity-based teaching program needed for enhancing prospective teachers’ creative potentials based on the theories of Sawyer and Renzulli. Neuroimaging tools such as fMRI were used to identify effects of the program on pre-service teachers’ neural activations on divergent thinking measured primarily by the Torrance Tests of Creative Thinking (TTCT). Since the research is still in progress, we present a theoretical model for the teaching program, and preliminary test results of comparing changes of neural recruitments in students’ brain participated in fMRI with the TTCT.},
 authors = {['Sun-Hyung Park', 'Kwang-Ki Kim', 'Kyung-Hwa Lee']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Creativity-based teaching programs', 'Divergent thinking', 'The TTCT', 'FMRI']},
 title = {A Neuro-educational Study of the Development of the Creativity-based Teaching Program and its Effect},
 year = {2015}
}

@Filtered Article{691c0c0e-ae79-4b04-9fc2-d31cf9deaf46,
 abstract = {Teaching is all about communication. Teachers who sharpen their communication skills are prepared to instruct, advise, and mentor their students. They communicate well to effectively collaborate within a healthy educational process. This research aims to Make sure the communication scale prepared for the current research has statistical validity to be applied in the current research, identify the teachers' verbal non-verbal communication skills level, to explore the differences in these variables due to gender. A sample of (376) elementary and preparatory stage teachers, Minia Governorate, Egypt (188 male, 188 female) was chosen. For data collection, the researchers utilized the verbal and non-verbal communication scale (prepared by the researchers). They were applied electronically during the 2020 academic year. The research was a descriptive research design. Results demonstrated The communication scale prepared for the current research has statistical validity to be applied in the current research, there was a high level of verbal and nonverbal communication skills among the research sample. Besides, there were no statistically significant differences between male and female teachers in the levels of verbal communication skills, non-verbal communication skills. Some recommendations regarding the necessity to specify courses for pre-service teachers on verbal communication skills, nonverbal communication skills, were presented. Also, suggestions for those in charge of the educational administration process to improve teachers and school health promotion were illustrated.},
 authors = {['Hossam {Mahmoud Zaki Ali}', 'Mohammed Hasan Ali Al-Abyadh']},
 journal = {Measurement: Sensors},
 keywords = {['Computation intelligent', 'Digital communication', 'Skills', 'Non-verbal communication skills', 'School health promotion', 'Teachers']},
 title = {The computation intelligent in teaching using digital communication},
 year = {2022}
}

@Filtered Article{692987e7-dd79-451c-a772-6c33bc7a6042,
 abstract = {Publisher Summary
Parallel computation represents not only a new mode of computation, but, a new intellectual paradigm. This chapter provides an overview of the technology of parallel computation in terms of hardware and programming languages; presents some of the fundamental classes of problems encountered in economics and the associated numerical methodologies for their solution; discusses the state-of-the-art computational techniques and focuses on parallel techniques and contrasts them with serial techniques for illumination and instructive purposes; and presents applications of the classes of problems and associated numerical methods to econometrics, microeconomics, macroeconomics, and finance. The emergence of computation as a basic scientific methodology in economics has given access to solutions of fundamental problems that pure analysis, observation, or experimentation could not have achieved. Parallel computation represents the wave of the future. It is considered to be cheaper and faster than serial computing and the only approach to faster computation currently foreseeable. Parallel computation is appealing, hence, for the economies of scale that are possible, for the potentially faster solution of large-scale problems, and also for the possibilities that it presents for imitating adjustment or tatonnement processes.},
 authors = {['Anna Nagurney']},
 journal = {Elsevier},
 title = {Chapter 7 Parallel computation},
 year = {1996}
}

@Filtered Article{692d6c14-face-41cd-bb8c-f6151ef54948,
 abstract = {The Less is More hypothesis suggests that one reason adults and children differ in their ability to learn language is that they also differ in other cognitive capacities. According to one version of this hypothesis, children’s relatively poor memory may make them more likely to regularize inconsistent input (Hudson Kam and Newport, 2005, Hudson Kam and Newport, 2009). This paper reports the result of an experimental and computational investigation of one aspect of this version of the hypothesis. A series of seven experiments in which adults were placed under a high cognitive load during a language-learning task reveal that in adults, increased load during learning (as opposed to retrieval) does not result in increased regularization. A computational model offers a possible explanation for these results. It demonstrates that, unless memory limitations distort the data in a particular way, regularization should occur only in the presence of both memory limitations and a prior bias for regularization. Taken together, these findings suggest that the difference in regularization between adults and children may not be solely attributable to differences in memory limitations during learning.},
 authors = {['Amy Perfors']},
 journal = {Journal of Memory and Language},
 keywords = {['Regularization', 'Less is More', 'Computational modeling', 'Language acquisition']},
 title = {When do memory limitations lead to regularization? An experimental and computational investigation},
 year = {2012}
}

@Filtered Article{6964a457-61db-43fa-aa32-ddaf703cdc54,
 abstract = {The present qualitative case study on mathematics majors’ visualization of eigenvector–eigenvalue concepts in a dynamic environment highlights the significance of student-generated representations in a theoretical framework drawn from Sierpinska's (2000) modes of thinking in linear algebra. Such an approach seemed to provide the research participants with mathematical freedom, which resulted in an awareness of the multiple ways that eigenvalue–eigenvector relationships could be visualized in a manner that widened students’ repertoire of meta-representational competences (diSessa, 2004) in coordination with their preferred modes of visualization. Students’ expression of visual fluency in the course of making sense of the eigenvalue problem Au=λu associated with a variety of matrices occurred in different, yet not necessarily hierarchical modes of visualizations that differed from matrix to matrix: (i) synthetic/analytic mode manifested in the process of detecting eigenvectors when the sought eigenvector and the matrix-applied product vector were aligned in the same/opposite directions; (ii) analytic arithmetic mode manifested in the case of singular matrices (in the determination of the zero eigenvalue) and invertible matrices with nonreal eigenvalues; (iii) analytic structural mode, though rarely occurred, manifested in making sense of the trajectory (circle, ellipse, line segment) of the matrix-applied product vector and relating trajectory behavior to matrix type. While the connection between the thinking modes (Sierpinska, 2000) and the concreteness–necessity–generalizability triad (Harel, 2000) was not sharp, math majors still frequently implemented the CNG principles, which proved facilitatory tools in the evolution of students’ thinking about the eigenvalue–eigenvector relationships.},
 authors = {['Günhan Caglayan']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Undergraduate mathematics education', 'Dynamic geometry software', 'Visualization', 'Representation', 'Connection', 'Linear algebra', 'Eigenvectors', 'Eigenvalues', 'Matrices']},
 title = {Making sense of eigenvalue–eigenvector relationships: Math majors’ linear algebra – Geometry connections in a dynamic environment},
 year = {2015}
}

@Filtered Article{69c3807b-b303-4b85-8a27-4e7f332a7842,
 abstract = {Objective
Although sleep difficulties are common after spinal cord injury (SCI), little is known about how day-to-day fluctuations in sleep quality affects health-related quality of life (HRQOL) among these individuals. We examined the effect of sleep quality on same-day HRQOL using ecological momentary assessment methods over a 7-day period.
Design
Repeated-measures study involving 7 days of home monitoring; participants completed HRQOL measures each night and ecological momentary assessment ratings 3 times throughout the day; multilevel models were used to analyze data.
Setting
Two academic medical centers.
Participants
A total of 170 individuals with SCI (N=170).
Interventions
Not applicable.
Main Outcome Measures
Daily sleep quality was rated on a scale of 0 (worst) to 10 (best) each morning. Participants completed end-of-day diaries each night that included several HRQOL measures (Sleep Disturbance, Sleep-related Impairment, Fatigue, Cognitive Abilities, Pain Intensity, Pain Interference, Ability to Participate in Social Roles and Activities, Depression, Anxiety) and ecological momentary assessment ratings of HRQOL (pain, fatigue, subjective thinking) 3 times throughout each day.
Results
Multilevel models indicated that fluctuations in sleep quality (as determined by end-of-day ratings) were significantly related to next-day ratings of HRQOL; sleep quality was related to other reports of sleep (Sleep Disturbance; Sleep-related Impairment; Fatigue) but not to other aspects of HRQOL. For ecological momentary assessment ratings, nights of poor sleep were related to worse pain, fatigue, and thinking. Generally, sleep quality showed consistent associations with fatigue and thinking across the day, but the association between sleep quality and these ecological momentary assessment ratings weakened over the course of the day.
Conclusions
Findings highlight the important association between sleep and HRQOL for people with SCI. Future work targeting sleep quality improvement may have positive downstream effects for improving HRQOL in people with SCI.},
 authors = {['Noelle E. Carlozzi', 'Jenna Freedman', 'Jonathan P. Troost', 'Traci Carson', 'Ivan R. Molton', 'Dawn M. Ehde', 'Kayvan Najarian', 'Jennifer A. Miner', 'Nicholas R. Boileau', 'Anna L. Kratz']},
 journal = {Archives of Physical Medicine and Rehabilitation},
 keywords = {['Ecological momentary assessment', 'Quality of life', 'Rehabilitation', 'Sleep', 'Spinal cord injuries']},
 title = {Daily Variation in Sleep Quality is Associated With Health-Related Quality of Life in People With Spinal Cord Injury},
 year = {2022}
}

@Filtered Article{69d0582e-67ba-4ba9-a52d-e17fb7a5417b,
 authors = {['Gilbert Ryle']},
 journal = {Acta Psychologica},
 title = {Thinking},
 year = {1953}
}

@Filtered Article{6a04078d-4aed-46fa-9839-ee87370a0753,
 abstract = {This paper addresses the implications of an emerging, increasingly important way of thinking about markets: systems thinking. A market is one of the most founational abstractions in marketing and business research; yet, it often receives too little attention. As a result, the taken-for-granted assumptions about markets spur from over-simplified conceptualizations of neoclassical economics that depict markets as static and mechanistic. Systems thinking represents a major change in perspective that involves transcending this mechanistic worldview and thinking instead in terms of wholes, relationships, processes, and patterns. We argue that building a theory of markets based on systems thinking, would enable scholars to develop more realistic models that correspond with fast-changing business environment and therefore, increase both the rigor and relevance of future research. To further this aim, we identify the main implications of systems thinking and formulate them into a research agenda to further the systemic understanding of markets.},
 authors = {['Stephen L. Vargo', 'Kaisa Koskela-Huotari', 'Steve Baron', 'Bo Edvardsson', 'Javier Reynoso', 'Maria Colurcio']},
 journal = {Journal of Business Research},
 keywords = {['Markets', 'Systems thinking', 'Marketing', 'Complex systems', 'Research agenda']},
 title = {A systems perspective on markets – Toward a research agenda},
 year = {2017}
}

@Filtered Article{6acd3ebf-48e9-4043-bc68-8d5c308f7cd5,
 abstract = {We report an experiment in which subjects named 120 pictures, consisting of series of five pictures drawn from each of 24 semantic categories (and intermixed with 45 fillers). The number of intervening trials (lag) between successive presentations of members of the same category varied from two to eight. Subjects' naming latencies were slowed by 30ms for each preceding member of the category. This effect was both cumulative and linear, and unrelated to the lag elapsing since the previous presentation of a category member. These results definitively demonstrate the occurrence of cumulative interference for word retrieval by prior retrieval of other exemplars of the same semantic category—cumulative semantic inhibition. We claim that this inhibition effect could only occur if the spoken word production system possesses three specific properties (competition, priming, and sharing of semantic activation). We provide computational-modelling evidence in support of this claim. We show that no current theory of spoken word production has all of these properties. In their current form, all these theories are falsified by these results. We briefly discuss the obstacles that may be encountered by current models were they modified to account for our findings.},
 authors = {['David Howard', 'Lyndsey Nickels', 'Max Coltheart', 'Jennifer Cole-Virtue']},
 journal = {Cognition},
 keywords = {['Semantic inhibition', 'Spoken word production', 'Picture naming', 'Competition', 'Word retrieval', 'Computational modelling', 'Priming']},
 title = {Cumulative semantic inhibition in picture naming: experimental and computational studies},
 year = {2006}
}

@Filtered Article{6ace1a2b-758a-4c5f-a666-b2c31d896378,
 abstract = {Traditionally, the sensitivity analysis of a Bayesian network studies the impact of individually modifying the entries of its conditional probability tables in a one-at-a-time (OAT) fashion. However, this approach fails to give a comprehensive account of each inputs' relevance, since simultaneous perturbations in two or more parameters often entail higher-order effects that cannot be captured by an OAT analysis. We propose to conduct global variance-based sensitivity analysis instead, whereby n parameters are viewed as uncertain at once and their importance is assessed jointly. Our method works by encoding the uncertainties as n additional variables of the network. To prevent the curse of dimensionality while adding these dimensions, we use low-rank tensor decomposition to break down the new potentials into smaller factors. Last, we apply the method of Sobol to the resulting network to obtain n global sensitivity indices, one for each parameter of interest. Using a benchmark array of both expert-elicited and learned Bayesian networks, we demonstrate that the Sobol indices can significantly differ from the OAT indices, thus revealing the true influence of uncertain parameters and their interactions.},
 authors = {['Rafael Ballester-Ripoll', 'Manuele Leonelli']},
 journal = {International Journal of Approximate Reasoning},
 keywords = {['Bayesian networks', 'Sensitivity analysis', 'Sobol indices', 'Tensor networks', 'Uncertainty quantification']},
 title = {Global sensitivity analysis of uncertain parameters in Bayesian networks},
 year = {2025}
}

@Filtered Article{6ae5d93a-14d9-4df7-aaf3-7ecf5112919b,
 abstract = {Much progress has been made in reducing and refining animal use in toxicology testing, but progress in the use of new approach methodologies (NAMs) to replace animals is disappointing. There are many highly sophisticated NAMs available, but societal, regulatory and political barriers to their implementation remain. Change requires vision, starting with imagining a future where we are successful. Specifically, this would comprise the registration of safe and effective medicines without animal tests. How do we achieve this vision? Thinking differently, in silico methods could be used to provide a detailed assessment of target- and modality-related toxicological risks, coupled with modelling of exposure. In vitro NAMs such as microphysiological systems, microelectrode array and ion channel panels could then be employed to address hypothetical risks. Finally, the safety of first time in human trials could be assessed and assured using circulating nanobots that measure conventional clinical pathology parameters alongside new biomarkers such as circulating tissue DNA. This may seem the stuff of fantasy, but imagination is key to shaping a better future and all change starts with a vision, however far-fetched it may seem today.},
 authors = {['Ruth A. Roberts']},
 journal = {Current Opinion in Toxicology},
 keywords = {['3Rs', 'NAMs', 'FTIH', 'Drug development', 'ICH', 'Nanobots']},
 title = {New approach methodologies (NAMs) in drug safety assessment: A vision of the future},
 year = {2024}
}

@Filtered Article{6afa3200-c5b4-4beb-a8ad-ba17edfcf4c1,
 abstract = {Belief elicitation is important in many different fields of economic research. We show that how a researcher elicits such beliefs – in particular, whether the belief is about the participant’s opponent, an unrelated other, or the population of others – strongly affects the belief reports. We study the underlying processes and find a clear consensus effect. Yet, when matching the opponent’s action would lead to a low payoff and the researcher asks for the belief about this opponent, ex-post rationalization kicks in and beliefs are re-adjusted again. Hence, we recommend to ask about unrelated others or about the population in such cases, as ‘opponent beliefs’ are even more detached from the beliefs participants had when deciding about their actions in the corresponding game. We find no evidence of wishful thinking in any of the treatments.},
 authors = {['Dominik Folli', 'Irenaeus Wolff']},
 journal = {Journal of Economic Psychology},
 keywords = {['Belief elicitation', 'Belief formation', 'Belief-action consistency', 'Framing effects', 'Projection', 'Consensus effect', 'Wishful thinking', 'rationalization']},
 title = {Biases in belief reports},
 year = {2022}
}

@Filtered Article{6b7ef563-b66a-4d21-a18c-b7e9a0933cc5,
 abstract = {We provide experimental evidence of behavior consistent with the sunk cost effect. Subjects who earned a lottery via a real-effort task were given an opportunity to switch to a dominant lottery; 23% chose to stick with their dominated lottery. The endowment effect accounts for roughly only one third of the effect. Subjects’ capacity for cognitive reflection is a significant determinant of sunk cost behavior. We also find stocks of knowledge or experience (crystallized intelligence) predict sunk cost behavior, rather than algorithmic thinking (fluid intelligence) or the personality trait of openness. We construct and validate a scale, the “SCE-8”, which encompasses many resources individuals can spend, and offers researchers an efficient way to measure susceptibility to the sunk cost effect.},
 authors = {['David Ronayne', 'Daniel Sgroi', 'Anthony Tuckwell']},
 journal = {Journal of Economic Behavior & Organization},
 keywords = {['Sunk cost effect', 'Sunk cost fallacy', 'Endowment effect', 'Cognitive ability', 'Psychological scales', 'Scale validation']},
 title = {Evaluating the sunk cost effect},
 year = {2021}
}

@Filtered Article{6bf155dc-743a-4bf5-9e13-da2e0464e1ff,
 abstract = {Monte Carlo simulations were performed to analyze the degree to which two-, three- and four-step learning histories of losses and gains correlated with escalation and persistence in extended extinction (continuous loss) conditions. Simulated learning histories were randomly generated at varying lengths and compositions and warranted probabilities were determined using Bayesian Updating methods. Bayesian Updating predicted instances where particular learning sequences were more likely to engender escalation and persistence under extinction conditions. All simulations revealed greater rates of escalation and persistence in the presence of heterogeneous (e.g., both Wins and Losses) lag sequences, with substantially increased rates of escalation when lags comprised predominantly of losses were followed by wins. These methods were then applied to human investment choices in earlier experiments. The Bayesian Updating models corresponded with data obtained from these experiments. These findings suggest that Bayesian Updating can be utilized as a model for understanding how and when individual commitment may escalate and persist despite continued failures.},
 authors = {['Shawn P. Gilroy', 'Donald A. Hantula']},
 journal = {Behavioural Processes},
 keywords = {['Escalation', 'Computer simulation', 'Decision-making', 'Bayes theorem']},
 title = {Inherently irrational? A computational model of escalation of commitment as Bayesian Updating},
 year = {2016}
}

@Filtered Article{6c604a2d-351a-4ca0-b1a9-5236f6b8990e,
 authors = {['E.N. Economou']},
 journal = {Computational Materials Science},
 title = {Activities, issues and perspectives in computational physics: a view from Greece},
 year = {1994}
}

@Filtered Article{6c80256f-aab0-44bb-8868-6e0571a13f34,
 abstract = {This essay addresses the nature of so–called ‘digital media’ in a literacy context from the perspectives of semiotics, theories of the ‘medium’, and computation. It argues that most accounts that attempt to work with some notion of ‘digital media’ anchor themselves insufficiently in semiotics and computation and the essential combination of these that is necessary when discussing digital media as an object of study. This weakens approaches, particularly when the concern is to develop ways of teaching engagement with contemporary communication practices at any level, i.e., improving ‘digital literacies’ of various kinds. Achieving more robust foundations is important for interventions which are not only more effective but also sustainable, minimizing the danger of obsolescence with each new technological turn of the screw. Foundations are also essential for a more balanced perspective on learning situations that does not dichotomize allegedly ‘digital’ and ‘non–digital’ practices and skills. Many such boundaries are deeply misleading and so unnecessarily compartmentalize thinking and restrict the application of relevant research results and methods. The focus of this essay is therefore to consider how a closer examination of media and their development, combined with the contributions made by information technologies, may help articulate notions of digital media that are more supportive of productive engagements with research and issues of literacy.},
 authors = {['John A. Bateman']},
 journal = {Discourse, Context & Media},
 keywords = {['Digital media', 'Digital information', 'Literacy', 'Models of communication', 'Multimodality', 'Medium', 'Development of media', 'Computational media']},
 title = {What are digital media?},
 year = {2021}
}

@Filtered Article{6ccef3f0-1b71-4c78-9a20-c6c7b7798a6c,
 abstract = {Abundant new information about signaling pathways in forebrain microcircuits presents many challenges, and opportunities for discovery, to computational neuroscientists who strive to bridge from microcircuits to flexible cognition and action. Accurate treatment of microcircuit pathways is especially critical for creating models that correctly predict the outcomes of candidate neurological therapies. Recent models are trying to specify how cortical circuits that enable planning and voluntary actions interact with adaptive subcortical microcircuits in the basal ganglia. The basal ganglia are strongly implicated in reinforcement learning, and in all behavior and cognition over which the frontal lobes exert flexible control. The persisting role of the basal ganglia shows that ancient vertebrate designs for motivated action selection proved adaptable enough to support many “modern” behavioral innovations, including fluent generation of language and speech. This paper summarizes how recent models have incorporated realistic representations of microcircuit features, and have begun to trace their computational implications. Also summarized are recent empirical discoveries that provide guidance regarding how to formulate the rules for synaptic modification that govern learning in cortico-striatal pathways. Such efforts are contributing to an emerging synthesis based on an interlocking set of computational hypotheses regarding cortical interactions with basal ganglia and thalamic nuclei. These hypotheses specify how specialized microcircuits solve learning and control problems inherent to the brain’s parallel design.},
 authors = {['Daniel Bullock', 'Can Ozan Tan', 'Yohan J. John']},
 journal = {Neural Networks},
 keywords = {['Basal ganglia', 'Acetylcholine', 'Dopamine', 'Striatum', 'Decision making']},
 title = {Computational perspectives on forebrain microcircuits implicated in reinforcement learning, action selection, and cognitive control},
 year = {2009}
}

@Filtered Article{6d834429-f9c4-47c8-9e76-9d8b90c36604,
 abstract = {This paper shows how the Independent Core Observer Model (ICOM) Cognitive Architecture for Artificial General Intelligence (AGI) can be applied to building a collective intelligence system called a mediated Artificial Superintelligence (mASI). The details include breaking down the ICOM implementation in the form of the mASI system and the general performance of initial studies with the mASI. Details of the primary difference between the Independent Core Observer Model Cognitive Architecture and the mASI architecture variant include inserting humanity in the contextual engine components of ICOM, creating a type of collective intelligence. Humans can ‘mediate’ new system-generated thinking keeping the thought process accessible and slow enough for humans to oversee and understand. This also allows the modification of emotional valences of the thought process of the mASI system to help the system generate complex contextual models (knowledge graphs) of new ideas and which speeds up the learning process. With the humans acting as control rods in a reactor and emotional drivers, the mASI system maintains safety where the system would cease to function if humans walked away.},
 authors = {['David Kelley']},
 journal = {Procedia Computer Science},
 keywords = {['Collective Intelligence Systems', 'Independent Core Observer Model', 'Artificial General Intelligence', 'mediated Artificial Superintelligence', 'Hive Mind', 'AGI', 'ICOM', 'mASI.']},
 title = {Applying Independent Core Observer Model Cognitive Architecture to a Collective Intelligence System},
 year = {2021}
}

@Filtered Article{6d9a2a3f-4894-4e04-8f04-53b2573d2e5c,
 abstract = {The development of generative AI has led to novel ways that technology can be integrated into creative activities. However, this has also raised concerns about how human creators will be affected, and what impact it may have on creative industries. As a result, there has been research into how we can design AI tools that work with human creators, rather than replacing them. In this paper we review approaches utilized to build AI tools that facilitate human creativity and allow users to engage fully and authentically in the creative process. These include leveraging AI models to help us shed light on elements of the creative process, building interfaces that encourage exploration of ideas, and designing technological affordances that can support the development of new creative practices.},
 authors = {["Katherine O'Toole", 'Emőke-Ágnes Horvát']},
 journal = {Journal of Creativity},
 keywords = {['Computational creativity', 'Generative AI', 'HCI']},
 title = {Extending human creativity with AI},
 year = {2024}
}

@Filtered Article{6de8793d-a9a6-4ed7-ad74-727e977d540e,
 abstract = {Eight new 2-aminothiols (69–96%) and three new sulfonic acids (51–76%) were synthesized and characterized by NMR and HRMS spectra. This study presents the inhibitory effects of a series of novel carvacrol-based 2-aminothiol and sulfonic acid derivatives (3a-f,4a-c) against human carbonic anhydrase I and II isozymes (hCA I and II) acetylcholinesterase (AChE), butyrylcholinesterase (BChE) and α-glycosidase. Ki values were calculated as 12.52±3.61–335.65±60.56 nM for hCA I, 12.20±3.59–389.69±119.41 nM for hCA II, 1.79±0.56–84.86±23.34 nM for AChE, 6.57±2.54–88.05±21.05 nM for BChE and 14.63±4.76–116.39±33.70 nM α-glucosidase enzymes. Also, the inhibition effects of novel carvacrol-based 2-aminothiol (3a-h) and sulfonic acid derivatives (4a-c) were compared to standard and clinically used inhibitors of acetazolamide, Tacrine and acarbose, respectively. Molecular modeling studies of novel compounds, docking scores, and free binding energies were calculated. The activity results of the compounds were found to be compatible with the docking scores. Molecular dynamics studies were conducted with the best activity against CA I and CA II compounds, 4b (IC50: 4.76 nM) and 4a (IC50: 4.36 nM), respectively. In Dynamic Simulation studies, it was observed that the compounds remained stable at the active sites of the proteins.},
 authors = {['Arlinda Bytyqi-Damoni', 'Eda Mehtap Uc', 'Rıfat Emin Bora', 'Hayriye Genc Bilgicli', 'Mehmet Abdullah Alagöz', 'Mustafa Zengin', 'İlhami Gülçin']},
 journal = {Journal of Molecular Structure},
 keywords = {['Carvacrol', 'Carbonic anhydrase', 'α-glucosidase', 'Acetylcholinesterase', 'Butyrylcholinesterase']},
 title = {Synthesis, characterization, and computational study of novel carvacrol-based 2-aminothiol and sulfonic acid derivatives as metabolic enzyme inhibitors},
 year = {2024}
}

@Filtered Article{6e086e6f-fb93-4ec3-be24-471d655cc984,
 abstract = {In this article, we discuss the relevance of electrophysiological data to the enterprise of analyzing and understanding the reading process. Specifically, we detail how the event-related brain potential (ERP) technique (and its magnetic counterpart) can aid in development of models of visual word recognition. Any viable and accurate account of reading must take into account the temporal and anatomical constraints imposed by the fact that reading is a human brain function. We believe that neurophysiological (especially, although not limited to electrophysiological) data can serve an essential reference in the development of biologically realistic models of reading. We assess just how well extant electrophysiological data comport with specific predictions of existing computational models and offer some suggestions for the kinds of research that can address some of the remaining open questions.},
 authors = {['Horacio A. Barber', 'Marta Kutas']},
 journal = {Brain Research Reviews},
 keywords = {['Language', 'Reading', 'Visual word recognition', 'Computational model', 'Event-related potential (ERP)', 'EEG', 'MEG', 'Cognitive electrophysiology']},
 title = {Interplay between computational models and cognitive electrophysiology in visual word recognition},
 year = {2007}
}

@Filtered Article{6e11c6c3-e59f-4b61-bb4c-6497f2b23f99,
 authors = {['Luigi Rizzi']},
 journal = {Lingua},
 title = {Introduction: Core computational principles in natural language syntax},
 year = {2013}
}

@Filtered Article{6e2b9152-ecaa-460b-9cad-ab7f90d0cec6,
 abstract = {Over the last decades, anti-seismic devices have gained increasing interest in the civil engineering field. The introduction of the base isolation system has led to a new concept in the construction panorama in terms of human life safety, a new way of thinking on new constructions, improvement and retrofitting on existent structures. Therefore, rubber and friction isolators have been deeply investigated to hence performances and predict dynamic behaviour during an earthquake. While the response of the former is characterised by the composition of the elastomeric compound, the latter features special materials able to dissipate energy by moving on smooth surfaces. This paper focuses on friction pendulum devices and addresses its attention on the behaviour of sliding materials. It is well-known that stick-slip phenomenon occurs when friction excitation is present and, in the anti-seismic field is important to reduce it and have a well-representative mathematical law able to describe it. Therefore, Hirun International after performing several treatments of the sliding materials has set up a special processing to guarantee a stable response of the HI-M material used on pendulum devices. The paper, after a brief presentation of the special sliding material, shows a comparison between the material with and without the treatment in terms of the force-displacement law. The paper also analyses in detail the cinematic behaviour of the sliding pendulum with one or two main sliding surfaces, with and without central articulation and determines the stress distribution in the sliding surfaces for the different cases.},
 authors = {['Ivan Marenda', 'Agostino Marioni', 'Marco Banfi', 'Roberto Dalpedri']},
 journal = {Procedia Structural Integrity},
 keywords = {['friction coefficient', 'pendulum device', 'contact', 'isolation system']},
 title = {Sliding pendulum isolators without secretes},
 year = {2023}
}

@Filtered Article{6ea35788-0566-4684-ba8d-4ad3b6376f9c,
 abstract = {This article presents a process for the design of innovative mechatronic products that integrates techniques of Design Thinking, Concurrent Engineering and Agilism to Intellectual Property Management activities. Design Thinking is employed in the early stages in order to better explore creativity, whereas Concurrent Engineering and Agilism are applied during the development of the product, in order to deal with emerging requirements and shrinking development times. The product development process is accompanied by Intellectual Property Management activities that address the protection of the project's intellectual assets. In this way, the proposed process represents an addition to theory and practice by smoothly integrating the three most influential product design philosophies of today, while, at the same time, introduces a direction for managing intellectual assets throughout the product lifecycle.},
 authors = {['Rogerio Atem {de Carvalho}', 'Henrique {da Hora}', 'Rodrigo Fernandes']},
 journal = {International Journal of Production Economics},
 keywords = {['Mechatronics', 'Product design', 'Design thinking', 'Concurrent engineering', 'Agilism', 'Product life cycle', 'Intellectual property', 'Innovation management']},
 title = {A process for designing innovative mechatronic products},
 year = {2021}
}

@Filtered Article{6ec6d968-0ca1-4157-9bb9-179995bc6a70,
 abstract = {Significant efforts have been devoted to assessing construction and demolition waste management (CDWM). However, there is little knowledge to understand the utilisation of the developed models for assessing CDWM performance, thus limiting the comparison and generalization of recognized methods and tools. By reviewing the prior published literature, this study assesses the current research methods, in particular, data collection. It also reviews the range of critical indicators for CDWM performance assessment considered by the literature and put forwards a new framework for better assessing CDWM performance. The proposed framework summarises the system boundary, research scale and performance assessment aspects documented by previous studies, and further integrate an integrated framework with procedures for better assessing CDWM performance. The literature review found that while some studies adopt a system thinking and life cycle thinking to assess CDWM performance, other research they adopt a sustainability based model to finalize CDWM performance assessment. The results also demonstrate that compared with environmental and economic aspects, the social aspect has attracted less attention. Social factors, however are crucial in CDWM. The findings about current performance assessment practices in CDWM and the proposed procedures are possible to implement for researchers and practitioners to develop sound CDWM approaches.},
 authors = {['Huanyu Wu', 'Jian Zuo', 'Hongping Yuan', 'George Zillante', 'Jiayuan Wang']},
 journal = {Resources, Conservation and Recycling},
 keywords = {['Construction and demolition waste', 'Waste management', 'Performance assessment methods', 'System thinking', 'Life cycle assessment']},
 title = {A review of performance assessment methods for construction and demolition waste management},
 year = {2019}
}

@Filtered Article{6edcb1d7-3172-4a54-a27b-3e3ae35f8007,
 abstract = {Maximum power point tracking (MPPT) technology plays a key role in improving the energy conversion efficiency of photovoltaic (PV) systems, especially when multiple local maximum power points (LMPPs) occur under partial shading conditions (PSC). It is necessary to modify the operating point efficiently and accurately with the help of MPPT technology to maximize the collected power. Even though a lot of research has been carried out and impressive progress achieved for MPPT technology, it still faces some challenges and dilemmas. Firstly, the mathematical model established for PV cells is not precise enough. Second, the existing algorithms are often optimized for specific conditions and lack comprehensive adaptability to the actual operating environment. Besides, a single algorithm may not be able to give full play to its advantages. In the end, the selection criteria for choosing the suitable MPPT algorithm/converter combination to achieve better performance in a given scenario is very limited. Therefore, this paper systematically discusses the current research status and challenges faced by PV MPPT technology around the three aspects of MPPT models, algorithms, and hardware implementation. Through in-depth thinking and discussion, it also puts forward positive perspectives on future development, and five forward-looking solutions to improve the performance of PV systems MPPT are suggested.},
 authors = {['Bo Yang', 'Rui Xie', 'Zhengxun Guo']},
 journal = {Energy Engineering},
 keywords = {['PV systems', 'MPPT', 'partial shading condition', 'DC-DC converter']},
 title = {Maximum Power Point Tracking Technology for PV Systems: Current Status and Perspectives},
 year = {2024}
}

@Filtered Article{6eeed7e7-00a7-4cd4-9254-3547c1c842d7,
 abstract = {Enrollment in Computing Science university programs is at a dangerously low level. A major reason for this is the general lack of interest in Computing Science by females. In this paper, we discuss our experience with using a computer game construction environment as a vehicle to encourage female participation in Computing Science. Experiments with game construction in grade 10 English classes showed that females enjoyed this activity as much as males and were just as successful. In this paper, we argue that: a) computer game construction is a viable activity for teaching higher-order thinking skills that are essential for Science; b) computer game construction that involves scripting teaches valuable Computing Science abstraction skills; c) this activity is an enjoyable introduction to Computing Science; and d) outcome measures for this activity are not male-dominated in any of the three aspects (higher-order thinking, Computing Science abstraction skills, activity enjoyment). Therefore, we claim that this approach is a viable gender-neutral approach to teaching Computing Science in particular and Science in general that may increase female participation in the discipline.},
 authors = {['Mike Carbonaro', 'Duane Szafron', 'Maria Cutumisu', 'Jonathan Schaeffer']},
 journal = {Computers & Education},
 keywords = {['Computing Science', 'Females in Science', 'Computer game construction']},
 title = {Computer-game construction: A gender-neutral attractor to Computing Science},
 year = {2010}
}

@Filtered Article{6f47d1df-70de-4942-8503-ce9c1f3b526a,
 abstract = {Affective computing (AfC) is a continuously growing multidisciplinary field, spanning areas from artificial intelligence, throughout engineering, psychology, education, cognitive science, to sociology. Therefore, many studies have been devoted to the aim of addressing numerous issues, regarding different facets of AfC solutions. However, there is a lack of classification of the AfC systems. This study aims to fill this gap by reviewing and evaluating the state-of-the-art studies in a qualitative manner. In this line of thinking, we put forward a threefold classification that breaks down to desktop and mobile AfC systems, and AfC machines. Moreover, we identified four types of AfC systems, based on the features extracted. In our opinion, the results of this study can serve as a guide for future affect-related research and design, on the one hand, and provide a better understanding on the role of emotions and affect in human-computer interaction, on the other hand.},
 authors = {['Paweł Weichbroth', 'Wiktor Sroka']},
 journal = {Procedia Computer Science},
 keywords = {['Affective Computing', 'Artificial Emotional Intelligence', 'Classification', 'System', 'Machine']},
 title = {A note on the affective computing systems and machines: a classification and appraisal},
 year = {2022}
}

@Filtered Article{6f5118a2-c79a-42c2-880a-729dcca4ad8a,
 abstract = {Present classification systems for thought disorder lack consistency and require one to remember long-winded definitions limiting their use to research settings. As an extension of recent work in this area (World Congress, 2008), we classify the characteristic thought disorder patterns seen in schizophrenia according to the location of the lesion in notional "threads" of mental computational processes that string speech together. These threads must take both semantics and syntax into consideration in performing their function. When we speak - just as when we write - there is a natural hierarchy topic thread (the topic of the ‘essay’) and multiples of paragraph threads, sentence threads, clause threads, word threads and phoneme threads. Intuitively, we grade the severity of thought disorder depending upon whether a particular thread gets stuck (S), reconnects abnormally (R) or is absent altogether: I.paragraph thread R: Disjointed sentences S: Circumstantiality;II.topic threadR: Tangentiality S: Preoccupatory thinking;III.sentence threads R: Knight's move thinking S: Clause perseveration;IV.clause threads R: Word salad S: Word perseveration, fusion;V.word threads R: Incoherent sounds/ neologisms/ paraphasias S: Phoneme/syllable perseveration;VI.phoneme threads - Failure of production: Mutism.Of course, one must record all the lesions that are present at any given time. This scale incorporates a intuitive progression from mild to severe thought disorder in Schizophrenia. Using the STDS would allow the straightforward ‘bedside’ quantification of the severity of thought disorder and enforce discipline into the thought assessment section of the Mental State Examination.},
 authors = {['C.P. Arun']},
 journal = {European Psychiatry},
 title = {P03-117 A bedside schizophrenia thought disorder scale},
 year = {2009}
}

@Filtered Article{6f525d20-d061-41c6-b0c5-956b35931c9b,
 abstract = {We show how to use the introductory econometrics textbook by Stock and Watson (2019) as a starting point for teaching and studying dynamic panel data methods. The materials are intended for undergraduate students taking their second econometrics course, undergraduate students in seminar-type courses, independent study courses, capstone, or thesis projects, and beginning graduate students in a research methods course. First, we distill the methodological core necessary to understand dynamic panel data methods. Second, we design an empirical and a theoretical case study to highlight the capabilities, downsides, and hazards of the method. The empirical case study is based on the cigarette demand example in Stock and Watson (2019) and illustrates that economic and methodological issues are interrelated. The theoretical case study shows how to evaluate current empirical practices from a theoretical standpoint. We designed both case studies to boost students’ confidence in working with technical material and to provide instructors with more opportunities to let students develop econometric thinking and to actively communicate with applied economists. Although we focus on Stock and Watson (2019) and the statistical software R, we also show how to modify the material for use with another introductory textbook by Wooldridge (2020) and Stata, and highlight some possible further pathways for instructors and students to reuse and extend our materials.},
 authors = {['Markus Fritsch', 'Andrew Adrian Yu Pua', 'Joachim Schnurbus']},
 journal = {International Review of Economics Education},
 keywords = {['Teaching econometrics', 'instrumental variables', 'linear dynamic panel data methods', 'cigarette demand', 'lagged variables']},
 title = {Teaching advanced topics in econometrics using introductory textbooks: The case of dynamic panel data methods},
 year = {2024}
}

@Filtered Article{6fb3eea6-1555-4035-9089-b7ed5f0d9c7a,
 abstract = {Lateral motion of material relative to the regional thermal and kinematic frameworks is important in the interpretation of thermochronology in convergent orogens. Although cooling ages in denuded settings are commonly linked to exhumation, such data are not related to instantaneous behavior but rather to an integration of the exhumation rates experienced between the thermochronological ‘closure’ at depth and subsequent exposure at the surface. The short spatial wavelength variation of thermal structure and denudation rate typical of orogenic regions thus renders thermochronometers sensitive to lateral motion during exhumation. The significance of this lateral motion varies in proportion with closure temperature, which controls the depth at which isotopic closure occurs, and hence, the range of time and length scales over which such data integrate sample histories. Different chronometers thus vary in the fundamental aspects of the orogenic character to which they are sensitive. Isotopic systems with high closure temperature are more sensitive to exhumation paths and the variation in denudation and thermal structure across a region, while those of lower closure temperature constrain shorter-term behaviour and more local conditions. Discounting lateral motion through an orogenic region and interpreting cooling ages purely in terms of vertical exhumation can produce ambiguous results because variation in the cooling rate can result from either change in kinematics over time or the translation of samples through spatially varying conditions. Resolving this ambiguity requires explicit consideration of the physical and thermal framework experienced by samples during their exhumation. This can be best achieved through numerical simulations coupling kinematic deformation to thermal evolution. Such an approach allows the thermochronological implications of different kinematic scenarios to be tested, and thus provides an important means of assessing the contribution of lateral motion to orogenic evolution.},
 authors = {['Geoffrey E. Batt', 'Mark T. Brandon']},
 journal = {Tectonophysics},
 keywords = {['Lateral motion', 'Thermochronology', 'Orogenic regions']},
 title = {Lateral thinking: 2-D interpretation of thermochronology in convergent orogenic settings},
 year = {2002}
}

@Filtered Article{70e52a7b-dcf6-4140-abde-45b2b8b4476b,
 abstract = {The authors explore the noncompliant pedagogy of the image based on their video Autopoietic Veering: Schizo Socius of Tokyo and Vancouver (2021). It is not the kind of trendy modelized video abstract or kinetic presentation eagerly promoted by international publishers; it is a cross-cultural collaborative work intended to generate affirmative temporal ruptures of entropic habitual modes of seeing, memorizing, and thinking of human and nonhuman life in the cities of Tokyo (Japan) and Vancouver (Canada). The authors elucidate Stiegler’s (2015b) concept of a “global mnemotechnical system” that stores and produces human memories in vast digital archives and databases (tertiary retentions) through “mnemonic control” (Parisi & Goodman, 2011). The authors repurpose video images to interrupt and recontrol human perception and memories as “living engines” (Lazzarato, 2006). They foreground the philosophical work of Deleuze, Heidegger, and Virilio to rethink and revive the creative act of “critique” (Foucault, 1997) through “metamodelization” (Guattari, 1995; Manning, 2020); therefore, they plug these apparently incommensurable modes of thinking into their readings of the video’s images. They read the images as “time-images” and focus on their five dimensions that possibly activate “spiritual automation” (Deleuze, 1989), which they assess as “negentropic bifurcatory” potentials (Bradley & Kennedy, 2019).},
 authors = {['Masayuki Iwase', 'Joff P. N. Bradley']},
 journal = {Video Journal of Education and Pedagogy},
 keywords = {['urban film-making', 'critique', 'metamodelization', 'global mnemotechnical system', 'proletarianized knowledge', 'mnemonic control', 'artificial and living engines', 'machinic enslavement', 'negentropic bifurcation', 'Deleuze', 'Heidegger', 'Virilio', 'time-image', 'lectosign', 'spiritual automation', 'zooming-in/out', 'autistic milieus', 'diffractive becoming', 'radical pedagogy']},
 title = {Towards a Noncompliant Pedagogy of the Image: Reading Negentropic Bifurcatory Potentials in Video Images},
 year = {2021}
}

@Filtered Article{7106d7e5-6cac-46a9-8824-504efacc1c9f,
 abstract = {The brain is first and foremost a control system that is capable of building an internal representation of the external world, and using this representation to make decisions, set goals and priorities, formulate plans, and control behavior with intent to achieve its goals. The internal representation is distributed throughout the brain in two forms: (1) firmware embedded in synaptic connections and axon-dendrite circuitry, and (2) dynamic state-variables encoded in the firing rates of neurons in computational loops in the spinal cord, midbrain, subcortical nuclei, and arrays of cortical columns. It assumes that clusters and arrays of neurons are capable of computing logical predicates, smooth arithmetic functions, and matrix transformations over a space defined by large input vectors and arrays. Feedback from output to input of these neural computational units enable them to function as finite-state-automata (fsa), Markov decision processes (MDP), or delay lines in processing signals and generating strings and grammars. Thus, clusters of neurons are capable of parsing and generating language, decomposing tasks, generating plans, and executing scripts. In the cortex, neurons are arranged in arrays of cortical columns that interact in tight loops with their underlying subcortical nuclei. It is hypothesized that these circuits compute sophisticated mathematical and logical functions that maintain and use complex abstract data structures. It is proposed that cortical hypercolumns together with their underlying thalamic nuclei can be modeled as a cortical computational unit (CCU) consisting of a frame-like data structure (containing attributes and pointers) plus the computational processes and mechanisms required to maintain it and use it for perception cognition, and sensory-motor behavior. In sensory processing areas of the brain, CCU processes enable focus of attention, segmentation, grouping, and classification. Pointers stored in CCU frames define relationships that link pixels and signals to objects and events in situations and episodes. CCU frame pointers also link objects and events to class prototypes and overlay them with meaning and emotional values. In behavior generating areas of the brain, CCU processes make decisions, set goals and priorities, generate plans, and control behavior. In general, CCU pointers are used to define rules, grammars, procedures, plans, and behaviors. CCU pointers also define abstract data structures analogous to lists, frames, objects, classes, rules, plans, and semantic nets. It is suggested that it may be possible to reverse engineer the human brain at the CCU level of fidelity using next-generation massively parallel computer hardware and software.},
 authors = {['James S. Albus']},
 journal = {Information Sciences},
 keywords = {['Brain modeling', 'Cognitive modeling', 'Human neocortex', 'Image processing', 'Knowledge representation', 'Perception', 'Reverse engineering the brain', 'Segmentation', 'Signals to symbols']},
 title = {A model of computation and representation in the brain},
 year = {2010}
}

@Filtered Article{71657793-416d-4b1d-bd87-985779422840,
 abstract = {A stabilized finite element formulation for three-dimensional unsteady incompressible flows is implemented on a distributed memory parallel computer. A matrix-free version of the GMRES algorithm is utilized to solve the equation systems in an implicit manner. The scalability of the computations on a 64-processor Linux cluster is evaluated for moderate to large size problems. A method for estimating the speedup for large-scale problems, where computations on a single processor is not possible, is proposed. Superlinear speedup is observed, perhaps for the first time, for a large-scale problem that is associated with more than 44 million nodes and 176 million equations. The performance of the various subactivities of the program is monitored to investigate the cause. It is found that the formation of the RHS vector and the preconditioner achieves a very high level of superlinear speedup as the number of processors increase. As a result, even though the network time for interprocessor communication increases with increase in processors, an overall superlinear speedup is realized for large-scale problems. The superlinear speedup is attributed to cache related effects. A comparison between the performance of matrix and matrix-free versions of the GMRES algorithm is carried out. It is found that for large-scale applications the matrix-free version outperforms its counterpart for reasonable dimensions of the Kyrylov subspace. The effect of mesh partitioning on the scalability is also studied. A significant reduction in communication time is observed with partitioning that leads to an overall improvement of speedup. The parallel implementation is utilized to study the wake instabilities in flow past a stationary circular cylinder at Re=150, 200 and 300. The Re=150 flow is found to be two-dimensional while mode-A and mode-B instabilities are observed at Re=200 and 300, respectively. The Re=300 flow is associated with a low frequency modulation in addition to the vortex shedding frequency.},
 authors = {['Suresh Behara', 'Sanjay Mittal']},
 journal = {Parallel Computing},
 keywords = {['Navier–Stokes equations', 'Parallel computing', 'Superlinear speedup', 'Wake', 'Transition', 'Wake instabilities']},
 title = {Parallel finite element computation of incompressible flows},
 year = {2009}
}

@Filtered Article{717f822d-e93f-4ab7-a714-6bbfa124c9b7,
 abstract = {Parallel programming of high-performance computers has emerged as a key technology for the numerical solution of large-scale problems arising in computational science and engineering (CSE). The authors believe that principles and techniques of parallel programming are among the essential ingredients of any CSE as well as computer science curriculum. Today, opinions on the role and importance of parallel programming are diverse. Rather than seeing it as a marginal beneficial skill optionally taught at the graduate level, we understand parallel programming as crucial basic skill that should be taught as an integral part of the undergraduate computer science curriculum. A practical training course developed for computer science undergraduates at Aachen University is described. Its goal is to introduce young computer science students to different parallel programming paradigms for shared and distributed memory computers as well as to give a first exposition to the field of computational science by simple, yet carefully chosen sample problems.},
 authors = {['H.M. Bücker', 'B. Lang', 'C.H. Bischof']},
 journal = {Future Generation Computer Systems},
 keywords = {['Parallel programming', 'Java', 'Computational science and engineering', 'Education']},
 title = {Parallel programming in computational science: an introductory practical training course for computer science undergraduates at Aachen University},
 year = {2003}
}

@Filtered Article{7225060b-2c72-4bca-8ad7-b00c916fac0f,
 abstract = {Background
The challenge to increase the diversity, inclusivity, and equity of nurse scientists is a critical issue to enhance nursing knowledge development, health care, health equity, and health outcomes in the United States.
Purpose
The purpose of this paper is to highlight the current nurse scholars in the Robert Wood Johnson Foundation (RWJF) Harold Amos Medical Faculty Development Program (AMFDP).
Discussion
Profiles and the programs of research and scholarship of the current AMFDP nurse scholars are described and discussed. Scholars share lessons learned, and how the AMFDP program has influenced their thinking and commitments to future action in service of nursing science, diversity efforts, legacy leadership, issues of health equity.
Conclusion
RWJF has a history of supporting the development of nursing scholars. AMFDP is an example of legacy leadership program that contributes to a culture of health and the development of next-generation nursing science scholars.},
 authors = {['Cindy M. Anderson', 'Nina Ardery', 'Daniel Pesut', 'Carmen Alvarez', 'Tamryn F. Gray', 'Karen M. Rose', 'Jasmine L. Travers', 'Janiece Taylor', 'Kathy D. Wright']},
 journal = {Nursing Outlook},
 keywords = {['Health professions diversity', 'Equity', 'Inclusive excellence', 'Robert Wood Johnson Foundation', 'Nurse faculty scholars', 'Harold Amos', 'Medical faculty development', 'Legacy leadership']},
 title = {Nurse scholars of the Robert Wood Johnson Foundation Harold Amos Medical Faculty Development Program},
 year = {2023}
}

@Filtered Article{727707e2-97b1-4d3d-950e-60258562237c,
 abstract = {RoboCupJunior is an international educational robotics initiative, aiming to promote STEM content and skill learning among participating youth through educational robotics competition inaugurated in 2000. What makes RoboCupJunior quite unique is its relationship with RoboCup which aims to promote robotics and AI research, by offering a publicly appealing, but formidable challenge including development of soccer robots, search and rescue robots, and robots functions at home and at work. This paper introduces a case of RoboCupJunior and the effectiveness of its practice for enhancing learning of STEM contents and skills for innovation and creativity among participating students. It presents the survey results from one of the World Championships held in 2012, the anecdotal and personal account of participating US students on their learning experience from their participation in 2013 World Championship, and participating students’ technological and innovative contributions to highlight the impacts RoboCupJunior has had through over a decade of its practice.},
 authors = {['Amy Eguchi']},
 journal = {Robotics and Autonomous Systems},
 keywords = {['Educational robotics', 'Robotics competitions', 'STEM education', 'Computational thinking', 'Engineering skills', '21st century skills']},
 title = {RoboCupJunior for promoting STEM education, 21st century skills, and technological advancement through robotics competition},
 year = {2016}
}

@Filtered Article{7299b463-c352-473a-91c1-25fed94913e8,
 abstract = {Globally, the cardiovascular diseases are the first cause of death. The early detection and quantification of these diseases can significantly reduce the mortality rate. Recent advances in cardiac MRI (CMRI) enable the detection of the left ventricle (LV) wall pathologies and the estimation of different quantification metrics that characterize the working of the heart. Examples of these metrics include the area of pathological tissue in the LV wall, the transmural extent of pathology, and other indexes such as wall thickening, functional strain, and the ejection fraction metrics. In the literature, several computational methods have been proposed in order to estimate these metrics based on using different CMRI acquisition techniques, such as cardiac-enhanced CMRI (CE-CMRI) and cine CMRI. This chapter overviews these computational methods and explains their basic ideas, focusing on the metrics extracted using CE-CMRI and cine CMRI.},
 authors = {['Ahmed Elnakib', 'Mohammed Ghazal', 'Fatma Taher', 'Ali H. Mahmoud', 'Ayman El-Baz']},
 journal = {Elsevier},
 keywords = {['Computational methods', 'Left ventricle', 'Heart', 'Pathologies', 'Cardiac MRI (CMRI)', 'Segmentation']},
 title = {3 - Computational methods for identifying left ventricle heart pathologies},
 year = {2021}
}

@Filtered Article{72a98a15-d61c-4fe1-a788-dfdec0db623c,
 abstract = {This paper looks at peace with an integrated perspective. As a state, peace cannot be measured directly and requires the use of proxies and indicators. This paper revisits the positive peace index (PPI) introduced by the Institute for Economy and Peace (IEP) through the lens of systems thinking and modeling. Three sets of systems tools (cross-impact analysis, network analysis, and system dynamics) are proposed to explicitly account for the different levels of influence and dependence among the eight domains used to determine the PPI at the country level. Although more comprehensive than the original IEP formulation, the integrated approach proposed herein requires decisionmakers to be systems thinkers and able to conduct a detailed analysis of how the eight domains influence (impact) or depend on (sensitive to) each other. The proposed approach allows decisionmakers to capture the multidimensional and cross-disciplinary nature of positive peace better. This paper also shows that the three components of peace (positive, negative, and cultural) initially proposed by Johan Galtung can be represented using three-dimensional geometric features.},
 authors = {['Bernard Amadei']},
 journal = {Technological Forecasting and Social Change},
 keywords = {['Complex systems', 'Systems thinking', 'System dynamics', 'Cross-impact analysis', 'Network analysis', 'Positive peace', 'Peace geometry']},
 title = {Revisiting positive peace using systems tools},
 year = {2020}
}

@Filtered Article{72e45277-a60c-4c37-8730-5ba1ea42f017,
 abstract = {This chapter reviews the origin and evolution of high-throughput screening (HTS) through the experience of the authors, who have either consulted for and/or provided courses to various pharmaceutical companies. It focuses on the role of HTS in natural product (phytochemicals) drug screening and drug discovery. Application of computational methods in HTS for phytochemical is highlighted. Commonly encountered difficulties and solutions to some of the problems are discussed together with selected ‘how to’ protocols to ensure investigators can set up and productively use HTS in their own natural product research. Relevant failures and successes in identifying interesting natural products are also outlined.},
 authors = {['Fyaz M.D. Ismail', 'Lutfun Nahar', 'Satyajit D. Sarker']},
 journal = {Elsevier},
 keywords = {['High-throughput screening (HTS)', 'Robotics', 'Dereplication', 'Liquid handling systems', 'Screening', 'Natural product prototypes', 'Drug discovery and development', '', '', '']},
 title = {Chapter 6 - High-Throughput Screening of Phytochemicals: Application of Computational Methods},
 year = {2018}
}

@Filtered Article{73228d13-ce6f-4694-b49e-93688e0008f0,
 abstract = {Sustainability is gaining attention, particularly in the building sector, owing to its significant influence on economy, society and environment. However, most assessment methods/frameworks available for this sector focus solely or dominantly on the environmental dimension of sustainability. Hence, a sustainability assessment framework for buildings that accounts for the interdependencies amongst social, economic and environmental aspects is essential. Further, buildings also undergo several time-induced changes in their characteristics, such as changes in electricity consumption, material properties, surrounding infrastructure and energy mix that can influence their sustainability. Therefore, this paper introduces a system dynamics-based methodological framework for Dynamic Life Cycle Sustainability Assessment (D-LCSA) capable of incorporating the dynamic changes in the building characteristics with time and capturing the interactions amongst different sustainability indicators. The usability and utility of the framework is demonstrated using a case study residential project in India. The case study results show that ignoring time-dependant dynamic aspects in sustainability assessment of buildings leads to underestimating the overall sustainability impacts by about 50 per cent and specific environmental impacts by about 12 per cent. Therefore, the study reinforces the need to adopt dynamic thinking through modelling and simulation to predict sustainability performance in the built environment.},
 authors = {['Ann Francis', 'Albert Thomas']},
 journal = {Sustainable Cities and Society},
 keywords = {['Sustainability assessment', 'System dynamics', 'Dynamic life cycle sustainability assessment (D-LCSA)', 'Computational modelling', 'Life cycle assessment']},
 title = {A framework for dynamic life cycle sustainability assessment and policy analysis of built environment through a system dynamics approach},
 year = {2022}
}

@Filtered Article{737be6ad-48dc-4099-bb80-5b1ad3e79846,
 abstract = {Neuroscientists often describe neural activity as a representation of something, or claim to have found evidence for a neural representation, but there is considerable ambiguity about what such claims entail. Here we develop a thorough account of what ‘representation’ does and should do for neuroscientists in terms of three key aspects of representation. (i) Correlation: a neural representation correlates to its represented content; (ii) causal role: the representation has a characteristic effect on behavior; and (iii) teleology: a goal or purpose served by the behavior and thus the representation. We draw broadly on literature in both neuroscience and philosophy to show how these three aspects are rooted in common approaches to understanding the brain and mind. We first describe different contexts that ‘representation’ has been closely linked to in neuroscience, then discuss each of the three aspects in detail.},
 authors = {['Ben Baker', 'Benjamin Lansdell', 'Konrad P. Kording']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['representation', 'information', 'coding', 'explanation', 'function', 'teleology', 'philosophy']},
 title = {Three aspects of representation in neuroscience},
 year = {2022}
}

@Filtered Article{73ca5769-bfa1-4f0a-8974-0b1c5abbc891,
 abstract = {This paper provides an overview of some of the CFD methods developed by the Team for Advanced Flow Simulation and Modeling (T*AFSM) [http://www.mems.rice.edu/TAFSM/]. The paper also provides many examples of three-dimensional flow simulations carried out with these CFD methods and advanced parallel supercomputers. The methods and tools described in this paper include: stabilized finite element formulations; formulations for flows with moving boundaries and interfaces; mesh update methods; iterative solution techniques for large nonlinear equation systems; and parallel implementation of these methods. Our target is to be able to address effectively certain classes of flow simulation problems. These include: unsteady flows with interfaces; fluid–object interactions; fluid–structure interactions; airdrop systems; aerodynamics of complex shapes; and contaminant dispersion.},
 authors = {['Tayfun E. Tezduyar']},
 journal = {Journal of Wind Engineering and Industrial Aerodynamics},
 keywords = {['CFD methods', 'T*AFSM', 'Three-dimensional flow simulations']},
 title = {CFD methods for three-dimensional computation of complex flow problems},
 year = {1999}
}

@Filtered Article{74057be2-3661-47cb-93f0-f405a9ac418d,
 abstract = {The production system and its maintenance system must be now developed on “system thinking” paradigm in order to guarantee that Key Performance Indicators (KPI) will be optimized all along the production system (operation) life. In a recursive way, maintenance system engineering has to integrate also KPI considerations with regards to its own enabling systems. Thus this paper develops a system-based methodology wherein a set of KPIs is computed in order to verify if the objectives of the production and maintenance systems are satisfied. In order to help the decision-making process for maintenance managers, a “unified” generic model have been developed. This model integrates (a) the interactions of the maintenance system with its enabling systems, (b) the impact of the maintenance strategies through the computation of some key performance indicators, and (c) different kinds of knowledge regarding the maintenance system and the system of interest, including quantitative and qualitative knowledge. This methodology is based on an executable unified model built with Probabilistic Relational Model (PRM). PRM allows a modular representation and inferences computation of large size models. The methodology added-value is shown on a test-bench.},
 authors = {['G. Medina-Oliva', 'P. Weber', 'B. Iung']},
 journal = {Reliability Engineering & System Safety},
 keywords = {['Maintenance strategies', 'Performances analysis', 'Decision-making', 'Bayesian Networks (BN)', 'Probabilistic Relational Model (PRM)']},
 title = {PRM-based patterns for knowledge formalisation of industrial systems to support maintenance strategies assessment},
 year = {2013}
}

@Filtered Article{742b5162-3565-416e-a947-4a0b26204ac8,
 authors = {['Matı&#x0301;as Alvarado', 'Leonid Cheremetov', 'Francisco Cantú']},
 journal = {Expert Systems with Applications},
 title = {Autonomous agents and computational intelligence: the future of AI application for petroleum industry},
 year = {2004}
}

@Filtered Article{747dceb6-4316-4d0b-81a8-d3ee6f9be84f,
 abstract = {Suspicions that the world might be some sort of a machine or algorithm existing “in the mind” of some symbolic number cruncher have lingered from antiquity. Although popular at times, the most radical forms of this idea never reached mainstream. Modern developments in physics and computer science have lent support to the thesis, but empirical evidence is needed before it can begin to replace our contemporary world view.},
 authors = {['Karl Svozil']},
 journal = {Chaos, Solitons & Fractals},
 title = {Computational universes},
 year = {2005}
}

@Filtered Article{749a9e93-9059-4114-8fa9-70b278d504fd,
 abstract = {Summary
The supplementary motor area (SMA) is believed to contribute to higher order aspects of motor control. We considered a key higher order role: tracking progress throughout an action. We propose that doing so requires population activity to display low "trajectory divergence": situations with different future motor outputs should be distinct, even when present motor output is identical. We examined neural activity in SMA and primary motor cortex (M1) as monkeys cycled various distances through a virtual environment. SMA exhibited multiple response features that were absent in M1. At the single-neuron level, these included ramping firing rates and cycle-specific responses. At the population level, they included a helical population-trajectory geometry with shifts in the occupied subspace as movement unfolded. These diverse features all served to reduce trajectory divergence, which was much lower in SMA versus M1. Analogous population-trajectory geometry, also with low divergence, naturally arose in networks trained to internally guide multi-cycle movement.},
 authors = {['Abigail A. Russo', 'Ramin Khajeh', 'Sean R. Bittner', 'Sean M. Perkins', 'John P. Cunningham', 'L.F. Abbott', 'Mark M. Churchland']},
 journal = {Neuron},
 keywords = {['supplementary motor area', 'motor control', 'motor cortex', 'population coding', 'recurrent neural network', 'neural dynamics', 'neural computation', 'population geometry']},
 title = {Neural Trajectories in the Supplementary Motor Area and Motor Cortex Exhibit Distinct Geometries, Compatible with Different Classes of Computation},
 year = {2020}
}

@Filtered Article{74c3e7dc-addf-45e3-8a93-1d12d5da4b0e,
 abstract = {The conformational space of methacrylamide was explored by quantum mechanical modeling and surveyed in the 59.6–104.0 GHz frequency range using a millimeter-wave Stark-modulated free-jet absorption spectrometer. According to the relative orientation of the two unsaturated bonds, two conformers were observed, namely s-trans (A=5234.360(1), B=3364.9717(8) and C=2173.099(1) MHz) and s-cis (A=5207.292(1), B=3470.930(1) and C=2113.496(1) MHz). The s-trans conformation is the global minimum, with relative energy 4(2) kJ mol−1 and calculated isomerization barrier 15 kJ mol−1. Except for the methyl hydrogen atoms, s-cis-methacrylamide is planar and its methyl internal rotation barrier is 10.2(1) kJ mol−1. In s-trans-methacrylamide the allyl and amino frames form a dihedral angle of about 30° and the methyl internal rotation barrier is 7.4 kJ mol−1. This different behaviour is explained in terms of attractive and repulsive intramolecular interactions between groups: CH2/CO and CH3/NH2 for s-cis, CH2/NH2 and CH3/CO for s-trans. The tunneling splitting related to the double-well potential describing the interconversion between the two equivalent s-trans forms is 837.97(2) MHz and was reproduced by a one-dimensional flexible model using a 3.6 kJ mol−1 interconversion barrier.},
 authors = {['Assimo Maris', 'Sonia Melandri', 'Luca Evangelisti', 'Annalisa Vigorito', 'Silvia Sigismondi', 'Camilla Calabrese', 'Imanol Usabiaga']},
 journal = {Journal of Molecular Structure},
 keywords = {['Amide', 'Gas-phase structure', 'Large amplitude motions', 'Nuclear quadrupole hyperfine structure', 'Rotational spectroscopy', 'Quantum mechanical calculations']},
 title = {Structure and dynamics of methacrylamide, a computational and free-jet rotational spectroscopic study},
 year = {2022}
}

@Filtered Article{750754f8-8d50-4983-8e0d-e052c6200bae,
 abstract = {The past few decades have established how digital technologies and platforms have provided an effective medium for spreading hateful content, which has been linked to several catastrophic consequences. Recent academic studies have also highlighted how online hate is a phenomenon that strategically makes use of multiple online platforms. In this article, we seek to advance the current research landscape by harnessing a cross-platform approach to computationally analyse content relating to the 2020 COVID-19 pandemic. More specifically, we analyse content on hate-specific environments from Twitter, Reddit, 4chan and Stormfront. Our findings show how content and posting activity can change across platforms, and how the psychological components of online content can differ depending on the platform being used. Through this, we provide unique insight into the cross-platform behaviours of online hate. We further define several avenues for future research within this field so as to gain a more comprehensive understanding of the global hate ecosystem.},
 authors = {['Fatima Zahrah', 'Jason R.C. Nurse', 'Michael Goldsmith']},
 journal = {Big Data Research},
 keywords = {['Social media analysis', 'Cross-platform analysis', 'Online hate', 'COVID-19']},
 title = {Unmasking hate in the pandemic: A cross-platform study of the COVID-19 infodemic},
 year = {2024}
}

@Filtered Article{7583318e-f003-4bd7-bbe0-ad7c694a67f5,
 abstract = {Decision making is an interdisciplinary field, which is explored with methods spanning from economic experiments to brain scanning. Its dominant paradigms such as utility theory, prospect theory, and the modern dual-process theories all resort to formal algebraic models or non-mathematical postulates, and remain purely phenomenological. An approach introduced by Grossberg deployed differential equations describing neural networks and bridged the gap between decision science and the psychology of cognitive–emotional interactions. However, the limits within which neural models can explain data from real people’s actions are virtually untested and remain unknown. Here we show that a model built around a recurrent gated dipole can successfully forecast individual economic choices in a complex laboratory experiment. Unlike classical statistical and econometric techniques or machine learning algorithms, our method calibrates the equations for each individual separately, and carries out prediction person-by-person. It predicted very well the behaviour of 15%–20% of the participants in the experiment–half of them extremely well–and was overall useful for two thirds of all 211 subjects. The model succeeded with people who were guided by gut feelings and failed with those who had sophisticated strategies. One hypothesis is that this neural network is the biological substrate of the cognitive system for primitive–intuitive thinking, and so we believe that we have a model of how people choose economic options by a simple form of intuition. We anticipate our study to be useful for further studies of human intuitive thinking as well as for analyses of economic systems populated by heterogeneous agents.},
 authors = {['George Mengov']},
 journal = {Neural Networks},
 keywords = {['Decision making', 'Economic choice', 'Experimental economics', 'Gated dipole', 'Intuitive thinking', 'Differential equations']},
 title = {Person-by-person prediction of intuitive economic choice},
 year = {2014}
}

@Filtered Article{75d297cf-c314-47e7-bb63-c332e7cfaea7,
 abstract = {Background
Persecutory delusions are among the most common delusions in schizophrenia and represent the extreme end of the paranoia continuum. Paranoia is accompanied by significant worry and distress. Identifying cognitive mechanisms underlying paranoia is critical for advancing treatment. We hypothesized that aberrant belief updating, which is related to paranoia in human and animal models, would also contribute to persecutory beliefs in individuals with schizophrenia.
Methods
Belief updating was assessed in 42 participants with schizophrenia and 44 healthy control participants using a 3-option probabilistic reversal learning task. Hierarchical Gaussian Filter was used to estimate computational parameters of belief updating. Paranoia was measured using the Positive and Negative Syndrome Scale and the revised Green et al. Paranoid Thoughts Scale. Unusual thought content was measured with the Psychosis Symptom Rating Scale and the Peters et al. Delusions Inventory. Worry was measured using the Dunn Worry Questionnaire.
Results
Paranoia was significantly associated with elevated win-switch rate and prior beliefs about volatility both in schizophrenia and across the whole sample. These relationships were specific to paranoia and did not extend to unusual thought content or measures of anxiety. We observed a significant indirect effect of paranoia on the relationship between prior beliefs about volatility and worry.
Conclusions
This work provides evidence that relationships between belief updating parameters and paranoia extend to schizophrenia, may be specific to persecutory beliefs, and contribute to theoretical models implicating worry in the maintenance of persecutory delusions.},
 authors = {['Julia M. Sheffield', 'Praveen Suthaharan', 'Pantelis Leptourgos', 'Philip R. Corlett']},
 journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
 keywords = {['Belief updating', 'Computational psychiatry', 'Delusions', 'Paranoia', 'Volatility', 'Worry']},
 title = {Belief Updating and Paranoia in Individuals With Schizophrenia},
 year = {2022}
}

@Filtered Article{75fd0dbd-4c63-4fdc-9332-219d37f1aafe,
 abstract = {This paper argues that the history of the computer, of the practice of computation and of the notions of ‘data’ and ‘programme’ are essential for a critical account of the emergence and implications of data-driven research. In order to show this, I focus on the transition that the investigations on the worm C. elegans experienced in the Laboratory of Molecular Biology of Cambridge (UK). Throughout the 1980s, this research programme evolved from a study of the genetic basis of the worm’s development and behaviour to a DNA mapping and sequencing initiative. By examining the changing computing technologies which were used at the Laboratory, I demonstrate that by the time of this transition researchers shifted from modelling the worm’s genetic programme on a mainframe apparatus to writing minicomputer programs aimed at providing map and sequence data which was then circulated to other groups working on the genetics of C. elegans. The shift in the worm research should thus not be simply explained in the application of computers which transformed the project from hypothesis-driven to a data-intensive endeavour. The key factor was rather a historically specific technology—in-house and easy programmable minicomputers—which redefined the way of achieving the project’s long-standing goal, leading the genetic programme to co-evolve with the practices of data production and distribution.},
 authors = {['Miguel García-Sancho']},
 journal = {Studies in History and Philosophy of Science Part C: Studies in History and Philosophy of Biological and Biomedical Sciences},
 keywords = {['', 'Genetics', 'Computer', 'Program', 'Software', 'Data', 'Genomics', 'Model organism']},
 title = {From the genetic to the computer program: the historicity of ‘data’ and ‘computation’ in the investigations on the nematode worm C. elegans (1963–1998)},
 year = {2012}
}

@Filtered Article{76207057-a4c1-45fd-8cde-76f96102b99d,
 abstract = {Elements of quantum computation are implemented in a vibrationally excited molecule applying optimal control theory. The two different IR-active modes of acetylene are taken as a two-qubit-system. Optimal control theory is used to design laser pulses that allow transitions within each qubit separately. Calculations for initial state preparation and basic quantum gates are presented.},
 authors = {['Carmen M. Tesch', 'Lukas Kurtz', 'Regina {de Vivie-Riedle}']},
 journal = {Chemical Physics Letters},
 title = {Applying optimal control theory for elements of quantum computation in molecular systems},
 year = {2001}
}

@Filtered Article{76476789-fbe0-4b55-bb37-1d9b4e3f6630,
 abstract = {With increasing recognition of the roles RNA molecules and RNA/protein complexes play in an unexpected variety of biological processes, understanding of RNA structure–function relationships is of high current importance. To make clean biological interpretations from three-dimensional structures, it is imperative to have high-quality, accurate RNA crystal structures available, and the community has thoroughly embraced that goal. However, due to the many degrees of freedom inherent in RNA structure (especially for the backbone), it is a significant challenge to succeed in building accurate experimental models for RNA structures. This chapter describes the tools and techniques our research group and our collaborators have developed over the years to help RNA structural biologists both evaluate and achieve better accuracy. Expert analysis of large, high-resolution, quality-conscious RNA datasets provides the fundamental information that enables automated methods for robust and efficient error diagnosis in validating RNA structures at all resolutions. The even more crucial goal of correcting the diagnosed outliers has steadily developed toward highly effective, computationally based techniques. Automation enables solving complex issues in large RNA structures, but cannot circumvent the need for thoughtful examination of local details, and so we also provide some guidance for interpreting and acting on the results of current structure validation for RNA.},
 authors = {['Swati Jain', 'David C. Richardson', 'Jane S. Richardson']},
 journal = {Academic Press},
 keywords = {['RNA crystallography', 'RNA backbone conformers', 'Ribose pucker', 'Clash correction', 'MolProbity', 'PHENIX', 'ERRASER', 'wwPDB validation']},
 title = {Chapter Seven - Computational Methods for RNA Structure Validation and Improvement},
 year = {2015}
}

@Filtered Article{764ae8ff-2826-4da8-b201-35137030734f,
 abstract = {This paper reviews studies of proposed creative machines applied to a prototypical creative task, i.e., the Alternative Uses Task (AUT). Although one system (OROC) did simulate some aspects of human strategies for the AUT, most recent attempts have not been simulation-oriented, but rather have used Large Language Model (LLM) systems such as GPT-3 which embody extremely large connectionist networks trained on huge volumes of textual data. Studies reviewed here indicate that LLM based systems are performing on the AUT at near or somewhat above human levels in terms of scores on originality and usefulness. Moreover, similar patterns are found in the data of humans and LLM models in the AUT, such as output order effects and a negative association between originality and value or utility. However, it is concluded that GPT-3 and similar systems, despite generating novel and useful responses, do not display creativity as they lack agency and are purely algorithmic. LLM studies so far in this area have largely been exploratory and future studies should guard against possible training data contamination.},
 authors = {['Ken Gilhooly']},
 journal = {Journal of Creativity},
 keywords = {['AI', 'Alternative uses', 'Divergent thinking']},
 title = {AI vs humans in the AUT: Simulations to LLMs},
 year = {2024}
}

@Filtered Article{7679eace-e35b-47f7-aa04-9e3f13c0f945,
 abstract = {Chomskyan generative grammar has long been committed to the ‘double-interface’ assumption that the faculty of language (FL) serves two interfaces, PF and LF, and correlatively that expressions have phonological and semantic properties. The paper argues this gives rise to (a) a grounding problem for syntax – i.e. for the interpretable content of syntax – and (b) a problem for the assumption that FL is a generative computation. It is argued these problems are resolved if we think of syntax as grounded exclusively in semantic/conceptual properties. Since this implies that FL is phonology-free, it is argued that FL should not be distinguished from a generative computation describable as ‘the language of thought’ (LOT). The paper explores to what extent this (FL=LOT) thesis is consistent with Chomsky's thinking. Chomsky's recent work can be seen as pointing in that direction but it is not consistent with the double-interface assumption, which he continues to regard as conceptually necessary. In the light of discussion of the issues, the paper concludes with a speculation on the role of phonology in human cognition and its evolution.},
 authors = {['Noel Burton-Roberts']},
 journal = {Lingua},
 keywords = {['Syntactic grounding', 'Interface interpretation', 'Language faculty', 'Language of thought', 'Phonology-free generativity', 'Phonology in human cognition']},
 title = {On the grounding of syntax and the role of phonology in human cognition},
 year = {2011}
}

@Filtered Article{76f9cdd0-c958-4cba-b8a5-6dad0529a5ae,
 abstract = {This chapter reviews the history of computer vision and artificial intelligence. Computer vision is the field of artificial intelligence that studies how computers can simulate the visual system of humans or other living things. It aims to enable computers to perceive and understand through the processing of visual information based on images or videos. From the 20th century onward, computer vision theory has been progressively developed. King-Sun Fu proposed syntactically structured representation and computation and constructed a top-down computational theory of vision. In the 1970s, David Marr then combined the knowledge of neuroscience, psychology, and other subjects of his time to systematically formulate a computational theory of vision, which made it possible to develop a more rigorous theory of the processing of visual information. Since then, computer vision has been flourishing.},
 authors = {['Wenbo Zheng', 'Fei-Yue Wang']},
 journal = {Academic Press},
 keywords = {['Computer vision', 'Artificial intelligence', 'Knowledge', 'Knowledge-based vision', 'Visual information']},
 title = {Chapter Two - Reviewing the past enables us to learn},
 year = {2024}
}

@Filtered Article{778151cd-37de-46ef-baa6-22d0f5daf0c0,
 abstract = {Summary
The understanding and treatment of psychiatric disorders present unique challenges due to these conditions' multifaceted nature, comprising dynamic interactions between biological, psychological, social, and environmental factors. Traditional reductionistic approaches often simplify these conditions into linear cause-and-effect relationships, overlooking the complexity and interconnectedness inherent in psychiatric disorders. Advances in complex systems approaches provide a comprehensive framework to capture and quantify the non-linear and emergent properties of psychiatric disorders. This Personal View emphasises the importance of identifying rules for generative models that govern brain and behaviour over time, which might contribute to personalised assessments and interventions for psychiatric disorders. For instance, mood fluctuations in bipolar disorder can be understood through dynamical systems modelling, which identifies modifiable parameters, such as circadian disruption, that can be addressed through targeted therapies such as light therapy. Similarly, recognition of depression as an emergent property arising from complex interactions highlights the need for integrated treatment strategies that enhance adaptive reactions in the individual. A framework for quantifying multilevel interactions and network dynamics can help researchers and clinicians to understand the interplay between neural circuits, behaviours, and social contexts. Probabilistic models and self-organisation concepts contribute to building concrete dynamical systems models of mental disorders, facilitating early identification of risk states and promoting resilience through adaptive interventions delivered with optimal timing. Embracing these complex systems approaches in psychiatry could capture the true nature of psychiatric disorders as properties of a dynamic complex system and not the manifestation of any lesion or insult. This line of thinking might improve diagnosis and treatment, offering new hope for individuals affected by psychiatric conditions and paving the way for more effective, personalised mental health care.},
 authors = {['Dost Öngür', 'Martin P Paulus']},
 journal = {The Lancet Psychiatry},
 title = {Embracing complexity in psychiatry—from reductionistic to systems approaches},
 year = {2025}
}

@Filtered Article{77d23702-5f93-49b6-8a9f-276f4ffa9272,
 abstract = {The impact of generative artificial intelligence (AI) on creative production in industry and education is just beginning to be experienced and understood. This impact is likely to accelerate and become even more significant as the computational potential of generative AI grows through training on more diverse and more extensive language models and data sets. Emerging research in this new field suggests that previous models of understanding the interactions between machine and human may no longer be sufficient in a world of generative AI. The significant question is how emerging generative AI technologies will relate to and be a part of human creativity and creative outputs. In this article, we adopt a posthuman stance and conceive of creative output involving generative AI and humans in terms of a yet-to-be-fully-realised and emergent relationship that will likely become more integrated and complex. To investigate and experiment with this relational notion, each of us (as part of an autoethnographic approach) developed a creative output using ChatGPT: a poem and a multimodal narrative. We then employed the idea of alterity relations from the American philosopher of technology, Don Ihde, to conceive of the possibilities and limitations in working relationally and productively with generative AI. As two academics working in teacher education, we applied our learning from this exploration to possibilities in educational contexts. In this article, we offer several important implications and provocations for practitioners, researchers, educators and policymakers, not only in terms of practical concerns but also for rethinking the nature of the creative output.},
 authors = {['Edwin Creely', 'Jo Blannin']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Generative AI', 'Creative production', 'Posthumanism', 'Education', 'Autoethnography', 'Alterity relations']},
 title = {Creative partnerships with generative AI. Possibilities for education and beyond},
 year = {2025}
}

@Filtered Article{78460eef-2d20-47fe-9c26-48e31b20fc12,
 abstract = {Uncovering the root causes of complex diseases requires complex approaches, yet many studies continue to isolate the effects of genetic and social determinants of disease. Epidemiologic efforts that under-utilize genetic epidemiology methods and findings may lead to incomplete understanding of disease. Meanwhile, genetic epidemiology studies are often conducted without consideration of social and environmental context, limiting the public health impact of genomic discoveries. This divide endures despite shared goals and increases in interdisciplinary data due to a lack of shared theoretical frameworks and differing language. Here, we demonstrate that bridging epidemiological divides does not require entirely new ways of thinking. Existing social epidemiology frameworks including Ecosocial theory and Fundamental Cause Theory, can both be extended to incorporate principles from genetic epidemiology. We show that genetic epidemiology can strengthen, rather than detract from, efforts to understand the impact of social determinants of health. In addition to presenting theoretical synergies, we offer practical examples of how genetics can improve the public health impact of epidemiology studies across the field. Ultimately, we aim to provide a guiding framework for trainees and established epidemiologists to think about diseases and complex systems and foster more fruitful collaboration between genetic and traditional epidemiological disciplines.},
 authors = {['Diane Xue', 'Anjum Hajat', 'Alison E. Fohner']},
 journal = {Global Epidemiology},
 title = {Conceptual frameworks for the integration of genetic and social epidemiology in complex diseases},
 year = {2024}
}

@Filtered Article{7848c676-19cd-4739-85a1-56648499fb9d,
 abstract = {Computational fluid dynamics (cfd) is a simulation tool, which uses powerful computer and applied mathematics to model fluid flow situations for the prediction of heat, mass and momentum transfer and optimal design in industrial processes. It is only in recent years that cfd has been applied in the food processing industry. This paper reviews the application of cfd in food processing industries including drying, sterilisation, refrigeration and mixing. The advantages of using cfd are discussed and the future of cfd applications is also outlined.},
 authors = {['Bin Xia', 'Da-Wen Sun']},
 journal = {Computers and Electronics in Agriculture},
 keywords = {['Computational fluid dynamics', '', 'Food', 'Refrigeration', 'Cooling', 'Drying', 'Sterilisation', 'Mixing', 'Chilling', 'Modelling', 'Simulation']},
 title = {Applications of computational fluid dynamics (cfd) in the food industry: a review},
 year = {2002}
}

@Filtered Article{789339b6-581b-4314-813d-ae5dad11286d,
 abstract = {Many cerebellar models use a form of synaptic plasticity that implements decorrelation learning. Parallel fibers carrying signals positively correlated with climbing-fiber input have their synapses weakened (long-term depression), whereas those carrying signals negatively correlated with climbing input have their synapses strengthened (long-term potentiation). Learning therefore ceases when all parallel-fiber signals have been decorrelated from climbing-fiber input. This is a computationally powerful rule for supervised learning and can be cast in a spike-timing dependent plasticity form for comparison with experimental evidence. Decorrelation learning is particularly well suited to sensory prediction, for example, in the reafference problem where external sensory signals are interfered with by reafferent signals from the organism’s own movements, and the required circuit appears similar to the one found to mediate classical eye blink conditioning. However, for certain stimuli, avoidance is a much better option than simple prediction, and decorrelation learning can also be used to acquire appropriate avoidance movements. One example of a stimulus to be avoided is retinal slip that degrades visual processing, and decorrelation learning appears to play a role in the vestibulo-ocular reflex that stabilizes gaze in the face of unpredicted head movements. Decorrelation learning is thus suitable for both sensory prediction and motor control. It may also be well suited for generic spatial and temporal coordination, because of its ability to remove the unwanted side effects of movement. Finally, because it can be used with any kind of time-varying signal, the cerebellum could play a role in cognitive processing.},
 authors = {['Paul Dean', 'John Porrill']},
 journal = {Elsevier},
 keywords = {['Cerebellum', 'eye blink conditioning', 'vestibulo-ocular reflex', 'spike-timing dependent plasticity', 'avoidance learning', 'long-term depression', 'long-term potentiation', 'supervised learning', 'reafference', 'least mean squares']},
 title = {Chapter 7 - Decorrelation Learning in the Cerebellum: Computational Analysis and Experimental Questions},
 year = {2014}
}

@Filtered Article{78a69127-8668-4cb3-9c3e-84980586ea7b,
 abstract = {Background
Large language models (LLMs) are computational artificial intelligence systems with advanced natural language processing capabilities that have recently been popularized among health care students and educators due to their ability to provide real-time access to a vast amount of medical knowledge. The adoption of LLM technology into medical education and training has varied, and little empirical evidence exists to support its use in clinical teaching environments.
Objective
The aim of the study is to identify and qualitatively evaluate potential use cases and limitations of LLM technology for real-time ward-based educational contexts.
Methods
A brief, single-site exploratory evaluation of the publicly available ChatGPT-3.5 (OpenAI) was conducted by implementing the tool into the daily attending rounds of a general internal medicine inpatient service at a large urban academic medical center. ChatGPT was integrated into rounds via both structured and organic use, using the web-based “chatbot” style interface to interact with the LLM through conversational free-text and discrete queries. A qualitative approach using phenomenological inquiry was used to identify key insights related to the use of ChatGPT through analysis of ChatGPT conversation logs and associated shorthand notes from the clinical sessions.
Results
Identified use cases for ChatGPT integration included addressing medical knowledge gaps through discrete medical knowledge inquiries, building differential diagnoses and engaging dual-process thinking, challenging medical axioms, using cognitive aids to support acute care decision-making, and improving complex care management by facilitating conversations with subspecialties. Potential additional uses included engaging in difficult conversations with patients, exploring ethical challenges and general medical ethics teaching, personal continuing medical education resources, developing ward-based teaching tools, supporting and automating clinical documentation, and supporting productivity and task management. LLM biases, misinformation, ethics, and health equity were identified as areas of concern and potential limitations to clinical and training use. A code of conduct on ethical and appropriate use was also developed to guide team usage on the wards.
Conclusions
Overall, ChatGPT offers a novel tool to enhance ward-based learning through rapid information querying, second-order content exploration, and engaged team discussion regarding generated responses. More research is needed to fully understand contexts for educational use, particularly regarding the risks and limitations of the tool in clinical settings and its impacts on trainee development.},
 authors = {['Anthony Skryd', 'Katharine Lawrence']},
 journal = {JMIR Formative Research},
 keywords = {['ChatGPT', 'medical education', 'large language models', 'LLMs', 'clinical decision-making']},
 title = {ChatGPT as a Tool for Medical Education and Clinical Decision-Making on the Wards: Case Study},
 year = {2024}
}

@Filtered Article{78b6d990-74e4-4b22-a685-28e96bb0d640,
 abstract = {Objective
Acoustic phonetic methods are useful in examining some symptoms of schizophrenia; we used such methods to understand the underpinnings of aprosody. We hypothesized that, compared to controls and patients without clinically rated aprosody, patients with aprosody would exhibit reduced variability in: pitch (F0), jaw/mouth opening and tongue height (formant F1), tongue front/back position and/or lip rounding (formant F2), and intensity/loudness.
Methods
Audiorecorded speech was obtained from 98 patients (including 25 with clinically rated aprosody and 29 without) and 102 unaffected controls using five tasks: one describing a drawing, two based on spontaneous speech elicited through a question (Tasks 2 and 3), and two based on reading prose excerpts (Tasks 4 and 5). We compared groups on variation in pitch (F0), formant F1 and F2, and intensity/loudness.
Results
Regarding pitch variation, patients with aprosody differed significantly from controls in Task 5 in both unadjusted tests and those adjusted for sociodemographics. For the standard deviation (SD) of F1, no significant differences were found in adjusted tests. Regarding SD of F2, patients with aprosody had lower values than controls in Task 3, 4, and 5. For variation in intensity/loudness, patients with aprosody had lower values than patients without aprosody and controls across the five tasks.
Conclusions
Findings could represent a step toward developing new methods for measuring and tracking the severity of this specific negative symptom using acoustic phonetic parameters; such work is relevant to other psychiatric and neurological disorders.},
 authors = {['Michael T. Compton', 'Anya Lunden', 'Sean D. Cleary', 'Luca Pauselli', 'Yazeed Alolayan', 'Brooke Halpern', 'Beth Broussard', 'Anthony Crisafio', 'Leslie Capulong', 'Pierfrancesco Maria Balducci', 'Francesco Bernardini', 'Michael A. Covington']},
 journal = {Schizophrenia Research},
 keywords = {['Acoustic resonance', 'Aprosody', 'Linguistics', 'Negative symptoms', 'Phonetics', 'Phonology', 'Psychosis', 'Schizophrenia']},
 title = {The aprosody of schizophrenia: Computationally derived acoustic phonetic underpinnings of monotone speech},
 year = {2018}
}

@Filtered Article{78c74ce4-e595-4983-814d-86add40aa95d,
 abstract = {Fine-tuning and inference on Large Language Models like BERT have become increasingly expensive regarding memory cost and computation resources. The recently proposed computation-flexible BERT models facilitate their deployment in varied computational environments. Training such flexible BERT models involves jointly optimizing multiple BERT subnets, which will unavoidably interfere with one another. Besides, the performance of large subnets is limited by the performance gap between the smallest subnet and the supernet, despite efforts to enhance the smaller subnets. In this regard, we propose layer-wise Neural grafting to boost BERT subnets, especially the larger ones. The proposed method improves the average performance of the subnets on six GLUE tasks and boosts the supernets on all GLUE tasks and the SQuAD data set. Based on the boosted subnets, we further build an inference framework enabling practical width- and depth-dynamic inference regarding different inputs by combining width-dynamic gating modules and early exit off-ramps in the depth dimension. Experimental results show that the proposed framework achieves a better dynamic inference range than other methods in terms of trading off performance and computational complexity on four GLUE tasks and SQuAD. In particular, our best-tradeoff inference result outperforms other fixed-size models with similar amount of computations. Compared to BERT-Base, the proposed inference framework yields a 1.3-point improvement in the average GLUE score and a 2.2-point increase in the F1 score on SQuAD, while reducing computations by around 45%.},
 authors = {['Ting Hu', 'Christoph Meinel', 'Haojin Yang']},
 journal = {Computer Speech & Language},
 keywords = {['Grafting', 'Dynamic inference', 'Large Language Models', 'Deep learning']},
 title = {A flexible BERT model enabling width- and depth-dynamic inference},
 year = {2024}
}

@Filtered Article{7990db96-1b2d-4af9-8137-b2053109f9ff,
 abstract = {Broadening participation in computing is more than providing access to computing for students; it requires reimagining and transforming teaching and learning to be more inclusive and culturally sustaining and it begins with elementary school children. In this study, we report on the fourth cycle of a participatory design-based research project in which researchers and children co-design culturally responsive-sustaining computational learning environments. We conducted user experience testing and co-design sessions with seven children on one level of a game-based learning environment in development. We model children's discourse through Epistemic Network Analysis models to investigate their feedback on character design, game narratives, and introductory activities. Our findings reveal 1) children's positive response to characters with counternarratives and visible intersectional identities in computing, 2) positive and negative experiences and feedback from children on game activities and narratives, and 3) suggestions for improvement.},
 authors = {['Golnaz {Arastoopour Irgens}', 'Cinamon Bailey', 'Tolulope Famaye', 'Atefeh Behboudi']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['Elementary education', 'User experience testing', 'Game-based learning', 'Culturally sustaining pedagogies', 'Intersectionality']},
 title = {User experience testing and co-designing a digital game for broadening participation in computing with and for elementary school children},
 year = {2024}
}

@Filtered Article{7a853711-9fa4-481e-8e70-5ed2f6fe4af7,
 abstract = {In the subject of smart sustainable cities, the underlying theories are a foundation for practice. Moreover, scholarly research in the field of smart sustainable cities operates out of the understanding that advances in the underlying knowledge necessitate pursuing multifaceted questions that can only be resolved from the vantage point of interdisciplinarity or transdisciplinarity. Indeed, research problems in this field are inherently too complex to be addressed by single disciplines. The PhD study addressing the topic of smart sustainable city development falls within the broad research field of sustainability transition and sustainability science where ICT is seen as a salient factor given its transformational, disruptive, and synergetic effects as an enabling, integrative, and constitutive technology. In light of this, the approach to the PhD study is of an applied theoretical kind, and its aim is to investigate and analyze how to advance and sustain the contribution of sustainable urban forms to the goals of sustainable development with support of ICT of pervasive computing. This is to primarily create a framework for strategic smart sustainable city development based on scientific principles, theories, and academic disciplines and discourses used to guide urban actors in their practice towards sustainability and analyze its impact. This involves the application of a set of integrative foundational elements drawn from urban planning, urban design, sustainability, sustainable development, sustainability science, data science, computer science, complexity science, systems theory, systems thinking, and ICT. Accordingly, it is deemed of high significance to devise a multidimensional framework consisting of relevant theories and academic disciplines and discourses that underpin the development of smart sustainable cities as a set of future practices. This framework in turn emphasizes the interdisciplinary and transdisciplinary nature and orientation of the topic of smart sustainable cities and thus the relevance of pursuing an interdisciplinary and transdisciplinary approach into studying this topic. Therefore, this paper endeavors to systematize the very complex and dense scientific area of smart sustainable cities in terms of identifying, distilling, and structuring the core dimensions of a foundational framework for smart sustainable city development as a set of future practices. In doing so, it focuses on a number of fundamental theories along with academic disciplines and discourses, with the aim of setting a framework that analytically relates city development, sustainability, and ICT, while emphasizing how and to what extent sustainability and ICT have particularly become influential in city development in modern society. In addition, this paper offers an in–depth interdisciplinary and transdisciplinary discussion covering topics of high relevance to the PhD study and at the heart of the very synergic relationship between the theoretical, disciplinary, and discursive dimensions of the foundational framework underpinning smart sustainable city development. These dimensions thus form the basis for the framework for strategic smart sustainable city development that is under investigation and will be developed based on a backcasting approach to strategic planning. This study provides an important lens through which to understand a set of influential theories and established academic disciplines and discourses with high potential for integration, fusion, and practicality in relation to the practice of smart sustainable city development.},
 authors = {['Simon Elias Bibri']},
 journal = {Sustainable Cities and Society},
 keywords = {['Smart sustainable cities', 'Theories', 'Academic disciplines', 'Academic discourses', 'Multidimensional framework', 'Interdisciplinarity and transdisciplinarity', 'Systems thinking', 'Complexity science', 'Sustainability', 'Computing and ICT']},
 title = {A foundational framework for smart sustainable city development: Theoretical, disciplinary, and discursive dimensions and their synergies},
 year = {2018}
}

@Filtered Article{7a9e0b6c-4027-477a-9e73-e65b8fb411b8,
 abstract = {Background
Effective shared decision-making between patients and physicians is crucial for enhancing health care quality and reducing medical errors. The literature shows that the absence of effective methods to facilitate shared decision-making can result in poor patient engagement and unfavorable decision outcomes.
Objective
In this paper, we propose a Collaborative Decision Description Language (CoDeL) to model shared decision-making between patients and physicians, offering a theoretical foundation for studying various shared decision scenarios.
Methods
CoDeL is based on an extension of the interaction protocol language of Lightweight Social Calculus. The language utilizes speech acts to represent the attitudes of shared decision-makers toward decision propositions, as well as their semantic relationships within dialogues. It supports interactive argumentation among decision makers by embedding clinical evidence into each segment of decision protocols. Furthermore, CoDeL enables personalized decision-making, allowing for the demonstration of characteristics such as persistence, critical thinking, and openness.
Results
The feasibility of the approach is demonstrated through a case study of shared decision-making in the disease domain of atrial fibrillation. Our experimental results show that integrating the proposed language with GPT can further enhance its capabilities in interactive decision-making, improving interpretability.
Conclusions
The proposed novel CoDeL can enhance doctor-patient shared decision-making in a rational, personalized, and interpretable manner.},
 authors = {['XiaoRui Guo', 'Liang Xiao', 'Xinyu Liu', 'Jianxia Chen', 'Zefang Tong', 'Ziji Liu']},
 journal = {Journal of Medical Internet Research},
 keywords = {['shared decision-making', 'speech acts', 'agent', 'argumentation', 'interaction protocol']},
 title = {Enhancing Doctor-Patient Shared Decision-Making: Design of a Novel Collaborative Decision Description Language},
 year = {2025}
}

@Filtered Article{7ade6fdd-c486-421b-ad2f-a0e13e4538d6,
 abstract = {Publisher Summary
This chapter focuses on semiempirical quantum-chemical methods describing their development over the past 40 years. One of the first semiempirical approaches in quantum chemistry was the p-electron method proposed by Hǖckel (1931) that generates molecular orbitals (MOs) essentially from the connectivity matrix of a molecule and provides valuable qualitative insights into the structure, stability, and spectroscopy of unsaturated molecules. Hoffmann extended this approach to include all valence electrons and applied in many qualitative studies of inorganic and organometallic compounds. These early semiempirical methods had a lasting impact on chemical thinking as they guided the development of qualitative MO theory that is commonly employed for rationalizing chemical phenomena in terms of orbitals interactions. They are normally not used any longer as computational tools. After a survey of the established methods such as MNDO, AM1, and PM3, recent methodological advances are described including the development of improved semiempirical models, new general-purpose and special-purpose parametrizations, and linear scaling approaches.},
 authors = {['Walter Thiel']},
 journal = {Elsevier},
 title = {Chapter 21 - Semiempirical quantum-chemical methods in computational chemistry},
 year = {2005}
}

@Filtered Article{7b1c1d33-221c-45af-a4e1-74292f34eca9,
 abstract = {E-learning systems output a huge quantity of data on a learning process. However, it takes a lot of specialist human resources to manually process these data and generate an assessment report. Additionally, for formative assessment, the report should state the attainment level of the learning goals defined by the instructor. This paper describes the use of the granular linguistic model of a phenomenon (GLMP) to model the assessment of the learning process and implement the automated generation of an assessment report. GLMP is based on fuzzy logic and the computational theory of perceptions. This technique is useful for implementing complex assessment criteria using inference systems based on linguistic rules. Apart from the grade, the model also generates a detailed natural language progress report on the achieved proficiency level, based exclusively on the objective data gathered from correct and incorrect responses. This is illustrated by applying the model to the assessment of Dijkstra’s algorithm learning using a visual simulation-based graph algorithm learning environment, called GRAPHs.},
 authors = {['M. Gloria Sánchez-Torrubia', 'Carmen Torres-Blanc', 'Gracian Trivino']},
 journal = {Expert Systems with Applications},
 keywords = {['Automatic learning assessment', 'Computing with words and perceptions', 'Granular linguistic model of a phenomenon']},
 title = {An approach to automatic learning assessment based on the computational theory of perceptions},
 year = {2012}
}

@Filtered Article{7be28b13-5031-494d-81b3-8a4b2f8cfa62,
 abstract = {We study the fundamental problem of distributed network formation among mobile agents of limited computational power that aim to achieve energy balance by wirelessly transmitting and receiving energy in a peer-to-peer manner. Specifically, we design simple distributed protocols consisting of a small number of states and interaction rules for the formation of arbitrary and k-ary tree networks. Furthermore, we evaluate (theoretically and also using computer simulations) a plethora of energy redistribution protocols that exploit different levels of knowledge in order to achieve desired energy distributions among the agents which require that every agent has exactly or at least twice the energy of the agents of higher depth, according to the structure of the network. Our study shows that without using any knowledge about the network structure, such energy distributions cannot be achieved in a timely manner, meaning that there might be high energy loss during the redistribution process. On the other hand, only a few extra bits of information seem to be enough to guarantee quick convergence to energy distributions that satisfy particular properties, yielding low energy loss.},
 authors = {['Adelina Madhja', 'Sotiris Nikoletseas', 'Alexandros A. Voudouris']},
 journal = {Computer Networks},
 keywords = {['Wireless power transfer', 'Tree network formation', 'Energy balance']},
 title = {Energy-aware tree network formation among computationally weak nodes},
 year = {2020}
}

@Filtered Article{7c782969-a9c3-4f55-804b-5b472c0da63b,
 abstract = {Regarding the massive sustainability challenge mankind is currently facing, there is an indisputable need to implement sustainability as the key reference point into higher engineering education in order to prepare the stakeholders of tomorrow. This requires networked thinking on the part of the learner and increases the learning goals’ complexity dramatically. The actual achieved learning outcomes are often evaluated by assessing factual knowledge in higher education. However, it has been shown many times that students choose the examination format for orientation when studying. Thus, the authors propose a gamified summative assessment approach that requires networked thinking to direct students’ learning efforts towards broad competency building. In a study with 25 students of a master engineering course, the effects of a gamified examination design are investigated.},
 authors = {['Mustafa Severengiz', 'Ina Roeder', 'Kristina Schindler', 'Günther Seliger']},
 journal = {Procedia Manufacturing},
 keywords = {['summative assessment', 'gamification', 'higher education', 'engineering education']},
 title = {Influence of Gaming Elements on Summative Assessment in Engineering Education for Sustainable Manufacturing},
 year = {2018}
}

@Filtered Article{7c91c745-9f8d-4b45-b95a-febdb0a6e8bb,
 authors = {['H.J. Eysenck']},
 journal = {Personality and Individual Differences},
 title = {Thinking clearly about psychology volume 2: Personality and psychopathology: William M. Grove and Dante Cicchetti: Minneapolis: University of Minnesota Press (1991). pp. v–vi, 3–467. Cloth, ISBN 0-8166-1892-5 v. 2.$45.00.},
 year = {1992}
}

@Filtered Article{7cbc191b-9828-4ccb-825d-8ef5598e2f10,
 abstract = {Quantum versions of random walks on the line and the cycle show a quadratic improvement over classical random walks in their spreading rates and mixing times, respectively. Non-unitary quantum walks can provide a useful optimisation of these properties, producing a more uniform distribution on the line, and faster mixing times on the cycle. We investigate the interplay between quantum and random dynamics by comparing the resources required, and examining numerically how the level of quantum correlations varies during the walk. We show numerically that the optimal non-unitary quantum walk proceeds such that the quantum correlations are nearly all removed at the point of the final measurement. This requires only O(logT) random bits for a quantum walk of T steps.},
 authors = {['Viv Kendon', 'Olivier Maloyer']},
 journal = {Theoretical Computer Science},
 keywords = {['Quantum computing', 'Quantum walks', 'Quantum algorithms']},
 title = {Optimal computation with non-unitary quantum walks},
 year = {2008}
}

@Filtered Article{7cbf01a6-9cd3-4502-8aa7-aa19db7cdc74,
 abstract = {In a rapidly changing and diverse world, the ability to reason about conflicting perspectives is critical for effective communication, collaboration, and critical thinking. The current pre-registered experiments with children ages 7 to 11 years investigated the developmental foundations of this ability through a novel social reasoning paradigm and a computational approach. In the inference task, children were asked to figure out what happened based on whether two speakers agreed or disagreed in their interpretation. In the prediction task, children were provided information about what happened and asked to predict whether two speakers will agree or disagree. Together, these experiments assessed children's understanding that disagreement often results from ambiguity about what happened, and that ambiguity about what happened is often predictive of disagreement. Experiment 1 (N = 52) showed that children are more likely to infer that an ambiguous utterance occurred after learning that people disagreed (versus agreed) about what happened and found that these inferences become stronger with age. Experiment 2 (N = 110) similarly found age-related change in children's inferences and also showed that children could reason in the forward direction, predicting that an ambiguous utterance would lead to disagreement. A computational model indicated that although children's ability to predict when disagreements might arise may be critical for making the reverse inferences, it did not fully account for age-related change.},
 authors = {['Jamie Amemiya', 'Gail D. Heyman', 'Tobias Gerstenberg']},
 journal = {Cognition},
 keywords = {['Disagreement', 'Inference', 'Prediction', 'Theory of mind', 'Ambiguous speech']},
 title = {Children use disagreement to infer what happened},
 year = {2024}
}

@Filtered Article{7ce1e2c2-7406-416a-a153-779949d7fe65,
 abstract = {For companies to stand out in increasingly competitive, dynamic and global markets, they must have customer satisfaction goals, create value through their processes, products and services and also aim for innovation. In this context, computer sciences combined with engineering processes constitutes a powerful way for companies to be able to improve process management, to interact with such markets in an efficient and effective way. The main objective of this article is to use Arena simulation software, to quantitatively predict the impact of improvements applied in metal surface treatment processes, based on tools to support Lean thinking. A case study in a Portuguese company in the metalworking sector is presented, in which it is verified that the proposed improvements in terms of the factory layout and resource management, suggested by the comparison between simulations of the current state of the company and the improved one, streamline the processes of finishing in metals, namely zinc coating and lacquering which prevent the occurrence of oxidation and the consequent corrosion of the base metals, by adding other metals and materials to their surface, which adhere and protect it. Through the results obtained, it is concluded that the reduction of waiting times and transport of stocks without production and of work-in-progress, as well as the increase of the productive capacity, make the company more able to guarantee the satisfaction of the requirements of its customers and improve its positioning in the market compared to its competitors.},
 authors = {['A.S.M.E. Dias', 'R.M.G. Antunes', 'A. Abreu', 'V. Anes', 'H.V.G. Navas', 'T. Morgado', 'J.M.F. Calado']},
 journal = {Procedia Computer Science},
 keywords = {['Process management', 'Arena software', 'Lean tools', 'Case study', 'Metal surface treatments']},
 title = {Utilization of the Arena simulation software and Lean improvements in the management of metal surface treatment processes},
 year = {2022}
}

@Filtered Article{7d3f9972-e167-4179-82d3-28fa2bd0b66f,
 abstract = {When products come to the end of their lives, the materials they contain are still there. Repurposing, repair or recycling can return them to active use, creating a technological cycle that, in some ways, parallels the carbon, nitrogen and hydrological cycles of the biosphere. In developed nations they lost urgency as the cost of materials fell and that of labour rose, making it cheaper to make new products than to fix old ones, leading to a materials economy that is largely linear, characterized by the sequence “take – make – use – dispose”. Increasing population and affluence, and the limited capacity for the planet to provide resources and absorb waste direct thinking towards a more circular way of using materials. Governments have sought to reduce waste by imposing take-back regulations, setting mandatory recycling targets and requiring minimum service lives. These, and the efficiency movement – eco-efficiency, material-efficiency, energy efficiency – seek to allow business as usual with reduced drain on natural resources without any real change of behaviour. The ‘circularity’ concept is a way of thinking that looks not just for efficiencies but also for new ways to provide functions. The idea of deploying rather than consuming materials, of using them not once but many times has economic as well as environmental appeal. This chapter examines the background, the successes, the difficulties, and the ultimate limits of implementing a circular materials economy.},
 authors = {['Michael F. Ashby']},
 journal = {Butterworth-Heinemann},
 keywords = {['Circularity', 'Material efficiency', 'Linear materials economy', 'Circular materials economy', 'Reuse', 'Repair', 'Recycling', 'Take-back legislation', 'Recycling targets', 'Increased product life', 'Urban mining', 'Business models', 'Measuring circularity', 'Modelling circularity', 'Limits to circularity']},
 title = {Chapter 10 - Circular Materials Economics},
 year = {2024}
}

@Filtered Article{7daf85fc-03df-4b8b-b892-e372f04179cb,
 abstract = {Prospection refers to thinking about the future, a capacity that has become the subject of increasing research in recent years. Here we first distinguish basic prospection, such as associative learning, from more complex prospection commonly observed in humans, such as episodic foresight, the ability to imagine diverse future situations and organize current actions accordingly. We review recent studies on complex prospection in various contexts, such as decision-making, planning, deliberate practice, information gathering, and social coordination. Prospection appears to play many important roles in human survival and reproduction. Foreseeing threats and opportunities before they arise, for instance, drives attempts at avoiding future harm and obtaining future benefits, and recognizing the future utility of a solution turns it into an innovation, motivating refinement and dissemination. Although we do not know about the original contexts in which complex prospection evolved, it is increasingly clear through research on the emergence of these capacities in childhood and on related disorders in various clinical conditions, that limitations in prospection can have profound functional consequences.},
 authors = {['T Suddendorf', 'A Bulley', 'B Miloyan']},
 journal = {Current Opinion in Behavioral Sciences},
 title = {Prospection and natural selection},
 year = {2018}
}

@Filtered Article{7db79649-d5d3-4b37-97f8-3a50a93f1357,
 abstract = {A major goal of education is to help students become effective problem solvers, that is, people who can generate useful and original solutions when they are confronted with problems they have never seen before. This article covers definitions of problem solving and reasoning, types of problems, cognitive processes and types of knowledge in problem solving, rigidity in thinking, problem-solving transfer, the distinction between productive and reproductive thinking, the nature of insight, problem space and search processes, and problem solving in realistic situations.},
 authors = {['R.E. Mayer']},
 journal = {Elsevier},
 keywords = {['Convergent thinking', 'Creativity', 'Deductive reasoning', 'Directed thinking', 'Divergent thinking', 'Einstellung', 'Everyday thinking', 'Expert problem solving', 'Functional fixedness', 'Ill-defined problem', 'Inductive reasoning', 'Insight', 'Means-ends analysis', 'Nonroutine problem', 'Problem solving', 'Problem space', 'Productive thinking', 'Reasoning', 'Reproductive thinking', 'Routine problem', 'Thinking', 'Transfer', 'Well-defined problem']},
 title = {Problem Solving and Reasoning},
 year = {2010}
}

@Filtered Article{7e445c30-5a7f-4563-8366-729543231846,
 abstract = {Millions of individuals die by suicide each year, and many more suffer from severe depression. Furthermore, these deaths harm education, the economy, and healthcare worldwide. An individual’s persistent feelings and activities, which include sleep disorders such as insomnia, sleeping overtime, or spending the majority of time lying down; pessimistic thinking about the future; and thoughts of committing suicide, help uncover the cause of depression. Several attempts have been made to prevent these losses and deaths. Our study proposes a simple fuzzy inference model that accurately predicts depression levels based on emotions and activities—even with incomplete data and uncertainties—and enhances mental health prediction, bridging theory and practice effectively. Psychologists and professors collaborated to create a survey to collect data for this study. The experiment was conducted using a Google survey form. This method effectively captures the ambiguity and imprecision in depression evaluation by combining linguistic elements of psychological traits. Using the Pearson correlation and R-squared methods, 15 features were chosen from 30 features, followed by five membership functions (poor, mediocre, average, decent, and good) and fuzzy rules to evaluate and create accurate forecasts of depression severity. Our proposed architecture can correctly classify depression levels based on human activities and feelings with 94% accuracy using a less sophisticated rules dictionary than previous pre-trained or hybrid models. Fuzzy logic performs better here by accurately categorizing ambiguous human emotional inputs into distinct degrees.},
 authors = {['Urmi Saha', 'Syed Mohammod {Minhaz Hossain}', 'Iqbal H. Sarker']},
 journal = {Data Science and Management},
 keywords = {['Depression Level Prediction', 'Human Activities', 'Human Feelings', 'Pearson Correlation Method', 'R-Squared Method', 'Uncertainty', 'Fuzzy Rules']},
 title = {Predicting depression level based on human activities and feelings: A fuzzy logic-based analysis},
 year = {2024}
}

@Filtered Article{7e6b139e-0932-4980-9416-3ac1796db15b,
 abstract = {The evolution of communication networks shows a clear shift of focus from just improving the communications aspects to enabling new important services, from Industry 4.0 to automated driving, virtual/augmented reality, the Internet of Things (IoT), and so on. This trend is evident in the roadmap planned for the deployment of the fifth-generation (5G) communication networks. This ambitious goal requires a paradigm shift toward a vision that looks at communication, computation, and caching (3C) resources as three components of a single holistic system. The further step is to bring these 3C resources closer to the mobile user, at the edge of the network, to enable very low latency and high reliability services. The scope of this chapter is to show that signal processing techniques can play a key role in this new vision. In particular, we motivate the joint optimization of 3C resources. Then we show how graph-based representations can play a key role in building effective learning methods and devising innovative resource allocation techniques.},
 authors = {['Sergio Barbarossa', 'Stefania Sardellitti', 'Elena Ceci', 'Mattia Merluzzi']},
 journal = {Academic Press},
 keywords = {['5G networks', 'Wireless communications', 'Graph-based learning']},
 title = {Chapter 16 - The Edge Cloud: A Holistic View of Communication, Computation, and Caching},
 year = {2018}
}

@Filtered Article{7eac134f-1d13-4e6d-9afe-5edf3531a842,
 abstract = {Online recognizing defects of the strip-steel surface on resource-constrained embedded devices is a difficult problem. The traditional deep learning model with deep network layers and large parameter counts cannot balance the efficiency and the accuracy. This paper proposes a specialized lightweight deep learning detection model (LSDNet) for strip-steel surface defects. Moreover, LSDNet effectively classifies and recognizes these defects with fewer model parameters. LSDNet adopts Mobilenetv2 as the basic framework and constructs a new feature extraction module. The SPD-Conv module enhances the feature learning capacity for small targets and reduces model redundancy, while the ECANet module improves feature extraction capabilities. Additionally, the parameter-free attention mechanism (SimAM) is incorporated after the initial and final convolutional layers to boost recognition accuracy. Computational efficiency is achieved by substituting fully connected layers with a spatially invariant global average pooling layer, thereby preserving essential depth information. Dropout layers are deployed to enhance generalization, and dynamic learning rate adjustments optimize the training process. Experimental results demonstrate that the proposed LSDNet achieves a classification accuracy of 98.60 %, an F1−score of 98.57 %, with only 0.76 million parameters and 0.095 billion FLOPs for strip-steel surface defects. Compared to Mobilenetv2, LSDNet reduces the parameter count by 2.749 million and improves the classification accuracy by 1.69 %. This method performs better than other classification models in balancing recognition efficiency and accuracy.},
 authors = {['Xuhui Xia', 'Jiale Guo', 'Zelin Zhang', 'Lei Wang', 'Yuyao Guo']},
 journal = {Optics and Lasers in Engineering},
 keywords = {['Cold-rolled strip steel', 'Defect classification', 'Lightweight network', 'Feature extraction']},
 title = {LSDNet: Lightweight strip-steel surface defect detection networks for edge device environment},
 year = {2025}
}

@Filtered Article{7eead5db-370e-4845-8239-cfff40b7c01b,
 abstract = {Summary
Adult neurogenesis in the hippocampus leads to the incorporation of thousands of new granule cells into the dentate gyrus every month, but its function remains unclear. Here, we present computational evidence that indicates that adult neurogenesis may make three separate but related contributions to memory formation. First, immature neurons introduce a degree of similarity to memories learned at the same time, a process we refer to as pattern integration. Second, the extended maturation and change in excitability of these neurons make this added similarity a time-dependent effect, supporting the possibility that temporal information is included in new hippocampal memories. Finally, our model suggests that the experience-dependent addition of neurons results in a dentate gyrus network well suited for encoding new memories in familiar contexts while treating novel contexts differently. Taken together, these results indicate that new granule cells may affect hippocampal function in several unique and previously unpredicted ways.},
 authors = {['James B. Aimone', 'Janet Wiles', 'Fred H. Gage']},
 journal = {Neuron},
 keywords = {['SYSNEURO', 'MOLNEURO', 'STEMCELL']},
 title = {Computational Influence of Adult Neurogenesis on Memory Encoding},
 year = {2009}
}

@Filtered Article{7f5b7766-533f-42e3-9a0c-941cc0225a11,
 abstract = {Educational games rapidly integrate entertainment technology and learning, engaging individuals in dynamic educational experiences. These games incorporate multimedia content to encourage critical thinking, problem-solving and information retention. Educational games employ immersive technology such as virtual and augmented reality to transfer individuals to simulated worlds, hence improving learning. Furthermore, artificial intelligence (AI) technologies optimize educational experiences by adjusting information to individual learning styles, providing focused feedback as well as encouraging a more effective and entertaining learning technology. The integration of educational games with immersive and AI technology provides great potential for transforming how individuals acquire and apply information sharing. This research determined the creation of significant educational applications that are personalized and adaptive through the use of image, emotional recognition and speech, intelligent agents that replicate the effects of an individual opponent and control over the complexities of game levels along with information. The study evaluated the different tools that educators and learners could utilize to develop immersive and artificial intelligence-based instructional games without a requirement for programming knowledge. The study demonstrates that immersive technology and AI technology could represent beneficial resources for creating educational video games and entertainment technology. The research highlights the novel possibilities of stochastic swing golf optimization (SSGOA) immersive and AI technologies providing an innovative approach to developing effective as well as attractive learning environments.},
 authors = {['Anuj Rapaka', 'S.C. Dharmadhikari', 'Kishori Kasat', 'Chinnem Rama Mohan', 'Kuldeep Chouhan', 'Manu Gupta']},
 journal = {Entertainment Computing},
 keywords = {['Artificial intelligence (AI)', 'Educational games', 'Learning styles', 'Stochastic swing golf optimization (SSGOA)']},
 title = {Revolutionizing learning − A journey into educational games with immersive and AI technologies},
 year = {2025}
}

@Filtered Article{7f769f78-8e8a-440e-adf0-00b4cc8bd617,
 abstract = {We use computers to study economics, but few people realize that we can use economics to study and design computational systems. The reason is that computer networks can be regarded as a community of processes that in their interactions, strategies and lack of perfect knowledge face the same issues as people in markets. This paper describes how computers have evolved to a point where economics approaches are useful for designing them and understanding their dynamics. Examples are given of existing computer systems that use market mechanisms and of novel phenomena, such as clustered volatility, that we uncovered when studying their evolution.},
 authors = {['Bernardo A. Huberman']},
 journal = {Journal of Economic Dynamics and Control},
 title = {Computation as economics},
 year = {1998}
}

@Filtered Article{7f95941b-4b21-4bb7-8d14-3ac074347dd3,
 abstract = {Linguistic self-representation and identity construction on social media have attracted much scholarly attention. However, relevant studies tend to overlook the temporal dimension of social media, potentially systematic patterning of linguistic behavior across time, and the attendant implications of such a temporal perspective on identity. Combining an automated lexical tool (LIWC) and the Box–Jenkins method of statistical time series analysis, this paper shows how the ‘modelability’ of linguistic choices across time can be interpreted as signatures of identity construction and complement existing frameworks for identity analysis. Two levels of modelability are discussed—the availability of a well-fitting time series model as evidence of temporal patterning, and specific parameters of that model interpreted in context. These are demonstrated with a case study of the construction of ‘amateur expertise’ over 109 consecutive makeup tutorial videos on the popular YouTube channel ‘Nikkiestutorials’. Results show that the linguistic display of ‘analytical thinking’ reflects a strategy of ‘short term momentum’, the display of ‘clout’ and ‘authenticity’ a strategy of ‘short term restoration’, while the display of ‘emotional tone’ fluctuates randomly across time. The approach is further discussed in terms of its general principles and potential applications in other contexts of identity and related research.},
 authors = {['Dennis Tay']},
 journal = {Journal of Pragmatics},
 keywords = {['Identity construction', 'Social media', 'LIWC', 'Modelability', 'ARIMA', 'Time series analysis']},
 title = {Modelability across time as a signature of identity construction on YouTube},
 year = {2021}
}

@Filtered Article{7fe0889c-675a-4c66-8260-96b4a60bfb86,
 abstract = {The cutting pattern plays a major role for the design of structural membranes, since it influences both their aesthetical appearance and structural behavior. A novel approach towards cutting pattern generation is the so-called Variation of Reference Strategy (VaReS) [1], which minimizes the total potential energy arising from the motion of a planar cutting pattern to its corresponding three-dimensional shape. With non-uniform rational B-Splines (NURBS) being the standard tool for geometry description in CAD, it is only consequent to use these for analysis as well. Isogeometric B-Rep Analysis (IBRA) [2] follows up on this idea and enriches the original Isogeometric Analysis (IGA), which was introduced by Hughes et al. [3], by the possibility of analysing trimmed NURBS geometries. This paper presents cutting pattern generation with the Variation of Reference Strategy in the context of IGA/IBRA. With this approach, the whole design of a membrane structure can be represented by NURBS geometries – including blueprint plans. To use the benefits of IBRA for cutting pattern generation, a NURBS-based membrane-element was developed for the VaReS routine. A developable surface serves as a benchmark example, since its analytical cutting pattern is known. Examples of double-curved geometries show the applicability and benefits of the proposed procedure for real structures.},
 authors = {['Ann-Kathrin Goldbach', 'Michael Breitenberger', 'Armin Widhammer', 'Kai-Uwe Bletzinger']},
 journal = {Procedia Engineering},
 keywords = {['Cutting pattern generation', 'Variation of Reference Strategy', 'Isogeometric Analysis', 'Isogeometric B-Rep Analysis', 'Design cycle of structural membranes']},
 title = {Computational Cutting Pattern Generation Using Isogeometric B-Rep Analysis},
 year = {2016}
}

@Filtered Article{80111f40-ebd7-4497-bd86-0e73f1ea2e35,
 abstract = {In this research, we investigated the role of multisensorial manipulation on creativity, and the influence of inspirational objects on creative outcomes. Object manipulation may support embodied cognition during a generative creative phase (emergence of motor, spatial, emotional ideas, etc.) then exploratory phase (creative fixation, development of a functional creation, etc.). Our protocol involved 136 engineering students divided into 34 groups which were provided with inspirational cubes illustrating manufacturing inventive principles or basic volumes from the Creative Mental Synthesis Task. They could manipulate these objects either in a visuo-haptic condition, or in a visuo-imaginative condition. Our results highlighted a main effect of manipulation, showing that visual-haptic condition led to higher creativity than visual-imaginative condition. We also observed several effects in favor of inspirational cubes with regard to basic volumes: significantly higher creativity, more subjective and inter-subjective facilitation behaviors, more cognitive and emotional operations. Participants also showed at an individual level a better mobilization of the multisensorial senses. Creative thinking may be stimulated when an active manipulation phase is set up before the creative production. This could contribute to improving practice for engineers, particularly for using additive manufacturing and/or during their training at school.},
 authors = {['Amandine Cimier', 'Beatrice Biancardi', 'Jérome Guegan', 'Frédéric Segonds', 'Fabrice Mantelet', 'Camille Jean', 'Claude Gazo', 'Stéphanie Buisine']},
 journal = {Journal of Creativity},
 keywords = {['Engineering', 'Manipulation', 'Embodied cognition', 'Kinesthesia', 'Creativity']},
 title = {Multisensory objects’ role on creativity},
 year = {2025}
}

@Filtered Article{804c9f05-3937-43e2-be96-98e5ca9b5cea,
 abstract = {According to the Food and Agriculture Organization of the United Nations (FAO), climate change is exponentially affecting agricultural production worldwide, with food prices expected to increase by up to 90 percent by 2030 and hunger and malnutrition rates to rise by 2050. This paper presents the development of a platform based on the Internet of Things (IoT) for monitoring urban gardens as a strategy to mitigate hunger, promote food sovereignty and circular economy in areas of food shortage. To this end, an Internet of Things (IoT) architecture is proposed and implemented that involves a social design layer that allows an effective transfer of knowledge to communities and a recommendation system based on evolutionary computation to optimize and maximize the productivity of urban orchards, and thus contribute to the 2030 agenda of the Sustainable Development Goals (SDGs). Finally, three experiments in urban gardens are shown to validate evolutionary computation and artificial intelligence models, such as multiple linear regression, genetic algorithms, ant colony algorithms and spatial estimation and inference algorithms such as the Kriging algorithm. The productivity of urban lettuce orchards is increased between 25 and 45%.},
 authors = {['Juan M. {Núñez V.}', 'Juan M. Corchado', 'Diana M. Giraldo', 'Sara Rodríguez-González', 'Fernando {De la Prieta}']},
 journal = {Internet of Things},
 keywords = {['Internet of Things', 'Bio-inspired algorithms', 'Urban orchards', 'Lettuce crops', 'Social design']},
 title = {Recommendation system using bio-inspired algorithms for urban orchards},
 year = {2024}
}

@Filtered Article{808631b9-febc-42fb-ad71-410d40605142,
 abstract = {The design of hospital spatial layouts is a critical aspect of healthcare architecture, directly influencing patient outcomes, staff efficiency, and the overall quality of care. A well-designed hospital layout is essential for ensuring smooth operations, minimizing errors, and improving both patient and staff experiences. This paper reviews the significant advances in the field, particularly focusing on the transition from traditional design methods to the integration of computational techniques and machine learning (ML) in hospital layout planning. Despite these technological advancements, there remains a notable gap in the full adoption and optimization of these methods to effectively address the inherent complexities of healthcare environments. This review identifies that while computational methods and machine learning-driven approaches have brought precision and innovation to hospital design, the challenge lies in balancing these technologies with the expertise and insights of human designers. Moreover, the need for interdisciplinary collaboration between architects, healthcare professionals, and engineers is emphasized as crucial for the successful implementation of advanced design strategies. Insights from this review highlight the potential of future research to bridge the existing gaps, proposing directions for the continuous integration of technology in hospital layout design.},
 authors = {['Aysegul Ozlem {Bayraktar Sari}', 'Wassim Jabi']},
 journal = {Journal of Building Engineering},
 keywords = {['Architectural spatial layout design', 'Hospital spatial layout design', 'Computational design', 'Facility layout planning', 'Machine learning (ML) driven layout design', 'Systematic review']},
 title = {Architectural spatial layout design for hospitals: A review},
 year = {2024}
}

@Filtered Article{8097d0bb-ec18-474b-a47f-b3a33d1fe363,
 abstract = {Printed Circuit Heat Exchangers are compact and efficient heat exchangers, widely used in nuclear engineering, very high-temperature reactors, and aerospace systems. This study investigates the heat transfer performance of a heat exchanger with anisotropic thermal conductivity, such as fiber reinforced composites. Numerical simulations were conducted to examine the synergistic effect of three-dimensional thermal resistance on heat exchanger performance. The most significant impact on performance is the z-direction thermal resistance, followed by the y-direction, while the x-direction has the least impact. Contrary to traditional design thinking, increasing the overall heat exchanger thermal resistance under the same thermal resistance ratio improves heat transfer efficiency at the studied conditions. The results suggest that it is necessary to design the lowest thermal conductivity direction as the z-direction and increase the y-direction thermal conductivity to enhance heat exchanger performance. In the numerical investigation presented in this study, the efficiency of the heat exchanger was improved by approximately 23 % under specific operating conditions by adjusting the thermal conductivity of anisotropic materials to control the thermal resistance in the x, y and z directions. It is evident that the manipulation of anisotropic material properties has a substantial influence on the performance of heat exchangers.},
 authors = {['Libo Li', 'Jiyuan Bi', 'Jingkai Ma', 'Xiaoxu Zhang', 'Qiuwang Wang', 'Ting Ma']},
 journal = {International Journal of Heat and Fluid Flow},
 keywords = {['Printed circuit heat exchanger', 'Anisotropic thermal conductivity', 'Numerical simulation', 'Thermal resistance', 'Heat exchanger efficiency']},
 title = {Numerical study on heat transfer performance of printed circuit heat exchanger with anisotropic thermal conductivity},
 year = {2024}
}

@Filtered Article{80faf5fa-b927-4fca-8f23-71c189ccb4d2,
 abstract = {Several gaps exist in the literature on coding. First, little exploration has focused on early elementary school students. In addition, close description of the overall context of coding tasks at this level is rare. Further, there is a need for both teacher and student voices around coding experiences to be heard. Moreover, a task engagement framework has not been used to evaluate the process or outcomes of early elementary coding tasks. Therefore, an exploratory holistic case study design was used to investigate student and teacher processes and outcomes of coding lessons in order to fill gaps in the literature. In this study, forty-six 2nd grade students, two teachers, and four researchers completed two one-week units on basic coding. Multiple descriptive and numeric data sources were employed to describe the process and outcomes of learning coding. Conclusions include: (1) teachers should start learning about coding first with short awareness sessions and then move to their own classrooms with knowledge brokers and other forms of assistance; (2) a focus on content and process, including problem-solving, is effective for coding with young children; (3) there can be a high level of engagement for teachers and students with the use of robots and welldesigned, age-appropriate coding tasks, and; (4) multiple data sources and the inclusion of both teacher and student data are essential in exploring coding in classrooms.},
 authors = {['Joy Egbert', 'Seyed Abdollah Shahrokni', 'Reima Abobaker', 'Nataliia Borysenko']},
 journal = {Computers & Education},
 keywords = {['Elementary education', 'Robotics', 'Coding', 'Teacher learning', 'Computational thinking']},
 title = {“It's a chance to make mistakes”: Processes and outcomes of coding in 2nd grade classrooms},
 year = {2021}
}

@Filtered Article{8159f87e-da60-4fc0-8bb5-5b41cfa58537,
 abstract = {The great improvement of algorithms and computing hardware in the last few years must be ranked as one of the most important turning points in the history of multiphase flow research. After a brief review of some of this recent progress, it is pointed out that, besides its application to solving actual problems, computational physics plays other key roles: (1) As a tool to develop and understand basic physics and as a guide toward asking more penetrating questions; (2) As an aid in closing the averaged equations; (3) As a means to learn to compute better. Roadblocks toward greater effectiveness are the huge complexity of many of the necessary computational tasks but also, at a more practical level, the transmission of “computational knowledge” from one researcher to another, much in the same way as experimentalists can rely on readily available equipment (e.g., lasers, etc.), without having to build each item themselves. The solution to this problem will require a cultural shift––from a “cottage industry” to a “big science” mentality––which can be aided by a different attitude on the part of the funding agencies. Great synergism can be achieved by a closer integration of the multiphase computational physics enterprise with both Applied Mathematics and Computer Science.},
 authors = {['Andrea Prosperetti', 'Grétar Tryggvason']},
 journal = {International Journal of Multiphase Flow},
 keywords = {['Computational multiphase flow', 'Direct numerical simulations', 'Numerical methods']},
 title = {Appendix 3: Report of study group on computational physics},
 year = {2003}
}

@Filtered Article{81c94da0-0067-4ce4-92fb-3fa829ed1f98,
 abstract = {Medical savings accounts (MSAs) and similar approaches based on flowing reimbursements through individuals/consumers rather than providers are unsuited for systems with universal coverage. Data from Manitoba, Canada reveal that, because expenditures for physician and hospital services are highly skewed in all age groups, MSAs would substantially increase both public expenditures and out-of-pocket costs for the most ill. The empirical distribution of health expenditures limits the potential impact of many current ‘demand-based’ approaches to cost control. Because most of the population is relatively healthy and uses few hospital and physician services, inducing the general population to spend less will not yield substantial savings.},
 authors = {['Raisa B Deber', 'Evelyn L Forget', 'Leslie L Roos']},
 journal = {Health Policy},
 keywords = {['Medical savings accounts', 'Canada', 'Health care financing', 'Distribution of expenditures']},
 title = {Medical savings accounts in a universal system: wishful thinking meets evidence},
 year = {2004}
}

@Filtered Article{81e88bb3-827a-4bfd-a370-1b93ca5d9994,
 abstract = {For creativity to be computed, it is paramount to understand the cognitive processes involved, which have been elucidated by either surveying creative people or discovering regions of the human brain that activate during creative endeavors. From this scattering, the author proposes a holistic framework to describe them and their interaction. Hence, creativity can be regarded as a meta process which coordinates autonomous cognitive processes such as planning or divergent thinking. To represent the interplay of cognitive processes around creativity, models are developed in the Agent Unified Modeling Language (AUML). Then, the execution of each process is delegated to autonomous agents and a global coordination protocol is devised. The implementation of the MAS is done on the JADE platform. Two modules of the resultant system are exemplified: opus planning and divergent exploration. The coordination protocol is also presented. The domain in which the software system is tested is the creation of musical pieces.},
 authors = {['Omar López-Ortega']},
 journal = {Expert Systems with Applications},
 keywords = {['Computer-assisted creativity', 'Cognitive processes', 'Agent-oriented programming', 'Recursive systems']},
 title = {Computer-assisted creativity: Emulation of cognitive processes on a multi-agent system},
 year = {2013}
}

@Filtered Article{820842d3-398f-43c5-8cff-492dded73f2d,
 abstract = {A teaching approach of programming experiment courses with a new education process is proposed in this study to improve the sustainability of engineering education and extend the accessibility of lab experiments across experiment courses, with the goal of encouraging innovative and scientific thinking among students. The Millikan Oil-Drop experiment combined with Java object-oriented programming is demonstrated as a case study to validate the feasibility and advantages of this teaching approach. The new education process of the experiment course is designed based on Jean Piaget's cognitive development theory, which has high practical potential for popularization among tertiary institutions without additional cost. Additionally, this work discussed the relevance of the general criterion of the Accreditation Board for Engineering and Technology on student outcomes to educational accreditation, further indicating that introducing programming into experiment education can improve students' all-round ability and strengthen the triangular relationship among the three main subjects in tertiary education, namely, students, faculty, and higher educational institutions. This study may serve as an educational guide for teachers and tertiary institutions to pursue innovative and sustainable education.},
 authors = {['Yizheng Li', 'Guandong Su', 'Haocheng Pan', 'Chengwei Tan', 'Gongsheng Li']},
 journal = {Journal of Cleaner Production},
 keywords = {['Laboratory instruction', 'Java object-oriented programming', 'Innovative and sustainable education', 'Student-centered learning']},
 title = {Programming experiment course for innovative and sustainable education: A case study of Java for Millikan Oil-Drop experiment},
 year = {2024}
}

@Filtered Article{82192fde-d9f4-44b0-ad91-c5014a453e8e,
 abstract = {The goal of computational neuroscience is to explain in computational terms how brains generate behaviors. Computational models of the brain explore how populations of highly interconnected neurons are formed during development and how they come to represent, process, store, act upon, and become altered by, information present in the environment. Techniques from computer science and mathematics are used to simulate and analyze these computational models and provide links between the widely ranging levels of investigation, from the molecular to the systems levels. Computational neuroscience is a relatively young discipline that is growing rapidly. Most of the models that have been developed thus far have been aimed at interpreting experimental data and providing a conceptual framework for the dynamic properties of neural systems. A more comprehensive theory of brain function should arise as we gain a broader understanding of the computational resources of nervous systems at all levels of organization.},
 authors = {['T.J. Sejnowski']},
 journal = {Pergamon},
 title = {Computational Neuroscience},
 year = {2001}
}

@Filtered Article{826a52bf-5fd2-490d-8a83-b616cb927569,
 authors = {['Andrzej K Konopka']},
 journal = {Computational Biology and Chemistry},
 title = {Selected dreams and nightmares about computational biology},
 year = {2003}
}

@Filtered Article{82c9530a-e212-4958-b33d-f2e979f0a76d,
 abstract = {The use of model membranes is currently part of the daily workflow for many biochemical and biophysical disciplines. These membranes are used to analyze the behavior of small substances, to simulate transport processes, to study the structure of macromolecules or for illustrative purposes. But, how can these membrane structures be generated? This mini review discusses a number of ways to obtain these structures. First, the problem will be formulated as the Membrane Packing Problem. It will be shown that the theoretical problem of placing proteins and lipids onto a membrane area differ significantly. Thus, two sub-problems will be defined and discussed. Then, different – partly historical – membrane modeling methods will be introduced. And finally, membrane modeling tools will be evaluated which are able to semi-automatically generate these model membranes and thus, drastically accelerate and simplify the membrane generation process. The mini review concludes with advice about which tool is appropriate for which application case.},
 authors = {['Björn Sommer']},
 journal = {Computational and Structural Biotechnology Journal},
 title = {MEMBRANE PACKING PROBLEMS: A SHORT REVIEW ON COMPUTATIONAL MEMBRANE MODELING METHODS AND TOOLS},
 year = {2013}
}

@Filtered Article{8300c24b-3551-48e9-a5a6-303162bff116,
 abstract = {How to improve pre-service teachers’ argumentation skills has been receiving more and more attention from teacher educators. Visual cognitive tool refers to tools which users can learn with and creatively use to construct knowledge online. Current research revealed that it could help to improve learners’ higher-order thinking skills. This experimental study aimed to investigate the effect of two kinds of cognitive tools, the text-based online visual cognitive tool and the visual concept map, on improving the pre-service teachers’ skills on constructing and evaluating arguments. Post-test argumentation measurement scores and attitude questionnaire showed that the text-based cognitive tool was more effective than the concept map on improving pre-service teachers’ argumentation skills. However, the concept map was useful for externalizing the pre-service teachers’ thinking process as well as collaborative learning. This study also found that the pre-service teachers with teaching experience were inferior to the ones without any teaching experience in the ability on constructing arguments.},
 authors = {['Guo Su', 'Taotao Long']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Argumentation skills', 'cognitive tools', 'pre-service teachers']},
 title = {Is the Text-Based Cognitive Tool More Effective Than the Concept Map on Improving the Pre-Service Teachers’ Argumentation Skills?},
 year = {2021}
}

@Filtered Article{8324f438-e9ac-44ba-8877-50faefb48a58,
 abstract = {Schools are increasingly integrating character education to facilitate improved moral thinking and pro social behavior among students. An effective method for delivering character education is problem solving moral and social situations represented visually as animated vignettes. However, schools are rarely able to use animated vignettes since existing tools do not allow them to be easily created and having them created externally is overly expensive. In this paper, we describe the design, use, and evaluation of a computational tool that enables students to construct their own animated vignettes. By building, sharing, and responding to vignettes, students become engaged in problem solving moral and social situations. Evaluations showed that users are able to build meaningful vignettes, our tool is easy to learn and fun to use, and our tool's multimedia features are often used and well-liked. Educators can download and use our tool while researchers can draw upon our design rationale and lessons learned when building similar tools.},
 authors = {['Brian P. Bailey', 'Sharon Y. Tettegah', 'Terry J. Bradley']},
 journal = {Interacting with Computers},
 keywords = {['Animation', 'Character education', 'Multimedia', 'Narratives', 'Vignettes']},
 title = {Clover: Connecting technology and character education using personally-constructed animated vignettes},
 year = {2006}
}

@Filtered Article{833e98eb-48c5-4239-ae2f-f25f31fb1d07,
 abstract = {This paper proposes a broader framework for understanding creativity by distinguishing different levels of creativity, namely personal and social-cultural creativity, and their interaction. Within this framework, the possible role that computer-aided design systems can play is discussed by analyzing the procedure of rule formation and the phenomena of seeing emergent subshapes.},
 authors = {['Yu-Tung Liu']},
 journal = {Design Studies},
 keywords = {['creativity', 'design cognition', 'computer-aided design', 'problem-solving', 'artificial intelligence']},
 title = {Creativity or novelty?: Cognitive-computational versus social-cultural},
 year = {2000}
}

@Filtered Article{835b3091-f25b-48d1-9890-912a2dfae243,
 abstract = {Publisher Summary
This chapter discusses the computational basis of learning and cognition. To deal with a continuously changing environment, living things have three choices: (1) evolve unvarying processes that usually succeed, (2) evolve genetically fixed effector, perceptual, and computational functions that are contingent on the environment, and (3) learn adaptive functions during their lifetimes. The theme of this chapter is the relation between (2) and (3): the nature of evolutionarily determined computational processes that support learning. The principal goal of this chapter has been to suggest that high-dimensional vector space computations based on empirical associations among very large numbers of components could be a close model of a fundamental computational basis of most learning in both verbal and perceptual domains. More powerful representational effects can be brought about by linear inductive combinations of the elements of very large vocabularies than has often been realized. Success of one such model to demonstrate many natural properties of language commonly assumed to be essentially more complex, nonlinear, and/or unlearned, along with evidence and argument that similar computations may serve similar roles in object recognition, are taken to reaffirm the possibility that a single underlying associational mechanism lies behind many more special and complex appearing cognitive phenomena.},
 authors = {['Thomas K Landauer']},
 journal = {Academic Press},
 title = {On the computational basis of learning and cognition: Arguments from LSA},
 year = {2002}
}

@Filtered Article{83a41a9f-b2f4-4a22-9b7a-87d74d037483,
 abstract = {Various methods have been developed for automated and semi-automated architecture generation in the computer aided ship design processes. The question remains as to how this can speed up the design process without losing the requirement elucidation intent for concept phase. This paper presents a novel approach with a software toolset to develop design and analysis approaches to early stage ship design and provide a sketching tool. This was done by enhancing the user interface and experience of the UCL Network Block Approach to achieve a “thinking sketch” in a way that is “quick” and “fluid” enough to promote inventive and creative sketching comparable to hand sketching. The UCL Network Block Approach draws on the UCL Design Building Block (DBB) approach and uses network methods applied to the synthesis of distributed ship service systems (DS3) and Computer Aided Ship Design (CASD) to expand DS3 definition in early stage ship design. The UCL originated inside-out/DBB approach to sketch driven synthesis has been made translatable to both DBB ship descriptions and ensuring early stage naval architectural “balance”. The proposed approach has been used for the first time successfully to not only carry out a rapid sketching exercise for a naval ship design but also enable quick preliminary analysis of a set of DS3 networks.},
 authors = {['M.H. Mukti', 'R.J. Pawling', 'D.J. Andrews']},
 journal = {Ocean Engineering},
 title = {Computer aided sketching in the early-stage design of complex vessels},
 year = {2024}
}

@Filtered Article{83f4a8de-7897-4a8e-a802-fa8230453595,
 abstract = {Data analytics is changing the audit environment and carries significant implications for auditing education. Both international auditing education (International Accounting Education Standards Board (IAESB), 2019a; IAESB, 2019b) and U.S.-based regulatory bodies (American Institute of Certified Public Accountants (AICPA), 2021c; AICPA & National Association of State Boards of Accountancy (NASBA), 2021) have made efforts to address the growing expectations for auditing education, citing fraud risk and going concern risk. While auditing courses have progressed to include some computerized audit software for case studies, the study of analytical procedures has been limited to the application of basic financial ratios, trend analyses and common-size financial statements. Demands for advanced analytics place most emphasis on computerized query and computational methods; however, several advanced analytical models, namely the Altman Z-score, Beneish M−score and the Sloan Accrual formula provide opportunities for greater insight on specific audit risks and do not require advanced computer-based skills. The ability to link audit procedures, specifically analytical procedures to the audit objectives of financial risk and going concern risk strengthens the rationale for introduction of these advanced models within the context of auditing education. This paper discusses the inherent value in these analytical models, links them to audit objectives, proposes the inclusion of these three analytical models as a component of auditing education, and suggests that future study be undertaken to assess implementation and student learning. In addition, we recommend future study of other analytical models that may provide further insight for auditing students.},
 authors = {['Michele S. Flint']},
 journal = {Journal of Accounting Education},
 keywords = {['Auditing education', 'Analytical procedures', 'Data analytics', 'Beneish M−score', 'Altman Z-score', 'Sloan Accrual']},
 title = {Expansion of analytical methods in auditing education},
 year = {2025}
}

@Filtered Article{843ea3c5-c241-4f72-ba0a-b38901ab4024,
 abstract = {Cognitive biases are systematic cognitive dispositions or inclinations in human thinking and reasoning that often do not comply with the tenets of logic, probability reasoning, and plausibility. These intuitive and subconscious tendencies are at the basis of human judgment, decision making, and the resulting behavior. Psychological frameworks consider biases as resulting from the use of (inappropriate) cognitive heuristics that people apply to deal with data-limitations, from information processing limitations, or from a lack of expertise. Neuro-evolutionary frameworks provide a more profound explanation of biases as originating from the inherent design characteristics of our brain as a neural network that was primarily developed to perform basic physical, perceptual and motor functions, and which also had to promote the survival of our hunter-gatherer ancestors.},
 authors = {['J.E. {(Hans) Korteling}', 'Alexander Toet']},
 journal = {Elsevier},
 keywords = {['Cognitive biases', 'Cognitive neuroscience', 'Decision making', 'Dual process theory', 'Expertise', 'Evolutionary psychology', 'Heuristics', 'Information processing capacity', 'Intuition', 'Neural networks', 'Rationality']},
 title = {Cognitive Biases},
 year = {2022}
}

@Filtered Article{844dbc88-258c-4b17-9ea5-42ce99b78bdd,
 abstract = {Abstraction is a powerful technique for speeding up planning and search. A problem that can arise in using abstraction is the generation of abstract states, called spurious states, from which the goal state is reachable in the abstract space but for which there is no corresponding state in the original space from which the goal state can be reached. Spurious states can be harmful, in practice, because they can create artificial shortcuts in the abstract space that slow down planning and search, and they can greatly increase the memory needed to store heuristic information derived from the abstract space (e.g., pattern databases). This paper analyzes the computational complexity of creating abstractions that do not contain spurious states. We define a property—the downward path preserving property (DPP)—that formally captures the notion that an abstraction does not result in spurious states. We then analyze the computational complexity of (i) testing the downward path preserving property for a given state space and abstraction and of (ii) determining whether this property is achievable at all for a given state space. The strong hardness results shown carry over to typical description languages for planning problems, including sas+ and propositional strips. On the positive side, we identify and illustrate formal conditions under which finding downward path preserving abstractions is provably tractable.},
 authors = {['Sandra Zilles', 'Robert C. Holte']},
 journal = {Artificial Intelligence},
 keywords = {['Abstraction', 'Heuristic search', 'Planning']},
 title = {The computational complexity of avoiding spurious states in state space abstraction},
 year = {2010}
}

@Filtered Article{85169c7e-7b66-4c76-a83d-ec0cae957e2b,
 abstract = {Background
There has been an increase of autistic students without intellectual disabilities (autisticWoID) placed in general education settings (Hussar et al., 2020), but there is a lack of understanding of how to best support classroom learning for these children. Previous research has pointed to subgroups of autisticWoID children who display difficulty with mathematics and reading achievement (Chen et al., 2018; Estes et al., 2011; Jones et al., 2009; Wei et al., 2015). Research has primarily focused on symptomatology and communication factors related to learning in subgroups of autistic children. The current study sought to expand upon this research by assessing the validity of these previous studies and by investigating the specific contribution of domain-general cognitive abilities to differences in these subgroups.
Method
Seventy-eight autisticWoID individuals (M = 11.34 years, SD = 2.14) completed measures of mathematics and reading achievement, IQ, working memory, inferential thinking, and Theory of Mind (ToM). A hierarchical cluster analysis was performed on the math and reading measures.
Results
The analysis revealed two unique achievement groups: one group that performed lower than expected on math and reading achievement and a second group that performed higher than expected. Groups differed significantly on IQ and working memory and were distinguished by performance on reading fluency. Groups did not differ on ToM, inferential thinking, or symptomatology.
Conclusion
These findings describe a group of autisticWoID individuals that may be more likely to experience difficulty learning, which should be accounted for in general education settings.},
 authors = {['Jennifer C. Bullen', 'Matthew C. Zajic', 'Nancy McIntyre', 'Emily Solari', 'Peter Mundy']},
 journal = {Research in Autism Spectrum Disorders},
 keywords = {['Autism spectrum disorder', 'Academic achievement', 'Hierarchical cluster analysis', 'Math achievement', 'Reading fluency']},
 title = {Patterns of math and reading achievement in children and adolescents with autism spectrum disorder},
 year = {2022}
}

@Filtered Article{85307d83-978f-4416-a243-f134594f57b7,
 abstract = {The distributive property plays a pivotal role in advancing students’ understanding of multiplication, enabling the decomposition of problems and the acquisition of new facts. However, this property of multiplication is difficult for students to understand. We used two unique data sets to explore middle school students’ use of the distributive property. Study 1 involved data from 1:1 structured interviews of students (N = 24) discussing worked examples and solving associated practice problems. We examined whether or not students used the distributive property to solve the problems and whether or not interviewers followed the recommended distributive property prompts or defaulted to more conventional methods. Despite exposure to worked examples using the distributive property and a protocol calling for attention to it, students and interviewers favored methods like PEMDAS (parentheses, exponents, multiplication, division, addition, subtraction) or long multiplication. Study 2 used a data set with middle school students’ (N = 131) item-level responses on Kirkland’s (2022; doctoral dissertation, University of Notre Dame) Brief Assessment of Mature Number Sense along with several related measures of domain-general and domain-specific skills. We extracted problems involving the distributive property for analysis. Surprisingly, there was no evidence that students’ use of the distributive property improved from sixth grade to eighth grade. However, both grade-level mathematics achievement and cognitive reflection uniquely predicted the correct use of the distributive property. Results suggest that middle school students who exhibit stronger reflective thinking tend to perform better on distributive property problems. Findings highlight cognitive reflection as a potentially important construct involved in the understanding and use of the distributive property.},
 authors = {['Sarah N. Clerjuste', 'Claire Guang', 'Dana Miller-Cotto', 'Nicole M. McNeil']},
 journal = {Journal of Experimental Child Psychology},
 keywords = {['Distributive property', 'Cognitive reflection', 'Multiplication', 'Worked examples']},
 title = {Unpacking the challenges and predictors of elementary–middle school students’ use of the distributive property},
 year = {2024}
}

@Filtered Article{857cef08-a585-43e3-871a-ab6d880e7fb9,
 abstract = {Diagnosis, being the first step in medical practice, is very crucial for clinical decision making. This paper investigates state-of-the-art computational intelligence (CI) techniques applied in the field of medical diagnosis and prognosis. The paper presents the performance of these techniques in diagnosing different diseases along with the detailed description of the data used. This paper includes basic as well as hybrid CI techniques that have been used in recent years so as to know the current trends in medical diagnosis domain. The paper presents the merits and demerits of different techniques in general as well as application specific context. This paper discusses some critical issues related to the medical diagnosis and prognosis such as uncertainties in the medical domain, problems in the medical data especially dealing with time-stamped (temporal) data, and knowledge acquisition. Moreover, this paper also discusses the features of good CI techniques in medical diagnosis. Overall, this review provides new insight for future research requirements in the medical diagnosis domain.},
 authors = {['Afzal Hussain Shahid', 'M.P. Singh']},
 journal = {Biocybernetics and Biomedical Engineering},
 keywords = {['Computational intelligence', 'Disease diagnosis', 'Prediction', 'Detection', 'Uncertainty', 'Medical data']},
 title = {Computational intelligence techniques for medical diagnosis and prognosis: Problems and current developments},
 year = {2019}
}

@Filtered Article{85c061de-6483-422b-9a9c-e083810c2838,
 authors = {['Emmanuel Daucé', 'Laurent Perrinet']},
 journal = {Journal of Physiology-Paris},
 title = {Computational neuroscience, from multiple levels to multi-level},
 year = {2010}
}

@Filtered Article{85d2d70e-b378-4c78-995f-b646e9de82b5,
 abstract = {We study the computational complexity of the physical problem of finding vacua of string theory which agree with data, such as the cosmological constant, and show that such problems are typically NP hard. In particular, we prove that in the Bousso–Polchinski model, the problem is NP complete. We discuss the issues this raises and the possibility that, even if we were to find compelling evidence that some vacuum of string theory describes our universe, we might never be able to find that vacuum explicitly. In a companion paper, we apply this point of view to the question of how early cosmology might select a vacuum.},
 authors = {['Frederik Denef', 'Michael R. Douglas']},
 journal = {Annals of Physics},
 title = {Computational complexity of the landscape: Part I},
 year = {2007}
}

@Filtered Article{86284817-9b36-453d-933e-712722aa6636,
 abstract = {Course-based undergraduate research experiences (CUREs) are an effective way to introduce students to contemporary scientific research. Research experiences have been shown to promote critical thinking, improve understanding and proper use of the scientific method, and help students learn practical skills including writing and oral communication. We aimed to improve scientific training by engaging students enrolled in an upper division elective course in a human microbiome CURE. The “Fiber Force” course is aimed at studying the effect of a wholesome high-fiber diet (40 to 50 g/day for two weeks) on the students’ gut microbiomes. Enrolled students participated in a noninvasive diet intervention, designed health surveys, tested hypotheses on the effect of a diet intervention on the gut microbiome, and analyzed their own samples (as anonymized aggregates). The course involved learning laboratory techniques (e.g., DNA extraction, PCR, and 16S sequencing) and the incorporation of computational techniques to analyze microbiome data with QIIME2 and within the R software environment. In addition, the learning objectives focused on effective student performance in writing, data analysis, and oral communication. Enrolled students showed high performance grades on writing, data analysis and oral communication assignments. Pre- and post-course surveys indicate that the students found the experience favorable, increased their interest in science, and heightened awareness of their diet habits. Fiber Force constitutes a validated case of a research experience on microbiology with the capacity to improve research training and promote healthy dietary habits.},
 authors = {['Julia Massimelli Sewall', 'Andrew Oliver', 'Kameryn Denaro', 'Alexander B. Chase', 'Claudia Weihe', 'Mi Lay', 'Jennifer B. H. Martiny', 'Katrine Whiteson']},
 journal = {Journal of Microbiology & Biology Education},
 title = {Fiber Force: A Fiber Diet Intervention in an Advanced Course-Based Undergraduate Research Experience (CURE) Course},
 year = {2020}
}

@Filtered Article{8651927e-27df-4174-bdc0-ccbd7a665f6a,
 abstract = {Summary
Single cells can perform surprisingly complex behaviors and computations, including primitive forms of learning like habituation. New work highlighted here uses mathematical modeling to show that relatively simple biochemical networks can recapitulate many features of habituation in animals.},
 authors = {['Deepa H. Rajan', 'Wallace F. Marshall']},
 journal = {Current Biology},
 title = {Cellular cognition: How single cells learn using non-neural networks},
 year = {2024}
}

@Filtered Article{8695b893-ff59-4464-b5f8-4611d009f2b4,
 abstract = {The construction industry has been found to cause damaging effects to the environment by means of waste generation, energy and water depletion and several other forms of damage to the environment. This damage has led to experts and environmentalist calling for a sustainable way of carrying out construction activities. Thus, this study addresses the challenges hindering the adoption of sustainable construction practices in the South Africa construction industry. The data used in this research were sourced from both primary and secondary sources. The primary data was collected through a questionnaire aimed at practicing construction professional in the South African construction industry. Indicative Findings from the questionnaire survey revealed that the foremost challenges faced by South African construction industry towards the adoption of sustainable construction practices is the assumption (a lazy view) of additional cost to building projects, followed by limited understanding of the benefits of sustainable construction amongst others. The study contributes to sustainability thinking in the South African construction industry; and it is recommended that strategies and actions should be pursued actively to speed up the process in creating a sustainable-oriented construction industry, which is paramount towards building a sustainable future.},
 authors = {['Clinton Aigbavboa', 'Ifije Ohiomah', 'Thulisile Zwane']},
 journal = {Energy Procedia},
 keywords = {['Climate change', 'sustainable thinking', 'sustainable construction practices', 'South Africa']},
 title = {Sustainable Construction Practices: “A Lazy View” of Construction Professionals in the South Africa Construction Industry},
 year = {2017}
}

@Filtered Article{86b6bc81-4902-4086-88cb-1233b857c376,
 authors = {['David P. Miller']},
 journal = {Journal of Mathematical Psychology},
 title = {Towards a believable theory of planning: D. E. Wilkins. Practical Planning. San Mateo, CA: Morgan Kaufmann, 1988. Pp. xii + 205. $49.95. S. L. Friedman. E. K. Scholnick, and R. R. Cocking. Blueprints for Thinking, London/New York: Cambridge Univ. Press, 1987. Pp. xv + 559. $58.50 K. Hammond. Case-Based Planning. San Diego: Academic Press, 1989. Pp. xviii + 277. $34.95},
 year = {1990}
}

@Filtered Article{86de4577-8941-4d20-8baf-047a6808a9da,
 abstract = {The high utility and usability of paper sticky notes support workflows and social dynamics of collaborative design activities and methods like affinity diagramming. In this chapter, we show how these natural collaboration activities can be blended with computational power by applying our framework Blended Interaction for a case study on affinity diagramming. Based on four domains of design, we embed our design solutions in a specific physical environment, preserve workflows, and emphasize individual and social interaction. Our proposed design solution to augment sticky notes with digital power blends the benefits of physical materials with the digital power of interactive surfaces, tangibles, and digital pens in an outstanding way. We hope that our design solutions inspire other researchers and practitioners to find innovative solutions that carefully blend real-world practices with the power of digital computing.},
 authors = {['Florian Geyer', 'Johannes Zagermann', 'Harald Reiterer']},
 journal = {Academic Press},
 keywords = {['Affinity diagramming', 'Blended interaction', 'Post-WIMP user interface', 'Interaction design', 'Tangible user interface', 'User interface design framework', 'Creativity tool']},
 title = {Chapter 6 - Physical meets digital: Blending reality and computational power with digital sticky notes},
 year = {2020}
}

@Filtered Article{878f2efb-24e8-41a1-a9f4-4a17e17eb4a2,
 authors = {['Howard E. Zimmerman']},
 journal = {Elsevier},
 title = {VIII - Development of Theory with Computation},
 year = {2005}
}

@Filtered Article{87b1288b-696b-4fa3-93e7-8ec7f7c72263,
 abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.},
 authors = {['Curtis B. Storlie', 'Laura P. Swiler', 'Jon C. Helton', 'Cedric J. Sallaberry']},
 journal = {Reliability Engineering & System Safety},
 keywords = {['Bootstrap', 'Confidence intervals', 'Meta-model', 'Nonparametric regression', 'Sensitivity analysis', 'Surrogate model', 'Uncertainty analysis', 'Variance decomposition']},
 title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
 year = {2009}
}

@Filtered Article{87b281a5-e571-42d7-8ff9-abf352cad08b,
 abstract = {The present study employed commercial computational fluid dynamics software ANSYS 2019R3 to explore the adiabatic film cooling effectiveness and the net heat flux reduction (NHFR) for the comparison of the five selected shaped holes and conventional cylindrical holes between the pressure surface, the suction surface and the leading edge. Amo4ng the shape parameters of shaped holes, the lateral divergence angle (β) and the forward divergence angle (δ) were fixed as 12° and 7° in all shaped holes structures, respectively. The others varied with different regions of the turbine vane. Results showed that different holes fit different positions of vanes. On the suction surface, laidback holes performed the worst net heat flux reduction in most blowing ratios conditions, which indicated the forward divergence angle was not conducive to the flow field and heat transfer characteristics on the suction surface. Whereas, the lateral divergence angle was beneficial to the film cooling and heat transfer characteristics. Laidback fan-shaped holes performed the best adiabatic film cooling effectiveness, but once simultaneously thinking about the heat transfer, fan-shaped holes performed better in net heat flux reduction due to less vortices at holes exit. On the leading edge, the divergence angle towards the upper wall (the lateral divergence angle of spanwise expansion holes) of vanes was not conducive to steady flow. And conical holes performed best, which indicated that coolant from holes with axial divergence angles (the lateral divergence angle in axial direction of conical holes) under the influence of mainstream impact could perform higher film cooling effectiveness and more stable flow fields. On the pressure surface, holes had a lateral divergence angle in the direction of vane height, which was conducive to increasing the coolant coverage area and improving the ability to attach to the pressure surface. Additionally, the laidback hole case was observed the lowest net heat flux reduction when the blowing ratio was less than 2, which revealed that holes that expanded only in flow direction was not conducive to film cooling and heat transfer characteristics.},
 authors = {['Yan Jiang', 'Haiwang Li', 'Runzhou Liu', 'Zhi Tao', 'Zhiyu Zhou']},
 journal = {Applied Thermal Engineering},
 keywords = {['Film cooling', 'NHFR', 'Shaped holes', 'Turbine guide vane']},
 title = {Film cooling comparison of shaped holes among the pressure surface, the suction surface and the leading edge of turbine vane},
 year = {2023}
}

@Filtered Article{87b44c04-3a67-4502-b821-b815a16c33f3,
 abstract = {The main purpose of this paper is quite uncontroversial. First, we recall some models of analog computations (including these allowed to perform Turing uncomputable tasks). Second, we support the suggestions that such hypercomputable capabilities of such systems can be explained by the use of infinite limits. Additionally, the inner restrictions of analog models of computations are indicated.},
 authors = {['Jerzy Mycka']},
 journal = {Applied Mathematics and Computation},
 title = {Analog computation beyond the Turing limit},
 year = {2006}
}

@Filtered Article{87e62885-065c-42ce-bcca-47f10f2e218d,
 abstract = {Physics, Neuroscience and Computation are concerned with finding the most appropriate representation spaces to describe the interaction of a dynamic system with its environment. In this work first we review the two basic conceptual approaches to the problem of representing an environment, Marr's ascending “constructivism” and Gibson's “direct perception” hypothesis. Later we review the basic neural mechanisms associated with creating meaning in both approaches: lateral inhibition and the creation of cortical maps by resonance to patterns of stimuli of families of spatially ordered neurons. We end by considering the usefulness in artificial intelligence of knowledge about the way in which biological systems construct their representation spaces. We stress the idea regarding events as representation entities and, consequently, using an event time, different from physical time. Semantics emerges from the mechanisms that detect these relevant events in each organisational level and their composition rules to specify the constitutive entities of the next level. This semantic is distributed in the cortical maps of the neuron groups that resound to the corresponding events.},
 authors = {['J. Mira', 'A.E. Delgado']},
 journal = {Neurocomputing},
 keywords = {['Representation space', 'Perception', 'Cortical maps', 'Semantic gap']},
 title = {Sensory representation spaces in neuroscience and computation},
 year = {2009}
}

@Filtered Article{8830a05f-734b-4a36-ae24-5144dce7da32,
 abstract = {Child welfare policy has historically emphasized the positive impact relative caregivers can have on foster children. This emphasis coupled with recent changes in the composition of the Temporary Assistance for Needy Families (TANF) caseload has led to interest in child-only, relative caregiver cases. Child-only research, however, ignores cases in which the relative caregiver is also receiving benefits. Using the universe of welfare cases in Maryland in October 2005, this article compares and contrasts the demographic and case characteristics of parental and relative caregiver cases, also analyzing differences between cases with and without an adult receiving benefits. Findings indicate that relative caregivers have service needs that differ from those of parents and that recipient relative caregivers are more disadvantaged than child-only cases.},
 authors = {['Correne Saunders', 'Andrea Hetling', 'Pamela C. Ovwigho', 'Catherine E. Born']},
 journal = {Children and Youth Services Review},
 keywords = {['Kinship care', 'Child-only cases', 'Temporary Assistance for Needy Families', 'Relative caregiver', 'Child welfare policy']},
 title = {Children without parents in the TANF caseload: Thinking beyond the child-only label},
 year = {2012}
}

@Filtered Article{88326af0-3d09-450c-bb2b-d2e3d41b3d36,
 abstract = {A drastic rise in construction waste observed has elicited a radical impact on the environment and economy of the world. It is, therefore, necessary to come up with waste minimization management strategies that reflect in-depth review of sources of waste. This in depth review demands understanding the intricacy of causative factors triggering generation of “waste at source” which is the main motive of study and is done through System Dynamics for design phase in context of developing countries. 8 most important causative factors in design phase were shortlisted along with their interrelationships via literature and questionnaire survey. Followed by system thinking approach that addressed the complexities caused by those factors in 2 stages. Firstly, a Causal loop diagram was developed that illustated interrelationship between factors in the form of loops. Later SD model built, evaluated the combinatorial effect of 3 evolved stocks over the fourth stock Design Generated Waste-an emanating phenomenon. Simulation result revealed increasing trend of the stock DGW over a course of time. Therefore, increase in effect of complexities of behavior of design waste causes, will consequently lead to increase in DGW. Managing the complex behavior of these design causes will help control over the DGW w.r.t. time.},
 authors = {['Sidra Muzaffar', 'Khurram Iqbal Ahmad Khan', 'Muhammad Bilal Tahir', 'Hamna Bukhari']},
 journal = {KSCE Journal of Civil Engineering},
 keywords = {['Construction & demolition waste', 'Design generated waste', 'Causal loop diagram', 'Systems thinking', 'System dynamics']},
 title = {Analysing the Causes of Design Generated Waste through System Dynamics},
 year = {2022}
}

@Filtered Article{8832b311-2097-477c-b512-fd3f11b30d7c,
 abstract = {Mindfulness foundation is an excellent method of the human spiritual development by the reasonable thinking and consideration, which was established by Lord Buddha a long time ago. There are four ways of thinking and consideration-(i) form (body), (ii) sensation, (iii) spiritual and (iv) Dhamma. In this paper, we propose the use of the form consideration for the spiritual development, in which the form can be considered thoroughly inside the body by the spiritual projection. By using the nonlinear microring resonator known as a Panda-ring resonator, the electromagnetic (EM) signals called polaritons can be generated by the coupling interaction between the intense EM fields and the ionic diploes within the almost closed system, where the dipoles can obtain from the coupling between the gold grating and the strong electromagnetic fields. In the manipulation, cells, tissues, and organs inside the human body can communicate with the spiritual (polaritonic) signals and investigation. The simulation results obtained have shown that the Lorentz factor of 0.99999959 is obtained. The successively filtering of the signal circulation within the body during the meditation can be formulated and the meditation behaviors modeled. The aura, the stopping, and the cold body states can be configured and explained.},
 authors = {['N. Pornsuwancharoen', 'I.S. Amiri', 'J. Ali', 'P. Youplao', 'P. Yupapin']},
 journal = {Results in Physics},
 keywords = {['Meditation science', 'Mindfulness Foundation', 'Buddhism philosophy', 'Mathematics foundation', 'Natural science']},
 title = {Meditation mathematical formalism and Lorentz factor calculation based-on Mindfulness foundation},
 year = {2018}
}

@Filtered Article{8833c209-b6b6-4ecc-9015-f431743163b2,
 abstract = {We present a new double-degree graduate (Master's) programme developed together by the ITMO University, Russia and University of Amsterdam, The Netherlands. First, we look into the global aspects of integration of d ifferent educational systems and list some funding opportunities fro m European foundations. Then we describe our double-degree program curricu lu m, suggest the time line of enrollment and studies, and give some e xa mples of student research topics. Finally, we d iscuss the peculiarities of joint progra ms with Russia, re flect on the first lessons learnt, and share our thoughts and experiences that could be of interest to the international community e xpanding the educational ma rkets to the vast countries like Russia, Ch ina or India. The paper is written for education professionals and contains useful information for potential students.},
 authors = {['Alexey V. Dukhanov', 'Valeria V. Krzhizhanovskaya', 'Anna Bilyatdinova', 'Alexander V. Boukhanovsky', 'Peter M.A. Sloot']},
 journal = {Procedia Computer Science},
 keywords = {['teaching computational science', "M aster's program", 'double degree', 'curriculum', 'enrollment', 'student research', 'funding opportunities']},
 title = {Double-degree Master's Program in Computational Science: Experiences of ITMO University and University of Amsterdam},
 year = {2014}
}

@Filtered Article{884c7dd5-e42d-4e03-afe0-d0d25ea3596a,
 abstract = {We inquire into the role of Turing’s biological thought in the development of his concept of intelligent machinery. We trace the possible relations between his proto-connectionist notion of ‘organising’ machines in Turing (1948) on the one hand and his mathematical theory of morphogenesis in developmental biology (1952) on the other. These works were concerned with distinct fields of inquiry and followed distinct paradigms of biological theory, respectively postulating analogues of Darwinian selection in learning and mathematical laws of form in organic pattern formation. Still, these strands of Turing’s work are related, first, in terms of being amenable in principle to his (1936) computational method of modelling. Second, they are connected by Turing’s scattered speculations about the possible bearing of learning processes on the anatomy of the brain. We argue that these two theories form an unequal couple that, from different angles and in partial fashion, point towards cognition as a biological and embodied phenomenon while, for reasons inherent to Turing’s computational approach to modelling, not being capable of directly addressing it as such. We explore ways in which these two distinct-but-related theories could be more explicitly and systematically connected, using von Neumann’s contemporaneous and related work on Cellular Automata and more recent biomimetic approaches as a foil. We conclude that the nature of ‘initiative’ and the mode of material realisation are the key issues that decide on the possibility of intelligent machinery in Turing.},
 authors = {['Hajo Greif', 'Adam P. Kubiak', 'Paweł Stacewicz']},
 journal = {Studies in History and Philosophy of Science},
 keywords = {['Universal computing machines', 'Mechanism', 'Connectionism', 'Morphogenesis', 'D’Arcy Thompson', 'Darwinian evolution', 'Cellular automata']},
 title = {Selection, growth and form. Turing’s two biological paths towards intelligent machinery},
 year = {2024}
}

@Filtered Article{88a7f97c-f79e-4355-b6b8-faaac16566d8,
 abstract = {In Korea, the pandemic has elevated scientists as trusted sources for both policy decisions and dinner table conversation. In an interview with Neuron, Eunji Cheong discusses how we need to support future generations by fostering scientific thinking, patience, and flexibility.},
 journal = {Neuron},
 title = {Eunji Cheong},
 year = {2021}
}

@Filtered Article{88ce7421-346c-43d1-8365-92b6bd6e405a,
 abstract = {An agent-based model is a virtual world comprising distributed heterogeneous agents who interact over time. In a spatial agent-based model the agents are situated in a spatial environment and are typically assumed to be able to move in various ways across this environment. Some kinds of social or organizational systems may also be modeled as spatial environments, where agents move from one group or department to another and where communications or mobility among groups may be structured according to implicit or explicit channels or transactions costs. This chapter focuses on the potential usefulness of computational laboratories for spatial agent-based modeling. Speaking broadly, a computational laboratory is any computational framework permitting the exploration of the behaviors of complex systems through systematic and replicable simulation experiments. By that definition, most of the research discussed in this handbook would be considered to be work with computational laboratories. A narrower definition of computational laboratory (or comp lab for short) refers specifically to specialized software tools to support the full range of agent-based modeling and complementary tasks. These tasks include model development, model evaluation through controlled experimentation, and both the descriptive and normative analysis of model outcomes. The objective of this chapter is to explore how comp lab tools and activities facilitate the systematic exploration of spatial agent-based models embodying complex social processes critical for social welfare. Examples include the spatial and temporal coordination of human activities, the diffusion of new ideas or of infectious diseases, and the emergence and ecological dynamics of innovative ideas or of deadly new diseases.},
 authors = {['Catherine Dibble']},
 journal = {Elsevier},
 keywords = {['agent-based simulation', 'computational laboratory', 'computational social science', 'computational economics', 'spatial economics', 'spatial social science', 'spatial networks', 'small-world networks', 'scale-free networks', 'synthetic landscape', 'inference']},
 title = {Chapter 31 Computational Laboratories for Spatial Agent-Based Models},
 year = {2006}
}

@Filtered Article{88d48488-8816-47ba-919f-8d9a167ff320,
 abstract = {Application of artificial intelligence in the accurate prediction of the rate of penetration (ROP), an important measure of drilling performance, has lately gained significant interest in oil and gas well drilling operations. Consequently, several computational intelligence techniques (CITs) for the prediction of ROP have been explored in the literature. This study explores the predictive capabilities of four commonly used CITs in the prediction of ROP and experimentally compare their predictive performance. The CIT algorithm utilizes predictors which are easily accessible continuous drilling data that have physical but complex relationship with ROP based on hydro-mechanical specific energy ROP model. The four CITs compared are the artificial neural network (ANN), extreme learning machine, support vector Regression and least-square support vector regression (LS-SVR). Two experiments were carried out; the first experiment investigates the comparative performance of the CITs while the second investigates the effect of reduced number of predictors on the performance of the models. The results show that all the CITs perform within acceptable accuracy with testing root mean square error range (RMSE) of 18.27–28.84 and testing correlation coefficient (CC) range of 0.71–0.94. LS-SVR has the best predictive performance in terms of accuracy with RMSE of 18.27 and CC of 0.94 while ANN has the best testing execution time at 0.03 s. Also utilizing the specific energy concept in chosen drilling parameters to be included among the predictors shows improved performance with five drilling parameters showing an improvement of 3%–9% in RMSE for LS-SVR in the two well studied. The utilization of the specific energy concept in the selection of the predictors in this study has demonstrated that the easily accessible drilling parameters have immense value to provide acceptable performance in the development of ROP model with CITs.},
 authors = {['Omogbolahan S. Ahmed', 'Ahmed A. Adeniran', 'Ariffin Samsuri']},
 journal = {Journal of Petroleum Science and Engineering},
 keywords = {['ROP prediction', 'Neural network', 'Least square support vector regression', 'Specific energy', 'Drilling efficiency', 'Extreme learning machine']},
 title = {Computational intelligence based prediction of drilling rate of penetration: A comparative study},
 year = {2019}
}

@Filtered Article{88d91ca2-e375-4097-b126-c58aa8142b97,
 abstract = {The Transient Reactor Analysis Code (TRAC), which features a two-fluid treatment of thermal-hydraulics, is designed to model transients in water reactors and related facilities. One of the major computational costs associated with TRAC and similar codes is calculating constitutive coefficients. Although the formulations for these coefficients are local, the costs are flow-regime- or data-dependent; i.e., the computations needed for a given spatial node often vary widely as a function of time. Consequently, a fixed, uniform assignment of nodes to parallel processors will result in degraded computational efficiency due to the poor load balancing. A standard method for treating data-dependent models on vector architectures has been to use gather operations (or indirect addressing) to sort the nodes into subsets that (temporarily) share a common computational model. However, this method is not effective on distributed memory data parallel architectures, where indirect addressing involves expensive communication overhead. Another serious problem with this method involves software engineering challenges in the areas of maintainability and extensibility. For example, an implementation that was hand-tuned to achieve good computational efficiency would have to be rewritten whenever the decision tree governing the sorting was modified. Using an example based on the calculation of the wall-to-liquid and wall-to-vapor heat-transfer coefficients for three nonboiling flow regimes, we describe how the use of the Fortran 90 WHERE construct and automatic inlining of functions can be used to ameliorate this problem while improving both efficiency and software engineering. Unfortunately, a general automatic solution to the load-balancing problem associated with data-dependent computations is not yet available for massively parallel architectures. We discuss why developers should either wait for such solutions or consider alternative numerical algorithms, such as a neural network representation, that do not exhibit load-balancing problems.},
 authors = {['S.B. Woodruff']},
 journal = {Nuclear Engineering and Design},
 title = {Some computational challenges of developing efficient parallel algorithms for data-dependent computations in thermal-hydraulics supercomputer applications},
 year = {1994}
}

@Filtered Article{88e1642f-b4e5-4aad-9e5b-5e803fb06731,
 abstract = {Fuzzy linear regression with crisp inputs and fuzzy output data constitutes an important modeling problem. Basic strategies used to solve this problem, i.e., the possibilistic method and the least squares method, together with their extensions, have some drawbacks. The possibilistic methods put emphasis on an inclusion property while the least squares methods focus on a central tendency property. Therefore, many researchers work on combining these two methods to obtain a better performance. In this paper, in contrast to most existing techniques which treat fuzzy linear regression as an optimization problem, we set the problem of constructing a fuzzy linear regression model in Bayesian statistics and propose a new fuzzy linear regression method based on approximate Bayesian computation (ABC). The method applies the likelihood-free inference algorithm ABC to generate independent samples of unknown model coefficients from Bayesian posterior distribution. This overcomes difficulty of defining likelihood function in fuzzy environment. By adjusting a prior distribution and a threshold of the ABC algorithm, the proposed approach can flexibly balance the inclusion property of the possibilistic methods and the central tendency property of the least squares methods. The convergence property of the proposed ABC algorithm is verified by a numerical example. Two measuring criteria, i.e., a distance metric and a degree of fitting index, which indicate the central tendency property and the inclusion property, respectively, are introduced to evaluate the quality of regression results. Three numerical examples are applied to show the performances of the proposed method. The numerical results are also compared with those obtained by some classical and recently proposed approaches. Additionally, a practical engineering application example is used to illustrate effectiveness of the proposed method.},
 authors = {['Ning Wang', 'Marek Reformat', 'Wen Yao', 'Yong Zhao', 'Xiaoqian Chen']},
 journal = {Applied Soft Computing},
 keywords = {['Fuzzy linear regression', 'Bayes statistics', 'Approximate Bayesian computation']},
 title = {Fuzzy Linear regression based on approximate Bayesian computation},
 year = {2020}
}

@Filtered Article{88fc3cd5-d1a7-4866-856a-f8e327dc8fae,
 abstract = {Large models are widely used in intelligent transportation systems (ITS) and autonomous vehicles (AV) due to their excellent new capabilities such as intelligence emergence, domain application adaptive, and multi-task learning. By integrating massive amounts of data, vehicles and transportation systems based on large models can understand the real-world environment, simulate the reasoning process of human drivers, optimize traffic flow, and improve driving safety and efficiency. However, in the practical implementation of large models, there are three key research questions: (1) How can model reasoning be consistent with the target task? (2) How to improve the redundancy of structures and weights caused by complex models? (3) How to solve the problems of supercomputing power, high latency, and large throughput of large models in practical deployment? These challenges have stimulated the development of deployment techniques for large models. Although existing review articles discuss large model technologies from singular or partial perspectives, there remains a lack of comprehensive systematic investigations into the application and deployment of large models for ITS and AV. Therefore, this paper conducted a quantitative analysis of scientific literature, demonstrating the necessity and significance of studying large models for ITS and AV. Subsequently, it outlined the concept and characteristics of large models and provided a detailed summary of the frontier progress of large models for ITS and AV. Moreover, to bridge the gap between large model inference and target tasks, address structural and weight redundancies, and tackle challenges in practical deployment such as high computational power, latency, and throughput, it explored efficient deployment techniques to accelerate the rapid deployment of large models. Finally, it discussed the challenges and future trends. This paper aims to provide researchers and engineers with an understanding of the forefront advancements and future trends of large models to facilitate the rapid implementation of large models and accelerate their development in ITS and AV.},
 authors = {['Lu Gan', 'Wenbo Chu', 'Guofa Li', 'Xiaolin Tang', 'Keqiang Li']},
 journal = {Advanced Engineering Informatics},
 keywords = {['Intelligent transportation systems', 'Autonomous vehicles', 'Survey', 'Large models', 'Efficient deployment techniques']},
 title = {Large models for intelligent transportation systems and autonomous vehicles: A survey},
 year = {2024}
}

@Filtered Article{89533024-f9fe-4f89-b68b-c15681c6514f,
 abstract = {In this new era, technological advancement toward the mission of a better tomorrow is reaching its limit because the exploration of the advanced possibilities of Artificial Intelligence is bounded with certain limitations. The application of analyzing various features of biosignal processing is key in the fields of medicine and healthcare. Biosignals such as Electroencephalogram (EEG), Electrocardiogram (ECG), Electromyography (EMG), Electrooculography (EOG), Galvanic Skin Response(GSR), and Magnetoencephalography (MEG) is already giving deep insight into the human body toward the identification of diverse nature and disorders. In recent years, the research toward analyzing biosignal gained interest among many researchers. The primary limitation for the algorithms to analyze these signals for more possibilities of insight is its uncertainty. Even though the algorithms of Artificial Intelligence have the capabilities to unravel the mysteries, it is bounded with specific difficulties. The machine learning algorithms designed to manage uncertain data but lacks accuracy due to many factors. Also, complete supervision is needed in a training process that involves the extraction and selection of adequate features for the training. The deep learning method (a subset of machine learning) comes into the picture due to one of these facts. This, indeed, as a supervised learning method, needs a massive volume of data to train to reach the accuracy goal. The deep learning algorithm plays a significant role in today's Artificial Intelligence–based applications. However, this platform needs many requirements, such as (a) high computational power like graphical processing units (GPU); (b) similar to machine learning methods, a massive labeled dataset for supervised learning; (c) adequate parameter selection to avoid overfitting or underfitting. To overcome the problems highlighted, the strategy of adopting the behaviors of unsupervised learning (performed by the clustering algorithm) in the deep learning methodology is needed. To achieve the goal, two-phase operations were processed, such as (1) transformation of the data elements into a latent feature space (Z) is processed through a nonlinear mapping of deep learning networks; (2) clustering the latent feature space to k-clusters, and simultaneously, the clustering loss is fed to the deep learning network for the next iteration of operation concerning the objective function convergence analyzed by the Kullback–Leibler divergence. Various strategies of enhancing the nature of deep learning methods and clustering methodologies for an unsupervised learning process are addressed in this chapter.},
 authors = {['Nagaraj Balakrishnan', 'Valentina E. Balas', 'Arunkumar Rajendran']},
 journal = {Academic Press},
 keywords = {['ANN', 'Clustering', 'Data classification', 'Data mining', 'Deep clustering networks', 'Deep learning']},
 title = {Chapter 2 - Computational intelligence in healthcare and biosignal processing},
 year = {2021}
}

@Filtered Article{8961f032-11eb-4f9e-b31e-dd8c56aea618,
 abstract = {Given the growing concern for developing students’ algebraic ideas and thinking in earlier grades (NCTM, 2000) it is important for students to have experiences that better prepare them for their formal introduction to algebra. Mobile puzzles seem to be an opportunity for exhibiting certain algebraic habits of mind as well as for demonstrating symbol-sense which might support students in their transition from arithmetic to algebra. These puzzles include multiple balanced collections of objects whose weights must be determined by the solver. The arms/beams must be perfectly balanced for it to hang properly. Therefore, they represent, in a pictorial way, systems of equations. Each arm/beam that balances two sets of objects (representing variables as unknown “weights”) represents an equation. The data derived from Grade-6 students who were asked to solve a collection of tasks reflect the presence of the “Puzzling and Persevering” and “Seeking and Using Structure” habits of mind. At the same time these data incorporate instances of some main components of symbol-sense such as “friendliness with symbols”, “manipulating and ‘reading through’ symbolic expressions”, and “choice of symbols”. Also discussed is the way this experience contributes to an intuitive application of the conventional rules for solving equations that will be later introduced to the students as the standard algebraic “moves”.},
 authors = {['Ioannis Papadopoulos']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Algebraic habits of mind', 'mobile puzzles', 'Symbol sense']},
 title = {Using mobile puzzles to exhibit certain algebraic habits of mind and demonstrate symbol-sense in primary school students},
 year = {2019}
}

@Filtered Article{89a1d4ef-96bd-4ad0-b392-7a34fde8071a,
 abstract = {We develop a conceptual framework for collaborative artificial intelligence (AI) in marketing, providing systematic guidance for how human marketers and consumers can team up with AI, which has profound implications for retailing, which is the interface between marketers and consumers. Drawing from the multiple intelligences view that AI advances from mechanical, to thinking, to feeling intelligence (based on how difficult for AI to mimic human intelligences), the framework posits that collaboration between AI and HI (human marketers and consumers) can be achieved by 1) recognizing the respective strengths of AI and HI, 2) having lower-level AI augmenting higher-level HI, and 3) moving HI to a higher intelligence level when AI automates the lower level. Implications for marketers, consumers, and researchers are derived. Marketers should optimize the mix and timing of AI-HI marketing team, consumers should understand the complementarity between AI and HI strengths for informed consumption decisions, and researchers can investigate innovative approaches to and boundary conditions of collaborative intelligence.},
 authors = {['Ming-Hui Huang', 'Roland T. Rust']},
 journal = {Journal of Retailing},
 keywords = {['Artificial intelligence', 'Collaborative AI', 'Collaborative intelligence', 'Augmentation', 'Replacement']},
 title = {A Framework for Collaborative Artificial Intelligence in Marketing},
 year = {2022}
}

@Filtered Article{89f128d4-bb8c-47a4-bf79-1f056d39755b,
 abstract = {We present the concepts of green and sustainable chemistry and related software tools. Making chemistry greener and more sustainable is a growing priority for researchers and software tools have been developed to aid in this pursuit. Software tools for green and sustainable chemistry have been developed to assess existing methods, propose new ones, and replace some experimental methods altogether with in silico approaches. We discuss the digitalization of chemistry and the computational advances that enable software tools to play a growing role in all aspects of chemical research. Barriers and limitations of current tools are discussed along with future trajectories.},
 authors = {['Joseph C. Davies', 'Jonathan D. Hirst']},
 journal = {Elsevier},
 keywords = {['Computer aided synthesis planning.', 'Digitalization', 'Electronic laboratory notebook', 'Green metrics', 'Solvent selection']},
 title = {Software Tools for Green and Sustainable Chemistry},
 year = {2025}
}

@Filtered Article{8a1168b4-6795-49c0-b604-686fc053abba,
 abstract = {Computational approaches are nowadays largely applied in drug discovery projects. Among these, molecular docking is the most used for hit identification against a drug target protein. However, many scientists in the field shed light on the lack of availability and reproducibility of the data obtained from such studies to the whole community. Consequently, sustaining and developing the efforts toward a large and fully transparent sharing of those data could be beneficial for all researchers in drug discovery. The purpose of this article is first to propose guidelines and recommendations on the appropriate way to conduct virtual screening experiments and second to depict the current state of sharing molecular docking data. In conclusion, we have explored and proposed several prospects to enhance data sharing from docking experiment that could be developed in the foreseeable future.},
 authors = {['Samia Aci-Sèche', 'Stéphane Bourg', 'Pascal Bonnet', 'Joseph Rebehmed', 'Alexandre G. {de Brevern}', 'Julien Diharce']},
 journal = {Data in Brief},
 keywords = {['3D coordinates', 'Docking', 'Files', 'SDF', 'Sharing', 'FAIR principles']},
 title = {A perspective on the sharing of docking data},
 year = {2023}
}

@Filtered Article{8a12a3c6-4aa1-4820-a0ae-5a01c41905b1,
 abstract = {Main goal of our research was to document differences on the types of modes linear algebra students displayed in their responses to the questions of linear independence from two different assignments. In this paper, modes from the second assignment are discussed in detail. Second assignment was administered with the support of graphical representations through an interactive web-module. Additionally, for comparison purposes, we briefly talk about the modes from the first assignment. First assignment was administered with the support of computational devices such as calculators providing the row reduced echelon form (rref) of matrices. Sierpinska’s framework on thinking modes (2000) was considered while qualitatively documenting the aspects of 45 matrix algebra students’ modes of reasoning. Our analysis revealed 17 categories of the modes of reasoning for the second assignment, and 15 categories for the first assignment. In conclusion, the findings of our analysis support the view of the geometric representations not replacing one’s arithmetic or algebraic modes but encouraging students to utilize multiple modes in their reasoning. Specifically, geometric representations in the presence of algebraic and arithmetic modes appear to help learners begin to consider the diverse representational aspects of a concept flexibly.},
 authors = {['Hamide Dogan-Dunlap']},
 journal = {Linear Algebra and its Applications},
 keywords = {['Mathematics education', 'Linear algebra', 'Thinking modes', 'Geometric representations']},
 title = {Linear algebra students’ modes of reasoning: Geometric representations},
 year = {2010}
}

@Filtered Article{8a375510-be28-43cb-82f9-32d81f5efc3a,
 abstract = {This paper studies a group of basic state reduction based dynamic programming (DP) algorithms for the multi-objective 0–1 knapsack problem (MKP), which are related to the backward reduced-state DP space (BRDS) and forward reduced-state DP space (FRDS). The BRDS is widely ignored in the literature because it imposes disadvantage for the single objective knapsack problem (KP) in terms of memory requirements. The FRDS based DP algorithm in a general sense is related to state dominance checking, which can be time consuming for the MKP while it can be done efficiently for the KP. Consequently, no algorithm purely based on the FRDS with state dominance checking has ever been developed for the MKP. In this paper, we attempt to get some insights into the state reduction techniques efficient to the MKP. We first propose an FRDS based algorithm with a local state dominance checking for the MKP. Then we evaluate the relative advantage of the BRDS and FRDS based algorithms by analyzing their computational time and memory requirements for the MKP. Finally different combinations of the BRDS and FRDS based algorithms are developed on this basis. Numerical experiments based on the bi-objective KP instances are conducted to compare systematically between these algorithms and the recently developed BRDS based DP algorithm as well as the existing FRDS based DP algorithm without state dominance checking.},
 authors = {['Aiying Rong', 'José Rui Figueira']},
 journal = {Computers & Mathematics with Applications},
 keywords = {['Multi-objective optimization', 'Bi-objective knapsack problem', 'Dynamic programming', 'Basic state reduction techniques']},
 title = {Computational performance of basic state reduction based dynamic programming algorithms for bi-objective 0–1 knapsack problems},
 year = {2012}
}

@Filtered Article{8a46e472-ba4b-4d0e-80ee-20676cbe0f4d,
 abstract = {Acoustic phonons have long been believed to dominate the lattice thermal conductivity (κl) and the contribution of optical phonons can be neglected in crystal structures. KCaBi, as a high-throughput screening semiconductor with ultralow κl [J. Am. Chem. Soc. 144, 4448 (2022)], has been demonstrated that the contribution of optical phonons plays an important role in thermal transport. In this work, by solving the Boltzmann transport equation, it is found that the κl of KCaBi is 2.2 at 300K, with acoustic phonons dominating the z-direction κl and optical phonons contributing around 50% to the x-direction κl under the four-phonon picture. The uncommon contribution of optical phonons also manifests the possibility of tuning the κl anisotropy based on optical phonons. Following this line of thinking, it is found that applying tensile strain can cause a more pronounced decrease of acoustic phonon contribution than that of optical counterpart due to the highly dispersive optical branches, thus enhancing the anisotropic ratio of κl. Moreover, the microscopic mechanism is elucidated by analyzing the phonon dispersion relation, phonon mode-wise contribution and phonon scattering rates. Our study could provide appealing alternatives for the regulation of phonon transport from the viewpoint of optical phonons.},
 authors = {['Xue-Kun Chen', 'Yue Zhang', 'Qing-Qing Luo', 'Pin-Zhen Jia', 'Wu-Xing Zhou']},
 journal = {International Journal of Heat and Mass Transfer},
 keywords = {['anisotropic thermal conductivity', 'optical phonons', 'four-phonon scattering', 'strain engineering', 'machine learning potential']},
 title = {Strain-driven anisotropic enhancement in the thermal conductivity of KCaBi: the role of optical phonons},
 year = {2025}
}

@Filtered Article{8a5b6fe5-52bf-4d34-b76e-8508deb2c6d2,
 abstract = {Management in complex environments requires knowledge about temporal contingencies. Expectations about durations enable us to prepare for important events in good time, but also to detect irregularities. Unfortunately, time perception is not invariant. Situational aspects as well as features of the task at hand may dramatically change our sense of time. Particularly under varying workload conditions, temporal distortions may lead to performance errors. A valid and reliable model of time perception must account for these characteristics. Based on the cognitive architecture ACT-R (Anderson et al., 2004), we developed a computational model in line with this requirement. Specific emphasis was placed on mechanisms of coordinative working memory which seem to influence time encoding and perception. The model’s assumptions were tested in three steps. First, the model was applied to account for time distortions ‘a posteriori’. Effects of varying working memory demands reported by Dutke (2005) were replicated and explained by simulations of the model. Second, the model was used for predicting effects ‘a priori’. Augmenting Dutke’s (2005) approach by switching between different degrees of memory demands, predictions of time distortions were derived from the model. These predictions were compared with experimental data. Central assumptions of the model were supported, but there were also some deviations that the model had not captured. Based on the conclusions from the results of the experiment, a second a priori testing addressed temporal expectations in a complex task using a micro-world scenario. The results support the interpretation of the previous experiment and provide new insights for modelling time perception. In summary, our results indicate that coordinative working memory – in contrast to general attention – causes differences in timing performance. This characteristic is captured by our approach. The model we propose heavily relies on mechanisms of working memory and can be applied to explain effects for different time intervals, under a variety of experimental conditions and in different task environments.},
 authors = {['Nele Russwinkel', 'Leon Urbas', 'Manfred Thüring']},
 journal = {Cognitive Systems Research},
 keywords = {['Cognitive modelling', 'Time perception', 'Working memory', 'Expectations', 'Surprise', 'ACT-R']},
 title = {Predicting temporal errors in complex task environments: A computational and experimental approach},
 year = {2011}
}

@Filtered Article{8a5b7cab-c712-4f9d-9b0d-f2b2deebc53e,
 abstract = {We prove NP-hardness results for five of Nintendo's largest video game franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokémon. Our results apply to generalized versions of Super Mario Bros. 1–3, The Lost Levels, and Super Mario World; Donkey Kong Country 1–3; all Legend of Zelda games; all Metroid games; and all Pokémon role-playing games. In addition, we prove PSPACE-completeness of the Donkey Kong Country games and several Legend of Zelda games.},
 authors = {['Greg Aloupis', 'Erik D. Demaine', 'Alan Guo', 'Giovanni Viglietta']},
 journal = {Theoretical Computer Science},
 keywords = {['Nintendo games', 'Video games', 'Computational complexity', 'NP-hardness', 'PSPACE-hardness']},
 title = {Classic Nintendo games are (computationally) hard},
 year = {2015}
}

@Filtered Article{8ac7c73c-0097-41d4-8e6a-1b686b4d5a3b,
 abstract = {In this paper I examine Paul Thagard's computational approach to studying science, which is a contribution to the cognitive science of science. I present several criticisms of Thagard's approach and use them to motivate some suggestions for alternative approaches in cognitive science of science. I first argue that Thagard does not clearly establish the units of analysis of his study. Second, I argue that Thagard mistakenly applies the same model to both individual and group decision making. Finally, I argue that in attempting to account for psychological and social processes as well as providing a philosophical model of successful reasoning Thagard attempts to explain too much with one model, thus straining the plausibility of his model.},
 authors = {['Stephen Downes']},
 journal = {New Ideas in Psychology},
 title = {Modeling scientific practice: Paul Thagard's computational approach},
 year = {1993}
}

@Filtered Article{8b4d25b8-d031-4939-b1a2-3b6f58c60ec7,
 abstract = {This paper presents a bio-inspired model of computations, the random PROLOG processor (RPP), used for analysis of collective intelligence (CI). In the RPP, clause_molecules (CMs) of facts, rules, goals, or higher-level logical structures enclosed by membranes move quasi-randomly in structured computational_PROLOG_space (CS). When CMs rendezvous, an inference process can occur iff the logical conditions are fulfilled. CI can be evaluated as follows: (1) the mapping is done of a given social structure into the RPP; (2) the beings and their behavior are translated into PROLOG expressions, carried by CMs; (3) the goal(s) of the social structure are translated into N-element inference (NEI); (4) the efficiency of the NEI is evaluated and given as the intelligence quotient of a social structure (IQS) projected onto NEI.},
 authors = {['Tadeusz Szuba']},
 journal = {Future Generation Computer Systems},
 keywords = {['Collective intelligence', 'IQ measure', 'Social structure', 'PROLOG', 'Model of computations', 'Brownian movements']},
 title = {A molecular quasi-random model of computations applied to evaluate collective intelligence},
 year = {1998}
}

@Filtered Article{8ba0e568-1ce6-4396-98f0-117ed28da10c,
 abstract = {Influence diagrams have become a popular tool for representing and solving decision making problems under uncertainty (Shachter, Operations Research 1986;34:871–82). We show here some practical difficulties when using them to construct a medical decision support system. Specifically, it is hard to tackle issues related to the problem structuring, like the existence of constraints on the sequence of decisions, and the time evolution modeling; related to the knowledge-acquisition, like probability and utility assignment; and related to computational limitations, in memory storage and evaluation phases, as well as the explanation of results. We have recently developed a complex decision support system for neonatal jaundice management — a very common medical problem — , encountering all these difficulties. In this paper, we describe them and how they have been undertaken, providing insights into the community involved in the design and solution of decision models by means of influence diagrams.
Scope and purpose
Decision Analysis is a very well-known discipline that deals with the practice of Decision Theory (Clemen, Making hard decisions: an introduction to decision analysis, 2nd ed. Pacific Grove, CA: Duxbury, 1996). It comprises various steps usually implemented in a decision support system: definition of the alternatives and objectives, modelization of the structure of the decision problem, as well as the beliefs and preferences of the decision maker. The recommended alternative is the one with maximum expected utility, once all the assignments have been refined via sensitivity analyses. However, there are a number of difficulties faced in practice when solving large problems, that require an attentive study.},
 authors = {['C. Bielza', 'M. Gómez', 'S. Rı́os-Insua', 'J.A.Fernández {del Pozo}']},
 journal = {Computers & Operations Research},
 keywords = {['Decision analysis', 'Influence diagrams', 'Implementation issues', 'Medical decision making', 'Neonatal jaundice']},
 title = {Structural, elicitation and computational issues faced when solving complex decision making problems with influence diagrams},
 year = {2000}
}

@Filtered Article{8bca2137-3061-46a4-ae57-f0f6e1cdd161,
 abstract = {In the field of energy system modelling, increasing complexity and optimization analysis are essential for understanding the most effective decarbonization options. However, the growing need for intricate models leads to increased computational time, which can hinder progress in research and policy-making. This study aims to address this issue by integrating machine learning algorithms with EnergyPLAN and EPLANopt, a coupling of EnergyPLAN software and a multi-objective evolutionary algorithm, to expedite the optimization process while maintaining accuracy. By saving computational time, we can increase the number of evaluations, thereby enabling deeper exploration of uncertainty in energy system modelling. Although machine learning models have been widely employed as surrogate models to accelerate optimization problems, their application in energy system modeling at the national scale, while preserving high temporal resolution and extensive sector-coupling, remains scarce. Several machine learning models were evaluated, and an artificial neural network was selected as the most effective surrogate model. The findings demonstrate that incorporating this surrogate model within the optimization process reduces computational time by 64 % compared to the conventional EPLANopt approach, while maintaining an accuracy level close to that obtained by running EPLANopt without the surrogate model.},
 authors = {['Matteo Giacomo Prina', 'Mattia Dallapiccola', 'David Moser', 'Wolfram Sparber']},
 journal = {Energy},
 keywords = {['Energy system modelling', 'Energy scenarios', 'Energy planning', 'Machine learning']},
 title = {Machine learning as a surrogate model for EnergyPLAN: Speeding up energy system optimization at the country level},
 year = {2024}
}

@Filtered Article{8c0f3ea9-8ee7-44df-b8dc-d7829bd2264f,
 abstract = {Creativity is an important skill that relates to innovation, problem-solving and artistic achievement. However, relatively little is known about the early development of creative potential in very young children, in part due to a paucity of tasks suitable for use during infancy. Current measures of creativity in early childhood include the Unusual Box Test, Torrance's Thinking Creatively in Action and Movement (TCAM) task and the Toca Kitchen Monsters task. These tasks are designed for children aged above 12, 36 and 18 months respectively, but very few measures of creativity can be used for infants aged below 2. Accordingly, here we report age-appropriate adaptations of TCAM and Toca Kitchen Monsters tasks for infants as young as 12 to 24 months. Considerations taken into account include (1) infants’ cognitive capacities (i.e., attention span, language comprehension skills, motor skills, and approach to play), and (2) practicality of the stimuli, including suitability for use amid the COVID-19 pandemic. The modified creativity battery for infants includes three tasks: Music Play, Object Play and Exploratory Play tasks. The task protocols elaborated in this paper are intended to facilitate studies on the early development of creativity in infants aged between 12 and 24 months. Primary highlights include:•Age-appropriate adaptation of creativity tasks for use with infants aged between 12 and 24 months.•Consideration of infants’ cognitive capacities and stimulus practicality.•Innovative use of movement as expression of infants’ creative behaviour.},
 authors = {['Ling Zheng Teo', 'Victoria Leong']},
 journal = {MethodsX},
 keywords = {['Precursors of creativity', 'Infancy', 'Measurements of creativity']},
 title = {Age-appropriate adaptation of creativity tasks for infants aged 12–24 months},
 year = {2024}
}

@Filtered Article{8c75ebe8-da98-42c1-b32c-e2a538e025a4,
 authors = {['J.R.A. Pearson']},
 journal = {Journal of Non-Newtonian Fluid Mechanics},
 title = {Report on University of Wales Institute of non-Newtonian Fluid Mechanics Mini-Symposium on “Continuum and Microstructural Modelling in Computational Rheology” Seiont Manor, Gwynedd, 11–12 April 1994},
 year = {1994}
}

@Filtered Article{8d48f7ca-8654-442a-921a-a2b76273f1e4,
 abstract = {The research and moral use of academic technology focuses on developing, implementing, and overseeing the use of suitable technical resources and procedures to enhance learning and achievement. Multimedia has found its position in some form as an educational technology platform in the contemporary environment of academic universities. The use of virtual reality software as an intellectual tool and learning provider allows students to perform cognitive rehabilitation of preexisting information frameworks. People are paying more and more attention to how preschoolers' holistic skills develop as education reform progresses. Drama education is incorporated into the school curriculum to enhance young children's artistic, intellectual, and linguistic skills. Therefore, this study aims to examine the potential of multimedia-based virtual reality technology (MVRT) in drama education. The participants in this study were students from different universities in China. Students were exposed to multimedia-based virtual reality technology, and its efficacy was assessed using a statistical analytic approach called Analysis of variance (ANOVA). Drama understanding rate, educational improvement ratio, teaching quality rate, student achievement ratio, computation time, and parental support rate are among the performance metrics used to assess performance. Multimedia-based virtual reality technology (MVRT) for drama education showed outstanding success, with a 98% improvement ratio in educational outcomes and higher teaching quality. Students exhibited improved performance, supported by solid parental approval, demonstrating the effectiveness of MVRT in enhancing educational experiences.},
 authors = {['Bingyu Zhang', 'Wenwen Jiang']},
 journal = {Entertainment Computing},
 keywords = {['Multimedia', 'Virtual reality', 'Academic technology', 'Drama education', 'and ANOVA']},
 title = {Research on the application value of Multimedia-Based virtual reality technology in drama education activities},
 year = {2024}
}

@Filtered Article{8d4b8542-fe6e-4565-9598-75a9b26ccd57,
 abstract = {Experimental FTIR, NMR and UV-visible spectrum analyses were used to describe the title compound 2-Hydroxy-1-Naphthaldehyde. The optimized molecular geometry and vibrational wave numbers were determined by using the DFT approach and B3LYP/6-311++G(d, p) basis set. VEDA was used to determine the vibrational assignments. The GIAO technique was used to compute carbon and proton NMR chemical shifts in CDCl3. The most reactive location of the 2H1NA molecule, according to MEP map analysis, is the site containing the oxygen atom. TD-DFT approach was used to produce the theoretical UV-visible spectrum in MeOH and gas phase. HOMO-LUMO and Donor-Acceptor (NBO) interactions were investigated for the title compound. In addition, nonlinear optical characteristics, ELF and Fukui activity were investigated. Temperature-dependent thermodynamic characteristics were also computed. The 3D intermolecular interactions of the crystal surface were characterised using Hirshfeld surface analysis, whereas the 2D interactions were explained using fingerprint plots. 2H1NA was stabilized by the development of H—H/H—C/H—O contacts. The bioactive probability of the title molecule was theoretically demonstrated by computing the electrophilicity index. In a biological study six different receptors, molecular docking was performed to evaluate the best ligand-protein interactions and likeness to the active substance. Biomolecular stability was investigated using a molecular dynamics simulation.},
 authors = {['Arun Sharma', 'Ghazala Khanum', 'Anuj Kumar', 'Aysha Fatima', 'Meenakshi Singh', 'Khamael M. Abualnaja', 'Khaled Althubeiti', 'S. Muthu', 'Nazia Siddiqui', 'Saleem Javed']},
 journal = {Journal of Molecular Structure},
 keywords = {['DFT studies', 'Fukui Function', 'MEP', 'ELF', 'Hirshfeld', 'Molecular docking']},
 title = {Conformational stability, quantum computational, spectroscopic, molecular docking and molecular dynamic simulation study of 2-hydroxy-1-naphthaldehyde},
 year = {2022}
}

@Filtered Article{8d7f5afa-2ac1-4229-8045-f22ea395d8e2,
 abstract = {The massive growth in mobile users and wireless technologies has resulted in increased data traffic and created demand for additional radio spectrum. This growing demand for radio spectrum has resulted in spectrum congestion and mandated the need for coexistence between radar and interfering communication emitters. To address the aforementioned issues, it is critical to review existing policies and evaluate new technologies that can utilize spectrum in an efficient and intelligent manner. Cognitive radio and cognitive radar are two promising technologies that exploit spectrum using dynamic spectrum access techniques. Additionally, introducing the bio-inspired concept ‘metacognition’ in a cognitive process has shown to increase the effectiveness and robustness of the cognitive radio and cognitive radar system. Metacognition is a high-order thinking agent that monitors and regulates the cognition process through a feedback and control process called the perception–action cycle. Extensive research has been done in the field of spectrum sensing in cognitive radio and spectral coexistence between radar and communication systems. This paper provides a detailed classification of spectrum sensing schemes and explains how dynamic spectrum access strategies share the spectrum between radar and communication systems. In addition to this, the fundamentals of cognitive radio, its architecture, spectrum management framework, and metacognition concept in radar are discussed. Furthermore, this paper presents various research issues, challenges, and future research directions associated with spectrum sensing in cognitive radar and dynamic spectrum access strategies in cognitive radar.},
 authors = {['Sumit Kumar Agrawal', 'Abhay Samant', 'Sandeep Kumar Yadav']},
 journal = {Physical Communication},
 keywords = {['Cognitive radio', 'Spectrum sensing', 'Spectrum sharing', 'Cognitive radar', 'Metacognition', 'Metacognitive radar']},
 title = {Spectrum sensing in cognitive radio networks and metacognition for dynamic spectrum sharing between radar and communication system: A review},
 year = {2022}
}

@Filtered Article{8d91e32e-fb46-4c28-b2e6-ced8e2295839,
 abstract = {Mild cognitive impairment (MCI) is a state that falls between the more severe decline of dementia and the typical aging-related loss of memory and thinking. MCI must be diagnosed earlier to avoid complete memory loss. Several Machine Learning (ML) and Deep Learning (DL) models employ standard feature extraction approaches to achieve effective MCI categorization. However, it has some drawbacks, including lower accuracy, longer time consumption, less feature learning, and increased model complexity. The proposed method introduces a novel deep learning model to address the limitations of existing MCI classification approaches. Initially, the Electroencephalography (EEG) signal is pre-processed using the Sequential Savitzky-Golay filtering model (SEQ-SG), which improves the signal’s quality and removes unnecessary noise. The Improved Tuneable Q Wavelet Transform (ITQWT) feature extraction model is used to extract relevant features. The Coati Stochastic Optimization (CSO) algorithm selects the most optimal channel features from the EEG signal. Finally, the proposed deep learning model, Dual Attention Assisted Compact Convolutional Network with Stacked Bi-LSTM (DCCN-SBiL), is used to classify EEG signals into three categories: Alzheimer’s disease, MCI, and normal. The proposed model is optimized using the Gazelle Optimization Algorithm (GOA), which tunes the classification model’s hyperparameters. The proposed classification model is evaluated using the Mendeley Dataset, which contains EEG signals from Alzheimer's disease, MCI and Normal. The proposed model has shown great performance in many performance parameters, including 97.25% accuracy, 95.94% recall, 96.03% precision, and 94.65% specificity in MCI classification.},
 authors = {['A. {Nirmala Devi}', 'M. Latha']},
 journal = {Expert Systems with Applications},
 keywords = {['Mild cognitive impairment', 'Deep learning', 'Compact convolutional neural network', 'EEG signal', 'Dual attention', 'Alzheimer’s disease', 'Improved tuneable Q wavelet transform']},
 title = {DCNN-SBiL: EEG signal based mild cognitive impairment classification using compact convolutional network},
 year = {2025}
}

@Filtered Article{8d9d6bd3-adec-424c-90c1-222223a51a9d,
 abstract = {The dataflow program graph execution model, or dataflow for short, is an alternative to the stored-program (von Neumann) execution model. Because it relies on a graph representation of programs, the strengths of the dataflow model are very much the complements of those of the stored-program one. In the last thirty or so years since it was proposed, the dataflow model of computation has been used and developed in very many areas of computing research: from programming languages to processor design, and from signal processing to reconfigurable computing. This paper is a review of the current state-of-the-art in the applications of the dataflow model of computation. It focuses on three areas: multithreaded computing, signal processing and reconfigurable computing.},
 authors = {['Walid A Najjar', 'Edward A Lee', 'Guang R Gao']},
 journal = {Parallel Computing},
 keywords = {['Computational models', 'Dataflow', 'Multithreaded computer architecture', 'von Neumann computer', 'Dataflow history', 'Memory models']},
 title = {Advances in the dataflow computational model},
 year = {1999}
}

@Filtered Article{8dbed2f4-9843-4d9a-9c22-73fff3f24e3b,
 abstract = {Most research into causal learning has focused on atemporal contingency data settings while fewer studies have examined learning and reasoning about systems exhibiting events that unfold in continuous time. Of these, none have yet explored learning about preventative causal influences. How do people use temporal information to infer which components of a causal system are generating or preventing activity of other components? In what ways do generative and preventative causes interact in shaping the behavior of causal mechanisms and their learnability? We explore human causal structure learning within a space of hypotheses that combine generative and preventative causal relationships. Participants observe the behavior of causal devices as they are perturbed by fixed interventions and subject to either regular or irregular spontaneous activations. We find that participants are capable learners in this setting, successfully identifying the large majority of generative, preventative and non-causal relationships but making certain attribution errors. We lay out a computational-level framework for normative inference in this setting and propose a family of more cognitively plausible algorithmic approximations. We find that participants’ judgment patterns can be both qualitatively and quantitatively captured by a model that approximates normative inference via a simulation and summary statistics scheme based on structurally local computation using temporally local evidence.},
 authors = {['Tianwei Gong', 'Neil R. Bramley']},
 journal = {Cognition},
 keywords = {['Causal learning', 'Time', 'Prevention', 'Structure induction', 'Summary statistics']},
 title = {Continuous time causal structure induction with prevention and generation},
 year = {2023}
}

@Filtered Article{8dd03c30-7c91-4d1e-ba4a-a9ebe1b8c020,
 abstract = {Purpose
There is a critical need to understand how to attract Black girls and other girls of color to the science, technology, engineering, math, and computer science (STEM+CS) field. This study aims to look at the design and implementation of a CS learning ecosystem that supports girls of color in acquiring critical CS skills starting in middle school.
Design/methodology/approach
This mixed-method case study included 53 girls, between the ages of 11 and 13, in four US middle schools. Study methods included the analysis of a pre-program student survey, longitudinal interviews and focus groups, weekly observations and computing artifacts.
Findings
Program participants were interested in CS, were confident in their ability to learn CS, had prior coding and CS experience and had parents and teachers who encouraged them to learn CS. But some students showed dependent learning behaviors while engaging in CS activities. These included relying on instructors and being reticent to make mistakes–behaviors that limit learning. The CS learning ecosystem supported students as they shifted from applying dependent learning approaches to applying independent learning approaches. Instructors sustained a growth mindset and supported productive struggle as students learned CS skills.
Originality/value
A CS learning system supported equitable learning experiences and helped students develop independent learning behaviors that led to deeper engagement in CS.},
 authors = {['Ryoko Yamaguchi', 'Veronica {Hankerson Madrigal}', 'Cyntrica N. Eaton', 'Jamika D. Burge']},
 journal = {Journal for Multicultural Education},
 keywords = {['Black girls', 'Computer science', 'Computational thinking', 'Dependent learning', 'Equity', 'Independent learning', 'Learning behaviors', 'Learning ecosystem', 'Middle school girls', 'STEM', 'STEM+CS']},
 title = {Equitable STEM+CS learning experiences for girls of color: nurturing an independent learning approach via a learning ecosystem},
 year = {2023}
}

@Filtered Article{8defba09-c4b9-458d-b235-a4717b84eb43,
 abstract = {Bio-intelligence in manufacturing integrates biological principles and advanced computational techniques to enhance industrial processes. This interdisciplinary approach is based on already known principles like biomimicry, bio-sensing, and bio-based materials with the aim to further innovate and optimize industrial production by combining manufacturing and biology / biotechnology. Although there is a raising interest in research, it is not yet clear where and how bio-intelligence will have practical implications for manufacturing enterprises. In this work we use systematic literature review methodology to identify and analyze the current status quo of scientific literature related to biointelligent manufacturing. A main result of this work was to deduce potential applications, their suitability for small and medium sized enterprises (SME) and to highlight still existing challenges such as scalability, integration with existing systems, and economic viability.},
 authors = {['Asja Emer', 'Matteo {De Marchi}', 'Angelika Hofer', 'Benedikt G. Mark', 'Walburga Kerschbaumer', 'Erwin Rauch', 'Dominik T. Matt']},
 journal = {Procedia Computer Science},
 keywords = {['Sustainable Manufacturing', 'Industry 4.0', 'Industry 5.0', 'Biological Transformation', 'Bio-Intelligent Manufacturing', 'SME']},
 title = {Examples of Potential Applications of Bio-intelligent Manufacturing},
 year = {2025}
}

@Filtered Article{8e16066a-fd40-477d-ae67-97574173e3d8,
 abstract = {The most recent information technologies have become an integral part of modern life. As the study shows, along with the obvious benefits, their use can lead to negative consequences, namely the loss of communication and soft skills, changes in the ability to absorb information, decreased motivation to acquire new knowledge among the younger age group. The authors propose a new methodology for organizing the educational process Study-Train-Explain. The aim of the method is to increase the Student Talking Time parameter to develop the skills of mathematical data analysis, systematic and analytical thinking to master the methods of description and construction of mathematical model of the phenomenon or process. These competencies are extremely in demand in the professional field related to the organization of transportation and operation of transport-technological machines and complexes under the conditions of digitalization of global processes. The article presents an algorithm and the results of the experimental training process carried out by the authors according to the specified methodology.},
 authors = {['L Bakeeva', 'L Brylevskaya', 'L Gonchar', 'E Pastukhova', 'Y Romanova', 'O Skepko']},
 journal = {Transportation Research Procedia},
 keywords = {['Digitalization of education', 'transport engineering', 'student talking time', 'study-train-explain methodology']},
 title = {Increasing the student talking time parameter under the digitalization in transport engineering learning},
 year = {2022}
}

@Filtered Article{8e4f7cc6-58d9-47e1-8c50-68de6031b487,
 authors = {['C. Brandon Ogbunu']},
 journal = {Current Biology},
 title = {C. Brandon Ogbunu},
 year = {2024}
}

@Filtered Article{8ea68139-b34e-4b2c-b087-131e85741bec,
 abstract = {Bone is probably the most frequently investigated biological material and finite element analysis (FEA) is the computational tool most commonly used for the analysis of bone biomechanical function. FEA has been used in bone research for more than 30years and has had a substantial impact on our understanding of the complex behavior of bone. Bone is structured in a hierarchical way covering many length scales and this chapter reflects this hierarchical organization. In particular, the focus is on the applications of FEA for understanding the relationship between bone structure and its mechanical function at specific hierarchical levels. Depending on the hierarchical level, different issues have been investigated with FEA ranging from more clinically oriented topics related to bone quality (e.g., predicting bone strength and fracture risk) to more fundamental problems dealing with the mechanical aspects of biological processes (e.g., stress and strain around osteocyte lacunae) as well as with the micromechanical behavior of bone at its ultrastructure. A better understanding of the relationship between structure and mechanical function is expected to be important for the current trends in (bio)materials design, where the structure of biological materials is considered as a possible source of inspiration, as well as for more successful approaches in the prevention and treatment of age- and disease-related fractures.},
 authors = {['D. Ruffoni', 'G.H. {van Lenthe}']},
 journal = {Elsevier},
 keywords = {['Bone imaging', 'Bone research', 'Computational modeling', 'Femur', 'Finite element analysis', 'Fracture', 'Hierarchical structure', 'Microcomputed tomography', 'Osteoporosis', 'Radius', 'Strength', 'Vertebra']},
 title = {3.307 - Finite Element Analysis in Bone Research: A Computational Method Relating Structure to Mechanical Function},
 year = {2011}
}

@Filtered Article{8ef84835-c7c5-4690-b8ed-b7338775a57b,
 abstract = {In recent work, Hess and Shipley [18] defined a theory of topological coHochschild homology (coTHH) for coalgebras. In this paper we develop computational tools to study this new theory. In particular, we prove a Hochschild–Kostant–Rosenberg type theorem in the cofree case for differential graded coalgebras. We also develop a coBökstedt spectral sequence to compute the homology of coTHH for coalgebra spectra. We use a coalgebra structure on this spectral sequence to produce several computations.},
 authors = {['Anna Marie Bohmann', 'Teena Gerhardt', 'Amalie Høgenhaven', 'Brooke Shipley', 'Stephanie Ziegenhagen']},
 journal = {Topology and its Applications},
 keywords = {['Topological Hochschild homology', 'Coalgebra', 'Hochschild–Kostant–Rosenberg']},
 title = {Computational tools for topological coHochschild homology},
 year = {2018}
}

@Filtered Article{8f5163d1-5381-434d-8b88-4e465e268364,
 abstract = {The transformation of empirical research due to the arrival of big data analytics and data science, as well as the new availability of methods that emphasize causal inference, are moving forward at full speed. In this Research Commentary, we examine the extent to which this has the potential to influence how e-commerce research is conducted. China offers the ultimate in data-at-scale settings, and the construction of real-world natural experiments. Chinese e-commerce includes some of the largest firms involved in e-commerce, mobile commerce, social media and social networks. This article was written to encourage young faculty and doctoral students to engage in research that can be carried out in near real-time, with truly experimental or quasi-experimental research designs, and with the clear intention of establishing causal inferences that relate the precursors and drivers of observable outcomes through various kinds of processes. We discuss: the relevant data sources and research contexts; the methods perspectives that are appropriate which blend Computer Science, Statistics and Econometrics, how the research can be made relevant for China; and what kinds of findings and research directions are available. This article is not a tutorial on big data analytics methods in general though, nor does it cover just those published works that demonstrate big data methods and empirical causality in other disciplines. Instead, the empirical research covered is mostly taken from Electronic Commerce Research and Applications, which has published many articles on Chinese e-commerce. This Research Commentary invites researchers in China and the Asia Pacific region to expand their coverage to bring into their empirical work the new methods and philosophy of causal data science.},
 authors = {['David C.W. Phang', 'Kanliang Wang', 'Qiuhong Wang', 'Robert J. Kauffman', 'Maurizio Naldi']},
 journal = {Electronic Commerce Research and Applications},
 keywords = {['Big data', 'Business insights', 'Causal inference', 'Causal methods', 'Computational social science (CSS)', 'Consumer behavior', 'China', 'Data analytics', 'Digital economy', 'E-commerce', 'Emerging markets', 'Empirical research', 'Information systems (IS) research', 'Machine learning (ML)', 'M-commerce', 'Policy analytics', 'Research design', 'Secondary data', 'Sensor data', 'Streaming data', 'Social insights', 'Theory testing']},
 title = {How to derive causal insights for digital commerce in China? A research commentary on computational social science methods},
 year = {2019}
}

@Filtered Article{8fa2b9ac-8873-4098-bc93-776c0002b1a5,
 abstract = {This study investigated introductory computer science (CS1) students’ implicit beliefs of intelligence. Referencing Dweck and Leggett’s (1988) framework for implicit beliefs of intelligence, we examined how (1) students’ implicit beliefs changed over the course of a semester, (2) these changes differed as a function of course enrollment and students’ motivated self-regulated engagement profile, and (3) implicit beliefs predicted student learning based on standardized course grades and performance on a computational thinking knowledge test. For all students, there were significant increases in entity beliefs and significant decreases in incremental beliefs across the semester. However, examination of effect sizes suggests that significant findings for change across time were driven by changes in specific subpopulations of students. Moreover, results showed that students endorsed incremental belief more strongly than entity belief at both the beginning and end of the semester. Furthermore, the magnitude of changes differed based on students’ motivated self-regulated engagement profiles. Additionally, students’ achievement outcomes were weakly predicted by their implicit beliefs of intelligence. Finally, results showed that the relationship between changes in implicit intelligence beliefs and student achievement varied across different CS1 courses. Theoretical implications for implicit intelligence beliefs and recommendations for STEM educators are discussed.},
 authors = {['Abraham E. Flanigan', 'Markeya S. Peteranetz', 'Duane F. Shell', 'Leen-Kiat Soh']},
 journal = {Contemporary Educational Psychology},
 keywords = {['Motivation', 'Implicit intelligence beliefs', 'Computer science', 'Self-regulation', 'Engagement']},
 title = {Implicit intelligence beliefs of computer science students: Exploring change across the semester},
 year = {2017}
}

@Filtered Article{8fd0a070-915f-4daf-9df5-44b799348e0e,
 abstract = {This study investigates the development of transversal skills and their association with academic performance in university students enrolled in on-campus programs with online activities. A cross-sectional, descriptive, and quantitative research was conducted with 252 students from a public university in Mexico. Transversal skills, socioformative project-based practices, learning strategies, and the relevance of online activities were assessed using validated rubrics. The results indicated a low level of development in three transversal skills: research, entrepreneurship, and English, with the latter being the poorest rated. Critical and creative thinking exhibited the highest level of development. In the didactic component, socioformative project-based pedagogical practices and learning strategies showed acceptable levels. Students expressed satisfaction with complementary online activities, showing a preference for interactive videos and short videos under 4 min. Regression analysis and structural equations were used to examine the relationships between various factors. Results demonstrated that socioformative project-based pedagogical practices, learning strategies, and online education positively correlated with the development of transversal skills. Furthermore, a higher level of transversal skills was associated with better academic averages among students. Socioformative project-based pedagogical practices also correlated with academic performance through transversal skills. The study concludes that integrating online activities into on-campus programs, based on the socioformative pedagogical model, can enhance the development of transversal skills and improve academic performance. Further research into the implementation of this educational model and its long-term impact on university education and professional success is recommended.},
 authors = {['Yolanda Guerra-Macías', 'Sergio Tobón']},
 journal = {Heliyon},
 keywords = {['Generic competencies', '21st-century skills', 'Virtual education', 'Higher education', 'Socioformation', 'Generic skills', 'Socioformative rubrics']},
 title = {Development of transversal skills in higher education programs in conjunction with online learning: relationship between learning strategies, project-based pedagogical practices, e-learning platforms, and academic performance},
 year = {2025}
}

@Filtered Article{9018a6b1-0edc-467b-807f-77ee962a6611,
 authors = {['Pieter Adriaans']},
 journal = {North-Holland},
 title = {LEARNING AND THE COOPERATIVE COMPUTATIONAL UNIVERSE},
 year = {2008}
}

@Filtered Article{902c5484-1b24-4b93-8d04-c73eca80ef26,
 abstract = {A modular motor organization may be needed to solve the degrees of freedom problem in biological motor control. Reflex elements, kinematic primitives, muscle synergies, force-field primitives and/or pattern generators all have experimental support as modular elements. We discuss the possible relations of force-field primitives, spinal feedback systems, and pattern generation and shaping systems in detail, and review methods for examining underlying motor pattern structure in intact or semi-intact behaving animals. The divisions of systems into primitives, synergies, and rhythmic elements or oscillators suggest specific functions and methods of construction of movement. We briefly discuss the limitations and caveats needed in these interpretations given current knowledge, together with some of the hypotheses arising from these frameworks.},
 authors = {['Simon Giszter', 'Vidyangi Patil', 'Corey Hart']},
 journal = {Elsevier},
 keywords = {['primitives', 'motor synergies', 'force-fields', 'modularity', 'feedback', 'motor pattern analysis', 'decomposition', 'rhythm generation', 'pattern shaping']},
 title = {Primitives, premotor drives, and pattern generation: a combined computational and neuroethological perspective},
 year = {2007}
}

@Filtered Article{90b2f2f2-9f41-4caa-a851-45a8eb7693d1,
 abstract = {A conventional collaborative beamforming (CB) system suffers from high sidelobes due to the random positioning of the nodes. This paper introduces a hybrid metaheuristic optimization algorithm called the Particle Swarm Optimization and Gravitational Search Algorithm-Explore (PSOGSA-E) to suppress the peak sidelobe level (PSL) in CB, by the means of finding the best weight for each node. The proposed algorithm combines the local search ability of the gravitational search algorithm (GSA) with the social thinking skills of the legacy particle swarm optimization (PSO) and allows exploration to avoid premature convergence. The proposed algorithm also simplifies the cost of variable parameter tuning compared to the legacy optimization algorithms. Simulations show that the proposed PSOGSA-E outperforms the conventional, the legacy PSO, GSA and PSOGSA optimized collaborative beamformer by obtaining better results faster, producing up to 100% improvement in PSL reduction when the disk size is small.},
 authors = {['S. Jayaprakasam', 'S.K.A. Rahim', 'Chee Yen Leow']},
 journal = {Applied Soft Computing},
 keywords = {['Collaborative beamforming', 'Random array', 'Sidelobe suppression', 'Particle swarm optimization (PSO)', 'Gravitational search algorithm (GSA)']},
 title = {PSOGSA-Explore: A new hybrid metaheuristic approach for beampattern optimization in collaborative beamforming},
 year = {2015}
}

@Filtered Article{9123e54a-218b-40bf-a086-9d31b61d1eda,
 abstract = {Professor Heather J. Kulik is a professor in chemical engineering and chemistry at MIT. She received her BE in chemical engineering from the Cooper Union in 2004 and her PhD from the Department of Materials Science and Engineering at MIT in 2009. She completed postdocs at Lawrence Livermore and Stanford prior to joining MIT as a faculty member in 2013. Her research in computational inorganic chemistry has been recognized by an ONR YIP, a DARPA Director’s fellowship, an NSF CAREER Award, a Sloan Fellowship, an AIChE CoMSEF Impact Award, and a Hans Fischer Senior Fellowship from TU Munich, among others.},
 authors = {['Heather J. Kulik']},
 journal = {Chem},
 title = {Reaction: The challenge of open-shell transition metal catalysis in “systems chemistry”},
 year = {2024}
}

@Filtered Article{913a5e3b-6c8f-4832-b69f-e057d562214e,
 abstract = {This paper reports two studies that examined the impact of early algebra learning and teachers’ beliefs on U.S. and Chinese students’ thinking. The first study examined the extent to which U.S. and Chinese students’ selection of solution strategies and representations is related to their opportunity to learn algebra. The second study examined the impact of teachers’ beliefs on their students’ thinking through analyzing U.S. and Chinese teachers’ scoring of student responses. The results of the first study showed that, for the U.S. sample, students who have formally learned algebraic concepts are as likely to use visual representations as those who have not formally learned algebraic concepts in their problem solving. For the Chinese sample, students rarely used visual representations whether or not they had formally learned algebraic concepts. The findings of the second study clearly showed that U.S. and Chinese teachers view students’ responses involving concrete strategies and visual representations differently. Moreover, although both U.S. and Chinese teachers value responses involving more generalized strategies and symbolic representations equally high, Chinese teachers expect 6th graders to use the generalized strategies to solve problems while U.S. teachers do not. The research reported in this paper contributed to our understanding of the differences between U.S. and Chinese students’ mathematical thinking. This research also established the feasibility of using teachers’ scoring of student responses as an alternative and effective way of examining teachers’ beliefs.},
 authors = {['Jinfa Cai']},
 journal = {The Journal of Mathematical Behavior},
 title = {Why do U.S. and Chinese students think differently in mathematical problem solving?: Impact of early algebra learning and teachers’ beliefs},
 year = {2004}
}

@Filtered Article{9151de69-b668-4b71-b4d6-06f1409d4404,
 abstract = {The last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then. The study of animal behavior or behavior biology has been one of the major contributors for this convergence. Behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves, the sensory and the motor systems. To some extent, behavior is similar to the output (or response) of a computer system or a network node if we consider an animal brain as a computer node. This paper is the first in a two-part series in which I review the state-of-the-art research in behavior biology inspired computing and communication, with the first part focusing on animal cognition and the second part on animal communication (Ma, 2014). The present article also assumes the task of presenting a general introduction on behavior biology literature, which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series. I sets three objectives in this ‘cognition’ part: (i) to present a brief overview on the literature of behavior biology for computer scientists; (ii) to summarize the state-of-the-art studies in several cognitive aspects of animal behavior: focusing on emerging research in cognitive ecology, social learning and innovation, as well as animal logics; (iii) to review some important existing studies inspired by animal behavior and further present a perspective on the future research. These cognition-related topics offer insights for research fields such as machine learning, human computer interactions (HCI), brain computer interfaces (BCIs), evolutionary computing, pervasive computing, etc. In perspective, I suggest that the interaction between behavioral biology and computer science should be bidirectional, and a new subject, behavioral informatics, or more general computational behavior biology, should be developed by the cooperative efforts between biologists and computer scientists.},
 authors = {['Zhanshan (Sam) Ma']},
 journal = {Cognitive Systems Research},
 keywords = {['Animal cognition', 'Cognitive ecology', 'Social learning', 'Bioinspired computing and communication', 'Behavioral informatics', 'Computational behavior biology']},
 title = {Towards computational models of animal cognition, an introduction for computer scientists},
 year = {2015}
}

@Filtered Article{91871868-81be-41fc-961c-1842095681f6,
 abstract = {Acquisition of knowledge must be interwoven with the process of applying it. However, traditional training methods which provide abstract knowledge have shown ineffective for gaining experience of the work. In order to solve this problem, more and more researchers have included narrative in simulation, which is known as narrative simulation. By providing the narratives, participants recognize the choices, decisions, and experience that lead to the consequences of those decisions. It has been proven that narrative simulation is very useful in facilitating in-depth learning and reflective learning. However, conventional methods of data collection and narrative construction for narrative simulation are labor intensive and time consuming. They make use of previous narratives manually and directly. They are inadequate to cope with the fast moving world where knowledge is changing rapidly. In order to provide a way for facilitating the construction of narrative simulation, a novel computational narrative construction method is proposed. By incorporating technologies of knowledge-based system (KBS), computational linguistics, and artificial intelligence (AI), the proposed method provides an efficient and effective way for collecting narratives and automating the construction of narratives. The method converts the unstructured narratives into a structural representation for abstraction and facilitating computing processing. Moreover, it constructs the narratives that combine multiple narratives into a single narrative by applying a forecasting algorithm. The proposed method was successfully implemented in early intervention in mental health care of a social service company in Hong Kong since the case records in that process have structural similarities to narrative. The accuracies of data conversion and predictive function were measured based on recall and precision and encouraging results were obtained. High recall and precision are achieved in the data conversion function, and high recall for the predictive function when new concepts are excluded. The results show that it is possible for converting multiple narratives into a single narrative automatically. Based on the approach, it helps to stimulate knowledge workers to explore new problem solving methods so as to increase the quality of their solutions.},
 authors = {['W.M. Wang', 'C.F. Cheung', 'W.B. Lee', 'S.K. Kwok']},
 journal = {Expert Systems with Applications},
 keywords = {['Narrative construction', 'Knowledge management', 'Concept mapping', 'Knowledge-based systems', 'Computational', 'Reflective learning', 'Narrative simulation']},
 title = {A computational narrative construction method with applications in organizational learning of social service organizations},
 year = {2009}
}

@Filtered Article{91e013a0-7fc6-4463-b76d-55d18ba82163,
 abstract = {This report describes a computational method developed to predict systemic exposure (s-exposure), chemical disposition {(CD) intestinal absorption, transport, membrane permeability, distribution, sequestration, phospholipidosis and toxicokinetics} of organic chemicals in humans. The method qualitatively and quantitatively estimates a chemical's CD activity profile based upon computed molecular descriptor properties (descriptors), and it facilitates in silico signal-detection of data-gaps, prioritization, risk-ranking, read-across, and re-assessments (if mandated) of large sets of chemicals in a safety evaluation setting. The investigation used a reference set of 2372 marketed human pharmaceuticals to define decision rules for an optimal chemical space (OCS) in which chemicals have high s-exposure, good CD, and a potential for chemical toxicity (CT); conversely, chemicals outside the OCS have low s-exposure, poor CD into the body, and low potential for CT. The method requires computation of 29 descriptors, identification of OCS molecular descriptor property violations (descriptor_violations), and alignment of descriptor_violations with specific decision rules for individual CD endpoint activities. The investigation predicted the CD activities of food and cosmetic preservatives, ingredients in GRAS (generally recognized as safe). Notices submitted to the FDA, reference pharmaceuticals, and it provides prioritization metrics and indices that facilitate prioritization of chemical in silico computed CD activities.},
 authors = {['Edwin John Matthews']},
 journal = {Computational Toxicology},
 keywords = {['Absorption', 'Bioavailability', 'Chemical disposition', 'Data-gaps', 'Distribution', 'Food ingredient', 'GRAS', 'Hazard identification', '', 'OCS (optimal chemical space)', 'Pharmacokinetics', 'Physicochemical property', 'Preservative', 'Prioritization', 'QSAR', 'QSPR', 'Read-across', 'Risk-ranking', 'Sequestration', 'Signal-detection', 'Toxicokinetics']},
 title = {Introducing a computational method to estimate and prioritize systemic body exposure of organic chemicals in humans using their physicochemical properties},
 year = {2019}
}

@Filtered Article{927c9996-cfda-42e6-9426-76fe5f2714b3,
 abstract = {In everyday life the use of vehicles has expanded immensely for some ventures and house hold applications, likewise the running time of engine cycle is exceptionally long. Thus because of the consistent running enormous measure of heat is produced. At the point when this heat isn't appropriately disseminated, the engine gets more fragile very soon and life of the engine declines because of the heat development. To build the life of the engine, heat dispersal is expanded by giving fins at external of engine chamber. The shape of the fins and the material used for the fin increases its heat dissipation capacity and in turn increases the cooling of the engine for proper functioning. The present work focuses on the design of fins of circular and tapered shapes for a 2-stroke engine. The temperature distribution and the heat dissipation along the fin surface of two shapes has been observed by a steady state thermal analysis. Alusil and Silumin has been selected as the fin materials and a computational evaluation has also been done using FEM. A better shape of the fin along with a suitable material has been selected based on the results observed by FEM and on comparison with the existing shape and material of the fin.},
 authors = {['P.L. Rupesh', 'K. Raja', 'N.V. {Sai Deepak Raj}', 'M. {Pruthviraj Bharmal}', 'Pandey {Aditya Ramjatan}']},
 journal = {Materials Today: Proceedings},
 keywords = {['Two stroke engine', 'Fin', 'Circular fin', 'Tapered fin', 'Silumin', 'Thermal Conductivity']},
 title = {Computational investigation of heat transfer on the surface of engine cylinder with fins of different shapes and materials},
 year = {2021}
}

@Filtered Article{92e3adb5-8ec7-4cdd-8d13-9786a10629fc,
 abstract = {Our work investigates how Making may be used in the context of scientific modeling in formal elementary school science classes. This paper presents an investigation of fourth- and fifth-grade students engaging in Making activities to create simulation, concept-process, and illustrative models in the science classroom. Based on video analyses of the Making-based class sessions, a generalized process model was developed for each type of science model. In addition, cross-cutting themes were found in Making-based science modeling: first, there are two loops that intersect and interact with each other (modeling for Making and modeling for Science content), and they interrelate in various ways depending on science model type; and second, showcasing Making products (sharing with peers, teachers, or helpers) is a primary factor that determines students’ overall engagement with science in the activity. We suggest that Making-based science kit and lesson design needs to support students to showcase their Making output, on top of science-related reflections, and to consider the balance between Making and science activity. We conclude that Making has the potential to support the development of scientific model thinking in the elementary science classroom, but much further research is needed in this area.},
 authors = {['Sharon Lynn Chu', 'Elizabeth Deuermeyer', 'Francis Quek']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['Making', 'Maker movement', 'Children', 'Science', 'Science models', 'Scientific modeling', 'Model thinking', 'Electronics', 'Programming']},
 title = {Supporting scientific modeling through curriculum-based making in elementary school science classes},
 year = {2018}
}

@Filtered Article{92faf01e-430a-496a-a8f6-1d24e75b5977,
 abstract = {The impact of metacognition on pupils' moral ideals and emotional development was investigated as well as it highlights on a collaborative research between metacognition and artificial intelligence that can bridge the gap (emotional, ethical, moral reasoning, common sense) existing in AI. A total of 200 pupils were selected in the study's sample. Participants (100 high metacognitive students and 100 low metacognitive students) were chosen at random and ranged in age from 17 to 21 years old. The influence of metacognition on students' moral ideals and emotional development was studied using a t-test. The outcome reveals that the mean score of moral reasoning on high metacognitive students as 66.77 and for low metacognitive students as 63.08, t value = 3.21, at the 0.01 level, statistically highly significant. The mean emotional maturity score for high metacognitive students was 29.99, while for low metacognitive students was 33.01, t value as 2.81, shows statistically significant at the 0.05 level. This demonstrates that the higher the score, the less emotionally stable the pupils are. The current findings show that metacognitive thinking has a major impact on moral reasoning and emotional maturity, and that as metacognition levels rise, so do moral reasoning and emotional maturity. Metacognition can strengthen the humanistic qualities which are majorly lacking in AI. In addition, there are new avenues being opened in the study of artificial intelligence via metacognitive study which is significant and futuristic.},
 authors = {['Sunder Kala Negi', 'Yaisna Rajkumari', 'Minakshi Rana']},
 journal = {Neuroscience Informatics},
 keywords = {['Metacognitive thinking', 'Moral reasoning', 'Emotional maturity', 'Artificial intelligence']},
 title = {A deep dive into metacognition: Insightful tool for moral reasoning and emotional maturity},
 year = {2022}
}

@Filtered Article{932dabc8-0885-49ae-9312-566058540516,
 abstract = {The traditional view of spontaneous neural activity as ‘noise’ has been challenged by recent findings suggesting that: (a) spontaneous activity in cortical populations is highly structured in both space and time, (b) the spatio-temporal structure of spontaneous activity is linked to the underlying connectivity of the cortical network, (c) spontaneous cortical activity interacts with external stimulation to generate responses to the individual presentations of a stimulus, (d) network connectivity is shaped in part by the statistics of natural signals and (e) ongoing cortical activity represents a continuous top-down prediction/expectation signal that interacts with incoming input to generate an updated representation of the world. These results can be integrated to provide a new framework for the study of cortical computation.},
 authors = {['Dario L Ringach']},
 journal = {Current Opinion in Neurobiology},
 title = {Spontaneous and driven cortical activity: implications for computation},
 year = {2009}
}

@Filtered Article{932e1473-6c29-41e8-a2ff-b4ddd4427a8a,
 abstract = {In this paper we report the work that jeKnowledge (Júnior Empresa da Faculdade de Ciências e Tecnologias da Universidade de Coimbra), a student-led initiative, has done in the ‘jeKnowledge academy’ courses to actively engage Portuguese high-school students in STEM education through hands-on projects based on the low-cost Arduino platform. F2F activities, based on a peer-assisted learning strategy, were complemented with tutorials and more advanced project suggestions in a blog. Pre and post surveys on students' attitudes towards programming and peer-coaching were administered to pre-university and first year college participants, finding an overall increase in the Likert scale for all the programming-related constructs under study (confidence, interest, gender, usefulness and professional) after the introductory course. As regards the peer-based learning approach, younger students seemed to be more eager to be taught in a less formal way than their older counterparts. The course resulted in high degrees of satisfaction for both the student tutors and their tutees.},
 authors = {['Pablo Martín-Ramos', 'Maria João Lopes', 'M. Margarida {Lima da Silva}', 'Pedro E.B. Gomes', 'Pedro S. {Pereira da Silva}', 'José P.P. Domingues', 'Manuela {Ramos Silva}']},
 journal = {Computers in Human Behavior},
 keywords = {['Attitudes survey', 'Arduino', 'High school', 'Programming', 'Peer coaching']},
 title = {First exposure to Arduino through peer-coaching: Impact on students' attitudes towards programming},
 year = {2017}
}

@Filtered Article{935a2e61-3b2d-49e6-8d13-775bb32f30cf,
 abstract = {This study focuses on the low-power Tegra X1 System-on-Chip (SoC) from the Jetson Nano Developer Kit, which is increasingly used in various environments and tasks. As these SoCs grow in prevalence, it becomes crucial to analyse their computational performance, energy consumption, and reliability, especially for safety-critical applications. A key factor examined in this paper is the SoC’s neutron radiation tolerance. This is explored by subjecting a parallel version of matrix multiplication, which has been offloaded to various hardware components via OpenMP, to neutron irradiation. Through this approach, this researcher establishes a correlation between the SoC’s reliability and its computational and energy performance. The analysis enables the identification of an optimal workload distribution strategy, considering factors such as execution time, energy efficiency, and system reliability. Experimental results reveal that, while the GPU executes matrix multiplication tasks more rapidly and efficiently than the CPU, using both components only marginally reduces execution time. Interestingly, GPU usage significantly increases the SoC’s critical section, leading to an escalated error rate for both Detected Unrecoverable Errors (DUE) and Silent Data Corruptions (SDC), with the CPU showing a higher average number of affected elements per SDC.},
 authors = {['Jose M. Badia', 'German Leon', 'Mario Garcia-Valderas', 'Jose A. Belloch', 'Almudena Lindoso', 'Luis Entrena']},
 journal = {Sustainable Computing: Informatics and Systems},
 keywords = {['Heterogeneous parallelism', 'System-on-Chip', 'Fault tolerance', 'Energy consumption', 'Neutron irradiation']},
 title = {Analysing the radiation reliability, performance and energy consumption of low-power SoC through heterogeneous parallelism},
 year = {2024}
}

@Filtered Article{939d6be1-5d4d-4ce2-b411-2c3c6f3111d8,
 abstract = {Self-regulative behaviors are dynamic and evolve as a function of time and context. However, dynamical fluctuations in behaviors are often difficult to measure and therefore may not be fully captured by traditional measures alone. Utilizing system log data and two novel statistical methodologies, this study examined emergent patterns of controlled and regulated behaviors and assessed how variations in these patterns related to individual differences in prior literacy ability and target skill acquisition. Conditional probabilities and Entropy analyses were used to examine nuanced patterns manifested in students’ interaction choices within a computer-based learning environment. Forty high school students interacted with the game-based intelligent tutoring system iSTART-ME, for a total of 11 sessions (pretest, 8 training sessions, posttest, and a delayed retention test). Results revealed that high and low reading ability students differed in their patterns of interactions and the amount of control they exhibited within the game-based system. However, these differences converged overtime along with differences in students’ performance within iSTART-ME. The findings from this study indicate that individual differences in students’ prior reading ability relate to the emergence of controlled and regulated behaviors during learning tasks.},
 authors = {['Erica L. Snow', 'G. Tanner Jackson', 'Danielle S. McNamara']},
 journal = {Computers in Human Behavior},
 keywords = {['Intelligent tutoring systems', 'Individual differences', 'Self-regulated learning', 'Agency', 'Log data', 'Dynamic analyses']},
 title = {Emergent behaviors in computer-based learning environments: Computational signals of catching up},
 year = {2014}
}

@Filtered Article{93e4987e-2ee2-4f80-bf49-b30040ed965e,
 abstract = {Food systems and our ability to secure food and nutrition for current and future generations is challenged by population growth, climate change, resource depletion and pollution. The current agricultural and supply chain systems are one of the main contributors to the issues. Transformational, not incremental change is needed to transition to sustainable food systems capable of feeding close to 10 billion people in less than 30 years. Artificial intelligence (AI) is pervading all parts of food systems in ways that indicate transformative system changes are possible. Designers, as mediators between people, technology and the environment have a responsibility to recognise and reflect on ways AI could bring the change needed to move to sustainable food systems. This literature review is situated at the intersection of Food systems, Design, Artificial Intelligence and Sustainability. The transdisciplinary approach reveals what exists across the disciplines, what can be done with AI to transition to sustainable food systems, how Design proposes to approach the change, and which ethical or philosophical considerations start to emerge. The discussion reflects on AI as a potential leverage point to bring changes in the system and on the designer's role in establishing the human-technology-environmental relationships. Further research and recommendations are provided.},
 authors = {['Stéphanie Camaréna']},
 journal = {Journal of Cleaner Production},
 keywords = {['Artificial intelligence', 'Design ethics', 'Transdisciplinary research', 'Design for sustainability', 'Sustainable food systems', 'Systems thinking']},
 title = {Artificial intelligence in the design of the transitions to sustainable food systems},
 year = {2020}
}

@Filtered Article{94174060-2cb9-4312-885e-29c34dae5787,
 abstract = {This paper draws a parallel between creative cognition and a multi-armed bandit problem involving learning from experience in an uncertain environment. Special emphasis is put on the optimal sequencing of divergent and convergent behavior by showing that divergence must be inhibited at one point to converge toward creative behavior so that excessive divergence is counterproductive. We test this hypothesis with a behavioral experiment, using measures of individual divergence and convergence components of creative potential in high school students. Results confirmed that a mix of divergence and convergence predicted high performance in a bandit task but not in a purely random task or in a simple repetitive task. These predictions are maintained after controlling for sex, personality, incentives, and other factors. As hypothesized, creative cognition was necessary for high performance under the appropriate conditions. However, it was not necessary to get high grades in a traditional school system.
Educational relevance statement
Relating to the goal of educators and public policies in the 21st century to make children and adolescents more creative, and schools more receptive to creative thinking, this research focuses on the creative potential and behavior of high school students. It provides an evidence-based policy argument in support of the screening and development by the educational sector of the creative potential of students.},
 authors = {['Louis Lévy-Garboua', 'Marco Gazel', 'Noémi Berlin', 'Jan Dul', 'Todd Lubart']},
 journal = {Learning and Individual Differences},
 keywords = {['Creative cognition', 'Multi-armed bandit problem', 'Education and creativity', 'Individual differences in creative potential', "Adolescents' behavior"]},
 title = {Creative cognition as a bandit problem},
 year = {2024}
}

@Filtered Article{949cf6ed-7a9d-44ec-a803-fe0dbd047e81,
 abstract = {This design-based research methodological paper is proposing a theoretical understanding in the form of STEAM + X framework that emerged from the empirical findings of implementing transdisciplinary STEAM practices featuring architecture, culture, and history. This paper shows how the proposed STEAM practices, involving creativities, to promote the integration of various disciplines with multiple cross-cultural iterations. These STEAM practices allow teachers to integrate cultural, architectural, environmental, or technological options into mathematics teaching and learning. These STEAM practices foster creativity and thinking skills in connecting disciplines in a transdisciplinary learning approach. Moreover, this paper introduces the study outcomes including the developed design principles and a framework that connects the underlying theoretical framework with emerging themes from our qualitative data analysis.},
 authors = {['Shereen El Bedewy', 'Zsolt Lavicza']},
 journal = {Thinking Skills and Creativity},
 keywords = {['STEAM', 'Design-based research', 'Culture', 'Technology', 'Design principles']},
 title = {STEAM + X - Extending the transdisciplinary of STEAM-based educational approaches: A theoretical contribution},
 year = {2023}
}

@Filtered Article{94b1cd3a-e00c-41bb-bfa0-712ca7afad68,
 abstract = {In this chapter on graduate and professional education, we explore doctoral and professional programs, posing a number of key questions you should ask yourself. Where to apply to graduate school or a postdoc, and why? With whom should you work? A PhD or a PharmD? What must you do to be successful? Moreover, we touch on traits important to becoming an independent researcher and ask whether success in graduate school or a postdoctoral fellowship requires different skills than undergraduate degrees. Critical thinking habits underpin this discussion. We outline possible career choices—jobs!—touching on the knowledge and expertise used by drug hunters, and also ask what might be of most value to potential employers. Each of us is different, and what’s best for you is something you will have to decipher, but hopefully you will consult with family, friends, and advisors or mentors before making a final decision. Regardless, “the big leap” is coming, so get ready.},
 authors = {['Susan Miller', 'Walter Moos', 'Barbara Munk', 'Stephen Munk', 'Charles Hart', 'David Spellmeyer']},
 journal = {Woodhead Publishing},
 keywords = {['Basic/applied/clinical', 'Career/job', 'Collaboration/teams', 'Critical thinking', '-index', 'PhD/PharmD', 'Postdoc/postdoctoral', 'Problem identification', 'Research design', 'Writing/publishing']},
 title = {Chapter 9 - Doctoral and professional programs},
 year = {2023}
}

@Filtered Article{94c10bb7-69a8-46dc-aabc-fa3d11e1d69a,
 abstract = {Emotion regulation, or the ability to regulate one’s own and other people’s emotions, is an important skill for human beings, enabling them to function adequately in their social environment. The development of computational models of emotion regulation opens up a range of interesting applications in human–computer interaction, varying from virtual characters to simulation-based training systems. To provide more insight in the underlying mechanisms as well as the application areas of computational emotion regulation models, the current chapter provides an overview of the state-of-the-art in this area. After briefly reviewing the psychological literature on emotion generation and regulation, I will explain how these phenomena can be formalized into computational models. Next, a computational model of emotion regulation is presented in detail, and a number of resulting simulation runs are shown. The chapter concludes with a discussion of potential applications of such models.},
 authors = {['Tibor Bosse']},
 journal = {Academic Press},
 keywords = {['emotion regulation', 'computational modeling', 'dynamics', 'virtual characters', 'simulation-based training']},
 title = {Chapter 13 - On Computational Models of Emotion Regulation and Their Applications Within HCI},
 year = {2017}
}

@Filtered Article{94e1f2e6-ee87-4bad-aed2-2d19b5173377,
 abstract = {In this study, hydrogen storage properties of the B24N24 and Al24N24 nanocages have been computationally investigated by the DFT method whose suitability was determined with a thorough methodological analysis. This analysis includes comparison of the performances of a number of DFT functionals against the CCSD(T) method for the determination of the best DFT method that is able to accurately model H2-BN and H2-AlN systems. The ɷB97X-D, B3LYP-D2, PBEPBE-D2, BHandH methods produced results close to that of the reference CCSD(T) method. Of all methods studied, ɷB97X-D, showing the best performance, is found to be the most appropriate DFT method for H2-B24N24 and Al24N24 systems including dispersive interactions between hydrogen and the host molecule. The ɷB97X-D calculations result in that H2 molecule make the tightest adsorptive bond with Al atom in Al24N24 having an adsorption energy of −0.116 eV, by forming much more stable complex than the H2-B24N24 one. This indicates that Al24N24 has better exohedral hydrogen storage properties. The calculations also revealed that H2 molecules cannot pass through hexagonal rings of B24N24 instead they chemisorb on the cage atoms by breaking BN bond while they can pass through hexagonal rings of Al24N24 without making any damage in the Al–N bond, leading the fact that the Al–N bond is stronger than the B–N bond. Moreover, endohedral addition of H2 molecules up to three can form thermodynamically stable nH2@Al24N24 complexes while endohedral hydrogen addition to B24N24 destabilizes the complexes. Thus, the Al24N24 nanocage is not only structurally more stable than B24N24 nanocage, but also it can accommodate more hydrogen molecules, so it is better candidate for both endohedrally and exohedrally hydrogen storage compared to B24N24.},
 authors = {['Sinan Sayhan', 'Armağan Kinal']},
 journal = {International Journal of Hydrogen Energy},
 keywords = {['Boron nitride nanocages', 'Aluminum nitride nanocages', 'Hyrdogen storage materials', 'BN', 'AlN', 'The DFT methods']},
 title = {Computational investigation and comparison of hydrogen storage properties of B24N24 and Al24N24 nanocages},
 year = {2017}
}

@Filtered Article{953ca31b-46da-4db8-a9a7-2285482f3633,
 abstract = {Computer Science Curriculum 2013 has become the guidance of computing education since it was released in 2013by the ACM/IEEE-Computer Society. This paper analyzes the CS curriculum development trend, trying to dig the programming-related core from CS2013 with respect to the knowledge areas, topics, organization of teaching, and the building of students’ capability. Considering the characteristic of our local institution and undergraduates, we present an updated teaching curriculum and lab curriculum for C Programming Language course in relation to CS2013 recommendations, which highlight the development of the students’ abilities on programming, problem-solving, self-regulated learning, and computational thinking. Finally, we present and assess the implementation of the resulting curriculum.},
 authors = {['Lingling Zhao', 'Xiaohong Su', 'Tiantian Wang']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['CS2013', 'C programming course', 'CS curriculum planning', 'CS major ;']},
 title = {Bring CS2013 Recommendations into c Programming Course},
 year = {2015}
}

@Filtered Article{957a1f69-66b3-4fef-a801-5c6fd6d7eed9,
 abstract = {Like my previous paper in this journal this commentary is focused on government statements published during the period 1974 to 1979. It is intended as an introductory guide aimed at two overlapping audiences. First, it is addressed to those interested in the reasoning which lies behind the Government's technical arguments on energy conservation in buildings. Secondly, it is directed towards those who seek to understand the social implications and consequences of this area of government endeavour. Not all the statements examined in this commentary represent official expressions of government policy. Some, indeed, are prefaced in their originals by specific disclaimers to this effect. Rather, they should be read as examples of arguments voiced by a variety of individuals and groups who are capable of informing, influencing or making decisions that affect this field of government activity. It should not be supposed that the government statements brought together in this commentary are necessarily consistent or coherent. Instead, in some cases at least, they seem incompatible and may even be irreconcilable. But, given that the source material is drawn from a wide range of documents with a broad range of authors and was published over a number of years, the extent of their unanimity is remarkable. As an introductory guide, this commentary is not offered as exhaustive, as representative of all aspects or shades of government thinking on this subject. On the contrary, only statements published in documents emanating from, or associated with, the Department of Energy have, for the most part, been cited. For the sake of brevity, statements published by other government departments with responsibility for the conservation of energy in buildings—such as the Department of the Environment—have not been drawn upon.},
 authors = {['Ian Cooper']},
 journal = {Applied Energy},
 title = {Energy conservation in buildings: Part 2-A commentary on British government thinking},
 year = {1982}
}

@Filtered Article{95a8762f-6d78-478c-bf75-4f0862830275,
 abstract = {Online forums afford individuals opportunities to take part in a community with shared interests and goals. This involves the sharing of experiences and advice (Attard and Coulson, 2012) and can lead to positive effects (Pendry and Salvatore, 2015). Online forums also afford access to rich sources of detailed data, personal experiences, and hard-to-reach or taboo communities. Such online research, though well-suited to qualitative analysis, leads to a number of practical problems in terms of range, depth, and ease of access to data. Even extensive data collection and manual analysis often only engage with a small percentage of the data available in online communities. In this article, we present a traditional manual collection and thematic analysis of data (2631 posts across 60 different threads, approximately 300,000 words) from forums where sex workers and men who pay for sex discuss matters relating to prostitution. This analysis revealed five themes of forum use: preference sharing, personal narrative sharing, practical advice, philosophical issues, and community maintenance. Further automated data collection and corpus analysis, such as keyness and topic modelling, are presented as a potential innovation within online qualitative research. This approach allowed for the analysis of a larger dataset of 255,891 posts, across 14,232 threads (16,472,006 words), revealing additional themes such as sexual hygiene, desire, legality, and ethnicity, as well as differences in the use of terms of address and slang by punters and sex workers. The automated methods presented allow for more comprehensive investigations of online communities than traditional approaches, but we also note that manual interpretation should still be incorporated into the analysis.},
 authors = {['Pelham Carter', 'Matt Gee', 'Hollie McIlhone', 'Harkeeret Lally', 'Robert Lawson']},
 journal = {Methods in Psychology},
 keywords = {['Sex work', 'Online forums', 'Language', 'gender and sexuality', 'Mixed methods', 'Corpus linguistics']},
 title = {Comparing manual and computational approaches to theme identification in online forums: A case study of a sex work special interest community},
 year = {2021}
}

@Filtered Article{95aaf1dd-1630-4947-aa05-042ac4dcc407,
 abstract = {Intramolecular net [2 + 2] cycloadditions between benzyne intermediates and an electron-deficient alkene to give benzocyclobutene intermediates are relatively rare. Benzynes are electrophilic and generally engage nucleophiles or electron-rich π-systems. We describe here reactions in which an alkene of a tethered enone traps thermally generated benzynes in a variety of interesting ways. The number of atoms that link the benzyne to C4 of a cyclohexa-2,5-dienone induces varying amounts of strain in the intermediates and products. This leads to a variety of different reaction outcomes by way of various strain-releasing events that are mechanistically intriguing. This work demonstrates an underappreciated class of strain that originates from the adjacent fusion of two rings to both C1–C2 and C2–C3 of a benzenoid ring – i.e. ‘ortho-annulation strain’. DFT computations shed considerable light on the mechanistic diversions among various reaction pathways as well as allow more fundamental evaluation of the strain in a homologous series of ortho-annulated carbocycles.},
 authors = {['Bhavani Shankar Chinta', 'Dorian S. Sneddon', 'Thomas R. Hoye']},
 journal = {Chemical Science},
 title = {Cascade reactions of HDDA-benzynes with tethered cyclohexadienones: strain-driven events originating from ortho-annulated benzocyclobutenes††Electronic supplementary information (ESI) available. CCDC 2302618–2302621. For ESI and crystallographic data in CIF or other electronic format see DOI: https://doi.org/10.1039/d4sc00571f},
 year = {2024}
}

@Filtered Article{95f91256-3be1-4dd2-be8f-787dc4278633,
 abstract = {Habitual behaviors significantly shape our daily actions. Furthermore, habit formation is proposed as a key mechanism contributing to the development and maintenance of addiction. However, the neural substrates underlying daily habitual tendencies and their contribution to behavioral addiction symptoms in everyday life remain poorly understood. To explore these questions, we conducted a comprehensive analysis of data from 219 individuals who underwent neuroimaging (structural MRI) assessments alongside evaluations of their daily habitual tendencies and symptoms of Internet Gaming Disorder (IGD) and Problematic Smartphone Use (PSU). Using voxel-based morphometry, meta-analytic decoding, and mediation analysis, we found that daily habitual tendencies were positively correlated with larger gray matter volumes in the ventromedial prefrontal cortex (vmPFC), precuneus, superior frontal gyrus (SFG), inferior temporal gyrus (ITG), and supplementary motor area (SMA). Notably, the midline regions, including the vmPFC and precuneus, play a crucial role in value-based computation, emotional regulation, social cognition, and self-referential thinking. Individual variations in gray matter volumes within these regions served as mediators, influencing the bidirectional relationship between daily habitual tendencies and IGD symptoms. However, vmPFC variations were specifically found to mediate the pathway from PSU to daily habitual tendencies. Our findings suggest that the morphological architecture of the vmPFC and precuneus is associated with habitual tendencies in daily life and may mediate the development of addictive behaviors. This study contributes to a more nuanced understanding of the neuroanatomical basis of daily habitual tendencies and their role in addictive behaviors.},
 authors = {['Xinqi Zhou', 'Qi Liu', 'Lan Wang', 'Xianyang Gan', 'Ran Zhang', 'Xiqin Liu', 'Guojuan Jiao', 'Christian Montag', 'Weihua Zhao', 'Benjamin Becker']},
 journal = {Computers in Human Behavior},
 keywords = {['Habit', 'Gray matter', 'vmPFC', 'Precuneus', 'Internet gaming disorder', 'Smartphone use']},
 title = {The neuroanatomical correlates of daily habitual tendencies and mediating effect on the association between daily habitual tendencies and symptoms of behavioral addictions},
 year = {2024}
}

@Filtered Article{96ca006d-55d0-4b22-accc-ed6c9fea6f39,
 abstract = {This study demonstrates a successful use of Generative Artificial Intelligence (AI) in teaching mathematical material to management students. We herein introduce the EOQ World Tour game, which substantially improves understanding of inventory-related concepts and long-term knowledge retention compared with traditional methods. Generative AI is revolutionizing management education, by offering innovative methods for teaching and learning. The integration of AI into quantitative business disciplines through novel learning mechanisms provides significant benefits, including enhanced data analysis, improved decision-making models, and sophisticated simulations for hands-on experience. This study introduces the EOQ World Tour game, specifically designed to teach the Economic Order Quantity concept in Operations Management. The game addresses challenges in integrating Generative AI into mathematics in management education by combining human oversight and instructor control through three innovative features: (1) a Generative AI-based simulation, (2) a macropowered Excel worksheet for validating the calculations of an AI chatbot, and (3) a Google Sheets dashboard for centralizing team-generated AI data for postgame analysis. Our study included 41 students divided into experimental and control groups. Pretest results indicated no significant differences in baseline knowledge. However, the post-test results showed that the experimental group achieved a better understanding of inventory-related concepts and practical applications, along with higher engagement, excitement, confidence, and long-term knowledge retention.},
 authors = {['Maria A.M. Trindade', 'Gihan S. Edirisinghe', 'Lan Luo']},
 journal = {The International Journal of Management Education},
 keywords = {['Generative artificial intelligence in education', 'Generative AI-Driven learning', 'Mathematics in management education', 'Operations management', 'Economic order quantity', 'Generative AI in management education']},
 title = {Teaching mathematical concepts in management with generative artificial intelligence: The power of human oversight in AI-driven learning},
 year = {2025}
}

@Filtered Article{97299fa3-0c36-4b6a-a45c-518bd4ad7b89,
 abstract = {For twenty years the Nuprl (“new pearl”) system has been used to develop software systems and formal theories of computational mathematics. It has also been used to explore and implement computational type theory (CTT)—a formal theory of computation closely related to Martin-Löf's intuitionistic type theory (ITT) and to the calculus of inductive constructions (CIC) implemented in the Coq prover. This article focuses on the theory and practice underpinning our use of Nuprl for much of the last decade. We discuss innovative elements of type theory, including new type constructors such as unions and dependent intersections, our theory of classes, and our theory of event structures. We also discuss the innovative architecture of Nuprl as a distributed system and as a transactional database of formal mathematics using the notion of abstract object identifiers. The database has led to an independent project called the Formal Digital Library, FDL, now used as a repository for Nuprl results as well as selected results from HOL, MetaPRL, and PVS. We discuss Howe's set theoretic semantics that is used to relate such disparate theories and systems as those represented by these provers.},
 authors = {['S.F. Allen', 'M. Bickford', 'R.L. Constable', 'R. Eaton', 'C. Kreitz', 'L. Lorigo', 'E. Moran']},
 journal = {Journal of Applied Logic},
 keywords = {['Martin-Löf type theory', 'Dependent intersection types', 'Union types', 'Polymorphic subtyping', 'Logic of events', 'Formal digital libraries', 'Computational type theory', 'Proofs as programs', 'Program extraction', 'Tactics']},
 title = {Innovations in computational type theory using Nuprl},
 year = {2006}
}

@Filtered Article{97de8a19-3759-4a97-8355-3b23e7f8fc05,
 abstract = {Sustainable development poses a grand challenge for society, addressed by organisations through their public relations activities. Grand challenges are complex by nature and call for nontrivial solutions whose effects show at the level of society. That is why studying public relations’ contribution to grand challenges requires a macro perspective that accounts for the dynamic interaction between individual, organisational, and system levels in a digital communication environment. This paper offers a new paradigm to analyse organisations’ significant and at times undue impact on grand challenges through public relations. It develops a framework inspired by complex adaptive systems thinking and adopts its ten properties for public relations: emergence, adaptivity, heterogeneous actors, nonlinear effects, feedback mechanisms, self-organisation, phase transitions, networks, scaling, and cooperation. The paper applies the framework to the example of sustainable development. It shows why research on grand challenges requires a holistic perspective and how it can help study digitally born communication phenomena. The proposed complex systems paradigm provides space for critical, social scientific, and interpretative research lines in public relations. Inquiries start from the grand challenge and study the communicative interactions between organisations and other actors from existing theory while accounting for the ten properties of complex adaptive systems. The paper outlines how future research can enrich the study of public relations and discusses its limits.},
 authors = {['Irina Lock']},
 journal = {Public Relations Review},
 keywords = {['Grand challenges', 'Public issues', 'Public relations', 'Strategic communication', 'Complex adaptive systems', 'Digital communication', 'Complexity']},
 title = {Conserving complexity: A complex systems paradigm and framework to study public relations’ contribution to grand challenges},
 year = {2023}
}

@Filtered Article{97e5e05a-52e4-4ef6-ba4e-13de81127bae,
 abstract = {The Fisher scoring and Gauss–Newton methods are two known methods for maximum likelihood computation. This paper provides a generalization for each method in a unified manner so that they can be used for some difficult maximum likelihood computation, when, for example, there exist constraints on the parameters. A generalized method does not use directly the Newton-type iteration formulas of these methods, but, instead, uses the corresponding quadratic functions transformed from them. It proceeds by repeatedly approximating the log-likelihood function with the quadratic functions in the neighborhoods of the current iterates and optimizing each quadratic function within the parameter space. It is shown that each quadratic function has a weighted linear regression formulation, which can be conveniently solved. This generalization also extends the applicability of the Fisher scoring method to situations when the expected Fisher information matrices are unavailable in closed form. Fast computation can generally be anticipated, owing to their small rates of convergence and a rapid solution of each linear regression problem. While the generalized Gauss–Newton method may sometimes suffer for the so-called large residual problem, the generalized Fisher scoring method has performed consistently well in the numerical experiments we conducted.},
 authors = {['Yong Wang']},
 journal = {Computational Statistics & Data Analysis},
 keywords = {['Maximum likelihood computation', 'Fisher scoring', 'Gauss–Newton method', 'Constrained optimization', 'Iteratively reweighted least-squares']},
 title = {Maximum likelihood computation based on the Fisher scoring and Gauss–Newton quadratic approximations},
 year = {2007}
}

@Filtered Article{9808e520-721b-430d-a1a4-27f915bc291e,
 abstract = {Summary
Creativity is a highly valued and beneficial skill that empirical research typically probes using “divergent thinking” (DT) tasks such as problem solving and novel idea generation. Here, in contrast, we examine the perceptual aspect of creativity by asking whether creative individuals are more likely to perceive recognizable forms in ambiguous stimuli –a phenomenon known as pareidolia. To this end, we designed a visual task in which participants were asked to identify as many recognizable forms as possible in cloud-like fractal images. We found that pareidolic perceptions arise more often and more rapidly in highly creative individuals. Furthermore, high-creatives report pareidolia across a broader range of image contrasts and fractal dimensions than do low creatives. These results extend the established body of work on DT by introducing divergent perception as a complementary manifestation of the creative mind, thus clarifying the perception-creation link while opening new paths for studying creative behavior in humans.},
 authors = {['Antoine Bellemare-Pepin', 'Yann Harel', 'Jordan O’Byrne', 'Geneviève Mageau', 'Arne Dietrich', 'Karim Jerbi']},
 journal = {iScience},
 keywords = {['Cognitive neuroscience', 'Social sciences', 'Psychology']},
 title = {Processing visual ambiguity in fractal patterns: Pareidolia as a sign of creativity},
 year = {2022}
}

@Filtered Article{980a0961-1859-42c0-a3f2-e1dcb0688573,
 abstract = {Summary
This article is about the neural conundrum behind the slowness of human behavior. The information throughput of a human being is about 10 bits/s. In comparison, our sensory systems gather data at ∼109 bits/s. The stark contrast between these numbers remains unexplained and touches on fundamental aspects of brain function: what neural substrate sets this speed limit on the pace of our existence? Why does the brain need billions of neurons to process 10 bits/s? Why can we only think about one thing at a time? The brain seems to operate in two distinct modes: the “outer” brain handles fast high-dimensional sensory and motor signals, whereas the “inner” brain processes the reduced few bits needed to control behavior. Plausible explanations exist for the large neuron numbers in the outer brain, but not for the inner brain, and we propose new research directions to remedy this.},
 authors = {['Jieyu Zheng', 'Markus Meister']},
 journal = {Neuron},
 keywords = {['human behavior', 'speed of cognition', 'neural computation', 'bottleneck', 'attention', 'neural efficiency', 'information rate', 'memory sports']},
 title = {The unbearable slowness of being: Why do we live at 10 bits/s?},
 year = {2025}
}

@Filtered Article{9822fc44-20d3-48ce-abb8-c9a951d9dbb9,
 abstract = {This text discusses today’s digital transformation through the lens of Horkheimer and Adornos’ study of the enlightenment. Policy and public discourse around digitalisation embrace and adhere to the narrow tenets enlightenment thinking; the idea that rationality, individual freedom, and a society free from superstition are necessary and attainable goals. The costs of what has come to be called ‘Modernity’ are many. Through the application of rationality to all spheres of life, married with disruptive technological advancement, humanity has diminished its’ imagination – its ability to seek new directions. To paraphrase Horkheimer and Adorno, Modernism fights against nature, of which we are a part, and thus, paradoxically, sets us in a fight against ourselves. Environmental degradation, the price of progress, being just one example of this – deadening work, consumerism and severed social connections being amongst others. In this framing, digitalisation itself comes to be understood itself as akin to a force of nature – one that we can do little about, other than adjust and adapt or be swept away. But this by no means a foregone conclusion, there is light at the end of the optical fibre. Albeit that recent technical developments around artificial intelligence appears to be pushing policy makers into hasty decisions, the pace of the technical development is not as fast as we believe, and in comparison with the Reformation – we have time. If we can restrain ourselves from the resist, adapt or die responses promoted in popular discourse in face of the shock of large language models and rising threat of automation, then we create room to consider economic, social, and ecological alignment and accord, in the decision making and design of future interactive artefacts and digital services. The article argues that through postdigital aesthetics, technology makers can embrace materiality and the inherent qualities of digital technology to formulate a critique of existing trajectories in digital transformation, with consequences for a more sustainable future.},
 authors = {['Rikard Lindell']},
 journal = {Futures},
 keywords = {['Dialectics', 'Digital transformation', 'Digitalisation', 'Postdigital']},
 title = {The dialectics of digitalisation: A critique of the modernistic imperative for the development of digital technology},
 year = {2024}
}

@Filtered Article{982eb760-c0af-48ad-8ba5-7a4f8024dbf3,
 abstract = {Many problems concerning the theory and technology of rhythm, melody, and voice-leading are fundamentally geometric in nature. It is therefore not surprising that the field of computational geometry can contribute greatly to these problems. The interaction between computational geometry and music yields new insights into the theories of rhythm, melody, and voice-leading, as well as new problems for research in several areas, ranging from mathematics and computer science to music theory, music perception, and musicology. Recent results on the geometric and computational aspects of rhythm, melody, and voice-leading are reviewed, connections to established areas of computer science, mathematics, statistics, computational biology, and crystallography are pointed out, and new open problems are proposed.},
 authors = {['Godfried Toussaint']},
 journal = {Computational Geometry},
 keywords = {['Musical rhythm', 'Melody', 'Voice-leading', 'Evenness measures', 'Rhythm similarity', 'Sequence comparison', 'Necklaces', 'Convolution', 'Computational geometry', 'Music information retrieval', 'Algorithms', 'Computational music theory']},
 title = {Computational geometric aspects of rhythm, melody, and voice-leading},
 year = {2010}
}

@Filtered Article{9891afbd-5258-4620-958b-b56b041f9664,
 abstract = {Two implicit finite element formulations for incompressible flows have been implemented on the Connection Machine supercomputers and successfully applied to a set of time-dependent problems. The stabilized space-time formulation for moving boundaries and interfaces, and a new stabilized velocity-pressure-stress formulation are both described, and significant aspects of the implementation of these methods on massively parallel architectures are discussed. Several numerical results for flow problems involving moving as well as fixed cylinders and airfoils are reported. The parallel implementation, taking full advantage of the computational speed of the new generation of supercomputers, is found to be a significant asset in fluid dynamics research. Its current capability to solve large-scale problems, especially when coupled with the potential for growth enjoyed by massively parallel computers, make the implementation a worthwhile enterprise.},
 authors = {['M. Behr', 'A. Johnson', 'J. Kennedy', 'S. Mittal', 'T. Tezduyar']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 title = {Computation of incompressible flows with implicit finite element implementations on the Connection Machine},
 year = {1993}
}

@Filtered Article{98d1c11a-8276-4c82-8bd2-2d37d9b88d70,
 abstract = {Publisher Summary
Lagrangian techniques deal with problems involving fast and transient loading. Lagrangian methods offer several advantages over the competition. Because Lagrangian codes cannot solve all the problems involving fast short-duration loading, other techniques shoulb be mentioned. The chapter describes the popular alternative methods, Euler codes, coupled Euler-Lagrange codes, arbitrary Lagrange-Euler (ALE) techniques, and meshless methods The advantages of Lagrange codes are offset by grid distortion. With large distortions, the time increment for advancing the computations is forced to approach zero, thus rendering the calculations uneconomical. The use of sliding interfaces and rezoning can extend the range of applicability of Lagrange codes to larger distortions. Similarly, the ability to handle large distortions in Euler codes is offset by the need to account for material transport. Pure Euler techniques are ideal for handling large distortions.},
 journal = {Elsevier},
 title = {Chapter 6 Alternatives to purely Lagrangian computations},
 year = {2004}
}

@Filtered Article{98dd7002-708b-4423-a179-4c7f6d26ef11,
 abstract = {To ensure optimal utilization and bioavailability, iron uptake, transport, subcellular localization, and assimilation are tightly regulated in plants. Herein, we examine recent advances in our understanding of cellular responses to Fe deficiency. We then use intracellular mechanisms of Fe homeostasis to discuss how formalizing cell biology knowledge via a mathematical model can advance discovery even when quantitative data is limited. Using simulation-based inference to identify plausible systems mechanisms that conform to known emergent phenotypes can yield novel, testable hypotheses to guide targeted experiments. However, this approach relies on the accurate encoding of domain-expert knowledge in exploratory mathematical models. We argue that this would be facilitated by fostering more “systems thinking” life scientists and that diversifying your research team may be a practical path to achieve that goal.},
 authors = {['Charles Hodgens', 'Belinda S. Akpa', 'Terri A. Long']},
 journal = {Current Opinion in Plant Biology},
 keywords = {['Iron homeostasis', 'Simulation-based inference (SBI)', 'Inclusivity']},
 title = {Solving the puzzle of Fe homeostasis by integrating molecular, mathematical, and societal models},
 year = {2021}
}

@Filtered Article{98ddd2bd-0978-48c0-86e4-297578ad0cf9,
 abstract = {In this paper, we explore the ideas that second grade students articulate about functional relationships. We adopt a function-based approach to introduce elementary school children to algebraic content. We present results from a design-based research study carried out with 21 second-grade students (approximately 7 years of age). We focus on a lesson from our classroom teaching experiment in which the students were working on a problem that involved a linear functional relationship (y=2x). From the analysis of students’ written work and classroom video, we illustrate two different approaches that students adopt to express the relationship between two quantities. Students show fluency recontextualizing the problem posed, moving between extra-mathematical and intra-mathematical contexts.},
 authors = {['María C. Cañadas', 'Bárbara M. Brizuela', 'Maria Blanton']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Quantities', 'Functional thinking', 'Early algebra', 'Elementary students']},
 title = {Second graders articulating ideas about linear functional relationships},
 year = {2016}
}

@Filtered Article{991616a0-e78f-4571-bfbe-9a2fca1ad022,
 abstract = {The aim of this work is to introduce a trust model, which is highly consistent with the social nature of trust in computational domains. To this end, we propose a hesitant fuzzy multi-criteria decision making based computational trust model capable of taking into account the fundamental building blocks corresponding to the concept of trust. The proposed model is capable of considering the contextuality property of trust and the subjective priorities of the trustor regarding the chosen goal. This is due to viewing trust not as a single label or an integrated concept, but as a collection of trustworthiness facets that may form the trust decision in various contexts and toward different goals. The main benefit of the proposed model is the consideration of the hesitancy of recommenders and the trustor in the process of trust decision making which can create a more flexible mapping between the social and computational requirements of trust. This type of formulation also allows for taking into account the vagueness of the provided opinions. In addition to the vagueness of the provided opinions, the model is capable of considering the certainty of recommendations and its effect on the aggregation process of gathered opinions. In the proposed model, the taste of the recommenders and the similarity of opinions are also considered. This will allow the model to assign more weight to recommendations that have a similar taste compared to the trustor. Finally, taking into consideration the attitudes of the trustors toward change of personality that may occur for various entities in the environment is another advantage of the proposed model. A step-by-step illustrative example and the results of several experimental evaluations, which demonstrate the benefits of the proposed model, are also presented in this paper.},
 authors = {['Mehrdad Ashtiani', 'Mohammad Abdollahi Azgomi']},
 journal = {Applied Soft Computing},
 keywords = {['Trust modeling', 'Hesitant fuzzy sets (HFS)', 'Comparative linguistic expressions', 'Vagueness', 'Uncertainty', 'Multi-criteria decision making (MCDM)']},
 title = {A hesitant fuzzy model of computational trust considering hesitancy, vagueness and uncertainty},
 year = {2016}
}

@Filtered Article{993a721b-f666-4942-942c-1dddab23f49e,
 abstract = {In this paper, we investigate how the impatience that results from placing a price on time impairs individuals' ability to derive happiness from pleasurable experiences. Experiment 1 demonstrated that thinking about one's income as an hourly wage reduced the happiness that participants derived from leisure time on the internet. Experiment 2 revealed that a similar manipulation decreased participants' state of happiness after listening to a pleasant song and that this effect was fully mediated by the degree of impatience experienced during the music. Finally, Experiment 3 showed that the deleterious effect on happiness caused by impatience was attenuated by offering participants monetary compensation in exchange for time spent listening to music, suggesting that a sensation of unprofitably wasted time underlay the induced impatience. Together these experiments establish that thinking about time in terms of money can influence how people experience pleasurable events by instigating greater impatience during unpaid time.},
 authors = {['Sanford E. DeVoe', 'Julian House']},
 journal = {Journal of Experimental Social Psychology},
 keywords = {['Time', 'Money', 'Impatience', 'Happiness']},
 title = {Time, money, and happiness: How does putting a price on time affect our ability to smell the roses?},
 year = {2012}
}

@Filtered Article{99a4588d-603c-4066-8b76-de102f378504,
 abstract = {Two experiments investigating information-processing consequences of entrenched and nonentrenched concepts are reported. An attempt is made to distinguish between these two kinds of concepts by using two variables—the naturalness of the occurrence described by a concept and the familiarity of the name used to refer to that occurrence. In each experiment a given conceptual system was expressed in four alternative forms by crossing concept familiarity (naturalness) with lexical familiarity. The experiments used a concept-selection task in which subjects were required to characterize an event based on a preliminary piece of information and a final, confirmatory piece of information. The results indicated that the locus of nonentrenchment lies in using a familiar name to identify an unfamiliar occurrence or in using an unfamiliar name to identify a familiar occurrence. An information-processing model of task performance provided a very good account of the latency data and scores from the concept-selection task correlated with scores from a set of psychometric reasoning tests. The distinction between entrenched and nonentrenched concepts can be interpreted in terms of interference theory, and it also has implications for the way we think about induction and human intelligence.},
 authors = {['Sheldon J Tetewsky', 'Robert J Sternberg']},
 journal = {Journal of Memory and Language},
 title = {Conceptual and lexical determinants of nonentrenched thinking},
 year = {1986}
}

@Filtered Article{99d74a89-27ff-41be-8445-d3ade085c1b3,
 abstract = {We propose a process algebra which is concerned with processes that have an implicit computational capital. This process algebra is intended to be helpful when designing computer-based systems of which the behaviour is related to money handling. It goes along with the development that the behaviour of computer-based systems, organizations and persons is increasingly more related to money handling.},
 authors = {['J.A. Bergstra', 'C.A. Middelburg']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['Process algebra', 'Implicit computational capital', 'Preservation of computational money']},
 title = {Parallel Processes with Implicit Computational Capital},
 year = {2008}
}

@Filtered Article{9a420083-e94c-47da-8b25-a00bcb7f0bb0,
 abstract = {This paper investigates the effect of strategic reasoning on financial markets with a level-k thinking framework. A level-k speculator performs k rounds of iterative reasoning to infer information from asset prices. In contrast to the static rational expectations equilibrium, the level-k framework produces a unified theory of momentum and contrarian trading strategies. Besides, this paper discusses how the distribution of sophistication levels affects several market variables and it sheds new light on empirical patterns such as: (1) overreaction of asset prices, (2) the excess volatility puzzle, and (3) the excessive trading volume puzzle. Moreover, this paper explores whether the level-k strategy converges to the rational expectations equilibrium.},
 authors = {['Hang Zhou']},
 journal = {Journal of Economic Theory},
 keywords = {['Level- thinking', "Investors' sophistication", 'Market instability']},
 title = {Informed speculation with k-level reasoning},
 year = {2022}
}

@Filtered Article{9a92f78f-5ce7-47af-937d-7f84013723ae,
 authors = {['Eugene Garfield']},
 journal = {Computer Compacts},
 title = {Artificial intelligence: Using computers to think about thinking, part I: Representing knowledge},
 year = {1984}
}

@Filtered Article{9ab90553-ee51-4f7d-9daa-e35bee7d4565,
 abstract = {The foreign language effect, or thinking in a foreign language, reduces judgment bias under uncertainty. This study investigates how language use (native versus foreign) affects accounting judgment on uncertainty expressions. We conducted two separate experiments: between-subjects and within-subjects, both of which included tasks requiring interpretations and probability estimations based on accounting standard uncertainty expressions. The results demonstrated that foreign language use affected the interpretation of uncertainty expressions and reduced judgment bias in probability estimation, particularly in the context of asset recognition. These findings have important implications for accounting research and reporting.},
 authors = {['Yuqian Zhang', 'Anura {De Zoysa}', 'Corinne Cortese']},
 journal = {Journal of International Accounting, Auditing and Taxation},
 keywords = {['Foreign language effect', 'Uncertainty expressions', 'Probability estimation', 'Accounting judgement', 'Interpretation']},
 title = {Foreign language effect in accounting uncertainty expressions: Interpretation and probabilistic estimation},
 year = {2023}
}

@Filtered Article{9abbf3e4-aa9e-47a0-85ec-4bfb87836bdb,
 abstract = {The Cognitive Neuroscience Institute held its first conference in September 1982, in Kusadasi, Turkey. The institute was recently established in New York to promote research in cognitive neuroscience, and in December 1982 it presented the Hermann von Helmholtz Award to Vernon Mountcastle (see TINS, January 1983, Vol. 6, p. 9). The meeting was attended by individuals whose specialities range from molecular biology to philosophy. Their common aim was to investigate the role of cognitive neuroscience in establishing a theory of mental processing which combines the knowledge derived from cognitive psychology and from neuroscience. How this synthesis is to be achieved, and indeed the extent to which it is possible, was the subject of wide-ranging and often vigorous debate. But, as many disciplines begin to converge on common problems, the prospects for cognitive neuroscience appear encouraging. Thus, as neuroscientists start to unravel the molecular mechanisms of learning and memory, it is interesting to consider what constraints such mechanisms might place on the operational rules for correlating single-neuron activity and behaviour in invertebrates, it has been argued that similar progress in understanding the mammalian brain will come from the application of models, derived from cognitive psychology, to neurophysiology. Artificial intelligence provides an opportunity to model many cognitive processes, but how close do the models come to reflecting underlying mental states? Indeed, the problem or non-problem of self-awareness dominated many conversations, tantalizing some participants by its intractability and accepted by others as a naturally emergent attribute of the mechanics of the mind.},
 authors = {['I.D. Thompson']},
 journal = {Trends in Neurosciences},
 title = {Thinking about thinking},
 year = {1983}
}

@Filtered Article{9ac3f7a1-c560-405c-b36b-c4841d8fbd19,
 abstract = {Understanding the fundamental organisational principles underlying the complex and multilayered process of angiogenesis is the mutual aim of both the experimental and theoretical angiogenesis communities. Surprisingly, these two fields have in the past developed in near total segregation, with neither fully benefiting from the other. However, times are changing and here we report on the new direction that angiogenesis research is taking, where from well-integrated collaborations spring new surprises, experimental predictions and research avenues. We show that several successful ongoing collaborations exist in the angiogenesis field and analyse what aspects of their approaches led them to achieve novel and impactful biological insight. We conclude that there are common elements we can learn from for the future, and provide a list of guidelines to building a successful collaborative venture. Specifically, we find that a near symbiosis of computation with experimentation reaps the most impactful results by close cyclical feedback and communication between the two disciplines resulting in continual refinement of models, experimental directions and our understanding. We discuss high impact examples of predictive modelling from the wider, more established integrated scientific domains and conclude that the angiogenesis community can do nothing but benefit from joining this brave new, integrated world.},
 authors = {['Katie Bentley', 'Martin Jones', 'Bert Cruys']},
 journal = {Experimental Cell Research},
 keywords = {['Computational modelling', 'Interdisciplinary', 'Angiogenesis', 'Prediction', 'Simulation']},
 title = {Predicting the future: Towards symbiotic computational and experimental angiogenesis research},
 year = {2013}
}

@Filtered Article{9b3042dc-8db0-4bb3-86d0-439337f019be,
 abstract = {After wrestling with the nature of the mind for over half a century, Daniel Dennett uploads his latest thinking on consciousness, word-based “mind viruses” and why we must doubt the power of artificial intelligence},
 authors = {['Anil Ananthaswamy']},
 journal = {New Scientist},
 title = {That's a termite colony between your ears},
 year = {2017}
}

@Filtered Article{9b92ee51-1634-4240-aa43-aec8f5e6a3f4,
 abstract = {Industry 4.0 represents a turning point in the thinking of the production model since it is based on digitalized production systems with the aim of improving productivity, product quality, and delivery time to the customer. The digitalization and evolution of information technology allowed the emulation of production system virtual models, namely in the concept of Digital Twin (DT), with the ability to simulate different scenarios providing support for better decision making. This concept not only represents a virtual copy of the physical world that obtains information about the state of the value chain but also illustrates a system capable of changing the development of the production activity according to the fulfillment of the intended business goals. In literature, the concept of the Digital Twin is exhaustively treated as a stand-alone factory (one digital factory represents one physical factory) and underestimates the possibility of a DT oriented to a customized product (a project) that requires decentralized production systems. This paper brings to discussion the relevance of product customized applying DT to smart customization, and the inclusion of decentralized production systems supported by Cloud Manufacturing.},
 authors = {['Hélio Castro', 'Fernando Câmara', 'Paulo Ávila', 'Luís Ferreira', 'Manuela Cruz-Cunha']},
 journal = {Procedia Computer Science},
 keywords = {['Industry 4.0', 'Digital Twin', 'Cyber-Physical System', 'Smart Factory', 'Product Customization', 'Cloud Manufacturing']},
 title = {Product Customization based on Digital Twin and Cloud Manufacturing within a Decentralized Production System},
 year = {2024}
}

@Filtered Article{9b956dde-41f6-4206-8dad-74cfa8718bd4,
 abstract = {This study used three pairs of problem-posing tasks to examine the impact of different prompts on students’ problem posing. Two kinds of prompts were involved. The first asked students to pose 2–3 different mathematical problems without specifying other requirements for the problems, whereas the second kind of prompt did specify additional requirements. A total of 2124 students’ responses were analyzed to examine the impact of the prompts along multiple dimensions. In response to problem-posing prompts with more specific requirements, students tended to engage in more in-depth mathematical thinking and posed much more linguistically and semantically complex problems with more relationships or steps required to solve them. The findings from this study not only contribute to our understanding of problem-posing processes but also have direct implications for teaching mathematics through problem posing.},
 authors = {['Jinfa Cai', 'Hua Ran', 'Stephen Hwang', 'Yue Ma', 'Jaepil Han', 'Faith Muirhead']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Problem posing', 'Problem-posing prompt', 'Problem-posing processes', 'Task variables', 'Task characteristics', 'Teaching mathematics through problem posing', 'P-PBL']},
 title = {Impact of prompts on students’ mathematical problem posing},
 year = {2023}
}

@Filtered Article{9bfa41cf-b798-4075-b78b-4a3c2d533246,
 abstract = {There has been a recent flurry of activity in consciousness research. Although an operational definition of consciousness has not yet been developed, philosophy has come to identify a set of features and aspects that are thought to be associated with the various elements of consciousness. On the other hand, there have been several recent attempts to develop computational models of consciousness that are claimed to capture or illustrate one or more aspects of consciousness. As a plausible substitute to evaluating how well the current computational models model consciousness, this study examines how the current computational models fare in modeling those aspects and features of consciousness identified by philosophy. Following a review of the literature on the philosophy of consciousness, this study constructs a list of features and aspects that would be expected in any successful model of consciousness. The study then evaluates, from the viewpoint of that list, some of the current self-claimed and implemented computational models of consciousness. The computational models studied are evaluated with respect to each identified aspect and feature of consciousness.},
 authors = {['Selvi Elif Gök', 'Erdinç Sayan']},
 journal = {Cognitive Systems Research},
 keywords = {['Consciousness', 'Computational cognitive modeling', 'Clarion', 'LIDA', 'ACT-R', 'Neuronal Work Space Model', 'ART', 'GMU-BICA']},
 title = {A philosophical assessment of computational models of consciousness},
 year = {2012}
}

@Filtered Article{9c0b263d-420e-45fa-a2f4-85f535406330,
 abstract = {Pattern separation is a basic principle of neuronal coding that precludes memory interference in the hippocampus. Its existence is supported by numerous theoretical, computational, and experimental findings in different species. However, I argue that recent evidence from single-neuron recordings suggests that pattern separation may not be present in the human hippocampus and that memories are instead coded by the coactivation of invariant and context-independent engrams. This alternative model prompts a reassessment of the definition of episodic memory and its distinction from semantic memory. Furthermore, I propose that a lack of pattern separation in memory coding may have profound implications that could explain cognitive abilities that are uniquely developed in humans, such as our power of generalization and of creative and abstract thinking.},
 authors = {['Rodrigo {Quian Quiroga}']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['episodic memory', 'Concept Cells', 'engram', 'conjunctive coding', 'neural coding', 'human intelligence']},
 title = {No Pattern Separation in the Human Hippocampus},
 year = {2020}
}

@Filtered Article{9c36e1db-a177-4337-8aa2-02bdf4f5ce32,
 abstract = {Periodic signals called Steady-State Visual Evoked Potentials (SSVEP) are elicited in the brain by flickering stimuli. They are usually detected by means of regression techniques that need relatively long trial lengths to provide feedback and/or sufficient number of calibration trials to be reliably estimated in the context of brain-computer interface (BCI). Thus, for BCI systems designed to operate with SSVEP signals, reliability is achieved at the expense of speed or extra recording time. Furthermore, regardless of the trial length, calibration free regression-based methods have been shown to suffer from significant performance drops when cognitive perturbations are present affecting the attention to the flickering stimuli. In this study we present a novel technique called Oscillatory Source Tensor Discriminant Analysis (OSTDA) that extracts oscillatory sources and classifies them using the newly developed tensor-based discriminant analysis with shrinkage. The proposed approach is robust for small sample size settings where only a few calibration trials are available. Besides, it works well with both low- and high-number-of-channel settings, using trials as short as one second. OSTDA performs similarly or significantly better than other three benchmarked state-of-the-art techniques under different experimental settings, including those with cognitive disturbances (i.e. four datasets with control, listening, speaking and thinking conditions). Overall, in this paper we show that OSTDA is the only pipeline among all the studied ones that can achieve optimal results in all analyzed conditions.},
 authors = {['Tania Jorajuría', 'Mina {Jamshidi Idaji}', 'Zafer İşcan', 'Marisol Gómez', 'Vadim V. Nikulin', 'Carmen Vidaurre']},
 journal = {Neurocomputing},
 keywords = {['Brain-computer interface', 'Steady-state visual evoked potential', 'Spatio-spectral decomposition', 'Higher order discriminant analysis', 'Analytical regularization', 'Tensor-based feature reduction']},
 title = {Oscillatory Source Tensor Discriminant Analysis (OSTDA): A regularized tensor pipeline for SSVEP-based BCI systems},
 year = {2022}
}

@Filtered Article{9c4b626f-3f00-47a6-bfab-b36b1b6c3a45,
 abstract = {Student-Centred Active Learning of Systems Engineering and Sustainability requires challenging metacognitive integration of high-level evaluation skills combined with discipline-based core knowledge. This two-part series aims to demonstrate the basic principles, methodology and specific examples of active learning with formative assessment implemented to achieve improved student academic performance. In this part I of the two-part series, firstly, a detailed description is introduced of the cognitive learning methodology which makes use of student-centered recognition, analysis and synthesis for decision-making when there is no entirely right or wrong decision. The concept of “decision situation” is described which combines several surrounding and contingency elements to arrive at a demonstration of the holistic decision-making through systems analysis. A Holistic thinking approach is further developed using a systems learning methodology that combines normative with descriptive analyses to arrive at a cognitive mode model of judgement and choice. Sustainability modelling using the three-gateway systems approach is introduced and compared with the multi-layered view of chemical and biochemical engineering education and research; see Gani et al. (2020). Holistic thinking strategy is applied most recently to integrating, backcasting and eco-design for the circular economy (CE); see Mendoza et al. (2017). A student-centred learning approach is advocated that makes use of these principles and enables the systematic embedding of sustainability modeling in industrial and economic activities whose success rely substantively on decision-making. Finally, the relative importance is evaluated using classroom data available with specific engineering topics of the didactic “rule-based” methods of knowledge transfer in contrast with the experiential accumulation of practical information amassed through social interactions in a co-operative learning environment that relies on sustained improvement through active communication and feedback between the teacher/instructor and the student/learner; see Stephan et al. (2017) and Shallcross and Alpay (2018).},
 authors = {['U. Tuzun']},
 journal = {Education for Chemical Engineers},
 keywords = {['Systems engineering & sustainability', 'Active learning', 'Formative assessment']},
 title = {Introduction to systems engineering and sustainability PART I: Student-centred learning for chemical and biological engineers},
 year = {2020}
}

@Filtered Article{9c7734e2-133d-4848-b1d8-44971f634bf8,
 abstract = {This study aims to decode guest satisfaction with peer-to-peer accommodations by analyzing the relationship between guests’ sentiment and online ratings and examining how analytical thinking and authenticity influence this relationship. Based on reviews of 4602 Airbnb listings in San Francisco, we empirically find that positive (negative) sentiment is linked to a high (low) rating. We further show that this link is stronger when guests manifest a higher extent of analytical thinking and authenticity. Both Tobit and ordered logit models yield consistent estimation results, showing the robustness of our findings. Our study contributes to the tourism and hospitality literature by theoretically explaining the association between sentiment and ratings. In addition, this paper enriches our knowledge regarding the trustworthiness of Airbnb ratings.},
 authors = {['Liang Zhu', 'Yan Lin', 'Mingming Cheng']},
 journal = {International Journal of Hospitality Management},
 keywords = {['Peer-to-peer accommodation', 'Guest satisfaction', 'Online ratings', 'Sentiment analysis', 'Analytical thinking', 'Authenticity']},
 title = {Sentiment and guest satisfaction with peer-to-peer accommodation: When are online ratings more trustworthy?},
 year = {2020}
}

@Filtered Article{9c7ebae8-9d33-4a48-9c5f-a60b3d970733,
 abstract = {This paper investigates how children who are engaged in a creative project with tangible technology kits make sense of the system feedback the technology provides. A micro-analytic video study was conducted of primary school children designing their own technologies using existing educational microcontrollers. Our investigation reveals that the roles feedback plays in children’s interactions cannot easily be assimilated within the existing approaches to understand feedback that have been articulated in HCI literature. Our qualitative analysis shows how children do not make sense of feedback as semantic communication from the system, but make sense of it with respect to its embeddedness in a sequence of activities they are performing with the system and each other. The principal contribution to emerge from our study is a conception of feedback as a process, rather than as a semantic communicative event, nor a direct coupling of action and system response. Our discussion identifies how feedback participates in the institutional agendas of classrooms (e.g. discovery, computational thinking), and draws out initial implications for the design of feedback in educational tangible technologies, identifying possibilities for how feedback might be redesigned to better promote children’s diagnostic practices with open-ended technology kits.},
 authors = {['Sarah Matthews', 'Ben Matthews']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['Tangible technologies', 'Feedback', 'Educational technologies', 'Creative material', 'Interaction design', 'Empirical studies']},
 title = {Reconceptualising feedback: Designing educational tangible technologies to be a creative material},
 year = {2021}
}

@Filtered Article{9d06155b-988b-42c5-b00f-57218fa26750,
 abstract = {Historical data indicate that tunnel fires often cause casualties and damage to both vehicles and tunnels. These severe consequences suggest that (1) humans seldom effectively learn from history, and (2) people lack optimal safety response strategies for tunnel fires. To investigate the root causes of accidental tunnel fires and learn from them, we first surveyed the literature on historical tunnel accidents and described the common timeline of accidental tunnel fires. We employed systems thinking, based on the past research, to depict a causal loop diagram of common accidental tunnel fires. We arrived at the following three findings: (1) the literature review proved that the causes of tunnel fires are far more complex than other types of fires, and the damage they generate is greater; (2) in the context of systems thinking, accidental tunnel fires involve many causal relationships which are both continuous and dynamic, including at least three systems, namely vehicles, tunnel control, and safety response; (3) the mental models “the experience of the operators at the tunnel operation control center is just as vital as the safety response” and “safety is more critical than the traffic volume in the tunnel”, can strengthen safety response systems and ensure safe driving in tunnels. Although the structure of each tunnel and the characteristics of each fire differ and present different causal relationships, this study elucidated lessons from accidental tunnel fires and provided required messages for establishing effective safety measures. The results of this study can be used to establish systems thinking models of tunnel fires and can serve as a reference for policy planning and establishing standard operating procedures for safety responses.},
 authors = {['Chien Liang Lin', 'Chao Fu Chien']},
 journal = {Tunnelling and Underground Space Technology},
 keywords = {['Systems thinking', 'Lessons learned', 'Accidental tunnel fires', 'Causal loop diagram']},
 title = {Lessons learned from critical accidental fires in tunnels},
 year = {2021}
}

@Filtered Article{9d2c6db2-5826-47d1-83d3-264e8bb45a90,
 abstract = {Systems Thinking theorist J. P. Monat has hypothesized that human-level organismal self-awareness will emerge spontaneously in a well-connected neural network as the number of interconnected nodes exceeds ∼70 billion; he speculates that computer networks may achieve self-awareness as the number of nodes approaches this figure. Forests have historically not been perceived as interconnected networks of trees; recently however, researchers have described the “wood-wide web” in which underground fungi interconnect large numbers of trees and plants via chemical and electrical signals. Some of earth’s forests number many billions of trees, and some of the world’s prairies and seagrass meadows also contain billions of individual plants. These plant ecosystems may thus be self-aware, and in fact there may be a multitude of self-aware plant-based ecosystems on earth already. The speed of signal transmission via fungi within each ecosystem is much slower than that in humans, and therefore their organismal self-awareness may be of a different nature than the self-awareness that we associate with humans and upper primates. However, the possibility that our plant systems may be aware of the environmental insults that are being wrought upon them should make us reconsider our anthropocentric activities, as well as the possibility that humanity may need to collaborate with other intelligent non-human earth-based life forms to ensure mutual survival.},
 authors = {['Jamie P. Monat']},
 journal = {Futures},
 keywords = {['Forest', 'Emergence', 'Systems thinking', 'Self-awareness', 'Neural network']},
 title = {The self-awareness of the forest},
 year = {2024}
}

@Filtered Article{9d44ca14-81f6-4caa-b0e2-b167cad46c9d,
 abstract = {Experimental tests of choice predictions in one-shot games show only little support for Nash equilibrium (NE). Poisson Cognitive Hierarchy (PCH) and level-k (LK) are behavioral models of the thinking-steps variety where subjects differ in the number of levels of iterated reasoning they perform. Camerer et al. (2004) claim that substituting the Poisson parameter τ=1.5 yields a parameter-free PCH model (pfPCH) which predicts experimental data considerably better than NE. We design a new multi-person game, the Minimizer Game, as a testbed to compare initial choice predictions of NE, pfPCH and LK. Data obtained from two large-scale online experiments strongly reject NE and LK, but are well in line with the point-prediction of pfPCH.},
 authors = {['Ulrich Berger', 'Hannelore {De Silva}', 'Gerlinde Fellner-Röhling']},
 journal = {Journal of Economic Behavior & Organization},
 keywords = {['Behavioral game theory', 'Experimental games', 'Poisson cognitive hierarchy', 'Level- model', 'Minimizer game']},
 title = {Cognitive hierarchies in the minimizer game},
 year = {2016}
}

@Filtered Article{9d475e51-c200-4aa4-ab44-0e2b414c4a79,
 abstract = {The paper presents the Affective Pertinentization model (APER), a theory of the affect and its role it plays in meaning-making. APER views the affect as the basic form of making sense of reality. It consists of a global, bipolar pattern of neurophysiological activity through which the organism maps the instant-by-instant variation of its environment. Such a pattern of neuropsychological activity is constituted by a plurality of bipolar affective dimensions, each of which maps a component of the environmental variability. The affect has a pluri-componential structure defining a multidimensional affective landscape that foregrounds (i.e., makes pertinent) a certain pattern of facets of the environment (e.g., its pleasantness/unpleasantness) relevant to survival, while backgrounding the others. Doing so, the affect grounds the following cognitive processes. Accordingly, meaning-making can be modeled as a function of the dimensionality of the affective landscape. The greater the dimensionality of the affective landscape, the more differentiated the system of meaning is. Following a brief review of current theories pertaining to the affect, the paper proceeds discussing the APER's core tenets – the multidimensional view of the affect, its semiotic function, and the concepts of Affective Landscape and Phase Space of Meaning. The paper then proceeds deepening the relationship between the APER model and other theories, highlighting how the APER succeeds in framing original conceptualizations of several challenging issues – the intertwinement between affect and sensory modalities, the manner in which the mind constitutes the content of the experience, the determinants of psychopathology, the intertwinement of mind and culture, and the spreading of affective forms of thinking and behaving in society. Finally, the unsolved issues and future developments of the model are briefly envisaged.},
 authors = {['Sergio Salvatore', 'Arianna Palmieri', 'Raffaele {De Luca Picione}', 'Vincenzo Bochicchio', 'Matteo Reho', 'Maria Rita Serio', 'Giampaolo Salvatore']},
 journal = {Physics of Life Reviews},
 keywords = {['Affective Pertinentization model', 'Affective Landscape', 'Phase Space of Meaning', 'Meaning dimensionality']},
 title = {The affective grounds of the mind. The Affective Pertinentization (APER) model},
 year = {2024}
}

@Filtered Article{9d6c6e40-6752-4730-8ef0-2ec3ac342309,
 abstract = {Demetis and Lee's paper outlines criteria for constructing theory in accordance with systems science. This is a laudable aim but in this comment I suggest that their view of systems thinking is both narrow and somewhat dated. Demetis and Lee equate systems science with only one aspect of it – General Systems Thinking (GST) – and they discuss in detail only one theorist – Niklas Luhmann. I draw attention to a range of other systems approaches including system dynamics, soft systems methodology, complexity theory, critical systems thinking, critical realism and multimethodology. I conclude with tentative guidelines of my own.},
 authors = {['John Mingers']},
 journal = {Information and Organization},
 title = {Back to the future: A critique of Demetis and Lee's “Crafting theory to satisfy the requirements of systems science”},
 year = {2017}
}

@Filtered Article{9daefea4-56ca-4da4-99dd-9d44535f0de6,
 abstract = {The unprecedented expansion of digital technologies has led to a rapid increase in the development and application of 3D digital environments for landscape and urban planning in the past two decades. Considering the significant challenges in guiding human societies towards sustainability, these technologies must not only assist decision-makers in adapting to changes but promote fast, transformative shifts in the relationship between human societies and nature. Based on a set of global exemplars, this Perspective Essay outlines six key factors that can enhance efficacy of 3D digital environments to guide knowledge-informed landscape and urban planning. We call for (1) explicitly representing dynamic interplay between the social, ecological, and technical systems, (2) exploring the integration of design with simulation models to address cross-scale dynamics, (3) developing features to foster imagination, (4) employing multisensory stimuli to encourage profound changes in environmentally and socially sustainable behavior, (5) tailoring the incorporation of active sensing by and with non-experts into 3D digital environments to better acknowledge indigenous and local knowledge systems, and finally, (6) carrying out a usability evaluation to facilitate participation and collaboration in an efficient co-creation process. We conclude by recommending the establishment of a collaborative knowledge platform that unites researchers, developers, and stakeholders for stimulating social-ecological-technological system thinking in the development of 3D digital environments and harnessing the technological advancements to accelerate and drive the needed transformative change within urban and landscape planning.},
 authors = {['Adrienne Grêt-Regamey', 'Nora Fagerholm']},
 journal = {Landscape and Urban Planning},
 title = {Key factors to enhance efficacy of 3D digital environments for transformative landscape and urban planning},
 year = {2024}
}

@Filtered Article{9e07f501-6e00-4b1c-9100-fc4bdeac0cd4,
 abstract = {The purpose of this chapter is to demonstrate how to solve optimal control problems, both for single-player games and multiplayer games with intermittent and continuous feedback in centralized and multi agent settings. We achieve this by using Q-learning, leveraging data measured along the trajectories. Importantly, our approaches do not require any prior knowledge of the system dynamics. This model-free and dynamic framework allows learning agents to adapt their objectives or optimality criteria on the fly. In addressing nonequilibrium results in shifting, dynamical environments, a control-oriented multi agent formulation of the interactions between different thinking agents is also shown.},
 authors = {['Kyriakos G. Vamvoudakis']},
 journal = {Elsevier},
 keywords = {['Adaptive control', 'Adaptive learning', 'Bounded rationality', 'Cyber-physical systems', 'Game theory', 'Model-free', 'Multi-agent systems', 'Nash games', 'Networked systems', 'Optimal control', 'Q-learning', 'Reinforcement learning']},
 title = {Multi Agent Q-Learning With Adversaries in Nash Equilibrium and Non Equilibrium Settings},
 year = {2024}
}

@Filtered Article{9e12518b-176e-4aad-8692-e3bd34ae0449,
 abstract = {Storytelling played a crucial role in human evolution. To this day, through stories humans gain declarative and procedural knowledge, and learn the skills that support learning itself. Research shows that reading stories to children enhances their reading and language skills. Does it also enhance their intelligence? To answer this question, we conducted three (N = 626, 254, 195) longitudinal, cluster-randomized control trials in Italian elementary and middle schools. Over a 4-month period, for half of the participants 1 h/day of standard, active language instructional activities were substituted with reading-aloud of stories by a teacher. Compared to those who kept doing language instructional activities, read-aloud condition children showed a stronger increase on two measures of intelligence focusing on knowing things and thinking skills. This result, which emerged in three independent trials conducted in different regions of Italy, suggests avenues for easily scalable interventions to improve children's intelligence.},
 authors = {['Federico Batini', 'Marco Bartolucci', 'Giulia Toti', 'Emanuele Castano']},
 journal = {Intelligence},
 keywords = {['Intelligence', 'Cognitive development', 'Narrative fiction', 'Storytelling', 'Reading']},
 title = {Shared reading aloud fosters intelligence: Three cluster-randomized control trials in elementary and middle school},
 year = {2025}
}

@Filtered Article{9e163d57-e921-44bb-9968-ae551cdb5bf6,
 abstract = {Early research investigated young students' understandings of science concepts using physical equipment, but technological advances now mean there are new options to introduce these ideas, through devices such as iPads and simulations. However, research investigating the use of simulations in early years' science learning is limited. This study applied revisions of Kolb's Experiential Learning theoretical model to determine if age-indicated science simulations were effective for teaching 5 year olds simple circuit building procedures and electricity concepts, and the function of circuit components. It also explored whether their engagement with the simulations provided worthwhile opportunities to exercise higher order capabilities such as reflective thinking and abstraction – skills oftencited in literature as valuable outcomes from older student and adult use of simulations. Findings indicate students developed a solid base of procedural knowledge about constructing different circuits, and functional knowledge about circuit components they applied to different circuit designs. The emergence of tentative, generalised theories about current and the effects of different circuit designs on the performance of resistors - linked to the exercise of reflective and descriptive thinking, were also noted in many students. However, examples were found of some simulations appearing to foster common misconceptions, such as current being ‘consumed’ by resistors – indicating teachers need to be highly vigilant and work closely with students, to ensure accurate understandings are developed. Overall, with appropriate teacher support and careful selection and review, the study concludes simulations can be effective for introducing young students to simple physical science concepts, and for providing them with opportunities to engage in higher order thinking processes.},
 authors = {['Garry Falloon']},
 journal = {Computers & Education},
 keywords = {['Simulations', 'Young students', 'Electricity', 'Circuits', 'Experiential learning']},
 title = {Using simulations to teach young students science concepts: An Experiential Learning theoretical analysis},
 year = {2019}
}

@Filtered Article{9ea4bf6c-d425-48bd-a999-f1c8baab1de3,
 abstract = {A brief overview of agent-based computational economics (ACE) is given, followed by a synopsis of the articles included in this special issue on ACE and in a companion special issue on ACE scheduled to appear in Computational Economics.},
 authors = {['Leigh Tesfatsion']},
 journal = {Journal of Economic Dynamics and Control},
 keywords = {['Agent-based computational economics']},
 title = {Introduction to the special issue on agent-based computational economics},
 year = {2001}
}

@Filtered Article{9f024c8c-8deb-4ebd-9341-948aadbc0707,
 abstract = {Vehicle-edge computing, as a promising paradigm, is employed to support applications that require low latency and high computational capability. In this study, we consider the idle resources of the surrounding parked vehicles (PVs) and roadside units (RSUs) as service providers to enhance the performance of User Equipment (UE). We propose a joint offloading architecture that uses parked vehicles. Additionally, owing to the dynamic and uncertain nature of the environment, we model computation offloading as a dynamic multi-objective optimization problem to simultaneously optimize the latency and energy consumption of UE applications. In this study, we propose a dynamic multi-objective evolutionary algorithm with a multi-strategy fusion response (DMOEA/D-MSFR). Specifically, we introduce a population center positioning strategy and a learnable prediction mechanism using Long Short-Term Memory (LSTM) in DMOEA-MSFR, which divides the prediction optimization process into two stages and exhibits a rapid response to environmental changes. In the static optimization phase, an adaptive weight vector adjustment strategy is employed, which significantly aids in the distribution and diversity of the solutions. Comprehensive experiments demonstrate that our proposed framework balances the trade-off between latency and energy consumption, and the convergence, feasibility, and diversity of the non-dominated solutions obtained.},
 authors = {['Yingbo Zhou', 'Zheng-Yi Chai', 'Ya-Lun Li', 'Jun-Jie Li']},
 journal = {Swarm and Evolutionary Computation},
 keywords = {['Vehicle edge computing', 'Dynamic multi-objective optimization', 'Evolutionary algorithms', 'Computational offloading', 'Vehicle collaboration']},
 title = {Parking Vehicle-Assisted Task Offloading in Edge Computing: A dynamic multi-objective evolutionary algorithm with multi-strategy fusion response},
 year = {2025}
}

@Filtered Article{9f48337c-ce7a-4516-9c0c-0c1a1c1fe356,
 abstract = {Rational choice theory says that operators and others make decisions by systematically and consciously weighing all possible outcomes along all relevant criteria. This paper first traces the long historical arm of rational choice thinking in the West to Judeo-Christian thinking, Calvin and Weber. It then presents a case study that illustrates the consequences of the ethic of rational choice and individual responsibility. It subsequently examines and contextualizes Rasmussen's legacy of pushing back against the long historical arm of rational choice, showing that bad outcomes are not the result of human immoral choice, but the product of normal interactions between people and systems. If we don't understand why people did what they did, Rasmussen suggested, it is not because people behaved inexplicably, but because we took the wrong perspective.},
 authors = {['Sidney W.A. Dekker']},
 journal = {Applied Ergonomics},
 keywords = {['Rasmussen', 'Rational choice', 'Human error', 'Second victim', 'Incidents']},
 title = {Rasmussen's legacy and the long arm of rational choice},
 year = {2017}
}

@Filtered Article{9fb4df29-ec28-4462-86e2-eccc1b4619f5,
 abstract = {Automation of teaching people new skills requires modeling of human reasoning because human cognition involves active reasoning over the new subject domain to acquire skills that will later become automatic. The article presents Thought Process Trees — a language for modeling human reasoning that was created to facilitate the development of intelligent tutoring systems, which can perform the same reasoning that is expected of a student and find deficiencies in their line of thinking, providing explanatory messages and allowing them to learn from performance errors. The methodology of building trees which better reflect human learning is discussed, with examples of design choices during the modeling process and their consequences. The characteristics of educational modeling that impact building subject-domain models for intelligent tutoring systems are discussed. The trees were formalized and served as a basis for developing a framework for constructing intelligent tutoring systems. This significantly lowered the time required to build and debug a constraint-based subject-domain model. The framework has already been used to develop five intelligent tutoring systems and their prototypes and is being used to develop more of them.},
 authors = {['Oleg Sychev']},
 journal = {Cognitive Systems Research},
 keywords = {['Reasoning modeling', 'Constraint-based modeling', 'Intelligent tutoring systems']},
 title = {Educational models for cognition: Methodology of modeling intellectual skills for intelligent tutoring systems},
 year = {2024}
}

@Filtered Article{9fe00662-e019-472f-9f21-c6721dd99325,
 abstract = {The Game-Design and Learning (GDL) initiative engages middle school students in the process of game-design in a variety of in-school, after-school, and summer camp settings. The goal of the GDL initiative is to leverage students' interests in games and design to foster their problem-solving and critical reasoning skills. The present study examines the effectiveness of an after-school version of the GDL program using a quasi-experimental design. Students enrolled in the GDL program were guided in the process of designing games aimed at solving problems. Compared to students in a control group who did not attend the program (n = 24), the children who attended the GDL program (n = 20) showed a significant increase in their problem-solving skills. The results provide empirical support for the hypothesis that participation in the GDL program leads to measurable cognitive changes in children's problem-solving skills. This study bears important implications for educators and theory.},
 authors = {['Mete Akcaoglu', 'Matthew J. Koehler']},
 journal = {Computers & Education},
 keywords = {['Game-design', 'Problem-solving', 'Quasi-experimental', 'Constructionism']},
 title = {Cognitive outcomes from the Game-Design and Learning (GDL) after-school program},
 year = {2014}
}

@Filtered Article{9fe351db-0765-41db-a30e-caa753b393a7,
 abstract = {Due to technological developments and societal needs cartography, scientific visualization, image analysis and remote sensing, information visualization, exploratory data analysis, visual analytics, and GIScience have undergone fundamental changes in recent years. Interactivity and dynamics allow not only maps and diagrams to present known facts but also to analyze and explore unknown data. The environment in which the maps and diagrams are used has also changed and often includes coordinated multiple views display via the Internet. Such environments allow for simultaneous alternative views of the data and stimulate visual thinking, resulting in geovisualization.},
 authors = {['Menno-Jan Kraak']},
 journal = {Elsevier},
 keywords = {['Alternative visualization', 'Cartography', 'Cognition', 'Coordinated multiple views', 'Geocomputation', 'Geovisualization', 'Information visualization', 'Interfaces', 'Maps', 'Representation', 'Spatiotemporal data', 'Usability', 'Visual analytics', 'Visual exploration', 'Visual representation', 'Visual thinking']},
 title = {Geovisualization},
 year = {2020}
}

@Filtered Article{a0329423-6304-4444-8340-b0e123e3bf27,
 abstract = {Enactivism is a philosophical and scientific approach that emphasizes the role of the body and its interactions with the environment in shaping cognitive processes and subjective experiences. Meanwhile, osteopathy is a person-centered health care discipline, highlighting the structure-function interrelationship of the body and its selfregulation mechanisms. Both approaches value the body and the environment in health. Several authors have been discussing the urgent need for a reconceptualization of osteopathy and also suggesting integrate biological, psychological and social aspects. Thinking osteopathy as a Therapeutic Rationality, implies recognize its fundamental dimensions: Human Morphology, Vital Dynamics, Medical Doctrine, Diagnostic System and Therapeutic System, all integrated by a philosophical Cosmology, as the original term Medical rationality states, but also embrace a broader perspective allowing an individual and unique process of each person, reflecting the transformation of contemporary medicine to a person approach. Enactivism principles can serve as a basis for a reconceptualization of osteopathy, integrating environmental, psychological, social, and spiritual factors. Osteopathic concepts can probably be updated through the convergence between enactivism and osteopathy, promoting more meaningful and evidence-based clinical practice. Advancing in this direction requires a collaborative dialogue between researchers, health professionals and interested people, seeking an integrated understanding of the relationship between body, mind, environment and health.},
 authors = {['Jacson Nesi', 'Michele Benites', 'Filipe Boeira Schedler']},
 journal = {Advances in Integrative Medicine},
 keywords = {['Enactivism', 'Osteopathy', 'Medical rationalities', 'Reconceptualization']},
 title = {Enactivism: A contemporary perspective of a reconceptualization of osteopathy},
 year = {2024}
}

@Filtered Article{a0405cd6-0293-4fd5-a5d5-868f13d87400,
 abstract = {We develop a formal model for distributed measurement-based quantum computations, adopting an agent-based view, such that computations are described locally where possible. Because the network quantum state is in general entangled, we need to model it as a global structure, reminiscent of global memory in classical agent systems. Local quantum computations are described as measurement patterns. Since measurement-based quantum computation is inherently distributed, this allows us to extend naturally several concepts of the measurement calculus [V. Danos, E. Kashefi and P. Panangaden, The measurement calculus (2004), arXiv:quant-ph/0412135], a formal model for such computations. Our goal is to define an assembly language, i.e. we assume that computations are well-defined and we do not concern ourselves with verification techniques. The operational semantics for systems of agents is given by a probabilistic transition system, and we define operational equivalence in a way that it corresponds to the notion of bisimilarity. With this in place, we prove that teleportation is bisimilar to a direct quantum channel, and this also within the context of larger networks.},
 authors = {['Vincent Danos', "Ellie D'Hondt", 'Elham Kashefi', 'Prakash Panangaden']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['Formal language', 'quantum communication', 'quantum computing', 'semantics']},
 title = {Distributed Measurement-based Quantum Computation},
 year = {2007}
}

@Filtered Article{a04164e3-d017-4c3c-bd5c-0d30ef7728e4,
 abstract = {The pursuit of artificial consciousness requires conceptual clarity to navigate its theoretical and empirical challenges. This paper introduces a composite, multilevel, and multidimensional model of consciousness as a heuristic framework to guide research in this field. Consciousness is treated as a complex phenomenon, with distinct constituents and dimensions that can be operationalized for study and for evaluating their replication. We argue that this model provides a balanced approach to artificial consciousness research by avoiding binary thinking (e.g., conscious vs. non-conscious) and offering a structured basis for testable hypotheses. To illustrate its utility, we focus on "awareness" as a case study, demonstrating how specific dimensions of consciousness can be pragmatically analyzed and targeted for potential artificial instantiation. By breaking down the conceptual intricacies of consciousness and aligning them with practical research goals, this paper lays the groundwork for a robust strategy to advance the scientific and technical understanding of artificial consciousness.},
 authors = {['K. Evers', 'M. Farisco', 'R. Chatila', 'B.D. Earp', 'I.T. Freire', 'F. Hamker', 'E. Nemeth', 'P.F.M.J. Verschure', 'M. Khamassi']},
 journal = {Physics of Life Reviews},
 title = {Preliminaries to artificial consciousness: A multidimensional heuristic approach},
 year = {2025}
}

@Filtered Article{a042376e-21a1-4d98-9557-f5ca4c7ee7e4,
 abstract = {This paper argues that there is a synchronicity among biological and computational levels on an organism and provides arguments and proofs based on experimental research gathered in the literature. The leading thread is the interplay between quantum biology (QB) and complexity. As the paper asks whether QB does contribute to complexity science (CS), five arguments are provided: (i) Firstly a state-of-the art of QB and its relationship to CS is sketched out. Thereafter, the attention is directed to answering the question set out; (ii) Secondly, it digs into the understanding of life toward deeper levels of reality; (iii) It is shown that non-trivial quantum effects shed insightful lights on the information processing of and within living beings; (iv) Once the distinction is made between increasing levels of complexity and increasing levels of organization, the focus lies in the importance of QB for organization, and not so much for complexity as such; (v) The role of information rises at the center of all concerns, and the intertwining of complexity and information processing. At the end some conclusions are drawn.},
 authors = {['Carlos E. Maldonado', 'Nelson A. Gómez-Cruz']},
 journal = {Procedia Computer Science},
 keywords = {['quantum biology', 'living systems', 'non-linear systems', 'complexity science', 'theory', 'health.']},
 title = {Synchronicity among Biological and Computational Levels of an Organism: Quantum Biology and Complexity},
 year = {2014}
}

@Filtered Article{a0bd736c-3844-4844-a9d3-c229408a5e13,
 abstract = {Purpose
This study aims to describe the pedagogical content knowledge (PCK) of a kindergarten educator who implements a lesson plan about informal inferential reasoning designed in a lesson study group.
Design/methodology/approach
To this end, we analyzed teaching interventions in two kindergarten lessons focused on the playful task of tossing two coins, associated with inferential statistical reasoning. The study highlights the importance of arguing and promoting this reasoning to develop statistical thinking. It is crucial to recognize how early students can be subject to learning experiences that promote a language of uncertainty, assess the evidence provided by the data, and make generalizations.
Findings
The results reveal that while the educator demonstrated knowledge and skills relevant to the curriculum and conceptual teaching strategies, the understanding of the content by the students and the integration of the PCK components still present a challenge.
Practical implications
The lesson study collaborative teaching practices that promote PCK have proven effective for informing the design and implementation of instructional practices supporting the development of early statistical thinking in young children.
Originality/value
The study enriches the knowledge regarding the potential of the lesson study (LS) in the professional learning of kindergarten educators. It also contributes to a comprehensive approach based on authentic playful experiences in grade K that supports the development of early statistical thinking in young children.},
 authors = {['Soledad Estrella', 'Maritza Mendez-Reina', 'Raimundo Olfos', 'Jocelyn Aguilera']},
 journal = {International Journal for Lesson and Learning Studies},
 keywords = {['Pedagogical content knowledge', 'Early statistics', 'Informal inferential reasoning', 'Lesson study']},
 title = {Early statistics in kindergarten: analysis of an educator's pedagogical content knowledge in lessons promoting informal inferential reasoning},
 year = {2022}
}

@Filtered Article{a0c84acd-8dd2-4102-bf14-62b573148c10,
 abstract = {This research shares progressions in thinking about equations and the equal sign observed in ten students who took part in an early algebra classroom intervention across Kindergarten and first grade. We report on data from task-based interviews conducted prior to the intervention and at the conclusion of each school year that elicited students’ interpretations of the equal sign and equations of various forms. We found at the beginning of the intervention that most students viewed the equal sign as an operational symbol and did not accept many equations forms as valid. By the end of first grade, almost all students described the symbol as indicating the equivalence of two amounts and were much more successful interpreting and working with equations in a variety of forms. The progressions we observed align with those of other researchers and provide evidence that very young students can learn to reason flexibly about equations.},
 authors = {['Ana Stephens', 'Ranza {Veltri Torres}', 'Yewon Sung', 'Susanne Strachota', 'Angela {Murphy Gardiner}', 'Maria Blanton', 'Rena Stroud', 'Eric Knuth']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Equal sign', 'Equations', 'Elementary grades', 'Early algebra', 'Algebraic thinking']},
 title = {From “You have to have three numbers and a plus sign” to “It’s the exact same thing”: K–1 students learn to think relationally about equations},
 year = {2021}
}

@Filtered Article{a25ea838-f669-41e0-9d32-986108aa352f,
 abstract = {This study proposes the Smart Design method to support the design decision making in the sustainable neighborhood development with multiple objectives. Instead of the “creative design” approach in the scenario making in traditional PSS and recent Geodesign frameworks, the Smart Design method applies the optimization algorithms to search for optimal design solutions in the design space. It integrates the design thinking, computational performance modeling and optimization techniques to efficiently and effectively approximate optimal designs. This method is applied to a hypothetical residential neighborhood design case study with three sustainability objectives: to maximize FAR, to minimize building energy use, and to minimize outdoor human discomfort. Based on the form parameterization, the Nondominated Sorting Genetic Algorithm II (NSGA-II) algorithm is utilized to guide the evolution of the neighborhood design throughout 80 generations, with neighborhood performance modeling tools. The Smart Design method is able to identify 38 representative design solutions as Pareto optimal which are equally optimal. Those solutions set a basis for discussions and negotiations among stake holders to make design decisions with the three objectives. Further research will be focused on addressing the challenges such as recursive objective definitions, parametrization of complex forms, quantification of performances and optimization uncertainties, from simple cases to more realistic and complex designs for sustainable neighborhood development.},
 authors = {['Steven Jige Quan']},
 journal = {Energy Procedia},
 keywords = {['Smart Design', 'Sustainable Neighborhood Development', 'Design Decision Making', 'Multi-objective Optimization', 'Genetic Algorithms', 'Pareto optimal']},
 title = {Smart Design for Sustainable Neighborhood Development},
 year = {2019}
}

@Filtered Article{a26345a3-8968-474a-b6b7-995353205446,
 abstract = {This research explores the effect of the structuredness of design concept generation techniques on temporal network neurocognition. Engineering graduate students (n = 30) completed three concept generation tasks using techniques with different levels of structuredness: brainstorming, morphological analysis, and TRIZ. Students’ brain activation in their prefrontal cortex (PFC) was measured using functional near-infrared spectroscopy (fNIRS). The temporal dynamic of central regions in brain networks were compared between tasks. Central regions facilitate functional interaction and imply information flow through the brain. A consistent central region appears in the medial PFC. Consistent network connections occurred across both hemispheres suggesting a concurrent dual processing of divergent and convergent thinking. This study offers novel insights into the underlying neurophysiological mechanism when using these concept generation techniques.},
 authors = {['Julie Milovanovic', 'Mo Hu', 'Tripp Shealy', 'John Gero']},
 journal = {Design Studies},
 keywords = {['design cognition', 'design process', 'problem solving', 'conceptual design', 'design neurocognition']},
 title = {Characterization of concept generation for engineering design through temporal brain network analysis},
 year = {2021}
}

@Filtered Article{a274083e-0730-48a3-8c98-417e202431d7,
 abstract = {Hybrid Intelligence (HI) represents the ability to solve problems by using human and artificial intelligence (AI) together, pursuing the strengths of both the former (e.g., ethical, creative, and common-sense thinking) and the latter (e.g., the fast and efficient ability). This study conducted a keyword network analysis and topic modelling to extract topics, examine research trends, and explore future development directions. HI-related research has increased rapidly since 2017, and the applied research fields have increased in diversity. The most active research fields are computer science, engineering, and mathematics. Based on the topic and keyword analysis, we found four trending topics (decision making, medical science, social factors, and automation) and three emerging topics (crowdsourcing, data science, and teamwork). Among the four types of knowledge (factual, conceptual, expectational, and methodological), previous papers have been lacking focus on methodological research in terms of basic research. Since HI can include both human and artificial intelligences, it offers endless possibilities for methodological knowledge in terms of the research process. We therefore propose the development of research methodologies leveraging HI as a promising future research topic.},
 authors = {['Jihye Lim', 'Junseok Hwang']},
 journal = {Futures},
 keywords = {['Hybrid Intelligence', 'Artificial Intelligence', 'Human Intelligence', 'Keywords Co-occurrence Network', 'Topic Modeling', 'Knowledge classification']},
 title = {Exploring trends and topics in hybrid intelligence using keyword co-occurrence networks and topic modelling},
 year = {2025}
}

@Filtered Article{a29ef0e7-fbe4-4bea-86de-e9ecf0da01cb,
 abstract = {The critical importance of sample context and geologic information for interpreting geochronologic data has long been fundamental to the Earth sciences. However, the lack of quantitative uncertainties associated with contextual, observational information means that much geologic data cannot be statistically treated in computational models using the same approaches as quantitative datasets. This challenge is showcased by the current debate over whether and how geologic data should be used when modeling thermochronologic results, which has important implications for deriving time-temperature (tT) paths from which burial and exhumation histories are interpreted. Holistically leveraging observational data to test hypotheses and determine the set of geologically reasonable thermal histories that can explain thermochronologic results has a longstanding history, but some recent studies have criticized this approach as one that arbitrarily limits tT solutions. Here, a geologic context approach to thermal history modeling, in which observational and thermochronologic datasets are combined to design geologically valid models and reach the most geologically likely interpretation, is illustrated using an example of constraining Great Unconformity exhumation in Colorado where this modeling philosophy has been questioned. Although the quality of geologic data and their applicability to modeled samples can vary and be debated, this does not mean that all geologic data are inherently unreliable and therefore discardable. Exploring models with varying or minimal constraints can be useful to test different hypotheses and determine the resolving power of the data, but using an endmember context-blind approach to interpret thermochronologic results can produce outcomes that violate fundamental aspects of the geology. The strategy outlined here is not the only valid approach to modeling thermochronologic data, but if the purpose of the modeling is to derive meaningful interpretations about sample tT paths in order to better illuminate the geologic history, then critical thinking about the sample context, first order geologic observations, and primary relationships should be integral components of the modeling process.},
 authors = {['R.M. Flowers', 'B.A. Peak']},
 journal = {Earth and Planetary Science Letters},
 keywords = {['Geologic context approach', '(U-Th)/He', 'Thermal history', 'Great Unconformity', 'Pikes Peak', 'Tava']},
 title = {Context matters: Modeling thermochronologic data in geologic frameworks using the Great Unconformity as a case study},
 year = {2025}
}

@Filtered Article{a2b858d3-b55e-4d19-9af4-c8d6f471c9b5,
 abstract = {When programming with sublinear space constraints one often needs to use special implementation techniques even for simple tasks, such as function composition. In this paper, we study how such implementation techniques can be supported in a functional programming language. Our approach is based on modelling computation by interaction using the Int construction of Joyal, Street & Verity. We apply this construction to a term model of a first-order programming language and use the resulting structure to derive the functional programming language intml. Intml can be understood as a programming language simplification of Stratified Bounded Affine Logic. We formulate intml by means of a type system inspired by Baillot & Terui's Dual Light Affine Logic. We show that it captures the complexity classes flogspace and nflogspace. We illustrate its expressiveness by showing how typical graph algorithms, such a test for acyclicity in undirected graphs, can be represented.},
 authors = {['Ugo {Dal Lago}', 'Ulrich Schöpp']},
 journal = {Information and Computation},
 keywords = {['Implicit computational complexity', 'Logarithmic space', 'Type system', 'Geometry of interaction', 'Functional programming']},
 title = {Computation by interaction for space-bounded functional programming},
 year = {2016}
}

@Filtered Article{a2c28ae0-e469-467e-ab6d-0f2f62325499,
 abstract = {Neurocognitive research is pertinent to developing mechanistic models of how humans generate creative thoughts. Such models usually overlook the role of the motor cortex in creative thinking. The framework of embodied or grounded cognition suggests that creative thoughts (e.g. using a shoe as a hammer, improvising a piano solo) are partially served by simulations of motor activity associated with tools and their use. The major hypothesis stemming from the embodied or grounded account is that, while the motor system is used to execute actions, simulations within this system also support higher-order cognition, creativity included. That is, the cognitive process of generating creative output, not just executing it, is deeply embedded in motor processes. Here, we highlight a collection of neuroimaging research that implicates the motor system in generating creative thoughts, including some evidence for its functionally necessary role in generating creative output. Specifically, the grounded or embodied framework suggests that generating creative output may, in part, rely on motor simulations of possible actions, and that these simulations may by partially implemented in the motor regions themselves. In such cases, action simulations (i.e. reactivating or re-using the motor system), do not result in overt action but instead are used to support higher-order cognitive goals like generating creative uses or improvising.},
 authors = {['Heath E. Matheson', 'Yoed N. Kenett']},
 journal = {NeuroImage},
 keywords = {['Motor system', 'Creativity', 'Simulations', 'Embodied cognition', 'Grounded cognition', 'Divergent thinking', 'Improvisation']},
 title = {The role of the motor system in generating creative thoughts},
 year = {2020}
}

@Filtered Article{a2ec5f55-92a5-4031-9467-c4836d5e14df,
 abstract = {This chapter presents the neurobiological basis for the convergence of interdisciplinary work (Nano-Bio-Info-Cogno) in the research of artificial neural networks. The neurobiological study was conducted from neuroscience and technology; the topics explained are genetics and cognition, complexity of information, information processing, brain and problem solving, emotions, and solutions as well as the relationship between the nervous system cells and biological synthesis of information as part of studies to the given problems. The most specific cognitive functions related to decision making and problem solving—attention, time, process, motion, relevance of information, and memory—as well as reasoning processes not typically associated with solving complex problems are reviewed.},
 authors = {['Luis Fernando {Cruz Quiroga}', 'Wilfrido Alejandro Moreno']},
 journal = {Academic Press},
 keywords = {['Complex problem solving', 'Neural networks', 'Neurobiology', 'Neuroscience']},
 title = {Chapter 5 - Neurobiological Computation and Neural Networks},
 year = {2016}
}

@Filtered Article{a2f62c06-6250-416c-b56a-c0b09553304b,
 abstract = {The era of Big Data has arrived along with large volume, complex and growing data generated by many distinct sources. Nowadays, nearly every aspect of the modern society is impacted by Big Data, involving medical, health care, business, management and government. It has been receiving growing attention of researches from many disciplines including natural sciences, life sciences, engineering and even art & humanities. It also leads to new research paradigms and ways of thinking on the path of development. Lots of developed and under-developing tools improve our ability to make more felicitous decisions than what we have made ever before. This paper presents an overview on Big Data including four issues, namely: (i) concepts, characteristics and processing paradigms of Big Data; (ii) the state-of-the-art techniques for decision making in Big Data; (iii) felicitous decision making applications of Big Data in social science; and (iv) the current challenges of Big Data as well as possible future directions.},
 authors = {['Hai Wang', 'Zeshui Xu', 'Hamido Fujita', 'Shousheng Liu']},
 journal = {Information Sciences},
 keywords = {['Big Data', 'Data deluge', 'Decision making', 'Data analysis', 'Data-intensive applications', 'Computational social science']},
 title = {Towards felicitous decision making: An overview on challenges and trends of Big Data},
 year = {2016}
}

@Filtered Article{a3056a35-3933-4d5e-9908-ec02ec1fae84,
 abstract = {Epiphany is a crucial stage in the process of creative thinking. The prototype heuristic theory has proved that the individual epiphany ability depends on the individual's ability to get out of the fetter of mental fixation, activate the prototype and acquire the key heuristic information from the activated prototype. Based on this theory, this present research combines the findings of extenics, TRIZ and theory of creativity to have developed a systematic method on enhancing individual epiphany ability. Supported by information technology, the method takes theory of creativity as its methodology, extension strategy generation as its framework, element theory its database and knowledge management its feedback chain. The research aims to cultivate creative thinking and eventually enhance the creativity of individuals.},
 authors = {['Ailing Chen', 'Wei Liu', 'Zhihui Wu', 'Jun Zhang']},
 journal = {Procedia Computer Science},
 keywords = {['Prototype heuristics', 'epiphany', 'extenics', 'Theory of Creativity', 'sytematic scheme ;']},
 title = {On the Systematic Method to Enhance the Epiphany Ability of Individuals},
 year = {2014}
}

@Filtered Article{a31c86f6-901f-4159-a882-d33ccede7563,
 abstract = {At the turn of the millennium, architects and educators, propelled by the demise of critical theory, found a space to speculate about technology’s role in the future of architecture. As Michael Speaks wrote in 2002, “if philosophy was the intellectual dominant of early twentieth century vanguards and theory the intellectual dominant of late twentieth century vanguards, then intelligence has become the intellectual dominant of twenty-first century post-vanguards” (Speaks, 2010, p. 211). This emphasis on intelligence fostered a progressive narrative around the increasing reliance on software in design processes. This paper examines architectural practice during this period, with a specific focus on the rise of a new set of values, priorities, and factors that transformed architectural thinking and making. In contrast to existing accounts of digital design’s history, this paper places less importance on outputs and more on the shifts in modes of working and enacting design labor. More specifically, it narrows in on a software application that, as will be argued, drastically changed both cultural values and design knowledge: Rhinoceros. Beyond simply facilitating the production of geometrically intricate and complex architectural assemblies, Rhinoceros helped shape the discourse on parametric, computational, and algorithmic design, redefining the role of the designer as a creative technologist. In doing so, it also engendered a specific community of practice, which in turn produced its own culture and folklore. The spread of this software greatly contributed to the rise of two new kinds of architectural technologists: the “parametric designer” and the “digital fabricator,” two actors who would significantly impact how architecture was imagined and produced from the mid-2000s through the 2010s.},
 authors = {['Galo Canizares']},
 journal = {Perspectives in Architecture and Urbanism},
 keywords = {['Software', 'History', 'Parametric design', 'Digital fabrication', 'Theory']},
 title = {Taming the Rhinoceros: A brief history of a ubiquitous tool},
 year = {2024}
}

@Filtered Article{a31f4627-0818-4c2c-bc43-9e43688faa8f,
 abstract = {The underground pipeline network (UPN) is an essential infrastructure and plays an irreplaceable role in national defense and urban activities. The complexity of structural environment and management makes its operation and maintenance difficult. To solve this problem, a distributed mixed reality (MR) and internet of things (IoT) system is developed through game thinking. Firstly, digital models are created based on design drawings and real-world environments, and then an MR system for the UPN is built by the game engine and the OpenXR platform. Secondly, an IoT cloud platform is built to connect with the MR system based on the API sets and cloud services; the data communication between sensors and MR devices is linked with the Socket method, and the data filtering model is constructed by the Kalman algorithm to realize the information exchange between the field workers and the backend managers. Finally, the National Center for Materials Service Safety at the University of Science and Technology Beijing (NCMS_USTB) is used as the experimental site to test this system, and its underground sewage and rainwater pipeline network are used to simulate the key problems in the operation and maintenance. The effect of the application shows that there is potential technical complementarity between the MR and IoT, and the distributed MR-IoT approach can be used as a new technical reference for the operation and maintenance of the UPN.},
 authors = {['Wei Li', 'Zhoujing Ye', 'Yajian Wang', 'Hailu Yang', 'Songli Yang', 'Zhenlong Gong', 'Linbing Wang']},
 journal = {Tunnelling and Underground Space Technology},
 keywords = {['Underground Pipeline Network', 'Mixed Reality', 'IoT Cloud Platform', 'Data Communication', 'Operation and Maintenance']},
 title = {Development of a distributed MR-IoT method for operations and maintenance of underground pipeline network},
 year = {2023}
}

@Filtered Article{a35cc4c3-07e5-4e8b-9ce0-23bf0f5e1d96,
 abstract = {Supply chain and global logistics are driven by strategically focusing on core competences, outsourcing manufacturing to pursue higher value proposition in the supply chain, radically improving the return of capital investments and providing total solutions to targeted customers. The container-loading research has important industrial and commercial application for global logistics. In practice, loading pooled shipment into containers is a complex procedure that has relied largely on the workers’ experience. We developed an efficient computational procedure involving three-dimensional cutting for determining near-optimal container-loading patterns to minimize the waste of container space. We used numerical examples from a motor company that imports key components from Japan, produces parts in Taiwan, and assembles cars in China to estimate its validity and discussed the effectiveness of the proposed solution. This study concludes with a discussion of future research.},
 authors = {['Chen-Fu Chien', 'Chia-Yen Lee', 'Yi-Chao Huang', 'Wen-Ting Wu']},
 journal = {Computers & Industrial Engineering},
 keywords = {['Global logistics', 'Container-loading', 'Cutting and packing', 'Three-dimension knapsack', 'Decision support system']},
 title = {An efficient computational procedure for determining the container-loading pattern},
 year = {2009}
}

@Filtered Article{a3724cb0-c6d7-48d5-aca9-0e7824451bf2,
 abstract = {The correlation between individuals’ creativity performances in a one-to-one interactive situation was preliminarily explored. Neuroimaging has provided many insights into the neural connectome that underlies creativity; however, the role of brain structure in individuals’ creativity performance when collaborating with others remains largely unexplored. Therefore, this study collected data from 74 single- and paired-player participants using an interactive creativity task platform, including the alternative use task (AUT) and the Chinese Radical Remote Associates Test (CRRAT). Participants’ AUT and CRRAT scores in the above two modes were collected to analyze the moderating effects of connective efficiency (CE). The results showed that the relationship between originality performance in the single- and paired-player modes was moderated by four theoretical graph measures. Specifically, in the case of a high clustering coefficient, local efficiency, global efficiency, or low characteristic path length, individual originality performance was more predictive of interactive originality. Additionally, the relationship between fluency performance in the above two modes was not moderated by CE, flexibility, or CRRAT performance. This study identified the effects of neural transmission efficiency on the relationship between creativity in the two modes. This study investigated neurocognitive factors influencing creativity performance in interactive situations.},
 authors = {['Ching-Lin Wu']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Connectome', 'Diffusion tensor imaging', 'Divergent thinking', 'Online creativity task', 'Remote associates test']},
 title = {The moderating effects of brain network connectivity on the relationship between individual and interactive creativity},
 year = {2024}
}

@Filtered Article{a3860b2b-9549-47f6-b799-9016fbc24139,
 abstract = {This article investigates the different ways that uncertainty is understood and approached across design disciplines. Structural attitudes toward uncertainty are assessed in design thinking literature before other possible ways of viewing uncertainty in the design process are introduced. Uncertainty is then presented as a source of epistemological difference between design disciplines, and this difference is explicated through a project that uses literature survey and analytical diagramming to map differences between discipline attitudes to uncertainty. Our review identifies uncertainty as a prevalent source of discipline difference with the goal of better describing barriers, and effective responses to them, in inter- and trans-disciplinary design agendas.},
 authors = {['Loren Dyer', 'Jacqueline Power', 'Andrew Steen', 'Louise Wallis', 'Aidan Davison']},
 journal = {Design Studies},
 keywords = {['design processes', 'design thinking', 'epistemology', 'interdisciplinarity', 'uncertainty']},
 title = {Uncertainty and disciplinary difference: Mapping attitudes towards uncertainty across discipline boundaries},
 year = {2021}
}

@Filtered Article{a38a68fe-7bd2-453f-9699-bf0115eca452,
 abstract = {Effective methods leading to automated, computer-based solution of complex engineering design problems are studied in this paper. In particular, methods of automation of the finite element analyses are of primary interest here. These include algorithmic approaches, based on error estimation, adaptivity and smart algorithms, as well as heuristic approaches based on methods of knowledge engineering. A computational environment, which interactively couples h-p adaptive finite element methods with object-oriented programming and expert system tools, is presented. Several examples illustrate the merit and potential of the approaches studied here.},
 authors = {['W.W. Tworzydlo', 'J.T. Oden']},
 journal = {Engineering Fracture Mechanics},
 title = {Knowledge-based methods and smart algorithms in computational mechanics},
 year = {1995}
}

@Filtered Article{a3b86c92-26f5-4c60-8e0f-064d33ba481e,
 abstract = {A computational algorithm based on recursive equations is developed in order to estimate multigluon production processes at high energy hadron colliders. The partonic reactions gg→(n−2)g with n≤9 are studied and comparisons with known approximations are presented.},
 authors = {['Petros Draggiotis', 'Ronald H.P. Kleiss', 'Costas G. Papadopoulos']},
 journal = {Physics Letters B},
 title = {On the computation of multigluon amplitudes},
 year = {1998}
}

@Filtered Article{a3f2425a-0c1e-42e4-a3ca-d6720aaa5fb8,
 authors = {['L. Hoyte', 'P. Krysl', 'G. Chukkapalli', 'A. Majumdar', 'D.J. Choi', 'A. Trivedi', 'S.K. Warfield', 'M.S. Damaser']},
 journal = {Journal of Biomechanics},
 title = {Computational model of levator ani muscle stretch during vaginal delivery},
 year = {2006}
}

@Filtered Article{a4566eb1-5fde-40c7-a0ba-b689a3e3766f,
 abstract = {Background
Cardiomyocytes differentiated from human induced pluripotent stem cells (iPSC-CMs) can be used to study genetic cardiac diseases. In patients these diseases are manifested e.g. with impaired contractility and fatal cardiac arrhythmias, and both of these can be due to abnormal calcium transients in cardiomyocytes. Here we classify different genetic cardiac diseases using Ca2+ transient data and different machine learning algorithms.
Methods
By studying calcium cycling of disease-specific iPSC-CMs and by using calcium transients measured from these cells it is possible to classify diseases from each other and also from healthy controls by applying machine learning computation on the basis of peak attributes detected from calcium transient signals.
Results
In the current research we extend our previous study having Ca-transient data from four different genetic diseases by adding data from two additional diseases (dilated cardiomyopathy and long QT Syndrome 2). We also study, in the light of the current data, possible differences and relations when machine learning modelling and classification accuracies were computed by using either leave-one-out test or 10-fold cross-validation.
Conclusions
Despite more complex classification tasks compared to our earlier research and having more different genetic cardiac diseases in the analysis, it is still possible to attain good disease classification results. As excepted, leave-one-out test and 10-fold cross-validation achieved virtually equal results.},
 authors = {['Martti Juhola', 'Henry Joutsijoki', 'Kirsi Penttinen', 'Disheet Shah', 'Katriina Aalto-Setälä']},
 journal = {Computer Methods and Programs in Biomedicine},
 keywords = {['Genetic cardiac diseases', 'Induced pluripotent stem cells', 'Cardiomyocytes', 'Transient profiles', 'Machine learning', 'Classification', 'Leave-one-out', '-fold cross-validation']},
 title = {On computational classification of genetic cardiac diseases applying iPSC cardiomyocytes},
 year = {2021}
}

@Filtered Article{a458dd33-f49c-4e57-85c1-4238b03450cf,
 abstract = {When solving problems, like making predictions or choices, people often “sample” possibilities into mind. Here, we consider whether there is structure to the kinds of thoughts people sample by default—that is, without an explicit goal. Across three experiments we found that what comes to mind by default are samples from a probability distribution that combines what people think is likely and what they think is good. Experiment 1 found that the first quantities that come to mind for everyday behaviors and events are quantities that combine what is average and ideal. Experiment 2 found, in a manipulated context, that the distribution of numbers that come to mind resemble the mathematical product of the presented statistical distribution and a (softmax-transformed) prescriptive distribution. Experiment 3 replicated these findings in a visual domain. These results provide insight into the process generating people’s conscious thoughts and invite new questions about the value of thinking about things that are both likely and good.},
 authors = {['Adam Bear', 'Samantha Bensinger', 'Julian Jara-Ettinger', 'Joshua Knobe', 'Fiery Cushman']},
 journal = {Cognition},
 keywords = {['Sampling', 'Decision-making', 'Consciousness', 'Computation']},
 title = {What comes to mind?},
 year = {2020}
}

@Filtered Article{a4604f70-6f82-4731-89c4-f64d58daff21,
 abstract = {The purpose of this paper is to examine the role of digital transformation in the agri-food sector. The study emphasizes digitalization as both an enabler of production efficiency and a radical innovator, redesigning business models and agricultural practices. The study explores the development of applications and products that connect consumers, supply chain actors, and producers, leading to customized food products. It highlights the notion of circular agri-food systems for feedback loops in the value chain, minimizing waste and integrating environmental and social values. Also, the paper explores the challenges in digital adoption, including technical barriers, privacy, and security concerns. To overcome these challenges, an interdisciplinary approach is proposed, merging technological, ecological, economic, and governance insights. Key needs identified for successful digital transformation include enhanced data processing, technological convergence, sustainability awareness, interoperability, and user adoption. The conclusion stresses the importance of invoking systemic thinking, user-friendly designs, and interdisciplinary collaboration in making sure that digital innovations enable a sustainable and resilient food production system.},
 authors = {['Mahdi Vahdanjoo', 'Claus Grøn Sørensen', 'Michael Nørremark']},
 journal = {Current Opinion in Food Science},
 title = {Digital transformation of the agri-food system},
 year = {2025}
}

@Filtered Article{a4d78c36-279e-4819-a187-0d9f1f6dc38b,
 abstract = {With the diversification of generative artificial intelligence (AI) applications, the interest in their use in every segment and field of society in recent years has been increasing rapidly. One of these areas is programming learning and program writing processes. One of the generative AI tools used for this purpose is ChatGPT. The use of ChatGPT in program writing processes has become widespread, and this tool has a certain potential in the programming process. However, when the literature is examined, research results related to using ChatGPT for this purpose have yet to be found. The existing literature has a gap that requires exploration. This study aims to analyze the students' perspectives on using ChatGPT in the field of programming and programming learning. The study encompassed a cohort of 41 undergraduate students enrolled in a public university's Computer Technology and Information Systems department. The research was carried out within the scope of the Object-Oriented Programming II course for eight weeks. Throughout the research process, students were given project assignments related to the course every week, and they were asked to use ChatGPT while solving them. The research data was collected using a form consisting of open-ended questions and analyzed through content analysis. The research findings revealed both the advantages and disadvantages of ChatGPT usage, as perceived by the students. The students stated that the main benefits of using ChatGPT in programming learning are providing fast and mostly correct answers to questions, improving thinking skills, facilitating debugging, and increasing self-confidence. On the other hand, the main limitations of using ChatGPT in programming education were getting students used to laziness, being unable to answer some questions, or giving incomplete/incorrect answers, causing professional anxiety in students. Based on the results of the research, it can be said that it would be useful to integrate generative AI tools into programming courses considering the advantages they provide in programming teaching. However, appropriate measures should be taken regarding the limitations it brings. Based on the research findings, several recommendations were proposed regarding the integration of ChatGPT into lessons.},
 authors = {['Ramazan Yilmaz', 'Fatma Gizem {Karaoglan Yilmaz}']},
 journal = {Computers in Human Behavior: Artificial Humans},
 keywords = {['Generative artificial intelligence', 'ChatGPT', 'Programming', 'Programming learning', 'Student opinions']},
 title = {Augmented intelligence in programming learning: Examining student views on the use of ChatGPT for programming learning},
 year = {2023}
}

@Filtered Article{a51865f2-0fc6-4976-b6ad-862d3baaeb07,
 abstract = {The power of mathematics is discussed as a way of expressing reasoning, aesthetics and insight in symbolic non-verbal communication. The human culture of discovering mathematical ways of thinking in the enterprise of exploring the understanding of the nature and the evolution of our world through hypotheses, theories and experimental affirmation of the scientific notion of algorithmic and non-algorithmic ‘computation’, is examined and commended upon.},
 authors = {['Panos A. Ligomenides']},
 journal = {Journal of Computational and Applied Mathematics},
 keywords = {['Languages of mathematics', 'Mathematical reality', 'Information science', 'Cyber-world']},
 title = {The reality of Mathematics},
 year = {2009}
}

@Filtered Article{a5378e24-45ce-4dc6-8aec-c571bb4fb3e5,
 abstract = {This paper focuses on the differences between interior design students' design processes as derived from an analysis of their sketching and design behavior. By implementing qualitative methodologies in the analysis of the sketches produced in the conceptual phase of the design process, the experiment allows identifying sketching characteristics and profiles. The motivation is to show that sketches can serve as a tool to differentiate between designers and recognize their personal approach and design strategies. The results point to three distinct sketching profiles that characterize designers' use of sketches as a tool for thinking and communicating ideas during their solution generation process. Awareness to differences between students' sketches and design behavior may support the development of pedagogical concepts, strategies and tools.},
 authors = {['Shoshi Bar-Eli']},
 journal = {Design Studies},
 keywords = {['design processes', 'design research', 'design behavior', 'problem solving', 'design tools']},
 title = {Sketching profiles: Awareness to individual differences in sketching as a means of enhancing design solution development},
 year = {2013}
}

@Filtered Article{a5cecec2-0cf4-4284-ad8a-4790e0ee1378,
 abstract = {This paper proposed a human–machine integrated conceptual design method based on ontology, aiming at eliminating the uncertainties and blindness during the design process of ultra-precision grinding machine, especially for its key component–the ultra-precision hydrostatic guideways. Both the required knowledge and the database of hydrostatic guideways are modelled using ontologies to provide a consensual understanding among collaborators. Moreover, a formalized knowledge searching interface is developed to obtain similar instances as references according to the design principles and rules. Based on the imaginal thinking theory, the search process and the results are attempted to be presented in the form of image in order to fit human's customary intuitive thinking frame, facilitating the decision making process. Finally, our design of hydrostatic guideways for an ultra-precision grinding machine is used to validate the effectiveness of the method.},
 authors = {['Haibo Hong', 'Yuehong Yin']},
 journal = {Journal of Industrial Information Integration},
 keywords = {['Human machine integrated conceptual design', 'Information integration', 'High dimensional information integration', 'Ontology']},
 title = {Ontology-based conceptual design for ultra-precision hydrostatic guideways with human–machine interaction},
 year = {2016}
}

@Filtered Article{a5e93499-04de-4a29-b4f1-ff9fc64772c4,
 abstract = {The paper establishes a conceptual and methodological link between destinations and simulacrum through gamified tourism. As a paradigm, gamified tourism provides a rationale and a setting within which to apply computational economics to tourism, an approach amounting to tourism computability. Algorithmic destinations serve as “petri dishes” for real destinations. Utilizing rule sets that embody destination growth dynamics and visitor behavioural norms, seeding points in a cellular automata model (CA) were grown into algorithmic destinations. This is followed by a morphological transformation of geo-tagged satellite images into spatial points. The overlap of this additive and subtractive approach is at the core of tourism computability. Finally, the spatio-temporal dynamics of economic resilience was traced out through a visual phenomenology of algorithmic destinations. The gamification of tourism should be embraced as it holds up a flicker of hope for mature destinations, amidst the onset of museumification and increased commoditization of heritage sites. Gamification is treated as part of the reflexive cycle for destination authenticity; a notion that that Cohen (1988) alluded to in his discussion of emergent authenticity in destination image formation. Seen in this light, the museumification of Venice and the proliferation of its simulacrum, such as the Venetian Hotel in Macao and Venice-themed hotels across the globe, are prefigures and archetypes of a glorious age of gamified tourism.},
 authors = {['Bernard Lew Shian Loong']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['gamified tourism', 'algorithmic destinations', 'simulacrum', 'computational economics', 'tourism computability', 'reflexivity']},
 title = {Tourism and Simulacrum: The Computational Economy of Algorithmic Destinations},
 year = {2014}
}

@Filtered Article{a61f4a7a-64b6-4b97-987f-2f96a962334f,
 abstract = {Summary
Widespread changes to climate-sensitive systems are placing increased demands on risk assessments as a foundation for managing risk. Recent attention to compounding and cascading risks, deep uncertainty, and “bottom-up” risk assessment frameworks have foregrounded the need to account for systemic complexity in risk assessment methodology. We describe the sources of systemic complexity and highlight the role of risk assessments as a formal sense-making device that enables learning and organizing knowledge of the dynamic interplay between the climate-sensitive system and its (climatological) environment. We highlight boundary judgments as a core concern of risk assessments, helping to create islands of analytical and cognitive tractability in a complex, uncertain, and ambiguous world. We then point to three key concepts—boundary critique, multi-methodology, and second-order learning—as critical elements of contemporary risk assessment practice, and we weave these into an overarching framework to better account for systemic complexity in the assessment of climate risk.},
 authors = {['Seth Westra', 'Jakob Zscheischler']},
 journal = {One Earth},
 title = {Accounting for systemic complexity in the assessment of climate risk},
 year = {2023}
}

@Filtered Article{a62ca9c5-6620-4599-990c-9c2451a36bb2,
 abstract = {Concepts are the basic elements of propositions. Concepts can be best understood as constituted by its subset of objects (Extent) and subset of attributes (Intent). Psychological capacities of human mind for example, learning, thinking, memorizing can be performed by concepts and their association. In this paper, we will explain how human will be able to generalize concrete concepts of Formal Concept Analysis into abstract concepts. In particular, we model the functionalities of concept algebra by making use of Formal Concept Analysis; we illustrate the proposed model with experiments on sample context. This model simulates the thinking process of human mind.},
 authors = {['Radhika Shivhare', 'Ch. Aswani Kumar']},
 journal = {Procedia Computer Science},
 keywords = {['Abstraction', 'Cognitive Informatics', 'Concept Algebra', 'Formal Concept Analysis.']},
 title = {On the Cognitive Process of Abstraction},
 year = {2016}
}

@Filtered Article{a6671162-6180-4c19-b64d-40c690ffab3c,
 abstract = {This essay examines the idea and potential of a ‘computational approach to theory’, discusses methodological issues raised by such computational methods, and outlines the problems associated with the dissemination of computational methods and the exposition of computational results. We argue that the study of a theory need not be confined to proving theorems, that current and future computer technologies create new possibilities for theoretical analysis, and that by resolving these issues we will create an intellectual atmosphere in which computational methods can make substantial contributions to economic analysis.},
 authors = {['Kenneth L. Judd']},
 journal = {Journal of Economic Dynamics and Control},
 keywords = {['Computational approach', 'Theoretical analysis']},
 title = {Computational economics and economic theory: Substitutes or complements?},
 year = {1997}
}

@Filtered Article{a67a6f32-fbe2-461e-be66-a3d445c44cab,
 abstract = {This shallow-water-directed paper plans to consider a (2＋1)-dimensional generalized modified dispersive water-wave (2DGMDWW) system, which describes the nonlinear and dispersive long gravity waves travelling along two horizontal directions in the shallow water of uniform depth. With symbolic computation, (1) a hetero-Bäcklund transformation is constructed, coupling the solutions as for the 2DGMDWW system with the solutions as for a known (2＋1)-dimensional Boiti-Leon-Pempinelli system describing the water waves in an infinitely narrow channel of constant depth, with that hetero-Bäcklund transformation dependent on the shallow-water coefficients in the 2DGMDWW system, with the former solutions indicating certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave, while with the latter solutions related to the horizontal velocity and elevation of the water wave; (2) two sets of the bilinear forms are obtained, each set of which is shown to depend on the shallow-water coefficients in the 2DGMDWW system and to be linked to certain shallow-water-wave patterns for the height of the water surface and the horizontal velocity of the water wave; and (3) two sets of the N-soliton solutions are also worked out, each set of which is seen to rely on the shallow-water coefficients in the 2DGMDWW system and to represent the existence of N-solitonic shallow-water-wave patterns with respect to the height of the water surface and the horizontal velocity of the water wave, with N as a positive integer.},
 authors = {['Xin-Yi Gao']},
 journal = {Chinese Journal of Physics},
 keywords = {['Shallow water', 'Nonlinear and dispersive long gravity waves', '(2＋1)-dimensional generalized modified dispersive water-wave system', 'Hetero-Bäcklund transformation', 'Bilinear form', 'Soliton', 'Symbolic computation']},
 title = {Hetero-Bäcklund transformation, bilinear forms and multi-solitons for a (2＋1)-dimensional generalized modified dispersive water-wave system for the shallow water},
 year = {2024}
}

@Filtered Article{a6c46b8e-698e-42e9-9276-da40abd0a882,
 abstract = {The purpose of this research was to investigate how diversification of the repertoire of digital design techniques affects the creative behaviors of designers in the early design phases. The principal results of practice-based pilot experiments on the subject indicate three key properties of the hybrid digital tooling strategy. The strategy features intelligent human-machine integration, facilitating three different types of synergies between the designer and the digital media: human-dominated, machine-dominated, and a balanced human-machine collaboration. This strategy also boosts the cognitive behaviors of the designer by triggering divergent, transformative and convergent design activities and allowing for work on various abstraction levels. In addition, the strategy stimulates the explorative behaviors of the designer by encouraging the production of and interaction with a wide range of design representations, including physical and digital, dynamic and static objects. Thus, working with a broader range of digital modeling techniques can positively influence the creativity of designers in the early conception stages.},
 authors = {['Malgorzata A. Zboinska']},
 journal = {Journal of Computational Design and Engineering},
 keywords = {['Early-stage design', 'Digital design', 'Computational design', 'Architectural design', 'Hybrid digital design systems', 'Intelligent human-machine integration']},
 title = {Influence of a hybrid digital toolset on the creative behaviors of designers in early-stage design},
 year = {2019}
}

@Filtered Article{a6e466ad-8337-49aa-9b6b-393929e80065,
 abstract = {Several judgment and decision-making tasks require overriding an incorrect response that is signaled by miserly information processes. The successful detection and override of conflict between heuristic and analytic processes has been a focus of dual processes models, especially in adult samples. These miserly processing tendencies have also been described in developmental samples. The measurement of resistance to miserly information processing has been assessed using several tasks, including ratio bias, belief bias syllogisms, cognitive reflection, and disjunctive thinking tasks. Several of these tasks have been studied in developmental samples, including in the longitudinal study described in this volume. There is evidence to suggest that resistance to miserly information processing is measurable in children and youth. While judgment and decision-making tasks vary in the degree to which override of miserly processing is required, individuals also vary in their ability to resist miserly processing tendencies. Individual differences in resistance to miserly information processing serve as an additional foundation to support rational thinking performance.},
 authors = {['Maggie E. Toplak']},
 journal = {Academic Press},
 keywords = {['Miserly information processing', 'Dual process models', 'Children and youth', 'Development', 'Ratio bias', 'Belief bias syllogisms', 'Cognitive reflection']},
 title = {3 - Development of the ability to detect and override miserly information processing},
 year = {2022}
}

@Filtered Article{a6ff3f04-c692-4069-a0ea-74146cacbaf8,
 abstract = {The presence of multiple criteria for evaluating wastewater reuse applications indicates the potential usage of Multi-Criteria Decision-Making (MCDM) methods for this purpose. However, there is currently a scarcity of studies in the domain literature that utilize MCDM approaches in this application topic. This paper therefore advances the domain literature in two distinctive ways. Firstly, it analyzes and advances the reuse agenda of wastewater from thermal power plants, recognized as large-scale users of water, thus promoting greater water circularity. Secondly, it provides a methodological advance by integrating the notion of Three-Way Decision (3WD) into the computational structure of MCDM methods by introducing a middle reference point. Such an initiative results in a novel 3WD extension of the Measurement of Alternatives and Ranking according to COmpromise Solution (MARCOS) method. Additionally, this work provides a proof that the MARCOS method utilizes a compromise solution in identifying priority alternatives, along with the integration of a Weighted Aggregated Sum Product ASsessment (WASPAS) metric. An initial hypothetical example illustrates how the proposed approach augments the canonical MARCOS method, particularly in promoting the “thinking in threes” as a more natural information processing approach and the high degree of distinguishability of priorities between decision alternatives. An actual case study in a thermal power plant then demonstrates the contributions of this work. With the best-worst method used to determine the priorities of the decision attributes, the findings reveal that wastewater reuse applications achieving reduced costs for needed infrastructures, operational simplicity, technological compatibility, consumer safety and household savings are preferred by stakeholders. The 3WD-MARCOS approach identifies industrial and commercial use, municipal use, environmental restoration, and household use as the high-priority alternatives, with cooking and drinking as least preferred. These insights guide stakeholders in their design of initiatives that allocate resources for greater wastewater reuse. A comparative analysis yields high consistency of these findings with similar MCDM methods. In addition, the efficacy of the novel 3WD-MARCOS method highlights its potential in handling MCDM problems, including those promoting water circularity.},
 authors = {['Lanndon Ocampo', 'Jenebyb Cabigas', 'Dylan Jones', 'Ashraf Labib']},
 journal = {Applied Soft Computing},
 keywords = {['Water circularity', 'Wastewater', 'Water reuse', 'MARCOS', 'Three-way decision']},
 title = {An integrated three-way decision methodology for sustainability of wastewater circularity in thermal power plants},
 year = {2024}
}

@Filtered Article{a7037d47-e80c-4297-aa7a-7e718f297ae6,
 abstract = {Wind-generated wave energy is a renewable energy source that exhibits a huge potential for sustainable growth. The design and deployment of wave energy converters at a given location require the prediction of the amount of available wave energy flux. This and other wave parameters can be estimated by means of Computational Intelligence techniques (Neural, Fuzzy, and Evolutionary Computation). This paper reviews those used in wave energy applications, both in the resource estimation and in the design and control of wave energy converters. In particular, most of the applications of Neural Computation techniques, considered here in a broad sense, focus on the prediction of a variety of wave energy parameters by means of Multilayer Perceptrons and, at a lesser extent, by Support Vector Machines, and Extreme Learning Machines. Fuzzy Computation is also applied to estimate wave parameters and control floating wave energy converter. Evolutionary Computation algorithms are used to estimate parameters and design wave energy collectors. We complete this paper with a case study that illustrates, for the first time to the best of our knowledge, the potential of hybridizing a Coral Reefs Optimization algorithm with an Extreme Learning Machine to tackle the problem of significant wave height reconstruction.},
 authors = {['L. Cuadra', 'S. Salcedo-Sanz', 'J.C. Nieto-Borge', 'E. Alexandre', 'G. Rodríguez']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Computational intelligence techniques', 'Wave energy', 'Renewable energy', 'Wave energy converters', 'Environmental impact']},
 title = {Computational intelligence in wave energy: Comprehensive review and case study},
 year = {2016}
}

@Filtered Article{a72b8c4f-06bf-416f-8fd8-f2c59dfde5bc,
 abstract = {As a rhetorical device, metaphor plays an instrumental role in facilitating human thinking, cognition and communication. The translation of metaphors into English represents a significant challenge, involving cross-cultural, cross-linguistic and cross-domain considerations. In recent years, the rapid development of artificial intelligence has provided a new method and approach for English metaphor translation. This article mainly discusses the basic concept of artificial intelligence, puts forward the key technologies of metaphor translation in artificial intelligence, and then analyzes the difficulties and methods of metaphor translation with the purpose of providing a reference point and helpful insights.},
 authors = {['Zikang Wang', 'Jinlian Chai']},
 journal = {Procedia Computer Science},
 keywords = {['Artificial intelligence', 'English translation of metaphor', 'Machine translation']},
 title = {On Metaphor Translation into English Based on Artificial Intelligence},
 year = {2024}
}

@Filtered Article{a73d45dc-e92f-4071-ac45-ba29d5c1e49c,
 abstract = {Technology-enhanced learning environments provide ample opportunities for learners to self-regulate their learning processes and activities for achieving the intended learning outcomes in various disciplines from soft to hard sciences and from humanities to the natural and social sciences. This special issue discusses the emerging technological advancements and cutting-edge research on self-regulated learning dealing with different cognitive, motivational, emotional, and social processes of learning both at the individual and group levels. Specifically, it discusses how to optimally use advanced technologies to facilitate learners’ self-regulated learning for achieving their own individual learning needs and goals. In this special issue, seven researchers/research teams from the fields of collaborative learning, computational thinking, educational psychology, and learning analytics presented contributions to self-regulated learning with the goal of stimulating cross-border discussion in the field.},
 authors = {['Omid Noroozi', 'Sanna Järvelä', 'Paul A. Kirschner']},
 journal = {Computers in Human Behavior},
 title = {Multidisciplinary innovations and technologies for facilitation of self-regulated learning},
 year = {2019}
}

@Filtered Article{a741818f-a9b6-4c52-a7a7-34b22ac07518,
 abstract = {Drainage networks determination from digital elevation models (DEM) has been a widely studied problem in the last three decades. During this time, satellite technology has been improving and optimizing digitalized images, and computers have been increasing their capabilities to manage such a huge quantity of information. The rapid growth of CPU power and memory size has concentrated the discussion of DEM algorithms on the accuracy of their results more than their running times. However, obtaining improved running times remains crucial when DEM dimensions and their resolutions increase. Parallel computation provides an opportunity to reduce run times. Recently developed graphics processing units (GPUs) are computationally fast not only in Computer Graphics but in General Purpose Computation, the so-called GPGPU. In this paper we explore the parallel characteristics of these GPUs for drainage network determination, using the C-oriented language of CUDA developed by NVIDIA. The results are simple algorithms that run on low-cost technology with a high performance response, obtaining CPU improvements of up to 8×.},
 authors = {['L. Ortega', 'A. Rueda']},
 journal = {Computers & Geosciences},
 keywords = {['GPU', 'GPGPU', 'CUDA', 'Drainage network', 'D8 algorithm']},
 title = {Parallel drainage network computation on CUDA},
 year = {2010}
}

@Filtered Article{a751e47b-726f-4029-821a-e2759eae0c03,
 abstract = {The past researches emphasize merely the avoidance of over-learning at the system level and ignore the problem of over-learning at the model level, which lead to the poor performance of the evolutionary computation based stock trading decision-making system. This study presents a new evaluation approach to focus on evaluating the generalization capability at the model level. An empirical study was provided and the results reveal four important findings. First, the decision-making system generated at the model design stage outperforms the system generated at the model validation stage, which shows over-learning at the model level. Secondly, for the decision-making system generated either at the model design stage or at the model validation stage, the investment performance in the training period is much better than that in the testing period, exhibiting over-learning at the system level. Third, employing moving timeframe approach is unable to improve the investment performance at the model validation stage. Fourth, reducing the evolution generation and input variables are unable to avoid the over-learning at the model level. The major contribution of this study is to clarify the issue of over-learning at the model and the system level. For future research, this study developed a more reliable evaluation approach in examining the generalization capability of evolutionary computation based decision-making system.},
 authors = {['I-Cheng Yeh', 'Che-hui Lien', 'Yi-Chen Tsai']},
 journal = {Expert Systems with Applications},
 keywords = {['Genetic algorithms', 'Neural networks', 'Decision system', 'Stock market', 'Over-learning']},
 title = {Evaluation approach to stock trading system using evolutionary computation},
 year = {2011}
}

@Filtered Article{a760e892-d264-408b-b333-16cf4ad8fc31,
 abstract = {Transcranial magnetic stimulation (TMS) is often targeted using a model of TMS-induced electric field (E). In such navigated TMS, the E-field models have been based on spherical approximation of the head. Such models omit the effects of cerebrospinal fluid (CSF) and gyral folding, leading to potentially large errors in the computed E-field. So far, realistic models have been too slow for interactive TMS navigation. We present computational methods that enable real-time solving of the E-field in a realistic five-compartment (5-C) head model that contains isotropic white matter, gray matter, CSF, skull and scalp. Using reciprocity and Geselowitz integral equation, we separate the computations to coil-dependent and -independent parts. For the Geselowitz integrals, we present a fast numerical quadrature. Further, we present a moment-matching approach for optimizing dipole-based coil models. We verified and benchmarked the new methods using simulations with over 100 coil locations. The new quadrature introduced a relative error (RE) of 0.3–0.6%. For a coil model with 42 dipoles, the total RE of the quadrature and coil model was 0.44–0.72%. Taking also other model errors into account, the contribution of the new approximations to the RE was 0.1%. For comparison, the RE due to omitting the separation of white and gray matter was >11%, and the RE due to omitting also the CSF was >23%. After the coil-independent part of the model has been built, E-fields can be computed very quickly: Using a standard PC and basic GPU, our solver computed the full E-field in a 5-C model in 9000 points on the cortex in 27 coil positions per second (cps). When the separation of white and gray matter was omitted, the speed was 43–65 cps. Solving only one component of the E-field tripled the speed. The presented methods enable real-time solving of the TMS-induced E-field in a realistic head model that contains the CSF and gyral folding. The new methodology allows more accurate targeting and precise adjustment of stimulation intensity during experimental or clinical TMS mapping.},
 authors = {['Matti Stenroos', 'Lari M. Koponen']},
 journal = {NeuroImage},
 keywords = {['Transcranial magnetic stimulation (TMS)', 'Navigated transcranial magnetic stimulation', 'Electric field calculation', 'Coil model', 'Volume conductor model']},
 title = {Real-time computation of the TMS-induced electric field in a realistic head model},
 year = {2019}
}

@Filtered Article{a776fe3f-f516-40b7-b7a6-1267db066adc,
 abstract = {Large Language Models (LLMs) demonstrate impressive ability in handling reasoning tasks. However, unlike humans who can instinctively adapt their problem-solving strategies to the complexity of task, most LLM-based methods adopt a one-size-fits-all approach. These methods employ consistent models, sample sizes, prompting methods and levels of problem decomposition, regardless of the problem complexity. The inflexibility of these methods can bring unnecessary computational overhead or sub-optimal performance. To address this limitation, we introduce an Adaptive-Solver (AS) framework that dynamically adapts solving strategies to suit various problems, enabling the flexible allocation of test-time computational resources. The framework functions with two primary modules. The initial evaluation module assesses the reliability of the current solution using answer consistency. If the solution is deemed unreliable, the subsequent adaptation module comes into play. Within this module, various types of adaptation strategies are employed collaboratively. Through such dynamic and multi-faceted adaptations, our framework can help reduce computational consumption and improve performance. Experimental results from complex reasoning benchmarks reveal that our method can significantly reduce API costs (up to 85%) while maintaining original performance. Alternatively, it achieves up to 4.5% higher accuracy compared to the baselines at the same cost. The datasets and code are available at https://github.com/john1226966735/Adaptive-Solver.},
 authors = {['Jianpeng Zhou', 'Wanjun Zhong', 'Yanlin Wang', 'Jiahai Wang']},
 journal = {Information Processing & Management},
 keywords = {['Large language models', 'Reasoning', 'Math word problems', 'Test-time computation allocation', 'Dynamic strategy selection']},
 title = {Adaptive-solver framework for dynamic strategy selection in large language model reasoning},
 year = {2025}
}

@Filtered Article{a777be6d-4f5c-49f5-a4b9-b4e9b4e187ed,
 abstract = {We present an account of a process by which different conceptualisations of number can be blended together to form new conceptualisations via recognition of common features, and judicious combination of their distinctive features. The accounts of number are based on Lakoff and Núñez’s cognitively-based grounding metaphors for arithmetic. The approach incorporates elements of analogical inference into a generalised framework of conceptual blending, using some ideas from the work of Goguen. The ideas are worked out using Heuristic-Driven Theory Projection (HDTP, a method based on higher-order anti-unification). HDTP provides generalisations between domains, giving a crucial step in the process of finding commonalities between theories. In addition to generalisations, HDTP can also transfer concepts from one domain to another, allowing the construction of new conceptual blends. Alongside the methods by which conceptual blends may be constructed, we provide heuristics to guide this process.},
 authors = {['Markus Guhe', 'Alison Pease', 'Alan Smaill', 'Maricarmen Martinez', 'Martin Schmidt', 'Helmar Gust', 'Kai-Uwe Kühnberger', 'Ulf Krumnack']},
 journal = {Cognitive Systems Research},
 keywords = {['Mathematical cognition', 'Metaphor', 'Mathematical reasoning', 'Analogy', 'Anti-unification', 'Conceptual blending', 'HDTP']},
 title = {A computational account of conceptual blending in basic mathematics},
 year = {2011}
}

@Filtered Article{a81ce8a2-0a3a-42c3-b124-d47fc0616e62,
 abstract = {Risk entails every action, every level and every perspective of our lives. The ability to make advantageous decisions, to deal with uncertainties, to infer and estimate more or less probable outcomes, to manage risky or riskless situations compose the wider notion of risk literacy and may be inserted from formal preschool education. The current paper aims to enlighten the notion of risk literacy and safety education as a necessity in establishing pupils ready to accept failure, to achieve success, to take initiatives, to become self-competent, to develop probabilistic and statistical thinking, to confront uncertainty and in turn to face the challenges of modern risk society. It is argued that within the formal settings of preschool education, through developmentally appropriate activities, opportunities may be implemented in order to encourage children as future citizens to construct risk literate personalities. It is concluded that risk perception and management imply awareness, assessment, avoidance and adaptation and are connected with growth, maturity, practice, experiences, intuitions and computations.},
 authors = {['Zoi Nikiforidou', 'Jenny Pange', 'Theodore Chadjipadelis']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['First keywords', 'second keywords', 'third keywords', 'forth keywords']},
 title = {Risk Literacy in Early Childhood Education Under a Lifelong Perspective},
 year = {2012}
}

@Filtered Article{a8200bd0-3a23-4bba-a7be-03c50990cf91,
 abstract = {Quantum computational logics are special examples of quantum logic where formulas are supposed to denote pieces of quantum information (qubit-systems or mixtures of qubit-systems), while logical connectives are interpreted as reversible quantum logical gates. Hence, any formula of the quantum computational language represents a synthetic logical description of a quantum circuit. We investigate a many-valued approach to quantum information, where the basic notion of qubit has been replaced by the more general notion of qudit. The qudit-semantics allows us to represent as reversible gates some basic logical operations of Łukasiewicz many-valued logics. In the final part of the article we discuss some problems that concern possible implementations of gates by means of optical devices.},
 authors = {['M.L. {Dalla Chiara}', 'R. Giuntini', 'G. Sergioli', 'R. Leporini']},
 journal = {Fuzzy Sets and Systems},
 keywords = {['Quantum logics', 'Quantum tomography', 'Logical gates']},
 title = {A many-valued approach to quantum computational logics},
 year = {2018}
}

@Filtered Article{a83a3c56-56ea-4277-8136-c37e7d9e3985,
 abstract = {While much of the current study on quantum computation employs low-level formalisms such as quantum circuits, several high-level languages/calculi have been recently proposed aiming at structured quantum programming. The current work contributes to the semantical study of such languages by providing interaction-based semantics of a functional quantum programming language; the latter is, much like Selinger and Valiron's, based on linear lambda calculus and equipped with features like the ! modality and recursion. The proposed denotational model is the first one that supports the full features of a quantum functional programming language; we prove adequacy of our semantics. The construction of our model is by a series of existing techniques taken from the semantics of classical computation as well as from process theory. The most notable among them is Girard's Geometry of Interaction (GoI), categorically formulated by Abramsky, Haghverdi and Scott. The mathematical genericity of these techniques—largely due to their categorical formulation—is exploited for our move from classical to quantum.},
 authors = {['Ichiro Hasuo', 'Naohiko Hoshino']},
 journal = {Annals of Pure and Applied Logic},
 keywords = {['Higher-order computation', 'Quantum computation', 'Programming language', 'Geometry of interaction', 'Denotational semantics', 'Categorical semantics']},
 title = {Semantics of higher-order quantum computation via geometry of interaction},
 year = {2017}
}

@Filtered Article{a882d6b6-8d9d-4617-b5d5-0d9f8d6d1276,
 abstract = {On the dark side of creativity, creative ideation is intentionally used to damage others. This first electroencephalogram (EEG) study on malevolent creativity investigated task-related power (TRP) changes in the alpha band while n = 89 participants (52 women, 37 men) generated original ideas for revenge in the psychometric Malevolent Creativity Test. TRP changes were assessed for different stages of the idea generation process and linked to performance indicators of malevolent creativity. This study revealed three crucial findings: 1) Malevolent creativity yielded topographically distinct alpha power increases similar to conventional creative ideation. 2) Time-related activity changes during malevolent creative ideation were reflected in early prefrontal and mid-stage temporal alpha power increases in individuals with higher malevolent creativity performance. This performance-related, time-sensitive pattern of TRP changes during malevolent creativity may reflect early conceptual expansion from prosocial to antisocial perspectives, and subsequent inhibition of dominant semantic associations in favor of novel revenge ideas. 3) The observed, right-lateralized alpha power increases over the entire ideation phase may denote an additional emotional load of creative ideation. Our study highlights the seminal role of EEG alpha oscillations as a biomarker for creativity, also when creative processes operate in a malevolent context.},
 authors = {['Corinna M. Perchtold-Stefan', 'Christian Rominger', 'Ilona Papousek', 'Andreas Fink']},
 journal = {Neuroscience},
 keywords = {['malevolent creativity', 'EEG', 'alpha power', 'time-course', 'divergent thinking']},
 title = {Functional EEG Alpha Activation Patterns During Malevolent Creativity},
 year = {2023}
}

@Filtered Article{a88a02ee-1ceb-4139-8978-5c7eb9c20bb4,
 abstract = {Undergraduate science, technology, engineering, and mathematics (STEM) students are often trained to use technical typesetting software in order to produce authentic mathematical prose, though little research exists about how this writing technology impacts students’ thinking and computation process. Drawing upon survey and interview research conducted at two liberal arts institutions, the authors investigate student writing practices across several undergraduate mathematics courses that required the use of LaTeX (a common markup language allowing users to specify the appearance of text and its layout on the printed page). This article presents findings about how the use of LaTeX slowed down students’ writing process, encouraging greater revision and reflection as well as allowing students to identify errors in their work at more than one stage in the process. We also explore the affective learning outcomes of STEM students using typesetting software, including increased feelings of confidence and professionalization. This article seeks to contribute to the growing conversation about how STEM students transfer knowledge about writing across disciplines.},
 authors = {['Patrick Bahls', 'Amanda Wray']},
 journal = {Computers and Composition},
 keywords = {['Computer-Mediated communication', 'STEM writing', 'Writing process', 'LaTeX', 'Markup languages', 'Typesetting tools']},
 title = {LaTeXnics: The effect of specialized typesetting software on STEM students’ composition processes},
 year = {2015}
}

@Filtered Article{a88ba500-dc38-4825-b3dc-81387733d613,
 abstract = {Publisher Summary
This chapter discusses how different types of entities such as individual objects, sets of objects, and ensembles can function as items to be maintained in working memory (WM). All of these types of representations can be relevant to thinking about quantities, and each supports different kinds of quantity-relevant computations. One way to overcome the three- to four-item limit of WM is to bind together representations of individual objects into representations of sets of objects. Binding multiple individuals into a single higher-order group can increase the number of individual items that can be remembered, as in the well-known demonstrations of chunking by adults. Set representations play a critical role in many numerical processes, including representing the nested relationships between as well as representing the cardinality of an array. Many real-world scenes contain stimuli that do not lend themselves to representation qua individual objects or sets of objects. One can imagine enumerating a flock containing dozens of individual birds.},
 authors = {['Lisa Feigenson']},
 journal = {Academic Press},
 title = {Chapter 2 - Objects, Sets, and Ensembles},
 year = {2011}
}

@Filtered Article{a8a56d64-4a2f-4fdd-8d37-a42c3ac5451b,
 abstract = {We extend the framework for complexity of operators in analysis devised by Kawamura and Cook (2012) to allow for the treatment of a wider class of representations. The main novelty is to endow represented spaces of interest with an additional function on names, called a parameter, which measures the complexity of a given name. This parameter generalises the size function which is usually used in second-order complexity theory and therefore also central to the framework of Kawamura and Cook. The complexity of an algorithm is measured in terms of its running time as a second-order function in the parameter, as well as in terms of how much it increases the complexity of a given name, as measured by the parameters on the input and output side. As an application we develop a rigorous computational complexity theory for interval computation. In the framework of Kawamura and Cook the representation of real numbers based on nested interval enclosures does not yield a reasonable complexity theory. In our new framework this representation is polytime equivalent to the usual Cauchy representation based on dyadic rational approximation. By contrast, the representation of continuous real functions based on interval enclosures is strictly smaller in the polytime reducibility lattice than the usual representation, which encodes a modulus of continuity. Furthermore, the function space representation based on interval enclosures is optimal in the sense that it contains the minimal amount of information amongst those representations which render evaluation polytime computable.},
 authors = {['Eike Neumann', 'Florian Steinberg']},
 journal = {Theoretical Computer Science},
 keywords = {['Second-order complexity', 'Type two complexity', 'Interval computation', 'Computable analysis']},
 title = {Parametrised second-order complexity theory with applications to the study of interval computation},
 year = {2020}
}

@Filtered Article{a8d1e74b-b600-4211-be3d-8fc9eb9c2992,
 abstract = {There is much debate over the degree to which language learning is governed by innate language-specific biases, or acquired through cognition-general principles. Here we examine the probabilistic language acquisition hypothesis on three levels: We outline a novel theoretical result showing that it is possible to learn the exact generative model underlying a wide class of languages, purely from observing samples of the language. We then describe a recently proposed practical framework, which quantifies natural language learnability, allowing specific learnability predictions to be made for the first time. In previous work, this framework was used to make learnability predictions for a wide variety of linguistic constructions, for which learnability has been much debated. Here, we present a new experiment which tests these learnability predictions. We find that our experimental results support the possibility that these linguistic constructions are acquired probabilistically from cognition-general principles.},
 authors = {['Anne S. Hsu', 'Nick Chater', 'Paul M.B. Vitányi']},
 journal = {Cognition},
 keywords = {['Child language acquisition', 'Poverty of the stimulus', 'No negative evidence', 'Bayesian models', 'Minimum description length', 'Simplicity principle', 'Natural language', 'Probabilistic models', 'Identification in the limit']},
 title = {The probabilistic analysis of language acquisition: Theoretical, computational, and experimental analysis},
 year = {2011}
}

@Filtered Article{a8e8b115-93d8-48c2-9f9f-8a3d2ca5611c,
 abstract = {Abstract:
This chapter reviews the latest researches and advances in the uses of the computational modeling and ab initio calculations on the study of the MAX phases and their properties. The fundamentals and approaches of the density functional theory in the ab initio quantum mechanical calculations and the importance of the theory in the study of the MAX phases are introduced. The studies of the electronic structures and properties, in particular, the energy band structures and total and/or partial density of states of the MAX phases, by using the means of the density function theory are illustrated and discussed. The stability and occurrence of the MAX phases predicted and confirmed by the density functional theory based energetic calculations are addressed. The ab initio calculated elastic and other physical properties of the MAX phases, and the effects of pressure, defects and impurities on the various structural and physical properties are also discussed.},
 authors = {['E. Wu']},
 journal = {Woodhead Publishing},
 keywords = {['computational modeling', 'calculations', 'density function theory', 'energy band', 'electronic properties', 'density of states']},
 title = {10 - Computational modeling and ab initio calculations in MAX phases – II},
 year = {2012}
}

@Filtered Article{a90fa0ac-6cda-49c2-b1b6-0daea7ec4c01,
 authors = {['Guillaume Herbet', 'Gilles Lafargue', 'Hugues Duffau']},
 journal = {Cortex},
 title = {Rethinking voxel-wise lesion-deficit analysis: A new challenge for computational neuropsychology},
 year = {2015}
}

@Filtered Article{a935318e-301a-4d47-8314-83c9384c1e1c,
 abstract = {Proponents of artificial intelligence boast its many promises, including the potential for creativity. Whether the realities of artificial cognition align with these promises, however, remains hotly debated. In this chapter, we explore the role of artificial intelligence in creative problem-solving through the lens of cognition. Through this lens, we advance the argument that, at present, creative problem-solving remains a distinctly human capability. Specifically, we examine how artificial cognition can and cannot engage in each stage of creative problem-solving, as well as the underlying mechanisms of divergent and convergent thinking. Although we find little evidence to support the creativity of artificial cognition, we advance several ways in which artificial cognition can augment human cognition to enhance creative problem-solving.},
 authors = {['Kelsey E. Medeiros', 'Rebecca L. Marrone', 'Srecko Joksimovic', 'David H. Cropley', 'George Siemens']},
 journal = {Academic Press},
 keywords = {['Creativity', 'Artificial intelligence', 'Cognition']},
 title = {Chapter 19 - Promises and realities of artificial creativity},
 year = {2023}
}

@Filtered Article{a951e3af-6541-458a-ac19-82dee0d6101b,
 abstract = {Over the past few years numerous proposals have appeared that attempt to characterize consciousness in terms of what could be called its computational correlates: Principles of information processing with which to characterize the differences between conscious and unconscious processing. Proposed computational correlates include architectural specialization (such as the involvement of specific regions of the brain in conscious processing), properties of representations (such as their stability in time or their strength), and properties of specific processes (such as resonance, synchrony, interactivity, or information integration). In exactly the same way as one can engage in a search for the neural correlates of consciousness, one can thus search for the computational correlates of consciousness. The most direct way of doing is to contrast models of conscious versus unconscious information processing. In this paper, I review these developments and illustrate how computational modeling of specific cognitive processes can be useful in exploring and in formulating putative computational principles through which to capture the differences between conscious and unconscious cognition. What can be gained from such approaches to the problem of consciousness is an understanding of the function it plays in information processing and of the mechanisms that subtend it. Here, I suggest that the central function of consciousness is to make it possible for cognitive agents to exert flexible, adaptive control over behavior. From this perspective, consciousness is best characterized as involving (1) a graded continuum defined over quality of representation, such that availability to consciousness and to cognitive control correlates with properties of representation, and (2) the implication of systems of meta-representations.},
 authors = {['Axel Cleeremans']},
 journal = {Elsevier},
 title = {Computational correlates of consciousness},
 year = {2005}
}

@Filtered Article{a97be045-c7ee-4fac-9217-c8feb21ac1b1,
 abstract = {Smart world is an attractive prospect with comprehensive development of ubiquitous computing involving penetrative intelligence into ubiquitous things, including physical objects (e.g., wearable devices), cyber entities (e.g., cloud services), social people (e.g., social networking) and human thinking (e.g., brain cognition). This work systematically overviews related works in the field of the smart world, and explains prospects in emerging areas. The smart world evolutions are discussed through four progressive phases, and the representative projects are accordingly introduced. Meanwhile, smart world elements and the smart world driven applications are respectively analyzed in the contexts of cyber–physical–social-thinking hyperspace. Moreover, enabling technologies including ubiquitous intelligence, web intelligence, brain informatics, social computing, big data, and security and privacy are respectively discussed. Finally, perspectives referring to ubiquitous sensing, ubiquitous object modeling, smart services, and philosophical, ethical and legal issues, are presented for identifying trends and challenges in the smart world.},
 authors = {['Hong Liu', 'Huansheng Ning', 'Qitao Mu', 'Yumei Zheng', 'Jing Zeng', 'Laurence T. Yang', 'Runhe Huang', 'Jianhua Ma']},
 journal = {Future Generation Computer Systems},
 keywords = {['Smart world', 'Ubiquitous computing', 'Ambient intelligence', 'Cyber–physical–social-thinking', 'Internet of Things']},
 title = {A review of the smart world},
 year = {2019}
}

@Filtered Article{a9de09d2-52ff-4a03-8aac-734fa92d9b7c,
 authors = {['Sydney Gregory']},
 journal = {Design Studies},
 title = {Current design thinking: 24 papers from Design 79, I Chem E Midlands Branch (available from (ChemE Rugby) 336 pp, £15},
 year = {1982}
}

@Filtered Article{aa411d59-d422-40c4-93df-471d51975334,
 abstract = {There are different algebraic structures that one can use to model notions of computation. The most well- known are monads, but lately, applicative functors have been gaining popularity. These two structures can be understood as instances of the unifying notion of monoid in a monoidal category. When dealing with non-determinism, it is usual to extend monads and applicative functors with additional structure. However, depending on the desired non-determinism, there are different options of interaction between the existing and the additional structure. This article studies one of those options, which is captured algebraically by dioids. We generalise dioids to dioid categories and show how dioids in such a category model non- determinism in monads and applicative functors. Moreover, we study the construction of free dioids in a programming context.},
 authors = {['Ezequiel Postan', 'Exequiel Rivas', 'Mauro Jaskelioff']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['dioid', 'monad', 'Haskell', 'computational effect']},
 title = {Dioids for Computational Effects},
 year = {2018}
}

@Filtered Article{aa6e419a-545c-498a-8a25-5148765616a1,
 abstract = {This paper examines four current theoretical approaches to the representation and recognition of visual objects: structural descriptions, geometric constraints, multidimensional feature spaces and shape-space approximation. The strengths and weaknesses of the four theories are considered, with a special focus on their approach to categorization — a computationally challenging task which is not widely addressed in computer vision, where the stress is rather on the generalization of recognition across changes of viewpoint.},
 authors = {['S. Edelman']},
 journal = {Trends in Cognitive Sciences},
 title = {Computational theories of object recognition},
 year = {1997}
}

@Filtered Article{aa92b75f-3746-4ae1-a29d-89e317358e5c,
 abstract = {Sustainability is a wicked problem, which is hard to define in a unique way. It cannot be solved and should be treated in a participatory approach involving as many stakeholders in the process as possible. Participatory modeling is an efficient method for dealing with wicked problems. It involves stakeholders in an open-ended process of shared learning and can be essential for developing sustainable technologies. While there may be various levels of participation, the process evolves around a model of the system at stake. The model is built in interaction with the stakeholders, it provides to formalism to synchronize stakeholder thinking and knowledge about the system and to move towards consensus about the possible decision-making.},
 authors = {['Alexey Voinov']},
 journal = {Elsevier},
 keywords = {['Biases', 'Modeling process', 'Social media', 'Stakeholders', 'Wicked problem']},
 title = {Participatory Modeling for Sustainability},
 year = {2024}
}

@Filtered Article{aac1a860-8e7f-4705-a5d8-bc02d101210d,
 abstract = {Organizations are transforming as they adopt new technologies and use new sources of data, changing the experiences of employees and pushing organizational researchers to respond. As employees perform their daily activities, they generate vast digital data. These data, when combined with established methods and new analytic techniques, create unprecedented opportunities for studying human behavior at work and have fueled the rise of people analytics as a new institutional field of practice. In this chapter, I describe the emerging field of people analytics and new organizational phenomena that accompany the use of data and algorithms. These practices are affecting how individuals, groups, and organizations function, ranging from decision-making processes and work procedures, to communication and collaboration, to attempts to monitor and control employees. In each of these domains, I describe recent research and propose new research directions. Many of these domains intersect with the emerging field of Computational Social Science, in which disciplinary scholars are applying computational methods to an expanding array of digitized data, pursuing interests that extend far into the organizational domain. Organizational scholars are well-positioned to bridge organizational and disciplinary advances to stay at the forefront of research on the future of work.},
 authors = {['Jeffrey T. Polzer']},
 journal = {Research in Organizational Behavior},
 keywords = {['People analytics', 'Algorithms', 'Decision-making', 'Networks', 'Teams', 'Meetings', 'Culture', 'Monitoring', 'Computational social science', 'Organizational behavior']},
 title = {The rise of people analytics and the future of organizational research},
 year = {2022}
}

@Filtered Article{ab2571a8-982f-47ea-9e23-4a4563235268,
 abstract = {Question-asking, an underexplored aspect of creativity, is integral to creative problem-solving and information-seeking. Previous research reveals that lower creativity correlates with asking simpler, closed questions, while higher creativity correlates with complex, open-ended inquiries. The present study explores the relation between question asking complexity and problem-solving tasks involving open- and close-ended thinking and how these abilities generalize and compare to AI. In Study 1, participants (N = 89) completed the alternative questions task (AQT), a close-ended riddles task (Stumpers), and the alternate uses task (AUT), a creativity measure. Our results show AQT question complexity wasn't correlated with stumpers performance, although it correlated with AUT originality (r = .3). In Study 2, participants (N = 100) completed the AQT, AUT, and open-ended creative problem-solving (CPS) task. CPS responses were evaluated for originality and quality. A positive correlation was observed between CPS quality and AQT complexity (r = .29) and originality (r = .34). In study 3, AI agents (N = 100) completed the AQT, AUT, stumpers, and CPS tasks. Like humans, AI's AQT originality and complexity were related with open, but not closed problem-solving. AI questions were also significantly more creative and complex, it solved more stumpers and gave higher quality CPS solutions. Surprisingly, human and AI CPS originality didn't differ. We find significant links between question complexity and open—but not closed-ended—problem-solving in humans, which generalize to AI. Our results highlight the significance of complex and creative question-asking in everyday life and as an integral part of our problem-solving toolkit.},
 authors = {['Tuval Raz', 'Roni Reiter-Palmon', 'Yoed N. Kenett']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Question asking', 'Problem-solving', 'AI', 'Creativity']},
 title = {Open and closed-ended problem solving in humans and AI: The influence of question asking complexity},
 year = {2024}
}

@Filtered Article{ab31828f-1f4f-4a23-aa6c-06ae9af9e7e1,
 abstract = {In the paper, we are going to show how robotics is undergoing a shift in a bionic direction after a period of emphasis on artificial intelligence and increasing computational efficiency, which included isolation and extreme specialization. We assemble these new developments under the label of the morphological paradigm. The change in its paradigms and the development of alternatives to the principles that dominated robotics for a long time contains a more general epistemological significance. The role of body, material, environment, interaction and the paradigmatic status of biological and evolutionary systems for the principles of control are crucial here. Our focus will be on the introduction of the morphological paradigm in a new type of robotics and to contrast the interests behind this development with the interests shaping former models. The article aims to give a clear account of the changes in principles of orientation and control as well as concluding general observation in terms of historical epistemology, suggesting further political-epistemological analysis.},
 authors = {['Sascha Freyberg', 'Helmut Hauser']},
 journal = {Studies in History and Philosophy of Science},
 keywords = {['Bionics', 'Embodied cognition', 'Morphology', 'Morphological computation', 'Soft robotics', 'Principles of orientation and control']},
 title = {The morphological paradigm in robotics},
 year = {2023}
}

@Filtered Article{ab351fe4-b6dc-44af-ad57-91edb2064ccd,
 abstract = {In this chapter, we describe how computational biology can be aided by informatics infrastructure to provide the basis for in silico studies that no longer require the generation of data, and instead facilitate the collection, organization, and analysis of existing datasets that can drive discovery. A new reality is that we are awash in data and tools to analyze these data and one of the most significant challenges is that of enabling the researcher to discover datasets relevant to their work, collect these data, assess its quality, and analyze it. The development of an adequate infrastructure within a researcher’s institution greatly facilitates progress on this front, both in terms of the development of tools and the development of local informatics expertise necessary to complement the domain-specific expertise of the researcher. As an informatics community we often ponder the heterogeneity of tools and resources on a global scale at the expense of the more immediate local problems encountered on a routine basis. Here, we suggest that getting our own houses in order by first employing interoperable solutions that support and facilitate collaboration amongst the complementary disciplines within our own institutions places the informatics community in a better position to address global informatics challenges. This approach can ensure that the solutions implemented employ an architecture and standards that support interoperability. Indeed, this is an organizational and cultural challenge rather than a technological one. Organizational structure and practices are described that provide a comprehensive base of talent capable of creating an environment that supports a sustainable informatics infrastructure, and that can quickly grow as needed to support the specific and rapidly evolving needs unique to that institution.},
 authors = {['T.S. Kalbfleisch', 'G.A. Rempala', 'K.S. Ramos']},
 journal = {Elsevier},
 title = {2.33 - Genomics, Bioinformatics, and Computational Biology},
 year = {2010}
}

@Filtered Article{ab57c260-2a27-47ae-aa56-2d57a7b4d0c7,
 authors = {['David Vernon']},
 journal = {Physics of Life Reviews},
 keywords = {['Internal simulation', 'Embodied cognition', 'Cognitive robotics', 'Episodic future thinking']},
 title = {Internal simulation in embodied cognitive systems: Comment on “Muscleless motor synergies and actions without movements: From motor neuroscience to cognitive robotics” by Vishwanathan Mohan et al.},
 year = {2019}
}

@Filtered Article{ab73f0b7-50e8-41b1-89b8-dae36fd64797,
 abstract = {Summary
Forming a four-component compound from the first 103 elements of the periodic table results in more than 1012 combinations. Such a materials space is intractable to high-throughput experiment or first-principle computation. We introduce a framework to address this problem and quantify how many materials can exist. We apply principles of valency and electronegativity to filter chemically implausible compositions, which reduces the inorganic quaternary space to 1010 combinations. We demonstrate that estimates of band gaps and absolute electron energies can be made simply on the basis of the chemical composition and apply this to the search for new semiconducting materials to support the photoelectrochemical splitting of water. We show the applicability to predicting crystal structure by analogy with known compounds, including exploration of the phase space for ternary combinations that form a perovskite lattice. Computer screening reproduces known perovskite materials and predicts the feasibility of thousands more. Given the simplicity of the approach, large-scale searches can be performed on a single workstation.},
 authors = {['Daniel\xa0W. Davies', 'Keith\xa0T. Butler', 'Adam\xa0J. Jackson', 'Andrew Morris', 'Jarvist\xa0M. Frost', 'Jonathan\xa0M. Skelton', 'Aron Walsh']},
 journal = {Chem},
 keywords = {['functional materials', 'computational chemistry', 'materials design', 'solar energy', 'high-throughput screening', 'water splitting', 'perovskites', 'structure prediction', 'SDG7: Affordable and clean energy']},
 title = {Computational Screening of All Stoichiometric Inorganic Materials},
 year = {2016}
}

@Filtered Article{ab923711-ce26-497a-abb1-079612d96593,
 abstract = {Home Energy Management (HEM) has a significantly growing impact on strategic energy policy, digital equity, as well as housing development and transport issues. With the proliferation of home working, reliance on electricity for heating and cooling and the increasing needs for electric charging for transportation, there is an urgent need to develop novel ways for efficient management of home energy use. Current efforts focus on HEM technologies at individual household levels, without considering the social or spatial context or their collective community-wide interrelated dependencies. We propose a multifaceted agenda at the intersection of disciplinary domains to tackle this problem by using a multidimensional lens that draws on energy behaviour, architectural research, biomimetics, and computational design, simultaneously. Optimal and effective behavioural patterns can be extracted and abstracted from nature, informing a more collective and interrelated behavioural dependencies approach that considers the complex multidimensional energy use patterns of different housing typologies. This paper discusses the analytical benefits of this new research approach through a study of home energy management behaviour. The approach though could be expanded to consider other similar empirical contexts whereby sustainable multidimensional resource management is sought such as water use, food distribution as well as transport and mobility.},
 authors = {['Sonja Oliveira', 'Lidia Badarnah', 'Merate Barakat', 'Anna Chatzimichali', 'Ed Atkins']},
 journal = {Energy Research & Social Science},
 keywords = {['Architecture', 'Biomimetics', 'Computational design', 'Cross-disciplinary methods', 'Home energy management']},
 title = {Beyond energy services: A multidimensional and cross-disciplinary agenda for home energy management research},
 year = {2022}
}

@Filtered Article{abcfbfe6-3e86-488b-8e36-97d3c2a51109,
 authors = {['Margaret King']},
 journal = {Artificial Intelligence},
 title = {Thinking: Readings in cognitive science: P.N. Johnson-Laird and P.C. Wason Cambridge University Press, 1977},
 year = {1980}
}

@Filtered Article{abd0170f-acb2-4cbc-a3b7-ef5748b088a6,
 abstract = {Research on insight problem solving sets itself a challenging goal: How to explain the origin of a new idea. It compounds the difficulty of this challenge by traditionally seeking to explain the phenomenon in strictly mental terms. Rather, we suggest that thoughts and actions are bound to objects, inviting a granular description of the world within which thinking proceeds. As the reasoner transforms the world, the physical traces of these changes can be mapped in space and time. Not only can the reasoner see these changes, and act upon them, the researcher can develop new inscription devices that captures the trajectory of the creative arc along spatial and temporal coordinates. Kinenoetic is a term we employ to capture the idea that knowledge comes from the movement of objects and that this knowledge is both at the level of the problem-solver and at the level of the researcher. This form of knowledge can only be constructed in problem solving environments where reasoners can manipulate physical elements. A kinenoetic analysis tracks and maps the changes to the object-qua-models of proto solutions, and in the process unveils the physical genesis of new ideas and creativity. Our aim here is to lay out a method for using the objects commonly employed in interactive problem-solving research, tracing the process of thought to elucidate underlying cognitive mechanisms. Thus, the focus turns from the effects of objects on thoughts, to tracing object-thought mutualities as they are enacted and made visible.},
 authors = {['Wendy Ross', 'Frédéric Vallée-Tourangeau']},
 journal = {Methods in Psychology},
 keywords = {['Insight', 'Case study', 'Observation']},
 title = {Kinenoetic analysis: Unveiling the material traces of insight},
 year = {2021}
}

@Filtered Article{ac492329-1335-477a-a6c6-a835119e28cb,
 abstract = {Long noncoding RNAs (lncRNAs) make up a large proportion of transcriptome in eukaryotes, and have been revealed with many regulatory functions in various biological processes. When studying lncRNAs, the first step is to accurately and specifically distinguish them from the colossal transcriptome data with complicated composition, which contains mRNAs, lncRNAs, small RNAs and their primary transcripts. In the face of such a huge and progressively expanding transcriptome data, the in-silico approaches provide a practicable scheme for effectively and rapidly filtering out lncRNA targets, using machine learning and probability statistics. In this review, we mainly discussed the characteristics of algorithms and features on currently developed approaches. We also outlined the traits of some state-of-the-art tools for ease of operation. Finally, we pointed out the underlying challenges in lncRNA identification with the advent of new experimental data.},
 authors = {['Jing Li', 'Xuan Zhang', 'Changning Liu']},
 journal = {Computational and Structural Biotechnology Journal},
 keywords = {['LncRNA identification', '', 'Algorithm', 'Feature', 'Coding potential', 'sORF']},
 title = {The computational approaches of lncRNA identification based on coding potential: Status quo and challenges},
 year = {2020}
}

@Filtered Article{ac5314c6-e4fc-4aa3-8176-b8fb08b4fa4b,
 abstract = {The technology of building mind model is often called mind modeling, which aims to explore and study the human thinking mechanism.},
 authors = {['Zhongzhi Shi']},
 journal = {Elsevier},
 keywords = {['Mind model', 'Turing machine', 'physical symbol system', 'SOAR model', 'ACT-R model', 'CAM model', 'cognitive cycle', 'PMJ model']},
 title = {Chapter 4 - Mind model},
 year = {2021}
}

@Filtered Article{ac8b6a4e-788c-4f50-a0f5-6b44529fb266,
 abstract = {AI technology has already gone through one “winter,” and alarmist thinking may cause yet another one. Calls for a moratorium on AI research increase the salience of the public request for comment on “AI accountability.” Prohibitive approaches are an overreaction, especially when leveraged on virtual (non-embodied) AI agents. While there are legitimate concerns regarding expansion of AI models like ChatGPT in society, a better approach would be to forge a partnership between academia and industry, and utilize infrastructure of campuses to authenticate users and oversee new AI research. The public could also be involved with public libraries authenticating users. This staged approach to embedding AI in society would facilitate addressing ethical concerns, and implementing virtual AI agents in society in a responsible and safe manner.},
 authors = {['Veljko Dubljević']},
 journal = {Technology in Society},
 keywords = {['Artificial intelligence (AI)', 'Ethics', 'Public policy', 'Legitimacy', 'Oversight']},
 title = {Colleges and universities are important stakeholders for regulating large language models and other emerging AI},
 year = {2024}
}

@Filtered Article{acccb721-c11e-4992-bc00-f76ced9fa5cd,
 abstract = {Background
Depression is linked to cognitive biases towards more negative and less positive self-relevant information. Rumination, perseverative negative thinking about the past and the self, may contribute to these biases.
Methods
159 adolescents (12–18 years), with a range of depression symptoms, completed the SRET during fMRI. Multiple regressions tested associations between conventional self-report and ecological momentary assessment (EMA) measured rumination, and neural and behavioral responses during a self-referent encoding task (SRET).
Results
Higher rumination (conventional self-report and EMA) was associated with more negative and fewer positive words endorsed and recalled. Higher self-reported (but not EMA) rumination was associated with higher accuracy in recognizing negative words and greater insula and dorsal anterior cingulate activity to negative versus positive words.
Limitations
The sample included mostly non-Hispanic White participants with household incomes above the national average, highlighting the need for replication in more diverse samples. Word endorsement discrepancies required fMRI analyses to model neural response to viewing negative versus positive words.
Conclusions
Adolescents with higher rumination endorsed and recalled more negative and fewer positive words and recognized more negative words during the SRET. Higher insula reactivity, a key region for modulating externally-oriented attention and internally-oriented self-referential processes, may contribute to links between rumination and negative memory biases. These findings provide insight into neurocognitive mechanisms underlying depression.},
 authors = {['Laura Murray', 'Nigel M. Jaffe', 'Anna O. Tierney', 'Kristina Pidvirny', 'Emma G. Balkind', 'Batool S. Abbasi', 'Miranda Brown', 'Christian A. Webb']},
 journal = {Journal of Affective Disorders},
 keywords = {['Depression', 'Adolescence', 'Ecological momentary assessment', 'fMRI', 'Rumination', 'Self-referent encoding task']},
 title = {Brain mechanisms of rumination and negative self-referential processing in adolescent depression},
 year = {2024}
}

@Filtered Article{ace2af36-c0aa-4b8d-a640-6b1d439d0544,
 abstract = {The current social reality is characterized by all-encompassing change, which disrupts existing social structures at all levels. Yet the approach based on the ontological primacy of stable and often hierarchical structures is still prevalent in theoretical and, most importantly, practical thinking about social systems. We propose a conceptual framework for thinking about a dynamically changing social system: the Living Cognitive Society. Importantly, we show how it follows from a much broader philosophical framework, guided by the theory of individuation, which emphasizes the importance of relationships and interactive processes in the evolution of a system. The framework addresses society as a living cognitive system – an ecology of interacting social subsystems – each of which is also a living cognitive system. We argue that this approach can help us to conceive sustainable social systems that will thrive in the circumstances of accelerating change. The Living Cognitive Society is explained in terms of its fluid structure, dynamics and the mechanisms at work. We then discuss the disruptive effects of Information and Communication Technologies on the mechanisms at work. We conclude by delineating a major topic for future research – distributed social governance – which focuses on processes of coordination rather than on stable structures within global society.},
 authors = {['Viktoras Veitas', 'David Weinbaum']},
 journal = {Technological Forecasting and Social Change},
 keywords = {['Cognitive system', 'Living society', 'Information and communication technologies', 'Future social governance', 'Individuation', 'Cognitive development']},
 title = {Living Cognitive Society: A ‘digital’ World of Views},
 year = {2017}
}

@Filtered Article{ad547b71-0a5d-41f4-9c47-b30a04de7101,
 abstract = {Three-dimensional numerical modeling of fluid flows is an important research tool to understand many fluid dynamical effects observed in nature. With the strong growth of available computational resources the use of such models has greatly increased over the last years. Because the available mass storage has not increased in the same order as the CPU speed many researchers nowadays face the problem of how to store and transfer the large data sets produced by the model calculations for post-processing. The use of lossy wavelet-based compression techniques on this data has been investigated in several publications. These techniques are often specialized to one problem and are not easy to implement. In the area of digital media, however, advances have been made for still image (JPEG, JPEG-2000) and motion image (MPEG) compression. In this paper we investigate the usefulness of these image compression algorithms for the storage of data from computational fluid dynamics on regular cartesian grids. We analyze both the compression ratios achieved and the error introduced by these lossy compression schemes. We found that, for our purposes, the JPEG compression scheme allows an easy-to-use, portable, robust, and computationally efficient lossy compression. For the easy use of these compression algorithms we present a simple wrapper library.},
 authors = {['Jörg Schmalzl']},
 journal = {Computers & Geosciences},
 keywords = {['Computational fluid dynamics', 'Data compression', 'Visualization', 'Post-processing']},
 title = {Using standard image compression algorithms to store data from computational fluid dynamics},
 year = {2003}
}

@Filtered Article{ad760c06-e9fb-49b0-8bca-f78f828ebf98,
 abstract = {Due to the widespread use of the internet, computer network systems may be exposed to different types of attacks. For this reason, the intrusion detection systems (IDSs) are often used to protect the network systems. Network traffic data (i.e., network packets) includes many features. However, most of them are irrelevant and can lead to a decrease in the runtime and/or the detection performance of the IDS. Although various data mining methods have been applied to improve the effectiveness of IDS, research regarding IDSs having high detection rates and better runtime performance (i.e., lower computational cost) is ongoing. On the other hand, the dimensionality reduction techniques help to eliminate unnecessary features and reduce the computation time of a classification algorithm. In the literature, the feature selection methods (i.e., filter and wrapper) have been widely used for the dimensionality reduction in IDSs. Although the wrapper feature selection techniques outperform the filters, they are time-consuming. Again, the ensemble classifiers can achieve higher detection rates for IDSs compared to the stand-alone classifiers, but they require more computation time to build the model. In order to improve the runtime performance and the detection rate of IDS, a swift wrapper feature selection and a speedy ensemble classifier are proposed in this study. For the dimensionality reduction, the swift wrapper feature selection (i.e., DBDE-QDA) is used, which consists of dichotomous binary differential evolution (DBDE) and quadratic discriminant analysis (QDA). For attack detection, the speedy ensemble classifier is used, which combines Holte's 1R, random tree, and reduced error pruning tree. In the experiments, the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets are used. According to the experimental results, the proposed IDS reaches 95%–97.4%, 82.7%, and 99.5%–99.9% detection rates for the NSL-KDD, UNSW-NB15, and CICDDoS2019 datasets. In this way, the proposed IDS competes with the state-of-the-art methods in terms of detection rate and false alarm rate. In addition, the proposed IDS has a lower computational cost than the state-of-the-art methods. Moreover, DBDE-QDA reduces the dimension by 60.97%–82.92%, 73.46%, and 96.55%–98.85% for the NSL-KDD, UNSW-NB15, and CICDoS2019 datasets.},
 authors = {['Ezgi Zorarpaci']},
 journal = {Engineering Applications of Artificial Intelligence},
 keywords = {['Data mining', 'Ensemble classifier', 'Feature selection', 'Intrusion detection system']},
 title = {A fast intrusion detection system based on swift wrapper feature selection and speedy ensemble classifier},
 year = {2024}
}

@Filtered Article{ad77a1f1-6dc4-4088-8010-6769cb6e4b80,
 abstract = {The real microstructures of ceramic matrix composites (CMCs) play a crucial role in determining their damage behavior. However, considering the real microstructure within the high-fidelity numerical simulation usually leads to expensive computational costs. In this study, an end-to-end deep-learning (DL) framework is proposed to predict the evolution of damage fields for CMCs from their real microstructures, which are characterized through computed tomography (CT). Three sub-networks, including the microstructure processing network (MPN), elastic deformation prediction network (EPN), and damage sequence prediction network (DPN), are used to construct a two-stage DL model. In the first stage, the geometrical characteristics of real microstructure are precisely captured by the MPN with over 92 % precision for the yarns and matrix. In the second stage, the elastic deformation predicted by the EPN is taken as the intermediate variable to motivate the damage prediction of DPN with the MPN-predicted microstructure as input. The damage evolution of real microstructure is finally predicted with a mean relative error of 10.8 % for the primary damage variable fields. The high-damage regions in the microstructure can also be accurately captured with a mean precision of 87.9 %. The proposed model is further validated by the in-situ tensile experiment. The micro-cracks are proven to initiate and propagate in the high-damage regions. Compared with the high-fidelity numerical methods, this DL-based method can predict the damage evolution on the fly, avoiding time-consuming computation and poor convergence during the damage analysis.},
 authors = {['Rongqi Zhu', 'Guohao Niu', 'Panding Wang', 'Chunwang He', 'Zhaoliang Qu', 'Daining Fang']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Deep learning', 'Damage evolution', 'Ceramic matrix composites', 'Microstructure', 'Computed tomography']},
 title = {Prediction of damage evolution in CMCs considering the real microstructures through a deep-learning scheme},
 year = {2025}
}

@Filtered Article{ad94dff7-eece-4641-a727-3e18c6997a4a,
 abstract = {Single port access surgery requires several specialized and one-time use devices to perform the surgery. By making the specialized devices suitable for multiple use may reduce surgical cost and increase the popularity of single port access surgery. However, this requires a new design thinking that emphasize on modular design and sterilization. We are exploiting simulation and computational intelligence methods to aid the design process that includes splitting an existing single use device design into modules and identifying the parts from the modules for manufacturing. Linking the design of the device with manufacturing can be achieved using feature graphs. This paper relates the development of a multiple use hand instrument for single port access surgery by re- using and enhancing the design of single-use devices with the proposed simulation-based methodology.},
 authors = {['Chee-Kong Chui', 'Han-Tong Loh', 'Jun-Fung Yam']},
 journal = {Procedia CIRP},
 keywords = {['Medical device design', 'Simulation', 'Single port access surgery']},
 title = {Simulation Method for Developing Multiple-Use Medical Devices from Re-using and Enhancing Design of Single-Use Device},
 year = {2013}
}

@Filtered Article{ada53b83-36f2-445d-8e86-65276a98faa5,
 abstract = {Focusing on the opposing ways of thinking of philosophers and scientists to explain the generation of form in biological development, I show that today's controversies over explanations of early development bear fundamental similarities to the dichotomy of preformation theory versus epigenesis in Greek antiquity. They are related to the acceptance or rejection of the idea of a physical form of what today would be called information for the generating of the embryo as a necessary pre-requisite for specific development and heredity. As a recent example, I scrutinize the dichotomy of genomic causality versus self-organization in 20th and 21st century theories of the generation of form. On the one hand, the generation of patterns and form, as well as the constant outcome in development, are proposed to be causally related to something that is "preformed" in the germ cells, the nucleus of germ cells, or the genome. On the other hand, it is proposed that there is no pre-existing form or information, and development is seen as a process where genuinely new characters emerge from formless matter, either by immaterial "forces of life," or by physical-chemical processes of self-organization. I also argue that these different ways of thinking and the research practices associated with them are not equivalent, and maintain that it is impossible to explain the generation of form and constant outcome of development without the assumption of the transmission of pre-existing information in the form of DNA sequences in the genome. Only in this framework of "preformed" information can "epigenesis" in the form of physical and chemical processes of self-organization play an important role.},
 authors = {['Ute Deichmann']},
 journal = {BioSystems},
 keywords = {['Genomic causality', 'Developmental gene regulatory networks', 'Physical-chemical selforganization in form generation', 'Preformation versus epigenesis', 'Morphogenesis', 'Stochastic fluctuations', 'Eric Davidson', 'Aristotle']},
 title = {Contrasting philosophical and scientific views in the long history of studying the generation of form in development},
 year = {2024}
}

@Filtered Article{ada939a5-3b6d-47f8-87bc-170e44cf0ef1,
 abstract = {We present a generalized approach to stability of static equilibria of nonlinearly elastic rods, subjected to general loading, boundary conditions and constraints (of both point-wise and integral type), based upon the linearized dynamics stability criterion. Discretization of the governing equations leads to a non-standard (singular) generalized eigenvalue problem. A new efficient sparse-matrix-friendly algorithm is presented to determine its few left-most eigenvalues, which, in turn, yield stability/instability information. For conservative problems, the eigenvalue problem arising from the linearized dynamics stability criterion is also shown to be equivalent to that arising in the determination of constrained local minima of the potential energy. We illustrate the method with several examples. A novel variational formulation for extensible and unshearable rods is also proposed within the context of one of the example problems.},
 authors = {['Ajeet Kumar', 'Timothy J. Healey']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Stability', 'Elasticity', 'Rods', 'Constraints']},
 title = {A generalized computational approach to stability of static equilibria of nonlinearly elastic rods in the presence of constraints},
 year = {2010}
}

@Filtered Article{adeb87e6-8b22-4010-9aec-d0fbe921632a,
 abstract = {Does positive thinking predict variance in school grades over and above that predicted by cognitive ability? Six hundred and thirty nine high school students participated in a three-year longitudinal study that predicted grades using cognitive ability and three positive thinking variables – self-esteem, hope, and attributional style. Hope, positive attributional style and cognitive ability predicted higher grades, whilst self-esteem was a less consistent predictor of academic performance. Structural equation modelling revealed significant paths from cognitive ability, gender, and a second order positive thinking factor to grades. The results suggest that intelligence, gender, and positive thinking each play a unique role in predicting academic performance in youth. Some suggestions for further research are made.},
 authors = {['Peter Leeson', 'Joseph Ciarrochi', 'Patrick C.L. Heaven']},
 journal = {Personality and Individual Differences},
 keywords = {['Cognitive ability', 'Personality', 'Academic performance', 'Adolescents', 'Hope', 'Self-esteem', 'Attributional style', 'Psychometric']},
 title = {Cognitive ability, personality, and academic performance in adolescence},
 year = {2008}
}

@Filtered Article{adff7ab1-17f2-4e1b-977b-2d8269645c99,
 abstract = {Copeland and others have argued that the Church–Turing thesis (CTT) has been widely misunderstood by philosophers and cognitive scientists. In particular, they have claimed that CTT is in principle compatible with the existence of machines that compute functions above the “Turing limit,” and that empirical investigation is needed to determine the “exact membership” of the set of functions that are physically computable. I argue for the following points: (a) It is highly doubtful that philosophers and cognitive scientists have widely misunderstood CTT as alleged.1 In fact, by and large, computability theorists and mathematical logicians understand CTT in the exact same way. (b) That understanding most likely coincides with what Turing and Church had in mind. Even if it does not, an accurate exegesis of Turing and Church need not dictate how today's working scientists understand the thesis. (c) Even if we grant Copeland's reading of CTT, an orthodox stronger version of it which he rejects (Gandy's thesis) follows readily if we only accept a highly plausible necessary condition for what constitutes a deterministic digital computer. Finally, (d) regardless of whether we accept this condition, the prospects for a scientific theory of hypercomputation are exceedingly poor because physical science does not have the wherewithal to investigate computability or to discover its ultimate “limit.”},
 authors = {['Konstantine Arkoudas']},
 journal = {Journal of Applied Logic},
 keywords = {['Hypercomputation', 'Church–Turing thesis', "Gandy's thesis", 'Mechanical computability', 'Algorithms', 'Turing limit', 'Physical science', 'Physical computability']},
 title = {Computation, hypercomputation, and physical science},
 year = {2008}
}

@Filtered Article{ae49ceab-3feb-47f0-b91d-96950f0c449d,
 abstract = {CLF (the Concurrent Logical Framework) is a language for specifying and reasoning about concurrent systems. Its most significant feature is the first-class representation of concurrent executions as monadic expressions. We illustrate the representation techniques available within CLF by applying them to an asynchronous pi-calculus with correspondence assertions, including its dynamic semantics, safety criterion, and a type system with latent effects due to Gordon and Jeffrey.},
 authors = {['Kevin Watkins', 'Iliano Cervesato', 'Frank Pfenning', 'David Walker']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['logical frameworks', 'type theory', 'linear logic', 'concurrency']},
 title = {Specifying Properties of Concurrent Computations in CLF},
 year = {2008}
}

@Filtered Article{ae679524-bd79-4bd2-9e92-3e23425eba27,
 abstract = {We propose a new heuristic algorithm to improve the computational efficiency of the general class of Multi-Echelon Technique for Recoverable Item Control (METRIC) problems. The objective of a METRIC-based decision problem is to systematically determine the location and quantity of spares that either maximizes the operational availability of a system subject to a budget constraint or minimizes its cost subject to an operational availability target. This type of sparing analysis has proven essential when analyzing the sustainment policies of large-scale, complex repairable systems such as those prevalent in the defense and aerospace industries. Additionally, the frequency of these sparing studies has recently increased as the adoption of performance-based logistics (PBL) has increased. PBL represents a class of business strategies that converts the recurring cost associated with maintenance, repair, and overhaul (MRO) into cost avoidance streams. Central to a PBL contract is a requirement to perform a business case analysis (BCA) and central to a BCA is the frequent need to use METRIC-based approaches to evaluate how a supplier and customer will engage in a performance based logistics arrangement where spares decisions are critical. Due to the size and frequency of the problem there exists a need to improve the efficiency of the computationally intensive METRIC-based solutions. We develop and validate a practical algorithm for improving the computational efficiency of a METRIC-based approach. The accuracy and effectiveness of the proposed algorithm are analyzed through a numerical study. The algorithm shows a 94% improvement in computational efficiency while maintaining 99.9% accuracy.},
 authors = {['David R. Nowicki', 'Wesley S. Randall', 'Jose Emmanuel Ramirez-Marquez']},
 journal = {European Journal of Operational Research},
 keywords = {['Inventory', 'Performance based logistics', 'Logistics', 'Supply chain', 'Optimization', 'Outcome based contracting']},
 title = {Improving the computational efficiency of metric-based spares algorithms},
 year = {2012}
}

@Filtered Article{ae6debf0-8c8b-43f1-a782-13e8bcf92f12,
 abstract = {The extent to which a person engages in reflective thinking while problem-solving is often measured using the Cognitive Reflection Test (CRT; Frederick, 2005). Some past research has attributed poorer performance on the CRT to impulsiveness, which is consistent with the close conceptual relation between Type I processing and dispositional impulsiveness (and the putative relation between a tendency to engage in Type I processing and poor performance on the CRT). However, existing research has been mixed on whether such a relation exists. To address this ambiguity, we report two large sample size studies examining the relation between impulsiveness and CRT performance. Unlike previous studies, we use a number of different measures of impulsiveness, as well as measures of cognitive ability and analytic thinking style. Overall, impulsiveness is clearly related to CRT performance at the bivariate level. However, once cognitive ability and analytic thinking style are controlled, these relations become small and, in some cases, non-significant. Thus, dispositional impulsiveness, in and of itself, is not a strong predictor of CRT performance.},
 authors = {['Shane Littrell', 'Jonathan Fugelsang', 'Evan F. Risko']},
 journal = {Personality and Individual Differences},
 keywords = {['Cognitive reflection', 'Impulsiveness', 'Intuitive thinking', 'Delay discounting dual process']},
 title = {Not so fast: Individual differences in impulsiveness are only a modest predictor of cognitive reflection},
 year = {2020}
}

@Filtered Article{aed61ebc-f5e1-45d3-bab1-9ce5e63e55f9,
 abstract = {Graphics processing unit (GPU) is an electronic circuit which manipulates and modifies the memory for better image output. Deep learning involves huge amounts of matrix multiplications and other operations which can be massively parallelized and thus sped up on GPUs. A single GPU might have thousands of cores while a CPU usually has no more than 12 cores. GPU's practical applicability is affected by two issues: long training time and limited GPU memory, which is greatly influenced as the neural network size grows. In order to overcome these issues, this chapter presents various technologies in distributed parallel processing which improve the training time and optimize the memory, and hardware engine architectures will be explored for data size reduction. The GPUs generally used for deep learning are limited in memory size compared to CPUs, so even the latest Tesla GPU has only 16 GB of memory. Therefore, GPU memory cannot be increased to that extent easily, so networks must be designed to fit within the available memory. This could be a factor limiting progress, overcoming which would be highly beneficiary to the computational intelligence area.},
 authors = {['M. Madiajagan', 'S. Sridhar Raj']},
 journal = {Academic Press},
 keywords = {['Deep learning', 'Parallelization', 'Graphics processing unit', 'Hardware architecture', 'Memory optimization', 'Computational intelligence']},
 title = {Chapter 1 - Parallel Computing, Graphics Processing Unit (GPU) and New Hardware for Deep Learning in Computational Intelligence Research},
 year = {2019}
}

@Filtered Article{af1556ab-85a3-474e-abdc-e16f81f043e8,
 abstract = {Abstract thinking is a cognitive process that involves the assimilation of concepts reduced from diffuse sensory input, organized, and interpreted in a manner beyond the obvious. There are multiple facets by which abstraction is measured that include semantic, visual-spatial and social comprehension. This study examined the prevalence and course of abstract and concrete responses to semantic proverbs and aberrant abstraction (composite score of semantic, visual-spatial, and social comprehension) over 20 years in 352 participants diagnosed with schizophrenia, affective psychosis, and unipolar non-psychotic depression. We utilized linear models, two-way ANOVA and contrasts to compare groups and change over time. Linear models with Generalized Estimation Equation (GEE) to determine association. Our findings show that regardless of diagnosis, semantic proverb interpretation improves over time. Participants with schizophrenia give more concrete responses to proverbs when compared to affective psychosis and unipolar depressed without psychosis. We also show that the underlying structure of concretism encompasses increased conceptual overinclusion at index hospitalization and idiosyncratic associations at follow-up; whereas, abstract thinking overtime encompasses increased visual-spatial abstraction at index and rich associations with increased social comprehension scores at follow-up. Regardless of diagnosis, premorbid functioning, descriptive characteristics, and IQ were not associated with aberrant abstraction. Delusions are highly and positively related to aberrant abstraction scores, while hallucinations are mildly and positively related to this score. Lastly, our data point to the importance of examining the underlying interconnected structures of ‘established’ constructs vis-à-vis mixed methods to provide a description of the rich interior world that may not always map onto current quantitative measures.},
 authors = {['Cherise Rosen', 'Martin Harrow', 'Liping Tong', 'Thomas H. Jobe', 'Helen Harrow']},
 journal = {Schizophrenia Research},
 keywords = {['Abstraction', 'Concretism', 'Aberrant abstraction', 'Schizophrenia', 'Affective psychosis', 'Unipolar depression non-psychotic']},
 title = {A word is worth a thousand pictures: A 20-year comparative analysis of aberrant abstraction in schizophrenia, affective psychosis, and non-psychotic depression},
 year = {2021}
}

@Filtered Article{af4d6368-7f1b-4647-9289-b0cbed0ddf28,
 abstract = {On all sandy coastlines fringed by dunes, understanding localised air flow allows us to examine the potential sand transfer between the beach and dunes by wind-blown (Aeolian) action. Traditional thinking into this phenomenon had previously included only onshore winds as effective drivers of this transfer. Recent research by the authors, however, has shown that offshore air-flow too can contribute significantly, through lee-side back eddies, to the overall windblown sediment budget to coastal dunes. Under rising sea levels and increased erosion scenarios, this is an important process in any post-storm recovery of sandy beaches. Until now though, full visualisation in 3D of this newly recognised mechanism in offshore flows has not been achieved. Here, we show for the first time, this return flow eddy system using 3D computational fluid dynamics modelling, and reveal the presence of complex corkscrew vortices and other phenomena. The work highlights the importance of relatively small surface undulations in the dune crest which act to induce the spatial patterns of airflow (and transport) found on the adjacent beach.},
 authors = {['Derek W.T. Jackson', 'Meiring Beyers', 'Irene Delgado-Fernandez', 'Andreas C.W. Baas', 'Andrew J. Cooper', 'Kevin Lynch']},
 journal = {Geomorphology},
 keywords = {['Computational fluid dynamics', 'Aeolian', 'Foredunes', 'Transport', 'Airflow modelling', 'Lee side eddies']},
 title = {Airflow reversal and alternating corkscrew vortices in foredune wake zones during perpendicular and oblique offshore winds},
 year = {2013}
}

@Filtered Article{af6970e3-ab77-430d-ace6-9f927960fb4b,
 abstract = {Moving mesh methods were devised to redistribute a mesh in a smooth way, while keeping the number of vertices of the mesh and their connectivity unchanged. A fruitful theoretical point-of-view is to take such moving mesh methods and think of them as an application of the diffeomorphic density matching problem. Given two probability measures μ0 and μ1, the diffeomorphic density matching problem consists of finding a diffeomorphic pushforward map T such that T#μ0=μ1. Moving mesh methods are seen to be an instance of the diffeomorphic density matching problem by treating the probability density as the local density of nodes in the mesh. It is preferable that the restructuring of the mesh be done in a smooth way that avoids tangling the connections between nodes, which would lead to numerical instability when the mesh is used in computational applications. This then suggests that a diffeomorphic map T is desirable to avoid tangling. The first tool employed to solve the moving mesh problem between source and target probability densities on the sphere was Optimal Transport (OT). Recently Optimal Information Transport (OIT) was rigorously derived and developed allowing for the computation of a diffeomorphic mapping by simply solving a Poisson equation. Not only is the equation simpler to solve numerically in OIT, but with Optimal Transport there is no guarantee that the mapping between probability density functions defines a diffeomorphism for general 2D compact manifolds. In this manuscript, we perform a side-by-side comparison of using Optimal Transport and Optimal Information Transport on the sphere for adaptive mesh problems. We choose to perform this comparison with recently developed provably convergent solvers, but these are, of course, not the only numerical methods that may be used. We believe that Optimal Information Transport is preferable in computations due to the fact that the partial differential equation (PDE) solve step is simply a Poisson equation. For more general surfaces M, we show how the Optimal Transport and Optimal Information Transport problems can be reduced to solving on the sphere, provided that there exists a diffeomorphic mapping Φ:M→S2. This implies that the Optimal Transport problem on M with a special cost function can be solved with regularity guarantees, while computations for the problem are performed on the unit sphere.},
 authors = {['Axel G.R. Turnquist']},
 journal = {Journal of Computational Physics},
 keywords = {['Optimal transport', 'Optimal information transport', 'Diffeomorphic density matching', 'Moving mesh methods', 'Convergent numerical methods', 'Compact manifolds']},
 title = {Adaptive mesh methods on compact manifolds via Optimal Transport and Optimal Information Transport},
 year = {2024}
}

@Filtered Article{af7c7626-262a-4937-8fc3-06155b296ecc,
 abstract = {Rainforests play an important role in the global ecosystem. However, significant regions of them are facing deforestation and degradation due to several reasons. Diverse government and private initiatives were created to monitor and alert for deforestation increases from remote sensing images, using different ways to deal with the notable amount of generated data. Citizen Science projects can also be used to reach the same goal. Citizen Science consists of scientific research involving nonprofessional volunteers for analyzing, collecting data, and using their computational resources to outcome advancements in science and to increase the public’s understanding of problems in specific knowledge areas such as astronomy, chemistry, mathematics, and physics. In this sense, this work presents a Citizen Science project called ForestEyes, which uses volunteer’s answers through the analysis and classification of remote sensing images to monitor deforestation regions in rainforests. To evaluate the quality of those answers, different campaigns/workflows were launched using remote sensing images from Brazilian Legal Amazon and their results were compared to an official groundtruth from the Amazon Deforestation Monitoring Project PRODES. In this work, the first two workflows that enclose the State of Rondônia in the years 2013 and 2016 received more than 35,000 answers from 383 volunteers in the 2,050 created tasks in only two and a half weeks after their launch. For the other four workflows, even enclosing the same area (Rondônia) and different setups (e.g., image segmentation method, image spatial resolution, and detection target), they received 51,035 volunteers’ answers gathered from 281 volunteers in 3,358 tasks. In the performed experiments, it was possible to observe that the volunteers achieved satisfactory overall accuracy, higher than 75%, in the classification of forestation and non-forestation areas using the ForestEyes project. Furthermore, considering an efficient segmentation and a better image spatial resolution, they achieved almost 66% accuracy in the classification of recent deforestation, which is a great challenge to overcome. Therefore, these results show that Citizen Science might be a powerful tool in monitoring deforestation regions in rainforests as well as in obtaining high-quality labeled data.},
 authors = {['Fernanda B.J.R. Dallaqua', 'Álvaro L. Fazenda', 'Fabio A. Faria']},
 journal = {Future Generation Computer Systems},
 keywords = {['Citizen Science', 'Deforestation area detection', 'Rainforest', 'Tropical forest', 'Volunteered Thinking']},
 title = {ForestEyes Project: Conception, enhancements, and challenges},
 year = {2021}
}

@Filtered Article{afa7d3bb-b706-429c-a617-28e6d79c0290,
 abstract = {Human brain imaging typically employs structured and controlled tasks to avoid variable and inconsistent activation patterns. Here we expand this assumption by showing that an extremely open-ended, high-level cognitive task of thinking about an abstract content, loosely defined as “abstract thinking” - leads to highly consistent activation maps. Specifically, we show that activation maps generated during such cognitive process were precisely located relative to borders of well-known networks such as internal speech, visual and motor imagery. The activation patterns allowed decoding the thought condition at >95%. Surprisingly, the activated networks remained the same regardless of changes in thought content. Finally, we found remarkably consistent activation maps across individuals engaged in abstract thinking. This activation bordered, but strictly avoided visual and motor networks. On the other hand, it overlapped with left lateralized language networks. Activation of the default mode network (DMN) during abstract thought was similar to DMN activation during rest. These observations were supported by a quantitative neuronal distance metric analysis. Our results reveal that despite its high level, and varied content nature - abstract thinking activates surprisingly precise and consistent networks in participants’ brains.},
 authors = {['Aviva Berkovich-Ohana', 'Niv Noy', 'Michal Harel', 'Edna Furman-Haran', 'Amos Arieli', 'Rafael Malach']},
 journal = {NeuroImage},
 keywords = {['Abstract-thoughts', 'Visual imagery', 'Default mode network', 'Language', 'fMRI']},
 title = {Inter-participant consistency of language-processing networks during abstract thoughts},
 year = {2020}
}

@Filtered Article{afae9361-ec38-4e4c-aee2-c11388a1dc02,
 abstract = {The intersection of mathematical cognition, metacognition, and advanced technologies presents a frontier with profound implications for human learning and artificial intelligence. This paper traces the historical roots of these concepts from the Pythagoreans and Aristotle to modern cognitive science and explores their relevance to contemporary technological applications. We examine how the Pythagoreans' view of mathematics as fundamental to understanding the universe and Aristotle's contributions to logic and categorization have shaped our current understanding of mathematical cognition and metacognition. The paper investigates the role of Boolean logic in computational processes and its relationship to human logical reasoning, as well as the significance of Bayesian inference and fuzzy logic in modelling uncertainty in human cognition and decision-making. We also explore the emerging field of Chemical Artificial Intelligence and its potential applications. We argue for unifying mathematical metacognition with advanced technologies, including artificial intelligence and robotics, while identifying the multifaceted benefits and challenges of such unification. The present paper examines essential research directions for integrating cognitive sciences and advanced technologies, discussing applications in education, healthcare, and business management. We provide suggestions for developing cognitive robots using specific cognitive tasks and explore the ethical implications of these advancements. Our analysis underscores the need for interdisciplinary collaboration to realize the full potential of this integration while mitigating potential risks.},
 authors = {['Alexios Kouzalis', 'Antonios Antoniou', 'Nicos Rossides', 'Rita Panaoura', 'Priyanka Yadav']},
 journal = {BioSystems},
 keywords = {['Mathematical cognition', 'Metacognition', 'Cognition', 'Artificial intelligence', 'Machine learning', 'Robotics', 'Boolean logic', 'Bayesian inference', 'Fuzzy logic', 'Chemical artificial intelligence']},
 title = {Advanced technologies and mathematical metacognition: The present and future orientation},
 year = {2024}
}

@Filtered Article{afe6fd26-f279-4331-a5ec-5b733f53e5b7,
 abstract = {This chapter discusses various aspects of cognition and creativity. Cognitive theories focus on thinking skills and intellectual processes. The approaches to creative cognition are extremely varied. There are bridges between basic cognitive processes and creative problem solving, as well as connections with intelligence, problem solving, language, and other indications of individual differences. The basic processes are generally nomothetic, meaning that they represent universals. Divergent thinking is employed when an individual is faced with an open-ended task. From this perspective divergent thinking is a kind of problem solving. Divergent thinking is not synonymous with creative thinking, but it does tell something about the cognitive processes that may lead to original ideas and solutions. Many theories of creative cognition look to associative processes. Associative theories focus on how ideas are generated and chained together. Cognitive theories of creativity often focus specifically on the problem-solving process. A problem can be defined as a situation with a goal and an obstacle. The stage models of creative cognition are also elaborated.},
 authors = {['Mark A. Runco']},
 journal = {Academic Press},
 keywords = {['Threshold theory', 'IQ', 'Structure of intellect', 'Associative theory', 'Problem solving', 'Problem finding', 'Incubation', 'Insight', 'Intuition', 'Meta-cognition', 'Mindfulness', 'Overinclusive thinking']},
 title = {Chapter 1 - Cognition and Creativity},
 year = {2014}
}

@Filtered Article{affb5a5b-4ed8-44d7-a8b7-86650060f0f3,
 abstract = {Darwin concludes The Origin of Species with a splendid one-phrase poem,From so simple a beginningendless forms most beautiful and most wonderfulhave been, and are being, evolved. Darwin's “simple beginning” may be identified, in today's terminology, with dissipation—evolution's basic fuel. All the rest is commentary—or, more precisely, corollary. One can aptly apply Darwin's phrase to another kind of “simple beginning,” from which as well “endless forms most beautiful and most wonderful have been, and are being, evolved.” What I have in mind is a concept that is apparently the very antithesis of dissipation, namely, physics' fundamental assumption of invertibility—or “microscopic reversibility.” To paraphrase Dobzhansky, no sensible step can be taken today in information, communication, and computer sciences, as well as in fundamental physics, except in the light of invertibility.},
 authors = {['Tommaso Toffoli']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['invertibility', 'irreversibility', 'computation', 'dynamics', 'thermodynamics', 'entropy', 'second law of thermodynamics']},
 title = {From Such Simple a Beginning: The Momentous Consequences of Physics' Microscopic Reversibility for Communication and Computation—and Almost Anything Else},
 year = {2010}
}

@Filtered Article{b03803d4-49d1-42db-b905-99df926ac4fd,
 abstract = {This chapter surveys research on agent-based models used in finance. It will concentrate on models where the use of computational tools is critical for the process of crafting models which give insights into the importance and dynamics of investor heterogeneity in many financial settings.},
 authors = {['Blake LeBaron']},
 journal = {Elsevier},
 keywords = {['learning', 'evolutionary finance', 'financial time series', 'asset pricing', 'efficient markets', 'behavioral finance', 'market microstructure', 'genetic algorithms', 'neural networks', 'artificial financial markets', 'evolutionary computation']},
 title = {Chapter 24 Agent-based Computational Finance},
 year = {2006}
}

@Filtered Article{b12486be-c360-4b8f-9584-9f8df20523e6,
 abstract = {We rely on our visual system to cope with the vast barrage of incoming light patterns and to extract features from the scene that are relevant to our well-being. The necessary reduction of visual information already begins in the eye. In this review, we summarize recent progress in understanding the computations performed in the vertebrate retina and how they are implemented by the neural circuitry. A new picture emerges from these findings that helps resolve a vexing paradox between the retina's structure and function. Whereas the conventional wisdom treats the eye as a simple prefilter for visual images, it now appears that the retina solves a diverse set of specific tasks and provides the results explicitly to downstream brain areas.},
 authors = {['Tim Gollisch', 'Markus Meister']},
 journal = {Neuron},
 title = {Eye Smarter than Scientists Believed: Neural Computations in Circuits of the Retina},
 year = {2010}
}

@Filtered Article{b15b1a4d-11c8-4bf0-9472-02247a6b63f8,
 abstract = {Simultaneous Localization and Mapping enable a mobile robot that is exploring an uncharted environment to localize itself and calculate its path within the map. In the context of green technologies and applications, there is a growing need for efficient SLAM solutions that not only provide accurate localization and mapping but also minimize power consumption. EKF-SLAM is a SLAM solution based on the Extended Kalman Filter, it is well known In the domain of robotics for its ability to handle non-linear models, its ability to handle noise related to the sensors, and its extremely high degree of precision. To guarantee real-time performance, the EKF-SLAM implementation requires a high-performance hardware architecture. In light of this challenge, researchers are thinking about using parallel processing platforms like FPGAs, which can provide the required level of performance and meet strict constraints on physics, computing capacity, and electrical power. This study describes a hardware architecture's implementation design for EKF-SLAM on an FPGA platform. The entire design is built on the Cyclone 2 FPGA, which has a maximum speed of 114 MHz and 18577 LUTs, creating a highly efficient hardware architecture.},
 authors = {['Slama Hammia', 'Anas Hatim', 'Abdelilah Haijoub']},
 journal = {Procedia Computer Science},
 keywords = {['EKF-SLAM', 'Simultaneous Localization and Mapping', 'Green Technologies', 'FPGA']},
 title = {Enhancing Real-time Simultaneous Localization and Mapping with FPGA-based EKF-SLAM's Hardware Architecture},
 year = {2024}
}

@Filtered Article{b1ec420a-64e5-4233-8d74-3b4a9ae81340,
 abstract = {This paper reviews and extends previous work on the domain-theoretic notion of Machine Development. It summarizes the concept of Developmental Computation and shows how Interactive Computation can be understood as a stepping stone in the pathway from Classical to Developmental Computation. A critical appraisal is given of Classical Computation, showing in which ways its shortcomings tend to restrict the possible evolution of real computers, and how Interactive and Developmental Computation overcome such shortcomings. The idea that Developmental Computation is more encompassing than Interactive Computation is stressed. A formal framework for Developmental Computation is sketched, and the current frontier of the work on Developmental Computation is briefly exposed.},
 authors = {['Antônio Carlos da Rocha Costa', 'Graçaliz Pereira Dimuro']},
 journal = {Electronic Notes in Theoretical Computer Science},
 keywords = {['Interactive computation', 'developmental computation', 'domain theory', 'classical theory of computation']},
 title = {Interactive Computation: Stepping Stone in the Pathway From Classical to Developmental Computation},
 year = {2005}
}

@Filtered Article{b1f7ac33-375f-44b8-abb8-e9b933cd6997,
 abstract = {We examine how crowdfunder experience affects their reliance on information available on projects. Drawing on elaboration likelihood model and using data from Kickstarter, we apply machine learning techniques and choice modeling to examine the information provided by creators, investigating not only the descriptions, but also the pictures and the videos. We found that more experienced crowdfunders react positively to descriptions exhibiting higher analytical thinking, while less experienced crowdfunders rely more on cues that arouse attention (e.g., number of pictures and positive emotions in videos). We highlight the importance of considering how experience influences crowdfunders’ interpretation of different types of information.},
 authors = {['Yan Lin', 'Wai Fong Boh']},
 journal = {Information & Management},
 keywords = {['Experience', 'Elaboration Likelihood Model', 'Information Asymmetry', 'Crowdfunding']},
 title = {Informational cues or content? Examining project funding decisions by crowdfunders},
 year = {2021}
}

@Filtered Article{b269cd3c-1caf-48d0-b033-840d7425825a,
 abstract = {Peptides, biopolymeric compounds connected by peptide bonds, have garnered significant attention in recent years as their potential wide applications in fields such as drug delivery, tissue engineering, and antibiotics. Peptides exhibit excellent biocompatibility and stability due to their structural similarities to many bioactive substances found in human bodies. The self-assembly of peptides has piqued considerable interest with groundbreaking advancements achieved in experimental research. However, it is still a big challenge to establish comprehensive theoretical model to accurately describe the behavior of peptide self-assembly. Current peptide self-assembly designs primarily rely on experimental outcomes and general rules, which is inefficient and susceptible to human errors. In recent years, thanks to rapid advancements in computer techniques and theoretical methods, computational research has become a vital tool in complementing experimental research with rapid development witted in this field. This review delves into the description of peptide self-assembly, covering relevant sequences, structures, morphologies, rules, and application areas. It places particular emphasis on the recent progress in computational methods such as molecular dynamics (MD) simulations and machine learning (ML) techniques in the study. Finally, we provide a perspective on the application of computational methods to expedite exploration in the realm of multi-peptide self-assembly.},
 authors = {['Nana Cao', 'Kang Huang', 'Jianjun Xie', 'Hui Wang', 'Xinghua Shi']},
 journal = {Nano Today},
 keywords = {['Peptides', 'Self-assembly', 'Molecular dynamics', 'Machine learning']},
 title = {Self-assembly of peptides: The acceleration by molecular dynamics simulations and machine learning},
 year = {2024}
}

@Filtered Article{b2f1befb-0c4e-4246-8808-6b266e86b8e9,
 abstract = {Optic ataxia (OA) is generally thought of as a disorder of visually guided reaching movements that cannot be explained by any simple deficit in visual or motor processing. In this paper we offer a new perspective on optic ataxia; we argue that the popular characterisation of this disorder is misleading and is unrepresentative of the pattern of reaching errors typically observed in OA patients. We begin our paper by reviewing recent neurophysiological, neuropsychological, and functional brain imaging studies that have led to the proposal that the medial parietal cortex in the vicinity of the parietal-occipital junction (POJ) – the key anatomical site associated with OA – represents reaching movements in eye-centred coordinates, and that this ability is impaired in optic ataxia. Our perspective stresses the importance of the POJ and superior parietal regions of the human PPC for representing reaching movements in both extrinsic (eye-centred) and intrinsic (postural) coordinates, and proposes that it is the ability to simultaneously represent multiple spatial locations that must be directly compared with one another that is impaired in non-foveal OA patients. In support of this idea we review recent fMRI and behavioural studies conducted by our group that have investigated the anatomical correlates of posturally guided movements, and the movements guided by postural cues in patients presenting with optic ataxia.},
 authors = {['Stephen R. Jackson', 'Roger Newport', 'Masud Husain', 'Jane E. Fowlie', 'Michael O’Donoghue', 'Nin Bajaj']},
 journal = {Neuropsychologia},
 keywords = {['Optic ataxia', 'Neuropsychology of action', 'Reaching']},
 title = {There may be more to reaching than meets the eye: Re-thinking optic ataxia},
 year = {2009}
}

@Filtered Article{b3105e56-ce07-4335-bd57-da5dd15c62f5,
 abstract = {Modern technologies, such as RFID, offer never-before seen learning abilities to parts moving in supply chains. Logistics systems may be understood as complex adaptive logistics systems (CALS). They also may be conceived as electronic auction markets as ‘smart parts’ bid for the best routing and pricing from transportation firms. To ensure the world-wide functionality and efficiency of CALS transportation markets, we suggest the utility of an agent-based computational market design based on Blake LeBaron's stock-market model. Given that parts may be more or less smart, markets more or less complex, and self-organizing CALS systems probabilistically subject to the bullwhip effect, we suggest nine different computational CALS market-design options, offering more adaptivity to unexpected environmental contingencies.},
 authors = {['Bill McKelvey', 'Christine Wycisk', 'Michael Hülsmann']},
 journal = {International Journal of Production Economics},
 keywords = {['Supply chain management', 'Electronic auction market', 'I&C technologies', 'Complexity theory', 'Neural networks']},
 title = {Designing an electronic auction market for complex ‘smart parts’ logistics: Options based on LeBaron's computational stock market},
 year = {2009}
}

@Filtered Article{b31ea908-fbcc-4a63-90d4-631a63bf1f88,
 abstract = {In this chapter, we introduce a proverbial crossroads in graduate and postgraduate education and jobs. We use medicinal chemistry as a core example for many topics, representative of what could also be said about pharmacology and other critical disciplines involved in drug discovery. Many factors are at play today for drug hunters, including an explosion of information, available now, at your fingertips, a move away from memorization toward critical thinking, the importance of learning by doing, and what has in the past been called “the gathering storm.” Core drug discovery disciplines are discussed, along with the importance of diversity and interdisciplinary skills and the value of academia-industry symbiosis. Challenges in making sure we continue to educate and engage the best and the brightest to tackle important biomedical problems are considered, especially in the context of personalized medicine and its interfaces with big data, bioinformatics, pharmacogenomics, and more. Finally, we scratch the surface on how to navigate graduate school, postdocs, employers, and careers.},
 authors = {['Susan Miller', 'Walter Moos', 'Barbara Munk', 'Stephen Munk', 'Charles Hart', 'David Spellmeyer']},
 journal = {Woodhead Publishing},
 keywords = {['Academia', 'Career', 'Critical thinking', 'Diversity', 'Education', 'Graduate school', 'Immigration', 'Industry', 'Jobs', 'Learn by doing', 'Medicinal chemistry', 'Online education', 'Organic chemistry', 'Pharmaceutical', 'Pharmacology', 'Postdoctoral', 'Postgraduate', 'Master’s degree', 'Doctorate']},
 title = {Chapter 7 - Graduate and postgraduate education at a crossroads},
 year = {2023}
}

@Filtered Article{b3544c93-b174-457c-a06d-7c7dc6ed1a8b,
 abstract = {In ordinary differential equations, one distinguishes two cases: autonomous and nonautonomous. Roughly speaking, the theory of the latter is built upon the theory of the former. The same distinction should be applied to partial differential equations, where much less is known. Here I will focus on the question of the generation of periodic solutions for autonomous partial differential equation boundary value problems. Specifically, I consider the incompressible Navier-Stokes equations, and the important driven cavity problem. For simplicity, attention is restricted to two bifurcation parameters, the Reynolds number and the Aspect ratio. Only Dirichlet velocity boundary conditions are considered. Both the known theory and known computational results for the driven cavity are surveyed. The importance of computationally adhering to the div u = 0 condition to accurately simulate unsteady flows which will be qualitatively correct for the incompressible Navier-Stokes equations is stressed. The dependence of sustained periodicity upon the existence of highly localized vortex shedding sequences somewhere along the boundary is pointed out. A new analysis of the pressure boundary condition, based upon a general regularity principle, is given. A conjectured Hopf bifurcation criticality curve is explained.},
 authors = {['K. Gustafson']},
 journal = {Mathematical and Computer Modelling},
 keywords = {['Navier-Stokes equations', 'Driven cavity problem', 'Pressure boundary condition', 'Vortex shedding', 'Hopf bifurcation']},
 title = {Theory and computation of periodic solutions of autonomous partial differential equation boundary value problems, with application to the driven cavity problem},
 year = {1995}
}

@Filtered Article{b3a1e2a9-78fd-4e11-8801-6fbad521d394,
 abstract = {This paper reviews concepts, modeling approaches, and recent findings along a spectrum of different levels of abstraction of neural network models including generalization across (1) Samples, (2) Distributions, (3) Domains, (4) Tasks, (5) Modalities, and (6) Scopes. Strategies for (1) sample generalization from training to test data are discussed, with suggestive evidence presented that, at least for the ImageNet dataset, popular classification models show substantial overfitting. An empirical example and perspectives from statistics highlight how models’ (2) distribution generalization can benefit from consideration of causal relationships and counterfactual scenarios. Transfer learning approaches and results for (3) domain generalization are summarized, as is the wealth of domain generalization benchmark datasets available. Recent breakthroughs surveyed in (4) task generalization include few-shot meta-learning approaches and the emergence of transformer-based foundation models such as those used for language processing. Studies performing (5) modality generalization are reviewed, including those that integrate image and text data and that apply a biologically-inspired network across olfactory, visual, and auditory modalities. Higher-level (6) scope generalization results are surveyed, including graph-based approaches to represent symbolic knowledge in networks and attribution strategies for improving networks’ explainability. Additionally, concepts from neuroscience are discussed on the modular architecture of brains and the steps by which dopamine-driven conditioning leads to abstract thinking.},
 authors = {['Chris Rohlfs']},
 journal = {Neurocomputing},
 keywords = {['Literature review', 'Deep learning', 'Overfitting', 'Causality', 'Domain generalization', 'Transfer learning', 'Foundation models', 'Multimodal', 'Semantic knowledge', 'Abstraction', 'Biologically-inspired']},
 title = {Generalization in neural networks: A broad survey},
 year = {2025}
}

@Filtered Article{b47770d4-dde4-41dd-82b8-9c5bd112f2fc,
 abstract = {How similar are judgements concerning how we expect to perform an action, to how we actually behave? The veracity of such prospective action judgements, and the mechanisms by which they are computed, was explored in a series of tasks that involved either grasping (MC conditions) or thinking about grasping (PJ conditions) a dowel presented in various orientations. PJs concerning limits of comfortable hand supination and pronation when turning a dowel in the picture plane were highly consistent with values obtained during actual hand rotation (Exp. 1). The same was true for judgements regarding the level of awkwardness involved in adopting a prescribed grip (e.g. overhand with right hand) for dowels in various picture plane orientations (Exp. 2). When allowed to select the most natural grip (overhand versus underhand) or hand (left versus right) for engaging dowels in these orientations, subjects preferred virtually identical responses in both PJ and MC conditions. In both instances, they consistently chose the least awkward response options. As would be expected for actual movements, PJs involving awkward hand postures had longer response times (RTs), and were less accurate. Likewise, latencies for both grip and hand judgements tended to increase as a function of the angular distance between the current positions of subjects’ hands, and the orientation of the chosen posture. Together, these findings are consistent with a the hypothesis that PJs involve mentally simulated actions, or motor imagery. These results suggest that motor imagery does not depend on the existence of a completed premotor plan (Jeannerod, 1994), but may instead be involved in the planning process itself. A provisional model for the involvement of imagery in motor planning is outlined, as are a set of criteria for evaluating claims of the involvement of motor imagery in problem solving.},
 authors = {['Scott H Johnson']},
 journal = {Cognition},
 keywords = {['Motor imagery', 'Prospective judgements', 'Prehension']},
 title = {Thinking ahead: the case for motor imagery in prospective judgements of prehension},
 year = {2000}
}

@Filtered Article{b4bdb48d-9772-48c9-a9d2-c21d5ec022e0,
 abstract = {The ubiquity of smartphone and tablet devices, combined with the increasing availability of serious games, has enabled students to learn various abstract concepts in an appealing and convenient manner. While several researchers have explored the use of Augmented Reality (AR) in serious games, many of these games have not been critically explained or evaluated. To that end, we employed game-based learning methodologies and Game Learning Analytics (GLA) to systematize the design and evaluation of an AR-based serious game to teach programming. We evaluated our game for usability and effectiveness by conducting a user study on twenty-seven undergraduate students. The evaluation primarily consisted of a learning test conducted twice – before and after playing the game – along with a usability questionnaire that players completed after playing the game. Our results showed that players made significant progress after playing the game. The game helped players improve their basic programming skills, especially for the group having lower prior programming skills. The results highlighted various ways in which GLA can be used to benefit different stakeholders in the game. Based on players’ qualitative responses, we also identified several areas of improvement, most prominently the trade-off between ease of use and game complexity. We provide suggestions and discuss implications for future work.},
 authors = {['Vandit Sharma', 'Kaushal Kumar Bhagat', 'Huai-Hsuan Huang', 'Nian-Shing Chen']},
 journal = {Computers & Graphics},
 keywords = {['Augmented Reality', 'Computational thinking', 'Learning Analytics', 'Gamification', 'Feedback design', 'System usability']},
 title = {The design and evaluation of an AR-based serious game to teach programming},
 year = {2022}
}

@Filtered Article{b4ce32d8-19bd-4e94-9466-50a13e93b3c5,
 abstract = {We consider the competing attributes of natural intelligence (NI) and artificial intelligence (AI). Attention is paid to conceptual, theoretical, stylistic and structural aspects of both, and non-human intelligence. Intelligence is related to information processing and current views of physical structuring. Means of distinguishing between NI and AI are noted, and neural and digital structures are described. Pribram's bi-computational neural networks are introduced, and high-level Pribram computation is discussed. We describe the hierarchical Aquarium scheme, along with an AI implementation, and conclude with a proposition for future quantum-based artificial intelligence.},
 authors = {['Ron Cottam', 'Roger Vounckx']},
 journal = {BioSystems},
 title = {Intelligence: Natural, artificial, or what?},
 year = {2024}
}

@Filtered Article{b51cdafa-9b3f-48e6-b95a-540a57d8d45b,
 abstract = {The current study investigates the Flynn Effect (FE) and its relation to abstract thinking ability. We compare two cohorts of Estonian students (1933/36, n=888; 2006, n=912) using the Concepts (Logical Selection) subtest of the Estonian adaptation of the National Intelligence Tests (NIT). The item presentation order of the subtest correlates with the abstractness of the words used in the items (r=.609) of the subtest. The different test results (right, wrong and missing answers) were analysed in order to make an estimate of the FE magnitude. The FE for abstract thinking ability of those samples was 1.06 Hedges' g (adjusted for guessing). The magnitude of the FE is dependent upon the degree of difficulty of the items (an item's difficulty is estimated by determining its abstractness and its familiarity to students). The more difficult part of the subtest (the second half) showed a FE=1.80 whereas the easier part (the first half) of the subtest showed a FE=.72. Word abstractness was a strong predictor of all the testing results in both cohorts (Beta=.700). The familiarity of words used in the test items has no correlation with the test results if word abstractness is controlled in both cohorts. Our findings support Flynn's explanation that the FE is primarily an indicator of the rise in abstract thinking ability.},
 authors = {['Olev Must', 'Aasa Must', 'Jaan Mikk']},
 journal = {Intelligence},
 keywords = {['Flynn Effect', 'National Intelligence Tests', 'Abstract thinking', 'Guessing', 'Tork', 'Estonia']},
 title = {Predicting the Flynn Effect through word abstractness : Results from the National Intelligence Tests support Flynn's explanation},
 year = {2016}
}

@Filtered Article{b5789bcc-0ea4-40b6-b6d9-0dcc50520f71,
 abstract = {Because the brain performs Bayesian inference for the causes of its sensory data, the synaptic gain could encode the precision (inverse variance) of its beliefs using a hierarchical generative model and predictive coding. Several neurobiological risk factors for schizophrenia (NMDAR and GABAergic interneuron hypofunction) reduce both synaptic and “oscillatory” gain at high hierarchical areas. This could impair the encoding of precision at higher levels of the brain’s hierarchical model and increase expected precision at lower levels. This imbalance can account for many neurobiological and phenomenological findings in schizophrenia. Striatal D2R hyperactivity may increase the precision of current policies by inhibiting behavioral or cognitive switching. This could be a (dysfunctional) consequence of or even an attempt to compensate for prefrontal or hippocampal pathology. This D2R hyperactivity may also reduce learning from positive outcomes and affect the encoding of motivational (or informational) salience.},
 authors = {['R.A. Adams', 'K.J. Friston']},
 journal = {Academic Press},
 keywords = {['Schizophrenia', 'Bayesian brain', 'precision', 'aberrant salience', 'reversal', 'predictive coding', 'NMDAR', 'GABA', 'delusions']},
 title = {Chapter 16 - Brain Computations in Schizophrenia},
 year = {2016}
}

@Filtered Article{b5a55edb-97f0-449a-81de-fa3c46959a7a,
 abstract = {This study sought to model the interconnected and multidimensional factors influencing the decarbonization of Chile's electricity sector. Factors were identified through a structured review of articles found in the Web of Science. Factor interactions were then characterized through a survey and participatory systems modeling workshop with stakeholders from various fields in the Chilean energy sector. The model emerging from the workshop was structurally analyzed to identify and evaluate system leverage points used to inform recommendations for future policy and practice. A key leverage point identified in this analysis underscores the importance of stakeholder awareness regarding the benefits of renewable energy projects, serving as a crucial catalyst towards decarbonization by fostering citizen support and driving the implementation of favorable public policies. Conversely, the model showed that public opposition to transmission line construction, stemming from health, environmental, and property value concerns, can potentially lead to project delays, increased costs, and challenges in modernizing electrical grids. These findings emphasize the need for public engagement and effective communication to prioritize decarbonization while balancing short-term impacts with long-term benefits. The systemic and process-oriented insights gained from the application of the participatory modeling approach presented in this study, highlight the value of utilizing systems thinking and modeling approaches to inform future decarbonization strategies on a global scale.},
 authors = {['Karina D. Véliz', 'Jeffrey P. Walters', 'Carlos Fica', 'Carolina Busco']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Decarbonization', 'Chile', 'Renewable energy', 'Systems thinking', 'Participatory modeling']},
 title = {Modeling the interconnected drivers of power sector decarbonization in Chile},
 year = {2025}
}

@Filtered Article{b5c95e44-c620-4de4-9b36-2ee71edb79a3,
 abstract = {A series of original computational models written in NEURON of increasing physiological and morphological complexity were developed to determine the dominant causes of epileptiform behavior. Current injections to a model hippocampal pyramidal neuron consisting of three compartments produced the sustained depolarizations (SD) and simple paroxysmal depolarizing shifts (PDS) characteristic of ictal and interictal behavior in a cell, respectively. Our results indicate that SDs are the result of the semi-saturation of Na+, Ca2+ and K+ active channels, particularly the CaN, with regular Na+/K+ spikes riding atop a saturated depolarization; PDS rides on a similar semi-saturated depolarization whose shape depends more heavily on interactions between low-threshold voltage-gated Ca2+ channels (CaT) and Ca2+-dependent K+ channels. Our results reflect and predict recent physiological data, and we report here a cellular basis of epilepsy whose mechanisms reside mainly in the membrane channels, and not in specific morphology or network interactions, advancing a possible resolution to the cellular/network debate over the etiology of epileptiform activity.},
 authors = {['Avram D. Heilman', 'James Quattrochi']},
 journal = {Biosystems},
 keywords = {['Paroxysmal depolarizing shifts (PDS)', 'Sustained depolarizations (SD)', 'Hippocampus', 'Autaptic CA1/CA3 pyramidal neuron', 'Voltage-gated Ca channels', 'Ca-dependent K channels']},
 title = {Computational models of epileptiform activity in single neurons},
 year = {2004}
}

@Filtered Article{b61528d6-343b-45ba-a0b6-585be337530d,
 abstract = {The classical notion of a ‘language of thought’ (LoT), advanced prominently by the philosopher Jerry Fodor, is an influential position in cognitive science whereby the mental representations underpinning thought are considered to be compositional and productive, enabling the construction of new complex thoughts from more primitive symbolic concepts. LoT theory has been challenged because a neural implementation has been deemed implausible. We disagree. Examples of critical computational ingredients needed for a neural implementation of a LoT have in fact been demonstrated, in particular in the hippocampal spatial navigation system of rodents. Here, we show that cell types found in spatial navigation (border cells, object cells, head-direction cells, etc.) provide key types of representation and computation required for the LoT, underscoring its neurobiological viability.},
 authors = {['Nina Kazanina', 'David Poeppel']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['language-of-thought', 'symbolic representation', 'computational theory of mind', 'spatial navigation', 'compositionality']},
 title = {The neural ingredients for a language of thought are available},
 year = {2023}
}

@Filtered Article{b65f2cc3-6fe2-41c2-9645-45214d1034b8,
 abstract = {In this paper we report validation efforts around the finite-to-finite strand of a provisional learning progression (LP) for the concept of function. We regard an LP as an empirically-verified account of how student understandings form over time and in response to instruction. The finite-to-finite strand of the LP was informed by literature on students’ thinking and learning related to functions as well as the Algebra Project’s curricular approach, which is designed for students who are traditionally underserved by mathematics education. Developing and validating an LP is a multi-step, cyclic process. Here we report on one step in this process, an item and response analysis. Data sources include 680 students’ responses to 13 multipart computer-delivered tasks. Results suggest that revisions to the items, associated scoring rubrics, and in some instances the LP are warranted. We illustrate this task, rubric, and LP revision process through an item analysis for a selected task.},
 authors = {['Cheryl L. Eames', 'Edith Aurora Graf', 'Peter W. {van Rijn}', 'Greg Budzban', 'Tammy Voepel']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Learning progressions', 'Concept of function', 'Representation', 'Secondary students']},
 title = {The finite-to-finite strand of a learning progression for the concept of function: A research synthesis and cognitive analysis},
 year = {2021}
}

@Filtered Article{b66c27c5-cf23-403f-bed5-0429858cb091,
 abstract = {This paper discusses the aspects of twelve first-year linear algebra students' thinking modes displayed on their interview responses to questions addressing linear independence ideas. Studying thinking modes allowed us to make inferences about the role of differing instructional modalities in shaping one's cognitive structures.},
 authors = {['Hamide Dogan']},
 journal = {Linear Algebra and its Applications},
 keywords = {['Mathematics education', 'Linear algebra', 'Thinking modes', 'Instructional modalities', 'Cognitive schemes']},
 title = {Differing instructional modalities and cognitive structures: Linear algebra},
 year = {2018}
}

@Filtered Article{b6873b50-78a2-4485-a086-57880a31ed19,
 abstract = {The rapid development of Advanced Air Mobility (AAM) in recent years suggests a promise to use electric vertical takeoff and landing aircraft (eVTOLs) for package delivery in metro areas. While eVTOL manufacturers and logistics service providers are actively developing prototype eVTOLs and exploring their potentials for moving freight, a system thinking about the suitability and ways to operate an eVTOL-based package delivery system remains scarce. A key aspect of the system thinking is the noise impact of eVTOL operations on surrounding communities. In this study, we provide an operation planning framework that aims to prepare AAM to be both economically efficient and community friendly for package delivery. We first develop a method to quantify the community noise impact of an eVTOL operation, using a “population exposure” measure which is based on the level of sound generated and accounts for both the number of people impacted and duration of the impact. Then, a bi-objective integer programming model is formulated which simultaneously optimizes total shipping cost and community noise impact of eVTOL operations. The optimization takes into consideration operational constraints including maximum distance for local delivery, latest package departure time from the warehouse, and eVTOL fleet size and carrying capacity. A tailored solution algorithm which augments non-dominated sorting genetic algorithm 2 (NSGA2) with compact solution representation, guided generation of initial population of solutions, and customized local search heuristics is devised. The model and the algorithm are implemented in a case study in the Chicago metro area. Numerical results reveal the trade-off between the minimization of shipping cost and community noise impact. Several operational insights about eVTOL-based package delivery are obtained. The computational efficiency and effectiveness of the proposed solution algorithm are also demonstrated in comparison with alternative solution methods.},
 authors = {['Nahid Parvez Farazi', 'Bo Zou']},
 journal = {Transportation Research Part E: Logistics and Transportation Review},
 keywords = {['Advanced air mobility (AAM)', 'eVTOL', 'Package delivery', 'Community noise impact', 'Bi-objective integer programming model', 'Tailored solution algorithm']},
 title = {Planning electric vertical takeoff and landing aircraft (eVTOL)-based package delivery with community noise impact considerations},
 year = {2024}
}

@Filtered Article{b6e1e5d1-2f41-4285-8fd8-881f990d67cb,
 abstract = {We review recent theoretical and algorithmic advances for the modeling of protein ligand binding free energies. We first describe a statistical mechanics theory of noncovalent association, with particular focus on deriving the fundamental formulas on which computational methods are based. The second part reviews the main computational models and algorithms in current use or development, pointing out the relations with each other and with the theory developed in the first part. Particular emphasis is given to the modeling of conformational reorganization and entropic effect. The methods reviewed are free energy perturbation, double decoupling, the Binding Energy Distribution Analysis Method, the potential of mean force method, mining minima and MM/PBSA. These models have different features and limitations, and their ranges of applicability vary correspondingly. Yet their origins can all be traced back to a single fundamental theory.},
 authors = {['Emilio Gallicchio', 'Ronald M. Levy']},
 journal = {Academic Press},
 keywords = {['Quasi-chemical description', 'Statistical mechanics', 'Potential of mean force', 'PDT', 'MM/PBSA', 'free energy perturbation', 'BEDAM', 'double decoupling']},
 title = {Recent theoretical and computational advances for modeling protein–ligand binding affinities},
 year = {2011}
}

@Filtered Article{b7063a26-f54c-4ae8-aa21-d6c7cb240761,
 abstract = {A large number of computational models of information processing in the basal ganglia have been developed in recent years. Prominent in these are actor–critic models of basal ganglia functioning, which build on the strong resemblance between dopamine neuron activity and the temporal difference prediction error signal in the critic, and between dopamine-dependent long-term synaptic plasticity in the striatum and learning guided by a prediction error signal in the actor. We selectively review several actor–critic models of the basal ganglia with an emphasis on two important aspects: the way in which models of the critic reproduce the temporal dynamics of dopamine firing, and the extent to which models of the actor take into account known basal ganglia anatomy and physiology. To complement the efforts to relate basal ganglia mechanisms to reinforcement learning (RL), we introduce an alternative approach to modeling a critic network, which uses Evolutionary Computation techniques to ‘evolve’ an optimal RL mechanism, and relate the evolved mechanism to the basic model of the critic. We conclude our discussion of models of the critic by a critical discussion of the anatomical plausibility of implementations of a critic in basal ganglia circuitry, and conclude that such implementations build on assumptions that are inconsistent with the known anatomy of the basal ganglia. We return to the actor component of the actor–critic model, which is usually modeled at the striatal level with very little detail. We describe an alternative model of the basal ganglia which takes into account several important, and previously neglected, anatomical and physiological characteristics of basal ganglia–thalamocortical connectivity and suggests that the basal ganglia performs reinforcement-biased dimensionality reduction of cortical inputs. We further suggest that since such selective encoding may bias the representation at the level of the frontal cortex towards the selection of rewarded plans and actions, the reinforcement-driven dimensionality reduction framework may serve as a basis for basal ganglia actor models. We conclude with a short discussion of the dual role of the dopamine signal in RL and in behavioral switching.},
 authors = {['Daphna Joel', 'Yael Niv', 'Eytan Ruppin']},
 journal = {Neural Networks},
 keywords = {['Basal ganglia', 'Dopamine', 'Reinforcement learning', 'Actor–critic', 'Dimensionality reduction', 'Evolutionary computation', 'Behavioral switching', 'Striosomes/patches']},
 title = {Actor–critic models of the basal ganglia: new anatomical and computational perspectives},
 year = {2002}
}

@Filtered Article{b71e6073-4e12-4e13-8720-8e4aa47525e3,
 abstract = {Intention recognition is one of the core components of mindreading, an important process in social cognition. Human beings, from age of 18months, have been shown to be able to extrapolate intentions from observed actions, even when the performer failed at achieving the goal. Existing accounts of intention recognition emphasize the use of an intent (plan) library, which is matched against observed actions for recognition. These therefore cannot account for recognition of failed sequences of actions, nor novel actions. In this paper, we begin to tackle these open questions by examining computational models for components of human intention recognition, which emphasize the ability of humans to detect and identify intentions in a sequence of observed actions, based solely on the rationality of movement (its efficiency). We provide a high-level overview of intention recognition as a whole, and then elaborate on two components of the model, which we believe to be at its core, namely, those of intention detection and intention prediction. By intention detection we mean the ability to discern whether a sequence of actions has any underlying intention at all, or whether it was performed in an arbitrary manner with no goal in mind. By intention prediction we mean the ability to extend an incomplete sequence of actions to its most likely intended goal. We evaluate the model, and these two components, in context of existing literature, and in a number of experiments with more than 140 human subjects. For intention detection, our model was able to attribute high levels of intention to those traces perceived by humans as intentional, and vice versa. For intention prediction as well, our model performed in a way that closely matched that of humans. The work highlights the intimate relationship between the ability to generate plans, and the ability to recognize intentions.},
 authors = {['Elisheva Bonchek-Dokow', 'Gal A. Kaminka']},
 journal = {Cognitive Systems Research},
 keywords = {['Intention recognition', 'Intention prediction', 'Cognitive modeling']},
 title = {Towards computational models of intention detection and intention prediction},
 year = {2014}
}

@Filtered Article{b72c3e6a-a4a5-4767-a4c3-1420953c45e3,
 abstract = {In 1965, Woodward and Hoffmann proposed a theory to predict the stereochemistry of electrocyclic reactions, which, after expansion and generalization, became known as the Woodward–Hoffmann Rules. Subsequently, Longuet-Higgins and Abrahamson used correlation diagrams to propose that the stereoselectivity of electrocyclizations could be explained by the correlation of reactant and product orbitals with the same symmetry. Immediately thereafter, Hoffmann and Woodward applied correlation diagrams to explain the mechanism of cycloadditions. We describe these discoveries and their evolution. We now report an investigation of various electrocyclic reactions using DFT and CASSCF. We track the frontier molecular orbitals along the intrinsic reaction coordinate and modeled trajectories and examine the correlation between HOMO and LUMO for thermally forbidden systems. We also investigate the electrocyclizations of several highly polarized systems for which the Houk group had predicted that donor–acceptor substitution can induce zwitterionic character, thereby providing low-energy pathways for formally forbidden reactions. We conclude with perspectives on the field of pericyclic reactions, including a refinement as the meaning of Woodward and Hoffmann’s “Violations. There are none!” Lastly, we comment on the burgeoning influence of computations on all fields of chemistry.},
 authors = {['Qingyang Zhou', 'Garrett Kukier', 'Igor Gordiy', 'Roald Hoffmann', 'Jeffrey I. Seeman', 'K. N. Houk']},
 journal = {The Journal of Organic Chemistry},
 title = {A 21st Century View of Allowed and Forbidden Electrocyclic Reactions},
 year = {2024}
}

@Filtered Article{b74a3b3a-d16f-415a-ad37-5c2b57591354,
 abstract = {Computational modeling plays an increasingly important role in neuroscience, highlighting the philosophical question of how computational models explain. In the particular case of neural network models, concerns have been raised about their intelligibility, and how these models relate (if at all) to what is found in the brain. We claim that what makes a system intelligible is an understanding of the dependencies between its behavior and the factors that are responsible for that behavior. In biology, many of these dependencies are naturally “top-down”, as ethological imperatives interact with evolutionary and developmental constraints under natural selection to produce systems with capabilities and behaviors appropriate to their evolutionary needs. We describe how the optimization techniques used to construct neural network models capture some key aspects of these dependencies, and thus help explain why brain systems are as they are — because when a challenging ecologically-relevant goal is shared by a neural network and the brain, it places constraints on the possible mechanisms exhibited in both kinds of systems. The presence and strength of these constraints explain why some outcomes are more likely than others. By combining two familiar modes of explanation — one based on bottom-up mechanistic description (whose relation to neural network models we address in a companion paper) and the other based on top-down constraints, these models have the potential to illuminate brain function.},
 authors = {['Rosa Cao', 'Daniel Yamins']},
 journal = {Cognitive Systems Research},
 keywords = {['Evolution', 'Contravariance', 'Intelligibility', 'Function', 'Optimization', 'No-miracles', 'Instrumentalism', 'Realism', 'Philosophy', 'Constraints', 'Evolutionary landscape', 'Models', 'Explanation', 'Evo-devo', 'Development', 'Learning', 'Deep learning', 'Abstraction']},
 title = {Explanatory models in neuroscience, Part 2: Functional intelligibility and the contravariance principle},
 year = {2024}
}

@Filtered Article{b754e0d2-9a83-4599-be6f-768a14d709b1,
 abstract = {NMs, whose dimensions are below 100 nm, provide unique mechanical properties from quantum effects, surface phenomena, and small-scale interactions that account for their importance in energy storage, biomedicine, nanoelectronics, etc. This review discusses computational prediction of mechanical properties (for example, elasticity, strength, and fracture behavior) in NMs, especially using Density Functional Theory as a central tool. By conducting DFT calculations, we can analyze how NMs will behave across different mechanical states, which is critical for designing properties for advanced applications. Problems related to the application of DFT (e.g., high computational cost and failure in modeling defects or exchange-correlation functionals) are discussed. Despite these challenges, DFT must provide insights that complement other tools and strategies. However, further development is essential for improving its quantitative predictability on temperature and multiscale models. Future work is needed to integrate ML with DFT further to refine the accuracy and computational efficiency, thereby extending the capability of DFT to accelerate the discovery of new NMs with superior mechanical properties.},
 authors = {['Md. Aminul Islam', 'Nayem Hossain', 'Zahid Ahsan', 'Masud Rana', 'Mustafizur Rahman', 'Md. Abdullah']},
 journal = {Results in Surfaces and Interfaces},
 keywords = {['DFT', 'NMs', 'Elasticity', 'Mechanical properties', 'Quantum effects', 'Surface phenomena']},
 title = {DFT insights into the mechanical properties of NMs},
 year = {2025}
}

@Filtered Article{b76280ae-f8cf-4516-95cd-a6423870e943,
 abstract = {It has been discovered more than a decade ago that autistic people cannot properly understand and reproduce mental states and emotions. We hypothesize that people with autism suffer from difficulties in learning social rules from examples. Many remediation strategies have not taken this into account. Therefore an appropriate remediation strategy is to teach not simply via examples but to teach the rule along with it. In this study we suggest a reasoning rehabilitation strategy, based on playing with a computer based mental simulator that is capable of modeling mental and emotional states of the real world. A model of the mental world is presented in 12 steps. We describe our implementation of a natural language multiagent system that simulates this model. In addition we describe the system’s user interface for autistic rehabilitation. This system is subject to short-term and long-term evaluation of rehabilitation of autistic reasoning. Case studies with children who used it extensively are presented. Implications specifically in terms of autistic rehabilitation as well as generally in terms of reasoning about mental states are discussed.},
 authors = {['Boris Galitsky']},
 journal = {Knowledge-Based Systems},
 keywords = {['Autistic reasoning', 'Rehabilitation', 'Theory of mind']},
 title = {A computational simulation tool for training autistic reasoning about mental attitudes},
 year = {2013}
}

@Filtered Article{b781d50b-313f-491e-8e20-de0d4d11660f,
 abstract = {We argue that failure can play a productive role in students’ creative knowledge-construction process. As evidence, we present a fine-grained analysis of a whole-class theory-building discussion with 8th grade students. The goal of the discussion was to construct a theoretical account for why a glass of cold milk warmed quickly at first and then more slowly as it approached room temperature. Though they initially produced scientifically non-normative explanations, by the end of the discussion the class had refined their ideas into an explanation of difference drives rate – a relationship at the heart of Newton’s law of heating and other equilibration phenomena. The students’ flawed initial explanations were productive in the knowledge-construction process, as the raw material they ultimately refined into a more scientific explanation. We argue that the theory-building discussion supported both creative and critical thinking and that this pedagogical approach has the power, more generally, to leverage failure productively for science learning.},
 authors = {['Hillary Swanson', 'Allan Collins']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Knowledge in pieces', 'Microgenetic learning analysis', 'Knowledge construction', 'Constructivist instruction', 'Science learning', 'Creative thinking', 'Critical thinking', 'Creative problem solving']},
 title = {How failure is productive in the creative process: Refining student explanations through theory-building discussion},
 year = {2018}
}

@Filtered Article{b7f03c02-f7c2-4bfa-8c13-132920049c88,
 abstract = {How can the same underlying psychological/neurobiological system result in both stable between-individual differences and high levels of within-individual variability in personality states over time and situations? We argue that both types of variability result from a psychological system based on structured, chronic motivations, where behavior at a specific point in time is a joint function of the current availability of motive affordances in the situation, current motivationally relevant bodily or interoceptive states, and the result of the competition among alternative active motives. Here we present a biologically-based theoretical framework, embodied in two different computational models, that shows how individuals with stable personality characteristics, can nevertheless exhibit considerable within-person variability in personality states across time and situations.},
 authors = {['Stephen J. Read', 'Benjamin J. Smith', 'Vitaliya Droutman', 'Lynn C. Miller']},
 journal = {Journal of Research in Personality},
 keywords = {['Virtual personalities', 'Within-person variability', 'Between-person variability', 'Social computational modeling']},
 title = {Virtual personalities: Using computational modeling to understand within-person variability},
 year = {2017}
}

@Filtered Article{b80c1d58-c5c4-42e0-b510-e96e88db04e8,
 abstract = {The paper reports both a theoretical analysis and a comparison of educational tools for computational modelling, and describes three prototype tools developed in the Programme for use in empirical studies of children reasoning with the aid of computational tools, together with an outline of the result obtained by using the tools with children.},
 authors = {['Rob Miller', 'Jon Ogborn', 'Jonathan Briggs', 'Derek Brough', 'Joan Bliss', 'Richard Boohan', 'Tim Brosnan', 'Harvey Mellar', 'Babis Sakonidis']},
 journal = {Computers & Education},
 title = {Educational tools for computational modelling},
 year = {1993}
}

@Filtered Article{b8321706-00b6-4d78-86cc-226496a1cc1e,
 abstract = {Maximum power point (MPP) tracking (MPPT) is an important technique for maximizing the power extraction from photovoltaic (PV) systems under varying climatic conditions. In an array of PV modules it is possible to observe multiple peaks in the power versus voltage (P-V) curve due to the current versus voltage (I–V) PV cell mismatch caused by differences in the received irradiance, such as occurs during partial shading. In these circumstances, the ability of the MPPT devices to track the global MPP of the PV array directly influences the system efficiency. In the literature, various MPPT techniques have been proposed. Among them, computational intelligence (CI) algorithm based MPPT methods have demonstrated the ability to find the global MPP. This paper presents a detailed and specific review of CI- based MPPT techniques. Each method type is classified into one of several subcategories according to its application strategy. The various ways of applying CI into MPPTs are analyzed in detail. The advantages and disadvantages of each method are discussed and compared. The purpose of this study is to provide a compendium on CI-based MPPT techniques for users to understand and select an appropriate method based on application requirements and system constraints.},
 authors = {['Lian L. Jiang', 'R. Srivatsan', 'Douglas L. Maskell']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Maximum power point tracking', 'PV system', 'Computational intelligence algorithm', 'Heuristic algorithm', 'Global tracking', 'Partial shading']},
 title = {Computational intelligence techniques for maximum power point tracking in PV systems: A review},
 year = {2018}
}

@Filtered Article{b8a04a0f-e40f-4c2c-8d85-4ccb4929dedd,
 abstract = {An important feature of human intelligence is the use of symbols. This is seen in our daily use of language and logical thinking. However, the use of symbols is not limited to humans. We observe planned action sequences in primate behavior and prediction-based action in higher mammals. For the representation and operation of symbols by the brain neural circuit, no specific construction principle or computational theory is known so far. In this paper, we regard the brain as a complex of associative memory and dynamic attentional system, and starting from two hypotheses on information representation and operation in the brain, we propose a model of primitive symbolic behavior emergence that is consistent with the conventional symbolic processing model. We also describe a computational theory of the symbolic processing model in associative memory. Through computer simulation studies on a language-like memory search and map learning by a moving robot, we discuss the validity of the model.},
 authors = {['T. Omori', 'A. Mochizuki', 'K. Mizutani', 'M. Nishizaki']},
 journal = {Neural Networks},
 keywords = {['Symbolic behavior', 'Associative memory', 'Attention', 'PATON', 'Inference', 'Hippocampus', 'Model', 'Computational theory']},
 title = {Emergence of symbolic behavior from brain like memory with dynamic attention},
 year = {1999}
}

@Filtered Article{b8bf9e6e-e9a8-4d72-8cca-3fd9d16b9a6a,
 abstract = {This chapter charts out the distinction between the concepts of digital placemaking and smart placemaking. It examines existing and speculative urban technology projects that combine spatial design thinking and physical computing to address placemaking design goals such as urban activation, amenitization, and safety and security. In ways different from conventional smart city initiatives that surveil urban dynamics en masse to imperceptibly calibrate large-scale urban services and infrastructural systems, the projects discussed in this chapter engage sensor-based technologies to create localized, responsive, and interactive urban environments to address the social, cultural, and aesthetic dimensions of urban livability.},
 authors = {['Nicole Gardner']},
 journal = {Elsevier},
 keywords = {['Cyber-physical systems', 'Design', 'Digital placemaking', 'Interaction', 'Placemaking', 'Smart design', 'Urban activation', 'Urban amenitization']},
 title = {Chapter 4 - Smart design for urban activation and placemaking},
 year = {2024}
}

@Filtered Article{b8e8e4d6-d0fe-44cd-8251-fa88e2b6e38e,
 abstract = {The brain disease model (BMDA) and psychosocial models of addiction attend to phenomena at different levels of biological organization, and evidence suggests neither is sufficient to explain substance use disorder (SUD). Here, we extend a Bayesian model of the emergence and persistence of delusions to SUD etiology and recovery, building upon efforts to link lower-level impacts of psychoactive compounds to higher-level phenomena such as attitudes, beliefs, and self-control. According to the resulting two-component model of SUD, psychoactive substances interact with genetic and environmental factors to produce delusions about the biological importance of substance use and its contexts by perturbating basic human affective systems. These delusions are most often revised or rejected based on individuals’ existing belief systems. But in some individuals, factors explaining the persistence of an array of delusions (e.g., lower levels of executive functioning) prevent the evaluation and revision system from rejecting or revising beliefs that attribute high salience to substance-related stimuli. This theory provides novel hypotheses regarding the potential roles of factors such as dichotomous thinking, positive illusions and self-deception, and denial or lack of awareness in SUD etiology and recovery. Furthermore, it provides an account of SUD that may result in less stigma than the BDMA.},
 authors = {['George B. Richardson', 'Nathan McGee']},
 journal = {New Ideas in Psychology},
 keywords = {['Brain disease model of addiction', 'Two-component model of delusion', 'Bayes Theorem', 'Belief', 'Substance use disorder']},
 title = {Extending the two-component model of delusion to substance use disorder etiology and recovery},
 year = {2022}
}

@Filtered Article{b94401dc-ae2d-437f-b457-e48959e2d15a,
 abstract = {Computational verb systems can help machines to implement, understand and use verbs as human perception of dynamics. By using computational verbs we can embed dynamical experiences of human experts into artificial intelligence. Computational verbs, which are models of verbs in nature languages, are basic building blocks of computational verb systems. In this paper, computational verbs are used to represent dynamical knowledge embedded by verbs as a new framework of knowledge representation. BE-transformations are used to transform statements containing dynamical verbs into statements only containing static verb BE; namely, BE-propositions. Based on BE-transformations, the computational verb logic can be built. Furthermore, reasoning with computational verbs can be built based on BE-transformations and basic verb logic operations.},
 authors = {['Tao Yang']},
 journal = {Information Sciences},
 keywords = {['Verbs', 'Computational verbs', 'Computational verb systems', 'Chaos', 'Artificial intelligence', 'Reasoning', 'Knowledge representation']},
 title = {Computational verb systems: a new paradigm for artificial intelligence},
 year = {2000}
}

@Filtered Article{b969b2fe-9c69-422d-b3f9-d60b10f2126b,
 abstract = {This paper presented both experimental and numerical assessments of separation gap effect on vented explosion pressure in and around the area of a tank group. A series of vented gas explosion layouts with different separation gaps between tanks were experimentally investigated. In order to qualitatively determine the relationship between the separation gap distance and explosion pressure, intensive computational Fluid Dynamics (CFD) simulations, verified with testing data, were conducted. Good agreement between CFD simulation results and experimental data was achieved. By using CFD simulation, more gas explosion cases were included to consider different gas cloud coverage scenarios. Separation gap effects on internal and external pressures at various locations were investigated.},
 authors = {['Jingde Li', 'Hong Hao', 'Yanchao Shi', 'Qin Fang', 'Zhan Li', 'Li Chen']},
 journal = {Journal of Loss Prevention in the Process Industries},
 keywords = {['Separation gap', 'Safety gap', 'External pressure', 'Vented gas explosion', 'CFD', 'FLACS']},
 title = {Experimental and computational Fluid Dynamics study of separation gap effect on gas explosion mitigation for methane storage tanks},
 year = {2018}
}

@Filtered Article{b9a65341-6512-453e-9e7d-aea88995a77b,
 abstract = {The use of computer simulated markets with individual adaptive agents in finance is a new, but growing field. This paper explores some of the early works in the area concentrating on a set of some of the earliest papers. Six papers are summarized in detail, along with references to many other pieces of this wide ranging research area. It also covers many of the questions that new researchers will face when getting into the field, and hopefully can serve as a kind of minitutorial for those interested in getting started.},
 authors = {['Blake LeBaron']},
 journal = {Journal of Economic Dynamics and Control},
 keywords = {['Agents', 'Heterogeneous information', 'Simulated markets', 'Learning', 'Evolution']},
 title = {Agent-based computational finance: Suggested readings and early research},
 year = {2000}
}

@Filtered Article{b9c61182-02dc-4943-9595-b1586a1662a0,
 abstract = {This chapter discusses the synergy between natural product-based drug discovery and methods used in computer-aided drug design. For centuries, Nature has been the source of compounds that are currently in the clinic or that have been used as molecular probes to identify therapeutic targets. In addition, Nature has inspired the development of a significant number of pharmaceutical agents. In contrast, computational approaches applied to drug discovery date back to only a few decades. Nonetheless, computational methods are evolving at an impressive speed and are making significant contributions to identifying and developing bioactive compounds of therapeutic relevance. Computational methods have a broad range of applications in natural product research including the organization and comprehensive analysis of molecular databases, systematic screening of natural products libraries, computer-aided optimization of lead compounds, and identification of biological activities for natural products of dietary origin.},
 authors = {['José L. Medina-Franco']},
 journal = {Elsevier},
 keywords = {['Chemical space', 'Chemoinformatics', 'Computer-aided drug design', 'Dietary components', 'Drug discovery', 'Molecular diversity', 'Pharmacological profiling', 'Structure–activity relationships', 'Target fishing', 'Virtual screening']},
 title = {Chapter 21 - Discovery and Development of Lead Compounds from Natural Sources Using Computational Approaches},
 year = {2015}
}

@Filtered Article{b9d87d58-ead4-4a2c-bd25-5dda89a9e176,
 abstract = {The production of biofuels is inextricably linked with the water-energy-food-land (WEFL) nexus. Understanding these linkages is necessary to formulate effective policies that can influence positive outcomes and contribute to the realization of long-term economic, environmental, and social goals. The use of biofuels can help achieve the United Nation's Sustainable Development Goals (SDGs) and implement the Paris Agreement on climate change. However, the biofuels sector must account for its interdependencies and trade-offs with other sectors. In this study, we formulate a qualitative analytical model that goes beyond the three water-energy-food nexus components by incorporating other elements, such as policy, innovation, governance, and labor to examine their effect as influencing factors and to understand how synergies, trade-offs, and long-overlooked interlinkages between sectors and among existing policies and institutions can become visible. This qualitative model was applied to the case of ethanol in Brazil, for which a large corpus was constructed from the scientific literature, documents and sustainability reports from sugarcane ethanol companies. We used a supervised latent Dirichlet allocation (sLDA) algorithm along with co-occurrence and network analyses. The results demonstrate this approach can be used to evaluate the interfaces between science, policy, and businesses within the WEFL-biofuels nexus. This is done by identifying how best to integrate the development of policies, governance, and stakeholder actions to support cost-effective decisions for optimal resource management and regulatory processes while enabling better integration of scientific insight and policy-making. We also identified how these four influencing factors are of vital importance within the nexus and, if properly addressed, can contribute to more holistic nexus thinking management.},
 authors = {['Lira Luz Benites Lazaro', 'Leandro Luiz Giatti', 'Celio Bermann', 'Angelica Giarolla', 'Jean Ometto']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Water–energy–food nexus', 'Nexus thinking', 'Governance', 'Policy', 'Biofuels', 'Nexus methodology', 'Nexus method', 'Innovation']},
 title = {Policy and governance dynamics in the water-energy-food-land nexus of biofuels: Proposing a qualitative analysis model},
 year = {2021}
}

@Filtered Article{b9fd9214-2cde-4574-8635-1dc2d67dbcf1,
 abstract = {Process Systems Engineering (PSE) deals with decision-making, at all levels and scales, by understanding complex process systems using a holistic view. Computer Aided Process Engineering (CAPE) is a complementary field that focuses on developing methods and providing solution through systematic computer aided techniques for problems related to the design, control and operation of chemical systems. The ‘PSE’ term suffers from a branding issue to the point that PSE does not get the recognition it deserves. This work aims to provide an informative industrial and academic perspective on PSE, arguing that the ‘systems thinking’ and ‘systems problem solving’ have to be prioritized ahead of just applications of computational problem solving methods. A multi-level view of the PSE field is provided within the academic and industrial context, and enhancements for PSE are suggested at their industrial and academic interfaces.},
 authors = {['Anton A. Kiss', 'Johan Grievink']},
 journal = {Elsevier},
 keywords = {['PSE', 'industry', 'education', 'research', 'interface', 'perspectives']},
 title = {Process Systems Engineering from an industrial and academic perspective},
 year = {2019}
}

@Filtered Article{ba23b9c4-ea76-4bd2-b5d0-489a9071efe7,
 abstract = {Recent studies have revealed the importance of cognitive abilities in creative thinking. However, most research addressed adults and only a few studies have examined the ways these correlations are manifested among young children. The present study explores the role of various cognitive abilities in creativity among school children. Measures of creativity, visual-spatial working memory (VSWM), verbal short-term memory (STM), working memory (WM), and visual processing (VP) were administered to 331 students in Grades 4 and 5. Cluster analysis was used to group students' creativity levels. A multivariate analysis of variance (MANOVA) was conducted to test differences in cognitive abilities across the three clusters. Group differences between high and moderate level creativity students and low creativity students were found regarding VP abilities in the following tests: VSWM, visual discrimination (VD), and Raven's Colored Progressive Matrices (RCPM). Group differences between high creativity students and low creativity students were also found on verbal STM and WM. Additionally, structural equation modeling (SEM) analysis revealed that VP can significantly account for unique variances associated with creativity, while verbal STM and WM are not significantly related to creativity. These findings enlighten the cognitive processes underlying creativity in young children.},
 authors = {['Nurit Paz-Baruch', 'Rotem Maor']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Cognitive abilities', 'Creativity', 'Working memory', 'Visual processing']},
 title = {Cognitive abilities and creativity: The role of working memory and visual processing},
 year = {2023}
}

@Filtered Article{ba4a60d2-8a82-450b-a85f-a059f22ddd9b,
 abstract = {There is an increasing emphasis on teaching young learners to code; yet, there are few tools designed to measure the effect of learning to code on children. The purpose of this study was to develop and validate a tool to assess changes in young learners' attitudes toward coding: the Elementary Student Coding Attitudes Survey (ESCAS). We validated the scale using Confirmatory Factory Analysis and Structural Equation Modeling with responses from over 6000 4th-6th grade students (aged 9–12 years). Survey validation revealed a scale consisting of five constructs that comprise young learners' attitudes toward coding: social value, coding confidence, coding interest, perception of coders, and coding utility. In our analysis, students' grade level, ethnicity, gender, coding frequency, coding experience, and math interest influenced social value, which in turn influenced coding interest, perception of coders, and coding utility. Students' math confidence, coding frequency, coding experience, ethnicity, and coding interest predicted their coding confidence. Among observable variables, coding frequency and math interest had the greatest influence on social value, which substantially influenced all other factors. We discuss how this tool can help those who teach coding to young children to better measure and understand the variables that may influence young learners’ attitudes toward coding over time.},
 authors = {['Stacie L. Mason', 'Peter J. Rich']},
 journal = {Computers & Education},
 keywords = {['Elementary education', 'Computational thinking', 'Coding', 'Attitude scale', 'Instrument validation']},
 title = {Development and analysis of the Elementary Student Coding Attitudes Survey},
 year = {2020}
}

@Filtered Article{bb42c701-eaa3-4a09-9192-e5aa0374503f,
 abstract = {A three-dimensional numerical study is conducted to better understand noise reduction results seen in the previous two-dimensional investigation of the acoustic effects of micro-tab device on airframe noise reduction. Without sacrificing the aerodynamic performance, it is possible to achieve high-lift noise reduction with the application of the micro-tab device attached to the pressure side of the flap surface near its trailing-edge. This study was carried out by numerical hybrid method, which combines Computational Fluid Dynamics and acoustic analogy to predict the farfield noise spectrum. The near-full-scale computational results show that the micro-tab device with reduced deflection of the high-lift devices achieves noise reduction in mid-to-high frequency domain, in particular the range that human beings are most sensitive to. In addition, a parametric study in terms of geometric variation of the micro-tab was also investigated and reported. The three-dimensional results obtained thus far show reduction in noise levels with use of micro-tab.},
 authors = {['Brian C. Kuo', 'Nesrin Sarigul-Klijn']},
 journal = {Aerospace Science and Technology},
 keywords = {['Computational aeroacoustics', 'High-lift devices', 'Micro-tab', 'Airframe noise']},
 title = {Conceptual study of micro-tab device in airframe noise reduction: (II) 3D computation},
 year = {2012}
}

@Filtered Article{bb993b44-4d5e-4563-b428-24dba0227e28,
 authors = {['Ben Goertzel', 'Harold Bowman']},
 journal = {Journal of Social and Evolutionary Systems},
 title = {Self-reference, computation, and mind},
 year = {1995}
}

@Filtered Article{bb9f0d3a-3664-41a8-aadc-dedac799416e,
 abstract = {Started doing Quantum Chemistry as a PhD student with Linnett in 1968 at Cambridge: configuration interaction (CI) calculations on small hydrogenic molecules with a mixed basis of Slater and Gaussian functions. Joined Hush at the University of Sydney in 1972 and worked with a number of graduate students on Coupled Cluster Theory, direct CI, the calculation of ionization energies, the computation of molecular properties and vibrational energies, studies of hydrogen bonded clusters and quadratically convergent Hartree-Fock theory. With Nordholm, worked on the implementation of Absorbing Boundary and Generalized Finite Element methods. As computational chemist, during the nineties and beyond, studied dihydrogen complexes of osmium and the catalytic properties of cis-platins, the potential energy surfaces relevant to thermal decomposition of small molecules, properties of halocarbenes, spectroscopy of C2, thermochemistry and covalent bonding. Retired in 2005. Research-active as Honorary Reader.},
 authors = {['György B. Bácskay']},
 journal = {Academic Press},
 keywords = {['Configuration interaction (CI)', 'Direct CI', 'Coupled cluster theory', 'Quadratically convergent-\xa0self-consistent field (QC-SCF) method', 'Molecular properties', 'Thermal decomposition', 'Chemical bonding', 'Thermochemistry']},
 title = {Fifty-five years of quantum chemistry},
 year = {2025}
}

@Filtered Article{bbe8d67a-2b67-4a34-8710-1044a7382011,
 abstract = {A long tradition of research in theoretical, experimental and computational pragmatics has investigated over-specification and under-specification in referring expressions. Along broadly Gricean lines, these studies compare the amount of information expressed by a referring expression against the amount of information that is required. Often, however, these studies offer no formal definition of what “required” means, and how the comparison should be performed. In this paper, we use a simple set-theoretic perspective to define some communicatively important types of over-/under-specification. We argue that our perspective enables an enhanced understanding of reference phenomena that can pay important dividends for the analysis of reference in corpora and for the evaluation of computational models of referring. To illustrate and substantiate our claims, we analyse two corpora, containing Chinese and English referring expressions respectively, using the new perspective. The results show that interesting new monolingual and cross-linguistic insights can be obtained from our perspective.},
 authors = {['Guanyi Chen', 'Kees {van Deemter}']},
 journal = {Journal of Pragmatics},
 keywords = {['Referring expressions', 'Over-specification', 'Under-specification']},
 title = {Varieties of specification: Redefining over- and under-specification},
 year = {2023}
}

@Filtered Article{bbe93d2d-c532-4ce0-bc49-8c0591755541,
 abstract = {Interest in the application of engineering methods to problems in congenital heart disease has gained increased popularity over the past decade. The use of computational simulation to examine common clinical problems including single ventricle physiology and the associated surgical approaches, the effects of pacemaker implantation on vascular occlusion, or delineation of the biomechanical effects of implanted medical devices is now routinely appearing in clinical journals within all pediatric cardiovascular subspecialties. In practice, such collaboration can only work if both communities understand each other's methods and their limitations. This paper is intended to facilitate this communication by presenting in the context of congenital heart disease (CHD) the main steps involved in performing computational simulation—from the selection of an appropriate clinical question/problem to understanding the computational results, and all of the “black boxes” in between. We examine the current state of the art and areas in need of continued development. For example, medical image-based model-building software has been developed based on numerous different methods. However, none of them can be used to construct a model with a simple “click of a button.” The creation of a faithful, representative anatomic model, especially in pediatric subjects, often requires skilled manual intervention. In addition, information from a second imaging modality is often required to facilitate this process. We describe the technical aspects of model building, provide a definition of some of the most commonly used terms and techniques (e.g. meshes, mesh convergence, Navier-Stokes equations, and boundary conditions), and the assumptions used in running the simulations. Particular attention is paid to the assignment of boundary conditions as this point is of critical importance in the current areas of research within the realm of congenital heart disease. Finally, examples are provided demonstrating how computer simulations can provide an opportunity to “acquire” data currently unobtainable by other modalities, with essentially no risk to patients. To illustrate these points, novel simulation examples of virtual Fontan conversion (from preoperative data to predicted postoperative state) and outcomes of different surgical designs are presented. The need for validation of the currently employed techniques and predicted results are required and the methods remain in their infancy. While the daily application of these technologies to patient-specific clinical scenarios likely remains years away, the ever increasing interest in this area among both clinicians and engineers makes its eventual use far more likely than ever before and, some could argue, only a matter of [computing] time.},
 authors = {['Irene E. Vignon-Clementel', 'Alison L. Marsden', 'Jeffrey A. Feinstein']},
 journal = {Progress in Pediatric Cardiology},
 keywords = {['Hemodynamics', 'Computer modeling', 'Boundary conditions', 'Clinical data', 'Congenital heart disease']},
 title = {A primer on computational simulation in congenital heart disease for the clinician},
 year = {2010}
}

@Filtered Article{bbfa49bf-8f1a-491f-b22d-3ed1f8b1584c,
 abstract = {The locality issue of quantum mechanics is a key issue to a proper understanding of quantum physics and beyond. What has been commonly emphasized as quantum nonlocality has received an inspiring examination through the notion of the Heisenberg picture of quantum information. Deutsch and Hayden established a local description of quantum information in a setting of quantum information flow in a system of qubits. With the introduction of a slightly modified version of what we call the Deutsch–Hayden matrix values of observables, together with our recently introduced parallel notion of the noncommutative values from a more fundamental perspective, we clarify all the locality issues based on such values as quantum information carried by local observables in any given arbitrary state of a generic composite system. Quantum information as the ‘quantum’ values of observables gives a transparent conceptual picture of all that. Spatial locality for a projective measurement is also discussed. The pressing question is if and how such information for an entangled system can be retrieved through local processes which can only be addressed with new experimental thinking.},
 authors = {['Otto C.W. Kong']},
 journal = {Chinese Journal of Physics},
 keywords = {['Quantum information', 'Quantum locality', 'Deutsch–Hayden descriptors', 'Noncommutative values of observables']},
 title = {On locality of quantum information in the Heisenberg picture for arbitrary states},
 year = {2024}
}

@Filtered Article{bc887e80-cbbe-4153-8599-8f8d4b3abf1a,
 abstract = {This study aimed to identify the role and nature of spatial visualization in the problem solutions of pre-service teachers solving school-mathematics tasks requiring measurement reasoning. The nuances in the pre-service teachers’ strategies were examined for the role of spatial visualization in the solution process. The findings suggest that inadequacies in visualizing the spatial configurations of the tasks led to incorrect numerical solutions despite the presence of conceptual knowledge. Furthermore, the tendency to rely on formula-based approaches appeared to have suppressed the preliminary spatial processing of the configurations. Theoretically, the paper offers insights into the mechanism that may be involved in the solution of spatially-related mathematical tasks. The findings imply that pre-service teachers need to be sufficiently engaged in spatial reasoning activities.},
 authors = {['Sitti Maesuri Patahuddin', 'Ajay Ramful', 'Tom Lowrie', 'Ajeevsing Bholoa']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Spatial visualization', 'Spatial reasoning', 'Mathematics', 'Geometry', 'Measurement', 'Pre-service teacher']},
 title = {Subtleties in spatial visualization maneuvers: Insights from numerical solutions},
 year = {2022}
}

@Filtered Article{bc9d882f-a032-4996-b3b9-5f9617407574,
 abstract = {We organize image types by their substantive relationship with textual claims and discuss their impact on attention, comprehension, memory, and judgment. Photos do not need to be false (altered or generated) to mislead; real photos can create a slanted representation or be repurposed from different events. Even semantically related non-probative photos, merely inserted to attract eyeballs, can increase message acceptance through increased fluency. Messages with images receive more attention and reach a wider audience. Text-congruent images can scaffold the comprehension of true and false claims and support the formation of correct and false memories. Standard laboratory procedures may underestimate the impact of images in natural media contexts: by drawing all participants' attention to a message that may be ignored without an image, they inflate message effects in the control condition. Misleading images are difficult to identify and their influence often remains outside of awareness, making it hard to curb their influence through critical-thinking interventions. Current concerns about deep fakes may reduce trust in all images, potentially limiting their power to mislead as well as inform. More research is needed to understand how knowing that an image is misleading influences inferences, impressions, and judgments beyond immediate assessments of the image's credibility.},
 authors = {['Eryn J. Newman', 'Norbert Schwarz']},
 journal = {Current Opinion in Psychology},
 keywords = {['Visual misinformation', 'Truthiness', 'Cognitive fluency', 'Artificial intelligence (AI)', 'Memory', 'Truth assessment']},
 title = {Misinformed by images: How images influence perceptions of truth and what can be done about it},
 year = {2024}
}

@Filtered Article{bcea5f22-5dca-4e66-8155-d3bef4935de8,
 abstract = {We have analysed rich, dynamic data about the behaviour of anaesthetists during the management of a simulated critical incident in the operating theatre. We use a paper based analysis and a partial implementation to further the development of a computational cognitive model for disturbance management in anaesthesia. We suggest that our data analysis pattern may be used for the analysis of behavioural data describing cognitive and observable events in other complex dynamic domains.},
 authors = {['K. Keogh', 'E.A. Sonenberg']},
 journal = {Cognitive Systems Research},
 keywords = {['Behavioural analysis', 'Computational cognitive modelling', 'Disturbance management']},
 title = {Keeping the patient asleep and alive: Towards a computational cognitive model of disturbance management in anaesthesia},
 year = {2007}
}

@Filtered Article{bd6a2486-56e7-4e80-9e7f-3c26f306d510,
 abstract = {We introduce in this paper a nonlinear model predictive control scheme for open-loop stable systems subject to input and state constraints. Closed-loop stability is guaranteed by an appropriate choice of the finite prediction horizon, independent of the specification of the desired control performance. In addition, this control scheme is likely to allow ‘real time’ implementation, because of its computational attractiveness. The theoretical results are demonstrated and discussed with a CSTR control application.},
 authors = {['H. Chen', 'F. Allgöwer']},
 journal = {Journal of Process Control},
 keywords = {['nonlinear predictive control', 'constraints', 'stability', 'terminal conditions']},
 title = {A computationally attractive nonlinear predictive control scheme with guaranteed stability for stable systems},
 year = {1998}
}

@Filtered Article{bdaa3ce2-8b6b-4b22-a495-b227a5fd0ebc,
 abstract = {Denis Noble was born in 1936 and obtained a BSc and PhD from University College London. He is one of the pioneers of computational biology related to cardiac cell electrophysiology and its incorporation into the first detailed biophysical models of the whole organ. He has made many major contributions to this work spanning from his groundbreaking work in 1960, showing that in heart, contrary to the situation in nerve, the first effect of membrane depolarisation is to greatly reduce potassium conductance, which in turn is greatly dependent on plasma potassium levels. His work over the last forty years has culminated in a highly successful virtual model of the heart, which has allowed theoretical interpretation of cardiac arrhythmias and the development of antiarrhythmic drugs. Professor Noble was made a fellow of the Royal Society in 1979, one of the highlights of his many awards. He is in great demand as a presenter of plenary lectures at many august meetings. In addition to his abilities as a computational biologist, Professor Noble is an accomplished linguist and has given lectures in French and Italian, and has significant abilities in Japanese, Korean and even Maori.},
 authors = {['Stephen Carney']},
 journal = {Drug Discovery Today: BIOSILICO},
 keywords = {['Interview', 'computer modeling', 'grid computing', 'cardiac electrophysiology', 'whole organ models', 'arrhythmia']},
 title = {Denis Noble discusses his career in computational biology},
 year = {2004}
}

@Filtered Article{bde803f5-7ffc-4151-93de-55fe83f74132,
 abstract = {This article presents two numerical procedures to speed up computations when dealing with a Reynolds Averaged Navier Stokes (RANS) solver based on the Volume of Fluid (VoF) or multifluid method to treat the free surface. The first one is a time-splitting procedure for the volume fraction equation, enabling the use of larger time steps for the resolution of the flow, without penalizing accuracy. However, these large time steps destabilize the coupling with the ship motion simulation when computing a dynamic equilibrium position in marine applications. The second procedure is therefore a quasi-static approach to solve the coupled problem of dynamic equilibrium. A comparison of these procedures with classical simulations shows that numerical solutions of realistic problems can be obtained up to four times faster.},
 authors = {['Alban Leroyer', 'Jeroen Wackers', 'Patrick Queutey', 'Emmanuel Guilmineau']},
 journal = {Ocean Engineering},
 keywords = {['Marine hydrodynamics', 'Free surface capturing', 'Dynamic equilibrium', 'RANS simulation']},
 title = {Numerical strategies to speed up CFD computations with free surface—Application to the dynamic equilibrium of hulls},
 year = {2011}
}

@Filtered Article{be2930e0-e45b-4273-8dff-a6c04b7824b9,
 abstract = {We present a framework for modeling biological pumping organs based on coupled spiral elastic band geometries and active wave-propagating excitation mechanisms. Two pumping mechanisms are considered in detail by way of example: one of a simple tube, which represents a embryonic fish heart and another more complicated structure with the potential to model the adult human heart. Through finite element modeling different elastic contractions are induced in the band. For each version the pumping efficiency is measured and the dynamics are evaluated. We show that by combining helical shapes of muscle bands with a contraction wave it is possible not only to achieve efficient pumping, but also to create desired dynamics of the structure. As a result we match the function of the model pumps and their dynamics to physiological observations.},
 authors = {['Anna Grosberg', 'Morteza Gharib']},
 journal = {Journal of Theoretical Biology},
 keywords = {['Cardiac modeling', 'Left ventricular twist', 'Myocardium macro-structure', 'Finite element simulations']},
 title = {Computational models of heart pumping efficiencies based on contraction waves in spiral elastic bands},
 year = {2009}
}

@Filtered Article{be7f9cf7-eb89-47ee-ad5f-1038065a6870,
 abstract = {In the article one of leading aims of educating mathematics is examined is aesthetic education of student facilities of mathematics. Presentation of aesthetic beauty at her decisions possibility of students is investigated, specifying them on the decision of one problem in several ways that assists the detailed consideration of idea of aesthetic education, through Open-source Software. The technical capabilities and elegant ease of use of systems Open-source Software provides a seamless, integrated and constantly expanding system that covers the breadth and depth of mathematical computing, and is available seamlessly through any web browser along with all modern systems used in the educational process. The article will describe understanding of beauty the decision of a problem; methods of decisions that are accompanied by the use make possible a uniquely flexible and convenient approach to charting and information visualization in a mathematical calculate. Such sort of activity assists aesthetic education, allowing to develop a culture and logical thinking, forming at students a different choice, grace of decision of problems.},
 authors = {['Alena Aktayeva', 'Elena Zubareva', 'Aibek Dautov', 'Kymbat Saginbayeva', 'Rozamgul Niyazova', 'Sergey Khan', 'Aigerim Shonasheva']},
 journal = {Transportation Research Procedia},
 keywords = {['Aesthetic education', 'mathematical education', 'software', 'computer programs']},
 title = {Aesthetic education: the process of teaching mathematics with the open-source software},
 year = {2022}
}

@Filtered Article{be847a38-7ae6-4d6f-bcba-a866c463973e,
 abstract = {Despite a surge in Foreign Direct Investment (FDI) in Mexico, like nearshoring, the slow growth in international investment in renewables challenges the country's progress in achieving Sustainable Development Goals (SDGs) related to clean energy. To the best of current knowledge, this research is one of the first to explore the integration of renewable energy (Green/Blue/Turquoise Hydrogen, Solar, and Wind plants) in Mexico, emphasizing a diverse portfolio of projects aligned with SDGs and Industry 5.0. While previous works have focused on the nexus between energy, Industry 4.0, and sustainability, the present study advances this discourse by incorporating Industry 5.0 principles and a comprehensive methodological approach. Through a comprehensive methodology involving Value-Focused Thinking (VFT), fuzzy Decision-Making Trial and Evaluation Laboratory (DEMATEL), and multi-objective mathematical programming, the study identifies key criteria encompassing social, economic, environmental, and technological dimensions. The resulting criteria form a robust framework for evaluating project sustainability. The fuzzy DEMATEL analysis reveals intricate interrelations among criteria, emphasizing the need for balanced considerations. Results highlighted job creation, income equality, and microfinance support as key social considerations, while energy-related criteria emphasized sustainable practices. The proposed multi-objective programming model and COmbined COmpromise SOlution (COCOSO) method facilitated the selection of eight projects, with one project as the top-ranked option across various scoring strategies. Overall, this research provides a nuanced roadmap for effective decision-making in renewable energy projects, offering insights into project strengths, weaknesses, and potential areas for improvement.},
 authors = {['Moein Khazaei', 'Fatemeh Gholian-Jouybari', 'Mahdi {Davari Dolatabadi}', 'Aryan {Pourebrahimi Alamdari}', 'Hamidreza Eskandari', 'Mostafa Hajiaghaei-Keshteli']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Renewable energies', 'Industry 5.0', 'Sustainable development goals', 'Portfolio selection', 'Nearshoring']},
 title = {Renewable energy portfolio in Mexico for Industry 5.0 and SDGs: Hydrogen, wind, or solar?},
 year = {2025}
}

@Filtered Article{bea0ecaf-d166-4e9b-b225-3a44ecbe7923,
 abstract = {The behaviour of a simple single-bolted-joint under tensile separating loads is analysed using conventional analytical methods, a finite element approach and experimental techniques. The variation in bolt force with external load predicted by the finite element analysis conforms well to the experimental results. It is demonstrated that certain detailed features such as thread interaction do not need to be modelled to ensure useful results. Behaviour during the pre-loading phase of use agrees with previous long-standing studies. However, the pre-loading analysis does not carry over to the stage when external loading is applied, as is normally assumed and it is shown that the current, conventional analytical methods substantially over-predict the proportion of the external load carried by the bolt. The basic reason for this is shown to be related to the non-linear variation in contact conditions between the clamped members during the external loading stage.},
 authors = {['J.G. Williams', 'R.E. Anley', 'D.H. Nash', 'T.G.F. Gray']},
 journal = {International Journal of Pressure Vessels and Piping},
 keywords = {['Bolted joints', 'Pre-loading', 'Bolt stiffness', 'Member stiffness', 'Member contact']},
 title = {Analysis of externally loaded bolted joints: Analytical, computational and experimental study},
 year = {2009}
}

@Filtered Article{beb61221-b324-43c0-af28-0b84a892041c,
 abstract = {Green toxicology is an emerging discipline that seeks to make toxicology a partner in the field of green chemistry, by providing chemists and toxicologists the tools necessary to identify potential hazards based on chemical structure alone, test efficiently for bioactivity, and better predict the human health impacts of chemicals. It seeks to make toxicology a data-driven, 21st century science by incorporating advanced techniques such as computational modeling, high-throughput screening, and alternative testing methods to assess chemical safety.},
 authors = {['Alexandra Maertens', 'Thomas Hartung']},
 journal = {Elsevier},
 keywords = {['Exposomics', 'In Silico', 'In Vitro', 'QSAR', 'SAR']},
 title = {Green Toxicology},
 year = {2025}
}

@Filtered Article{beb6c6d8-cf89-49bc-b858-99456683ac30,
 abstract = {Effective treatment choices to the severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2) are limited because of the absence of effective target-based therapeutics. The main object of the current research was to estimate the antiviral activity of cannabinoids (CBDs) against the human coronavirus SARS-CoV-2. In the presented research work, we performed in silico and in vitro experiments to aid the sighting of lead CBDs for treating the viral infections of SARS-CoV-2. Virtual screening was carried out for interactions between 32 CBDs and the SARS-CoV-2 Mpro enzyme. Afterward, in vitro antiviral activity was carried out of five CBDs molecules against SARS-CoV-2. Interestingly, among them, two CBDs molecules namely Δ9 -tetrahydrocannabinol (IC50 = 10.25 μM) and cannabidiol (IC50 = 7.91 μM) were observed to be more potent antiviral molecules against SARS-CoV-2 compared to the reference drugs lopinavir, chloroquine, and remdesivir (IC50 ranges of 8.16–13.15 μM). These molecules were found to have stable conformations with the active binding pocket of the SARS-CoV-2 Mpro by molecular dynamic simulation and density functional theory. Our findings suggest cannabidiol and Δ9 -tetrahydrocannabinol are possible drugs against human coronavirus that might be used in combination or with other drug molecules to treat COVID-19 patients.},
 authors = {['Vinit Raj', 'Jae Gyu Park', 'Kiu-Hyung Cho', 'Pilju Choi', 'Taejung Kim', 'Jungyeob Ham', 'Jintae Lee']},
 journal = {International Journal of Biological Macromolecules},
 keywords = {['Cannabinols', 'antiviral assay', 'SARS-CoV-2 and M enzyme']},
 title = {Assessment of antiviral potencies of cannabinoids against SARS-CoV-2 using computational and in vitro approaches},
 year = {2021}
}

@Filtered Article{bf0fcee5-75a6-4c48-a941-0cd692fe121e,
 authors = {['Mark Michael Diacopoulos', 'Helen Crompton']},
 journal = {Computers & Education},
 title = {A systematic review of mobile learning in social studies},
 year = {2020}
}

@Filtered Article{bf139590-1d0b-4e89-830f-7fb95596f97c,
 abstract = {A long tradition of research on mathematical thinking has focused on procedural knowledge, or knowledge of how to solve problems and enact procedures. In recent years, however, there has been a shift toward focusing, not only on solving problems, but also on conceptual knowledge. In the current work, we reviewed (1) how conceptual knowledge is defined in the mathematical thinking literature, and (2) how conceptual knowledge is defined, operationalized, and measured in three mathematical domains: equivalence, cardinality, and inversion. We uncovered three general issues. First, few investigators provide explicit definitions of conceptual knowledge. Second, the definitions that are provided are often vague or poorly operationalized. Finally, the tasks used to measure conceptual knowledge do not always align with theoretical claims about mathematical understanding. Together, these three issues make it challenging to understand the development of conceptual knowledge, its relationship to procedural knowledge, and how it can best be taught to students. In light of these issues, we propose a general framework that divides conceptual knowledge into two facets: knowledge of general principles and knowledge of the principles underlying procedures.},
 authors = {['Noelle M. Crooks', 'Martha W. Alibali']},
 journal = {Developmental Review},
 keywords = {['Mathematical thinking', 'Conceptual knowledge', 'Equivalence', 'Cardinality', 'Inversion']},
 title = {Defining and measuring conceptual knowledge in mathematics},
 year = {2014}
}

@Filtered Article{bf1bcea1-934b-4bf0-b74c-7194c8805c46,
 abstract = {I provide some science and reflections from my experiences working in geophysics, along with connections to computational and data sciences, including recent developments in machine learning. I highlight several individuals and groups who have influenced me, both through direct collaborations as well as from ideas and insights that I have learned from. While my reflections are rooted in geophysics, they should also be relevant to other computational scientific and engineering fields. I also provide some thoughts for young, applied scientists and engineers.},
 authors = {['Robert L. Nowack']},
 journal = {Earthquake Science},
 keywords = {['geophysics', 'computational and data science', 'applied science and engineering']},
 title = {Science and reflections: With some thoughts to young applied scientists and engineers},
 year = {2024}
}

@Filtered Article{bf53fbe4-f99e-4f2b-a5bb-c4bdcecba2d7,
 abstract = {I incorporate the level-k thinking solution concept into a simplified (Brummermeier and Pedersen, 2005) predatory trading model to investigate the possibility of arbitraging arbitrageurs. While naive financial predators prey upon a single distressed investor, higher-level thinkers best respond to this and prey upon fellow predators. For some parameter values, sophisticated predators are able to reason their way to the Nash equilibrium strategy, and prices do not oscillate. As parameter values are perturbed, the system undergoes a bifurcation and predators select strategies from a mean-preserving spread of the Nash equilibrium strategy. In these settings, prices display excess volatility and a single shock can send predators into an oscillatory trading frenzy.},
 authors = {['Keisuke Teeple']},
 journal = {Journal of Mathematical Economics},
 keywords = {['Behavioral finance', 'Level- models', 'Front running', 'Price overshooting']},
 title = {Level-k predatory trading},
 year = {2023}
}

@Filtered Article{bf672989-d7f5-48ce-96f0-58f4650ef0c9,
 abstract = {We are rapidly moving into an era where AI and robots are part of everyday interactions in society and education, and there are immense discussions today about current and future technologies. Still, children are often not included in this discussion, while there is much to learn from current uses and children's understandings of AI and robotics. The study is based on a seven-month ethnographical work that details the implementation of a robot in two preschool groups of children aged 1–2 and 3–5 (n = 38). The study descriptively combines a framework for children's play analysis with explorative qualitative child interviews (n = 6) with the 3-5-year-olds to examine how children play with the robot and their thinking about a future with robots and AI. The results show how children's play with robots spans all of Hughes's (2011) sixteen play types and integrates robots into play in ways specific to child-robot interaction. The interviews indicate that children have well-formed knowledge about the current uses of robots and AI and elaborate imaginaries about a future with them, including critical boundaries toward robots and AI agents. The evidence shows emerging ways children relate to these. The potential of including children's actions and voices in the ongoing societal and educational debates on AI is discussed.},
 authors = {['Robin Samuelsson']},
 journal = {Computers and Education: Artificial Intelligence},
 keywords = {['Early childhood', 'Robots', 'AI', 'Play', 'Playful learning', 'Sociotechnical imaginaries']},
 title = {A shape of play to come: Exploring children's play and imaginaries with robots and AI},
 year = {2023}
}

@Filtered Article{bfe3f232-e2a9-430b-ab29-aa21df736211,
 abstract = {The growing interest in the role of social support in mental and physical health has led to the development of several intelligent systems that aim to use social mechanisms to simulate healthy behaviour. In this paper a computational model of a human agent is presented which describes the effect of social support on mood. According to the literature, social support can either refer to the social resources that individuals perceive to be available or to the support that is actually provided in problematic situations. The proposed model distinguishes between both roles of social support. The role of social network characteristics has been taken into account, as an individual can perceive or receive social support through his/her social network. In addition, the number of connections (friends), strength of ties (relationships), social isolation and social integration have been studied. Simulation experiments have been done to analyze the effect of the different types of support in different scenarios and also to analyze the role of various social network characteristics on the mood level. It is shown that support can help to reduce the induced stress and thus can contribute to healthy mood regulation and prevention of depression. The presented model provides a basis for an intelligent support system for people with mood regulation problems that take the social network of people into account.},
 authors = {['Seyed Amin Tabatabaei', 'Altaf Hussain Abro', 'Michel Klein']},
 journal = {Cognitive Systems Research},
 keywords = {['Social support', 'Stress buffering', 'Human ambient agent', 'Perceived support', 'Social network characteristics']},
 title = {With a little help from my friends: A computational model for the role of social support in mood regulation},
 year = {2018}
}

@Filtered Article{bff3f9c5-4c6e-4f88-ad27-d6e1becb8578,
 abstract = {Objective
We aim to investigate whether EEG dynamics differ in adults with ASD (Autism Spectrum Disorders), ADHD (attention-deficit/hyperactivity disorder), compared with healthy subjects during the performance of an innovative cognitive task: Aristotle's valid and invalid syllogisms. We follow the Neuroanatomical differences type of criterion in assessing the results of our study in supporting or not the dual-process theory of Kahneman, 2011) (Systems I & II of thinking).
Method
We recorded EEGs from 14 scalp electrodes in 30 adults with ADHD, 30 with ASD and 24 healthy, normal subjects. The subjects were exposed in a set of innovative cognitive tasks (inducing varying cognitive loads), the Aristotle's four types of syllogism mentioned above. The multiscale entropy (MSE), a nonlinear information-theoretic measure or tool was computed to extract features that quantify the complexity of the EEG.
Results
The dynamics of the curves of the grand average of MSE values of the ADHD and ASD participants was significantly in higher levels for the majority of time scales, than the healthy subjects over a number of brain regions (electrodes locations), during the performance of both valid and invalid types of syllogism. This result is seemingly not in accordance of the broadly accepted ‘theory’ of complexity loss in ‘pathological’ subjects, but actually this is not the case as explained in the text. ADHD subjects are engaged in System II of thinking, for both Valid and Invalid syllogism, ASD and Control in System I for valid and invalid syllogism, respectively. A surprising and ‘provocative’ result of this paper, as shown in the next sections, is that the Complexity-variability of ASD and ADHD subjects, when they face Aristotle's types of syllogisms, is higher than that of the control subjects. An explanation is suggested as described in the text. Also, in the case of invalid type of Aristotelian syllogisms, the linguistic and visuo-spatial systems are both engaged ONLY in the temporal and occipital regions of the brain, respectively, of ADHD subjects. In the case of valid type, both above systems are engaged in the temporal and occipital regions of the brain, respectively, of both ASD and ADHD subjects, while in the control subjects only the visuo-spatial type is engaged (Goel et al., 2000; Knauff, 2007).
Conclusion
Based on the results of the analysis described in this work, the differences in the EEG complexity between the three groups of participants lead to the conclusion that cortical information processing is changed in ASD and ADHD adults, therefore their level of cortical activation may be insufficient to meet the peculiar cognitive demand of Aristotle's reasoning.
Significance
The present paper suggest that MSE, is a powerful and efficient nonlinear measure in detecting neural dysfunctions in adults with ASD and ADHD characteristics, when they are called on to perform in a very demanding as well as innovative set of cognitive tasks, that can be considered as a new diagnostic ‘benchmark’ in helping detecting more effectively such type of disorders. A linear measure alone, as the typical PSD, is not capable in making such a distinction. The work contributes in shedding light on the neural mechanisms of syllogism/reasoning of Aristotelian type, as well as toward understanding how humans reason logically and why ‘pathological’ subjects deviate from the norms of formal logic.},
 authors = {['Anastasia G. Papaioannou', 'Eva Kalantzi', 'Christos C. Papageorgiou', 'Kalliopi Korombili', 'Anastasia Βokou', 'Artemios Pehlivanidis', 'Charalabos C. Papageorgiou', 'George Papaioannou']},
 journal = {Heliyon},
 keywords = {['Multiscale entropy', 'Power Spectral Density', "Aristotle's syllogism", 'ASD-ADHD', 'Systems of thinking I&II', 'Cognitive load']},
 title = {Complexity analysis of the brain activity in Autism Spectrum Disorder (ASD) and Attention Deficit Hyperactivity Disorder (ADHD) due to cognitive loads/demands induced by Aristotle's type of syllogism/reasoning. A Power Spectral Density and multiscale entropy (MSE) analysis},
 year = {2021}
}

@Filtered Article{c06f1526-f125-4642-9f61-2266c91efa9d,
 abstract = {Summary
Executive control, the ability to organize thoughts and action plans in real time, is a defining feature of higher cognition. Classical theories have emphasized cortical contributions to this process, but recent studies have reinvigorated interest in the role of the thalamus. Although it is well established that local thalamic damage diminishes cognitive capacity, such observations have been difficult to inform functional models. Recent progress in experimental techniques is beginning to enrich our understanding of the anatomical, physiological, and computational substrates underlying thalamic engagement in executive control. In this review, we discuss this progress and particularly focus on the mediodorsal thalamus, which regulates the activity within and across frontal cortical areas. We end with a synthesis that highlights frontal thalamocortical interactions in cognitive computations and discusses its functional implications in normal and pathological conditions.},
 authors = {['Mathieu Wolff', 'Michael M. Halassa']},
 journal = {Neuron},
 keywords = {['thalamus', 'mediodorsal', 'prefrontal cortex', 'cognition', 'cognitive flexibility', 'computation', 'neural architectures']},
 title = {The mediodorsal thalamus in executive control},
 year = {2024}
}

@Filtered Article{c09ac776-c75f-4bf9-8183-e8a3ffd705d0,
 abstract = {The ripple effect measures impact, or how likely it is that a change to a particular module may cause problems in the rest of a program. It can also be used as an indicator of the complexity of a particular module or program. Central to this paper is a reformulation in terms of matrix arithmetic of the original ripple effect algorithm produced by Yau and Collofello in 1978. The main aim of the reformulation is to clarify the component parts of the algorithm making the calculation more explicit. The reformulated algorithm has been used to implement REST (Ripple Effect and Stability Tool) which produces ripple effect measures for C programs. This paper describes the reformulation of Yau and Collofello’s ripple effect algorithm focusing on the computation of matrix Zm which holds intramodule change propagation information. The reformulation of the ripple effect algorithm is validated using fifteen programs which have been grouped by type. Due to the approximation spurious 1s are contained within matrix Zm. It is discussed whether this has an impact on the accuracy of the reformulated algorithm. The conclusion of this research is that the approximated algorithm is valid and as such can replace Yau and Collofello’s original algorithm.},
 authors = {['Sue Black']},
 journal = {Information and Software Technology},
 keywords = {['Software measurement', 'Ripple effect', 'Matrix algebra']},
 title = {Deriving an approximation algorithm for automatic computation of ripple effect measures},
 year = {2008}
}

@Filtered Article{c0bc346e-0689-4475-998e-3c5fd6edd75f,
 abstract = {Analytical decision making strategies rely on weighing pros and cons of multiple options in an unbounded rationality manner. Contrary to these strategies, recognition primed decision (RPD) model which is a primary naturalistic decision making (NDM) approach assumes that experienced and professional decision makers when encounter problems in real operating conditions are able to use their previous experiences and trainings in order to diagnose the problem, recall the appropriate solution, evaluate it mentally, and implement it to handle the problem in a satisficing manner. In this paper, a computational form of RPD, now called C-RPD, is presented. Unified Modeling Language was used as a modeling language to represent the proposed C-RPD model in order to make the implementation easy and obvious. To execute the model, RoboCup Rescue agent simulation environment, which is one of the best and the most famous complex and multi-agent large-scale environments, was selected. The environment simulates the incidence of fire and earthquakes in urban areas where it is the duty of the police forces, firefighters and ambulance teams to control the crisis. Firefighters of SOS team are first modeled and implemented by utilizing C-RPD and then the system is trained using an expert’s experience. There are two evaluations. To find out the convergence of different versions developed during experience adding, some of the developed versions are chosen and evaluated on seven maps. Results show performance improvements. The SOS team ranked first in an official world championship and three official open tournaments.},
 authors = {['Alireza Nowroozi', 'Mohammad E. Shiri', 'Angeh Aslanian', 'Caro Lucas']},
 journal = {Information Sciences},
 keywords = {['Naturalistic decision making', 'Recognition primed decision model', 'Computational modeling', 'Disaster management', 'RoboCup', 'Multi-agent rescue simulation benchmark']},
 title = {A general computational recognition primed decision model with multi-agent rescue simulation benchmark},
 year = {2012}
}

@Filtered Article{c2a8bc95-a043-440f-bf14-30e6d9053fda,
 abstract = {Until fairly recently, creativity was a human-specific characteristic. Computational creativity or artificial creativity was established as a domain in the late 90’s, with different fields such as verbal, musical, or graphical creativity. With the latest technological advances and the appearance of Large Language Models (LLMs), creativity as a feature of machines gained more and more interest in the scientific community. The scope of this study is twofold: to design a comprehensive benchmark for verbal creativity assessment of LLMs and then to run the same creativity tests on different LLMs as well as on humans, for a direct comparison. We aimed to raise the replicability and extensibility of the creativity assessment of LLMs. Hence, we adapted different types of creativity tests and different criteria from psychology to fit the LLMs profile. We also employed computer-assisted evaluation methods, by using the Open Creativity Scoring with Artificial Intelligence (OCSAI), as we wanted to focus exclusively on automated approaches to assessing creativity. We quantitatively and qualitatively analyzed the data set of both human and machine-generated answers and interpreted the results. Finally, we provide both the original verbal creativity test that we have designed, and the curated data comprising all the collected answers, from the LLMs and from the humans that participated in this research.},
 authors = {['Anca Dinu', 'Andra Maria Florescu']},
 journal = {Procedia Computer Science},
 keywords = {['LLM creativity', 'AI creativity', 'verbal creativity tests', 'human-machine comparison']},
 title = {An integrated benchmark for verbal creativity testing of LLMs and humans},
 year = {2024}
}

@Filtered Article{c2de4263-7b60-40c6-991a-2ad8672ad5ff,
 abstract = {Prior research on collaborative learning identifies student behaviors that significantly predict student achievement, such as giving explanations of one’s thinking. Less often studied is the role of teachers’ instructional practices in collaboration among students. This article investigates the extent to which teachers engage in practices that support students’ explanations of their thinking, and how these teacher practices might be related to the nature of explanations that students give when asked by the teacher to collaborate with each other. The teachers observed here, all of whom received specific instruction in eliciting the details of student thinking, varied significantly in the extent to which they asked students to elaborate on their suggestions. This variation corresponded to variation across classrooms in the nature and extent of student explanations during collaborative conversations and to differences in student achievement.},
 authors = {['Noreen M. Webb', 'Megan L. Franke', 'Marsha Ing', 'Angela Chan', 'Tondra De', 'Deanna Freund', 'Dan Battey']},
 journal = {Contemporary Educational Psychology},
 keywords = {['Instructional practices', 'Student collaboration']},
 title = {The role of teacher instructional practices in student collaboration},
 year = {2008}
}

@Filtered Article{c37a6da1-f7bd-4f5a-999f-d3eb7794e6fe,
 abstract = {Summary
Attempts to understand psychosis—the experience of profoundly altered perceptions and beliefs—raise questions about how the brain models the world. Standard predictive coding approaches suggest that it does so by minimising mismatches between incoming sensory evidence and predictions. By adjusting predictions, we converge iteratively on a best guess of the nature of the reality. Recent arguments have shown that a modified version of this framework—hybrid predictive coding—provides a better model of how healthy agents make inferences about external reality. We suggest that this more comprehensive model gives us a richer understanding of psychosis compared with standard predictive coding accounts. In this Personal View, we briefly describe the hybrid predictive coding model and show how it offers a more comprehensive account of the phenomenology of delusions, thereby providing a potentially powerful new framework for computational psychiatric approaches to psychosis. We also make suggestions for future work that could be important in formalising this novel perspective.},
 authors = {['Jessica Niamh Harding', 'Noham Wolpe', 'Stefan Peter Brugger', 'Victor Navarro', 'Christoph Teufel', 'Paul Charles Fletcher']},
 journal = {The Lancet Psychiatry},
 title = {A new predictive coding model for a more comprehensive account of delusions},
 year = {2024}
}

@Filtered Article{c39a464d-e797-4c11-91e2-78a6b455af15,
 abstract = {The merging of the computational theory of mind and evolutionary thinking leads to a kind of rationalism, in which enduring truths about the world have become implicit in the computations that enable the brain to cope with the experienced world. The dead reckoning computation, for example, is implemented within the brains of animals as one of the mechanisms that enables them to learn where they are (Gallistel, 1990, Gallistel, 1995). It integrates a velocity signal with respect to a time signal. Thus, the manner in which position and velocity relate to one another in the world is reflected in the manner in which signals representing those variables are processed in the brain. I use principles of information theory and Bayesian inference to derive from other simple principles explanations for: (1) the failure of partial reinforcement to increase reinforcements to acquisition; (2) the partial reinforcement extinction effect; (3) spontaneous recovery; (4) renewal; (5) reinstatement; (6) resurgence (aka facilitated reacquisition). Like the principle underlying dead-reckoning, these principles are grounded in analytic considerations. They are the kind of enduring truths about the world that are likely to have shaped the brain's computations.},
 authors = {['C.R. Gallistel']},
 journal = {Behavioural Processes},
 keywords = {['Acquisition', 'Extinction', 'Partial reinforcement', 'Spontaneous recovery', 'Renewal', 'Reinstatement', 'Resurgence', 'Information theory', 'Bayesian inference']},
 title = {Extinction from a rationalist perspective},
 year = {2012}
}

@Filtered Article{c3ae7dd6-4d75-45fa-a5cf-cfdca5f96d7c,
 abstract = {Conceptual design is a pivotal stage of new product development in manufacturing industries. Since multiple design alternatives are put forward at this stage, developing advanced evaluation methods is of great importance. Existing methods adopt additive models to integrate evaluation data. They face some inconsistency issues, e.g. inconsistency in the independent assumption and interdependent data, since evaluation criteria are interactional. Fuzzy measure that replaces the additivity with monotonicity has enabled advances in addressing such issues. This work proposes a two-additive fuzzy measure-based information integration approach to product design alternative evaluation for the first time. The evaluation data given by experts are in the form of intuitionistic linguistic numbers. They are more in accordance with the thinking habits of experts because the hesitation degree in linguistic assessment can be revealed. In order to reduce the subjective bias, the decision-making trial and evaluation laboratory method combining with grey relational analysis is applied to adjust evaluation data. Then monotonous two-additive fuzzy measure is identified by nonlinear programming using these data. It makes a good trade-off between computational complexity and presentation capability. Hence, evaluation data can be integrated by non-additive Choquet integral for ranking design alternatives. In comparison to additive model-based methods, the extra effect on the simultaneous satisfaction of criteria can be effectively revealed by the proposed approach. And the robustness of it is demonstrated by the sensitivity analysis. A case study on an elevator's design alternative evaluation is conducted to illustrate the feasibility and practicability of the proposed approach.},
 authors = {['Shanhe Lou', 'Yixiong Feng', 'Zhiwu Li', 'Jianrong Tan']},
 journal = {Journal of Industrial Information Integration},
 keywords = {['Multi-criteria decision-making', 'Two-additive fuzzy measure', 'Information integration', 'Intuitionistic linguistic number']},
 title = {Two-additive fuzzy measure-based information integration approach to product design alternative evaluation},
 year = {2022}
}

@Filtered Article{c3f25fb4-0298-4a0b-b7dc-9bfb0c1a6435,
 abstract = {Cholinergic (Ach), Noradrenergic (NE), and Dopaminergic (DA) pathways play an important role in the regulation of spatial attention. The same neurotransmitters are also responsible for inter-individual differences in temperamental traits. Here we explored whether biologically defined temperamental traits determine differences in the ability to orient spatial attention as a function of the probabilistic association between cues and targets. To this aim, we administered the Structure of Temperament Questionnaire (STQ-77) to a sample of 151 participants who also performed a Posner task with central endogenous predictive (80 % valid/20 % invalid) or non-predictive cues (50 % valid/50 % invalid). We found that only participants with high scores in Plasticity and Intellectual Endurance showed a selective abatement of attentional costs with non-predictive cues. In addition, stepwise regression showed that costs in the non-predictive condition were negatively predicted by scores in Plasticity and positively predicted by scores in Probabilistic Thinking. These results show that stable temperamental characteristics play an important role in defining the inter-individual differences in attentional behaviour, especially in the presence of different probabilistic organisations of the sensory environment. These findings emphasize the importance of considering temperamental and personality traits in social and professional environments where the ability to control one's attention is a crucial functional skill.},
 authors = {['Stefano Lasaponara', 'Gabriele Scozia', 'Silvana Lozito', 'Mario Pinto', 'David Conversi', 'Marco Costanzi', 'Tim Vriens', 'Massimo Silvetti', 'Fabrizio Doricchi']},
 journal = {Cortex},
 keywords = {['Attention', 'Temperament', 'Personality', 'Posner task', 'Neurotransmitters']},
 title = {Temperament and probabilistic predictive coding in visual-spatial attention},
 year = {2024}
}

@Filtered Article{c3fdb290-7bcc-4705-8ebb-5e36e03e5ec6,
 abstract = {This paper presents an efficient and reliable evolutionary based approach to solve the economic load dispatch (ELD) with security constraints. A new approach is proposed which employs attractive and repulsive particle swarm optimization (ARPSO) algorithm for ELD. Incorporation of ARPSO as a derivative-free optimization technique in solving ELD with security (voltages and line-flows) constraints significantly relieves the assumptions imposed on the optimized objective function. The proposed approach has been implemented on three representative systems, i.e. IEEE 14 bus, IEEE 30 bus and IEEE 57 bus systems, respectively. The feasibility of the proposed method is demonstrated and the results are compared with linear programming, quadratic programming and genetic algorithm, respectively. The premature convergence problem, that is common in all evolutionary computation techniques, is solved in ARPSO by including the diversity factor in the Type 1 PSO algorithm. The developed algorithms are computationally faster (in terms of the number of load flows carried out) than the other methods because only one run is required.},
 authors = {['K.S. Swarup', 'P. Rohit Kumar']},
 journal = {International Journal of Electrical Power & Energy Systems},
 keywords = {['Power system optimization', 'Economic load dispatch', 'Particle swarm optimization', 'Line-flow', 'Voltage constraints']},
 title = {A new evolutionary computation technique for economic dispatch with security constraints},
 year = {2006}
}

@Filtered Article{c45fbd07-2b6e-4073-9d3b-6fc0e92e9b9a,
 abstract = {A design methodology for producing cognitively efficient visualizations is introduced. The method involves seven steps (1) a high-level cognitive task description; (2) a data inventory; (3) cognitive task refinement; (4) identification of appropriate visualization types; (5) applying visual thinking design patterns (VTDPs); (6) prototype development; and (7) evaluation and design refinement. Most of the chapter is devoted to a set of VTDPs. These are descriptions of interactive visualization methods that have demonstrated value, together with a description of perceptual and cognitive issues relating to their use and guidelines for applicability. VDTPs provide a method for taking into account perceptual and cognitive issues in designing interaction especially with respect to key bottlenecks in the visual thinking process, such as limited visual working memory capacity. They also provide a way of reasoning about semiotic issues in perceptual terms via the concept of the visual query.},
 authors = {['Colin Ware']},
 journal = {Morgan Kaufmann},
 keywords = {['Interactive visualization design', 'Visual thinking', 'Visual thinking design patterns', 'Visual working memory', 'Visualization design', 'Visualization types']},
 title = {Chapter Twelve - Designing Cognitively Efficient Visualizations},
 year = {2021}
}

@Filtered Article{c4e0a30f-b232-4838-aec3-e6ccb4fc9603,
 abstract = {We present a critical analysis of the effectiveness of the current field of CAD, and discuss some of the forces that have taken it so far off course from its strikingly foresighted origins. Armed with the ensuing understanding of the operational forces that have taken CAD adrift, we conclude that the disparity between CAD’s mired state-of-the-art condition relative to more appropriate, inspired and achievable goals for CAD calls for more drastic measures. It is asserted that, well beyond the evolutionary progression of incremental steps characteristic of next version system releases, the field is overdue for developing a class of genuine design-centric, ab initio, CAD systems architectures effecting the original CAD vision through the powerful instruments of contemporary computing tools and technologies.},
 authors = {['Richard F. Riesenfeld', 'Robert Haimes', 'Elaine Cohen']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Multidisciplinary analysis driven design', 'Integrated computational engineering', 'CAD/CAE/CAM/IGA']},
 title = {Initiating a CAD renaissance: Multidisciplinary analysis driven design: Framework for a new generation of advanced computational design, engineering and manufacturing environments},
 year = {2015}
}

@Filtered Article{c51ed198-40ba-48f1-8320-3f77cd455fe0,
 abstract = {Divergent thinking (DT) ability (i.e., the ability to come up with creative ideas) is a complex cognitive construct that has been associated with several specific components of the Cattel-Horn-Carroll (CHC) model. In this study, we employed a nested latent variable approach to examine the specific role of mental speed (Gs) and general retrieval ability (Gr) in DT ability, which was assessed by DT tasks that instructed to be creative and were scored for creative quality. Specifically, Gs was assumed to facilitate both Gr and DT, and Gr was assumed to contribute to DT. Successive latent variable models with orthogonal factors were tested to reflect these nested cognitive basic abilities. The proposed model of nested factors fit the data well: Latent Gs accounted for variation in Gs, Gr, and DT creative quality scores, latent Gr predicted performance in Gr and DT scores beyond Gs, and latent DT explained variation in DT scores beyond Gs and Gr. In addition, we related the resulting orthogonal latent variables to the external criteria of school grades to illustrate the explanatory power of the modeling approach. This study provides evidence that divergent thinking performance relies on mental speed and retrieval ability, as well as cognitive abilities unique to divergent thinking. We discuss consequences for the understanding of divergent thinking ability in the context of the CHC model.},
 authors = {['Boris Forthmann', 'David Jendryczko', 'Jana Scharfen', 'Ruben Kleinkorres', 'Mathias Benedek', 'Heinz Holling']},
 journal = {Intelligence},
 keywords = {['Divergent thinking', 'Broad retrieval ability', 'Processing speed', 'Structural equation modeling']},
 title = {Creative ideation, broad retrieval ability, and processing speed: A confirmatory study of nested cognitive abilities},
 year = {2019}
}

@Filtered Article{c56449cf-0f0a-4528-b831-fc670894e180,
 abstract = {This article is a reply to Foss et al.’s (2022) contribution to the special issue of the Scandinavian Journal of Management on The Great Reset of management and organization theory. In their article, the authors make a strong case that “reset thinking” geared towards a more “sustainable” redesign of the global economy promotes extensive state interventionism and cronyism capitalism, and therefore reject the idea of a need for “a fundamental rethink of existing management theory”. Whereas I do agree with the authors on most points, I am less convinced that “existing management theory” will suffice to address the problem of “reset thinking”. In this article, I demonstrate that the economy-bias of existing theories is a gateway for “reset thinking” geared towards an allegedly necessary re-/socialisation of management and organisation. A research agenda on cronyism must therefore be complemented by one on privilege and hierarchy not only as undesirable side-effects of cronyism, but also as desired outcomes of advocacy for specific minorities or missions. As self-identifications with group interests or calls for missions have become popular in management theory, I conclude that this new appetite for privilege might undermine not only the higher ideals of many management theorists, but also the foundations of modern society.},
 authors = {['Steffen Roth']},
 journal = {Scandinavian Journal of Management},
 keywords = {['The Great Reset', 'Management theory', 'Cronyism', 'Stratification', 'Conservatism', 'Restorism']},
 title = {Reset and restoration. The looming conservative turn of management theory: An extension of Foss et al.},
 year = {2023}
}

@Filtered Article{c57d97d3-c1ee-49cf-a4de-4e602d9ad25e,
 abstract = {I show how a conversational process that takes simple, intuitively meaningful steps may be understood as a sophisticated computation that derives the richly detailed, complex representations implicit in our knowledge of language. To develop the account, I argue that natural language is structured in a way that lets us formalize grammatical knowledge precisely in terms of rich primitives of interpretation. Primitives of interpretation can be correctly viewed intentionally, as explanations of our choices of linguistic actions; the model therefore fits our intuitions about meaning in conversation. Nevertheless, interpretations for complex utterances can be built from these primitives by simple operations of grammatical derivation. In bridging analyses of meaning at semantic and symbol-processing levels, this account underscores the fundamental place for computation in the cognitive science of language use.},
 authors = {['Matthew Stone']},
 journal = {Cognitive Science},
 keywords = {['Dialogue', 'Pragmatics', 'Tree adjoining grammar']},
 title = {Intention, interpretation and the computational structure of language},
 year = {2004}
}

@Filtered Article{c595b292-5002-455a-a0d8-111bfe58080a,
 abstract = {An increasing number of dissident voices claim that the standard neo-Darwinian view of genes as ‘leaders’ and phenotypes as ‘followers’ during the process of adaptive evolution should be turned on its head. This idea is older than the rediscovery of Mendel’s laws of inheritance, with the turn-of-the-twentieth-century notion eventually labeled as the ‘Baldwin effect’ as one of the many ways in which the standard neo-Darwinian view can be turned around. A condition for this effect is that environmentally induced variation such as phenotypic plasticity or learning is crucial for the initial establishment of a trait. This gives the additional time for natural selection to act on genetic variation and the adaptive trait can be eventually encoded in the genotype. An influential paper published in the late 1980s claimed the Baldwin effect to happen in computer simulations, and avowed that it was crucial to solve a difficult adaptive task. This generated much excitement among scholars in various disciplines that regard neo-Darwinian accounts to explain the evolutionary emergence of high-order phenotypic traits such as consciousness or language almost hopeless. Here, we use analytical and computational approaches to show that a standard population genetics treatment can easily crack what the scientific community has granted as an unsolvable adaptive problem without learning. Evolutionary psychologists and linguists have invoked the (claimed) Baldwin effect to make wild assertions that should not be taken seriously. What the Baldwin effect needs are plausible case-histories.},
 authors = {['Mauro Santos', 'Eörs Szathmáry', 'José F. Fontanari']},
 journal = {Journal of Theoretical Biology},
 keywords = {['Evolutionary search', 'Genetic algorithm', 'Learning', 'The Baldwin effect', 'Speed of evolution']},
 title = {Phenotypic plasticity, the Baldwin effect, and the speeding up of evolution: The computational roots of an illusion},
 year = {2015}
}

@Filtered Article{c5b8219f-1cd7-49f0-838b-33c068eee875,
 abstract = {This paper proposes a big data integrated framework to assist with prevention and control of HIV/AIDS, TB and silicosis (HATS) in the mining industry. The linkage between HATS presents a major challenge to the mining industry globally. When the immune system is compromised by HIV/AIDS and silicosis, it makes it easier for tuberculosis to infect the body. In addition, the silica dust which affects the lungs may also cause silicosis and tuberculosis. The objective of this paper is to posit a big data integrated framework to assist in the prevention and control of HATS in the mining industry. Literature was reviewed in order to build a conceptual framework. Although this study is not the first to apply big data in healthcare, to the researcher's knowledge, it is the first to apply big data in understanding the linkage between HATS in the mining industry. The literature review indicates only a few studies using big data in healthcare with no research found on big data and HATS. It therefore makes a contribution to existing body of literature on the control of HATS. The proposed big data framework has the potential of addressing the needs of predictive epidemiology which is important in forecasting and disease control in the mining industry. The paper therefore lays a foundation for the use of viable systems model and big data to address the challenges of HATS in the mining industry. As part of future work, the framework will be validated using sequential explanatory mixed methods case study approach in mining organizations.},
 authors = {['Osden Jokonya']},
 journal = {Procedia Technology},
 keywords = {['Tuberculosis', 'Big Data', 'HIV/AIDS', 'Silicosis', 'Systems Approach', 'Viable Systems Model', 'Organizational Cybernetics', 'Hard Systems Thinking', 'Soft Systems Thinking', 'Emancipatory Systems Thinking', 'Critical Systems Thinking', 'Epidemiology']},
 title = {Towards a Big Data Framework for the Prevention and Control of HIV/AIDS, TB and Silicosis in the Mining Industry},
 year = {2014}
}

@Filtered Article{c64cca67-badd-4046-b1e3-e5e136680fa5,
 abstract = {The paper reviews the evolution of knowledge and its current state regarding the evaluation of wind loads on low buildings by placing particular emphasis on Alan Davenport's contributions. These contributions have paved the way to the current state-of-the-art and have influenced the thinking of not only Alan's closest collaborators but also of other researchers in this area around the world. The paper will provide a brief historical perspective, followed by some detailed description of the University of Western Ontario's research on wind loads on low buildings carried out in the 1970s. Visualizing the wake of Davenport's contributions in this area, the paper will refer to the influence of this knowledge in the formulation of design load provisions in contemporary wind standards and codes of practice. The paper will also discuss the status of computational wind engineering as well as the so-called computer-aided wind engineering in the evaluation of wind pressures on low buildings.},
 authors = {['Ted Stathopoulos']},
 journal = {Journal of Wind Engineering and Industrial Aerodynamics},
 keywords = {['Building', 'Code', 'Computational wind engineering', 'Design', 'Load', 'Pressure', 'Standard', 'Time series', 'Wind']},
 title = {Wind loads on low buildings: in the wake of Alan Davenport's contributions},
 year = {2003}
}

@Filtered Article{c6620c2f-9d66-4414-8930-cb00c9c91c02,
 abstract = {Synopsis
It has been alleged that, in appropriate verbal contexts, man and he are generic, i.e. that the words include women as well as men, as for example in, Man is mortal, or One must watch his language. Many feminists argue for the elimination of this generic use of man and he and the substitution of such non-male words as people and they. Others argue on various grounds that these changes are unnecessary. This paper isolates the issues involved in such arguments and provisionally concludes that a reduction in the generic use of man and he would result in a long term reduction in sexist thinking. Recent feminist research on man and he is carefully reviewed. In its final section, the paper develops the implication that women experience more alienation than men in the presence of the generic man and he.},
 authors = {['Jeanette Silveira']},
 journal = {Women's Studies International Quarterly},
 title = {Generic masculine words and thinking},
 year = {1980}
}

@Filtered Article{c678d28f-8cf1-4f50-b3e3-aadd6e55af99,
 abstract = {Publisher Summary
It is the purpose of this chapter to examine the origins, development, and present status of those key cybernetic notions that provide an information-flow framework within which to attack one aspect of the question of how a person thinks— that is,.the question of the information mechanisms and processes that underlie and are correlated with thinking. After an introductory survey of the scope and ramifications of the information sciences, the cybernetic way of looking at the information processing in the nervous system is examined, so as to see in what sense it provides new and sharp tools of analysis for the neurophysiologist. With this as background, the problem of artificial intelligence is considered and with that the logical and linguistic difficulties in talking about the relationship between thinking and brain activity. An information-flow model of an artificial brain mechanism is described whose activity; it is argued is the correlate to activity, such as perceiving, learning, thinking, knowing, etc. This leads finally to a consideration of the impact of these notions on theoretical neurophysiology and its attempt to frame suitable hypotheses and on epistemology that is concerned with the logical analysis of measures, methods, and techniques, which can justify the activity of knowing.},
 authors = {['M.E. Maron']},
 journal = {Elsevier},
 title = {On Cybernetics, Information Processing, and Thinking},
 year = {1965}
}

@Filtered Article{c6838497-9712-4be8-bfb0-02b0efe2f51e,
 abstract = {Modern science having embarked on the thorough and accurate interpretation of natural and physical phenomena has proven to provide successful models for the analysis of complex systems and harnessing of control over the various processes therein. Computational complexity, in this regard, comes to the foreground by providing applicable sets of ideas or integrative paradigms to recognize and understand the complex systems' intricate properties. Thus, while making the appropriate, adaptable and evolutive decisions in complex dynamic systems, it is essential to acknowledge different degrees of acceptance of the problems and construct the model it to account for its inherent constraints or limits. In this respect, while hypothesis-driven research has its inherent limitations regarding the investigation of multifactorial and heterogeneous diseases, a data-driven approach enables the examination of the way variables impact one another, which paves the way for the interpretation of dynamic and heterogeneous mechanisms of diseases. Fractional Calculus (FC), in this scope characterized by complexity, provides the applicable means and methods to solve integral, differential and integro-differential equations so FC enables the generalization of integration and differentiation possible in a flexible and consistent manner owing to its capability of reflecting the systems' actual state properties, which exhibit unpredictable variations. The fractional integration and differentiation of fractional-order is capable of providing better characterization of nonstationary and locally self-similar attributes in contrast to constant-order fractional calculus. It becomes possible to model many complex systems by fractional-order derivatives based on fractional calculus so that related syntheses can be realized in a robust and effective way. To this end, our study aims at providing an intermediary facilitating function both for the physicians and individuals by establishing accurate and robust model based on the integration of fractional-order calculus and Artificial Neural Network (ANN) for the diagnostic and differentiability predictive purposes with the diseases which display highly complex properties. The integrative approach we have proposed in this study has a multistage quality the steps of which are stated as follows: first of all, the Caputo fractional-order derivative, one of the fractional-order derivatives, has been used with two-parametric Mittag-Leffler function on the stroke dataset and cancer cell dataset, manifesting biological and neurological attributes. In this way, new fractional models with varying degrees have been established. Mittag-Leffler function, with its distributions of extensive application domains, can address irregular and heterogeneous environments for the solution of dynamic problems; thus, Mittag-Leffler function has been opted for accordingly. Following this application, the new datasets (mlf_stroke dataset and mlf_cancer cell dataset) have been obtained by employing Caputo fractional-order derivative with the two-parametric Mittag-Leffler function (α,β). In addition, classical derivative (calculus) was applied to the raw datasets; and cd_stroke dataset and cd_cancer cell dataset were obtained. Secondly, the performance of the new datasets as obtained from the Caputo fractional derivative with the two-parametric Mittag-Leffler function, the datasets obtained from the classical derivative application and the raw datasets have been compared by using feed forward back propagation (FFBP) algorithm, one of the algorithms of ANN (along with accuracy rate, sensitivity, precision, specificity, F1-score, multiclass classification (MCC), ROC curve). Based on the accuracy rate results obtained from the application with FFBP, the Caputo fractional-order derivative model that is most suitable for the diseases has been generated. The experimental results obtained demonstrate the applicability of the complex-systems-grounded paradigm scheme as proposed through this study, which has no existing counterpart. The integrative multi-stage method based on mathematical-informed framework with comparative differentiability prediction analyses can point toward a new direction in the various areas of applied sciences to address formidable challenges of critical decision making and management of chaotic processes in different complex dynamic systems.},
 authors = {['Yeliz Karaca', 'Dumitru Baleanu']},
 journal = {Academic Press},
 keywords = {['Complexity', 'Artificial neural network', 'Classical calculus', 'Computational complexity', 'Data-driven fractional modeling', 'Differentiability prediction analyses', 'Fractional calculus', 'Mathematical biology and neuroscience', 'Mittag-Leffler function', 'Optimized fractional-order calculus']},
 title = {Chapter 9 - Computational fractional-order calculus and classical calculus AI for comparative differentiability prediction analyses of complex-systems-grounded paradigm},
 year = {2022}
}

@Filtered Article{c6b66f7b-b27f-4cba-9e7c-cfa808ee9475,
 abstract = {The structure of human memory seems to be optimized for efficient prediction, planning, and behavior. We propose that these capacities rely on a tripartite structure of memory that includes concepts, events, and contexts—three layers that constitute the mental world model. We suggest that the mechanism that critically increases adaptivity and flexibility is the tendency to contextualize. This tendency promotes local, context-encoding abstractions, which focus event- and concept-based planning and inference processes on the task and situation at hand. As a result, cognitive contextualization offers a solution to the frame problem—the need to select relevant features of the environment from the rich stream of sensorimotor signals. We draw evidence for our proposal from developmental psychology and neuroscience. Adopting a computational stance, we present evidence from cognitive modeling research which suggests that context sensitivity is a feature that is critical for maximizing the efficiency of cognitive processes. Finally, we turn to recent deep-learning architectures which independently demonstrate how context-sensitive memory can emerge in a self-organized learning system constrained by cognitively-inspired inductive biases.},
 authors = {['Martin V. Butz', 'Maximilian Mittenbühler', 'Sarah Schwöbel', 'Asya Achimova', 'Christian Gumbsch', 'Sebastian Otte', 'Stefan Kiebel']},
 journal = {Neuroscience & Biobehavioral Reviews},
 keywords = {['Prediction', 'Cognitive modeling', 'Events', 'Active inference', 'Learning', 'Behavior', 'Free energy', 'Abstraction', 'Context inference', 'Deep learning']},
 title = {Contextualizing predictive minds},
 year = {2025}
}

@Filtered Article{c6d3b08e-5bd3-4b1f-bad0-4a9f2618947c,
 abstract = {The intersection of electromagnetic radiation and neuronal communication, focusing on the potential role of biophoton emission in brain function and neurodegenerative diseases is an emerging research area. Traditionally, it is believed that neurons encode and communicate information via electrochemical impulses, generating electromagnetic fields detectable by EEG and MEG. Recent discoveries indicate that neurons may also emit biophotons, suggesting an additional communication channel alongside the regular synaptic interactions. This dual signaling system is analyzed for its potential in synchronizing neuronal activity and improving information transfer, with implications for brain-like computing systems. The clinical relevance is explored through the lens of neurodegenerative diseases and intrinsically disordered proteins, where oxidative stress may alter biophoton emission, offering clues for pathological conditions, such as Alzheimer's and Parkinson's diseases. The potential therapeutic use of Low-Level Laser Therapy (LLLT) is also examined for its ability to modulate biophoton activity and mitigate oxidative stress, presenting new opportunities for treatment. Here, we invite further exploration into the intricate roles the electromagnetic phenomena play in brain function, potentially leading to breakthroughs in computational neuroscience and medical therapies for neurodegenerative diseases.},
 authors = {['Aysin Erboz', 'Elif Kesekler', 'Pier Luigi Gentili', 'Vladimir N. Uversky', 'Orkid Coskuner-Weber']},
 journal = {Progress in Biophysics and Molecular Biology},
 keywords = {['Electromagnetic radiation', 'Biophotons', 'Neurodegenerative diseases', 'Neuron communication', 'Intrinsically disordered proteins']},
 title = {Electromagnetic radiation and biophoton emission in neuronal communication and neurodegenerative diseases},
 year = {2025}
}

@Filtered Article{c715dd8c-24be-48bd-9d96-103fc9fcfd16,
 abstract = {This paper presents a comprehensive review of all significant research applying computational optimisation to sustainable building design problems. A summary of common heuristic optimisation algorithms is given, covering direct search, evolutionary methods and other bio-inspired algorithms. The main summary table covers 74 works that focus on the application of these methods to different fields of sustainable building design. Key fields are reviewed in detail: envelope design, including constructions and form; configuration and control of building systems; renewable energy generation; and holistic optimisations of several areas simultaneously, with particular focus on residential and retrofit. Improvements to the way optimisation is applied are also covered, including platforms and frameworks, algorithmic comparisons and developments, use of meta-models and incorporation of uncertainty. Trends, including the rise of multi-objective optimisation, are analysed graphically. Likely future developments are discussed.},
 authors = {['Ralph Evins']},
 journal = {Renewable and Sustainable Energy Reviews},
 keywords = {['Review', 'Optimisation', 'Sustainable', 'Building', 'Design', 'Multi-objective']},
 title = {A review of computational optimisation methods applied to sustainable building design},
 year = {2013}
}

@Filtered Article{c781237d-c9dd-4435-ad7e-2192dd23b524,
 abstract = {The ability to acquire, remember and use information about locations of objects in one’s proximal surrounding is a fundamental aspect of human spatial cognition. In this paper, we present a computational model of this ability. The model provides a possible explanation of contradictory results from experimental psychology related to this ability, namely explanation of why some experiments have reproduced the so-called “disorientation effect” while others have failed to do so. Additionally, in contrast to other computational models of various aspects of spatial cognition, our model is integrated within an intelligent virtual agent. Thus, on a more general level, this paper also demonstrates that it is possible to use intelligent virtual agents as a powerful research tool in computational cognitive sciences.},
 authors = {['Cyril Brom', 'Jan Vyhnánek', 'Jiří Lukavský', 'David Waller', 'Rudolf Kadlec']},
 journal = {Cognitive Systems Research},
 keywords = {['Spatial cognition', 'Paradigm of pointing', 'Disorientation effect', 'Intelligent virtual agent']},
 title = {A computational model of the allocentric and egocentric spatial memory by means of virtual agents, or how simple virtual agents can help to build complex computational models},
 year = {2012}
}

@Filtered Article{c82bbe7e-c079-4911-bdf6-2dc6a97d8fc5,
 abstract = {In this work we explored undergraduate students’ geometric and visual interpretations of the inscription for contour integrals, ∫Cfzdz, without them having any prior knowledge of integration of complex-valued functions. Our research participants drew from various sources of geometric and visual interpretations to productively investigate the components of contour integrals, which they conveyed with diagrams and gesture. Although this enabled significant progress, they were overwhelmed coordinating the multiple quantitative relationships and reverted to simplified interpretations such as summing values of z,fz, or ∆z. In other words, they were unable to maintain focus on what was accumulated. Our participants also engaged in the thinking real, doing complex phenomenon which sometimes provided productive feedback to assess their interpretations. We offer potential reasons for students’ struggles including various interpretations for integration of real-valued integration and the layering of inscriptions. We also provide potential instructional strategies based on the participants’ interpretations.},
 authors = {['Hortensia Soto', 'Michael Oehrtman']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Complex functions', 'Contour integration', 'Emerging models']},
 title = {Undergraduates’ exploration of contour integration: What is Accumulated?},
 year = {2022}
}

@Filtered Article{c8817f97-42fc-4d8d-8979-6606da72ed10,
 abstract = {Most theories of cognitive control assign a vital role to human prefrontal cortex (PFC). Although models of PFC function are abundant, most fail to capture the complexity of this part of the brain. Here we argue that an improved understanding of the evolution of PFC can aid in the formulation of better models. By better understanding what PFC is, why it evolved, and what benefit it provided to our ancestors, we can constrain our thinking and put the plethora of neuroimaging data showing PFC activation into context.},
 authors = {['Rogier B. Mars']},
 journal = {Elsevier},
 keywords = {['Prefrontal cortex', 'Brain evolution', 'Foraging', 'Granular prefrontal cortex', 'Dorsolateral prefrontal cortex', 'Cognitive control', 'Comparative neuroscience', 'Primate', 'Connectivity', 'Human']},
 title = {What every cognitive neuroscientist should know about prefrontal cortex evolution},
 year = {2025}
}

@Filtered Article{c8a41060-e999-43ce-b07a-f0f681efafd2,
 abstract = {Cognitive psychology seeks to understand how we acquire knowledge about ourselves and the world, how this knowledge is represented in the mind and brain, and how we use knowledge to guide behavior. Major topics in cognitive psychology include sensation and perception, attention, memory, categorization, learning, language and communication, and thinking, reasoning, judgment, and decision-making. Cognitive development is discussed from both an ontogenetic and phylogenetic point of view. Cognitive neuroscience explores the neural substrates of cognitive processes. The cognitive point of view has been extended to personality, social, and clinical psychology, as well as to sociology, anthropology, and other social-science disciplines.},
 authors = {['John F. Kihlstrom', 'Lillian Park']},
 journal = {Elsevier},
 keywords = {['Cognition', 'Sensation', 'Perception', 'Attention', 'Memory', 'Categorization', 'Learning', 'Language', 'Reasoning', 'Judgment', 'Decision-making', 'Choice', 'Cognitive development', 'Cognitive neuroscience', 'Cognitive sociology']},
 title = {Cognitive Psychology: Overview☆},
 year = {2018}
}

@Filtered Article{c8b6d6a6-48db-4254-8b3d-9405154079bf,
 abstract = {Kunen's expository work is described, bringing out both his way of assimilating and thinking about set theory and how it had a meaningful hand in its promulgation into the next generations.},
 authors = {['Akihiro Kanamori']},
 journal = {Annals of Pure and Applied Logic},
 keywords = {['Handbook chapters', 'Set theory text', 'Late texts']},
 title = {Kunen the expositor},
 year = {2024}
}

@Filtered Article{c9094ad6-ed05-4b76-a6c8-f6b9437f0648,
 abstract = {Is vision a necessary building block for the foundations of mathematical cognition? A straightforward model to test the causal role visual experience plays in the development of numerical abilities is to study people born without sight. In this review we will demonstrate that congenitally blind people can develop numerical abilities that equal or even surpass those of sighted individuals, despite representing numbers using a qualitatively different representational format. We will also show that numerical thinking in blind people maps onto regions typically involved in visuo-spatial processing in the sighted, highlighting how intrinsic computational biases may constrain the reorganization of numerical networks in case of early visual deprivation. More generally, we will illustrate how the study of arithmetic abilities in congenitally blind people represents a compelling model to understand how sensory experience scaffolds the development of higher-level cognitive representations.},
 authors = {['Virginie Crollen', 'Olivier Collignon']},
 journal = {Neuroscience & Biobehavioral Reviews},
 keywords = {['Blindness', 'Mathematical cognition', 'Brain plasticity']},
 title = {How visual is the « number sense »? Insights from the blind},
 year = {2020}
}

@Filtered Article{c94cd877-c504-4b45-bdde-fb80fc43e723,
 abstract = {To further advance the crop residue-based bioenergy (CRB) industry for climate change mitigation, it is crucial to better understand the influence of stakeholders' behaviours on greenhouse gases (GHG) mitigation potentials. However, the heterogeneity and social dynamics of stakeholders, particularly farmers, have received less attention. This study develops an Agent-based Environmental and Economic assessment (AEE) model that integrates agent-based model and life cycle thinking methods to simulate the CRB system. The AEE model was applied in Heilongjiang Province of China, to investigate how stakeholder decisions affect CRB's GHG reduction potential and government subsidies. Scenario analyses explore the effects of grain markets, subsidies, and collection distance on environmental and economic outcomes. The findings indicate that more farmers are willing to adopt crop residues collection than those currently practicing it, primarily due to logistical constraints. Key factors influencing adoption include farming income, age, farm size and crop types. CRB contributed to 70.6 % of overall GHG reductions with only 41.6 % of the subsidy, demonstrating higher mitigation efficiency. In conclusion, the government must address the deficiency in crop residues logistics to promote CRB development. Additionally, agricultural policies play a crucial role in ensuring CRB feedstock availability by guiding crop types selection. The results suggest that AEE model is adequate in simulating both micro and macro dynamics in the context of CRB, highlighting the robustness of integrating agent-based model and life cycle thinking methods to study complex issues.},
 authors = {['Jiaqi Zhang', 'Chengxiang Zhuge', 'Qitong Huang', 'Bin Wang', "Yu'e Li", 'Peter Oosterveer']},
 journal = {Sustainable Production and Consumption},
 keywords = {['Agent-based model', 'Crop residue-based bioenergy', 'Life cycle thinking', 'Environmental and economic impacts', 'Farmer decision making']},
 title = {Farmers’ decisions on crop residues utilization, greenhouse gases reduction and subsidy of crop residue-based bioenergy: An agent-based life cycle model},
 year = {2025}
}

@Filtered Article{c97e6fa0-e133-404a-97e7-731ebd4f5523,
 abstract = {This paper is an editorial guide for the second special issue on Computational Intelligence in economics and finance, which is a continuation of the special issue of Information Sciences, Vol. 170, No. 1. This second issue appears as a part of the outcome from the 3rd International Workshop on Computational Intelligence in Economics and Finance, which was held in Cary, North Carolina, September 26–30, 2003. This paper offers some main highlights of this event, with a particular emphasis on some of the observed progress made in this research field, and a brief introduction to the papers included in this special issue.},
 authors = {['Shu-Heng Chen']},
 journal = {Information Sciences},
 keywords = {['Computational intelligence', 'Agent-based computational economics']},
 title = {Computationally intelligent agents in economics and finance},
 year = {2007}
}

@Filtered Article{c9877bd4-2c2d-4276-b70c-afb69087f7c8,
 abstract = {Abstract:
Today biology is overwhelmed with ‘big data’, amassed from genomic projects carried out in various laboratories around the world using efficient high throughput technologies. Biologists are co-opting mathematical and computational techniques developed to address these data and derive meaningful interpretations. These developments have led to new disciplines: systems and synthetic biology. To explore these two evolving branches of biology one needs to be familiar with technologies such as genomics, bioinformatics and proteomics, mathematical and computational modeling techniques that help predict the dynamic behavior of the biological system, ruling out the trial-and-error methods of traditional genetic engineering. Systems and synthetic biology have developed hand-in-hand towards building artificial biological devices using engineered biological units as basic building blocks. Systems biology is an integrated approach for studying the dynamic and complex behaviors of biological components, which may be difficult to interpret and predict from properties of individual constituents making up the biological systems. While, synthetic biology aims to engineer biologically inspired devices, such as cellular regulatory circuits that do not exist in nature but are designed using well characterized genes, proteins and other biological components in appropriate combinations to perform a desired function. This is analogous to an electronic circuit board design that is fabricated using well characterized electrical components such as resistors, capacitors and so on. The in silico abstractions and predictions should be tightly linked to experimentation to be proved in vitro and in vivo systems for their successful applications in biotechnology. This chapter focuses on mathematical approaches and computational tools available to engineer biological regulatory circuits and how they can be implemented as next generation therapeutics in infectious disease.},
 authors = {['Milsee Mol', 'Shailza Singh']},
 journal = {Bentham Science Publishers},
 keywords = {['Abstraction', 'bioengineering', 'bioinspired', 'biological parts', 'computational modelling', 'computational tools', 'constructs', 'dynamic', 'infectious disease', 'interdisciplinary', 'linearization', 'mathematical framework', 'nextgen therapeutics', 'omics', 'ordinary differential equations', 'parameters', 'physical systems', 'reactions', 'regulatory circuits', 'simulation']},
 title = {Chapter 5 - Computational Design of Biological Systems: From Systems to Synthetic Biology},
 year = {2015}
}

@Filtered Article{c9d11b8e-be7f-4d2a-a664-b6b900c2bc72,
 abstract = {In this paper the authors propose a novel semiotic approach to the design of interactive systems and computational systems, grounded in the most recent contributions within the debate around semiotic theory and analysis. This approach, that is here called Semiotics of Configurations (SoC), is proposed for its analytic power in describing material artifacts and settings with a purposely a-conceptualistic stance. The resulting analysis informs a kind of design that is aimed at reproducing and supporting the programs of action detected in the use of artifacts, as this use is “abducted” from the physical and material form of the artifacts themselves and from the observation of how content is transformed within and across them. This approach to design, called immanent design, has inspired a platform for the user-driven development and use of electronic documents and forms in cooperative and organizational domains. The framework is illustrated with a case drawn from a study performed in the domain of hospital work.},
 authors = {['Federico Cabitza', 'Alvise Mattozzi']},
 journal = {Journal of Visual Languages & Computing},
 keywords = {['Semiotics of Configurations', 'Immanent design', 'End-User Development platforms', 'Document management systems', 'Electronic Health Record']},
 title = {The semiotics of configurations for the immanent design of interactive computational systems},
 year = {2017}
}

@Filtered Article{ca1f3007-1307-4cb5-80a5-b55943b1acaa,
 abstract = {Diverse explanations or theories of consciousness are arrayed on a roughly physicalist-to-nonphysicalist landscape of essences and mechanisms. Categories: Materialism Theories (philosophical, neurobiological, electromagnetic field, computational and informational, homeostatic and affective, embodied and enactive, relational, representational, language, phylogenetic evolution); Non-Reductive Physicalism; Quantum Theories; Integrated Information Theory; Panpsychisms; Monisms; Dualisms; Idealisms; Anomalous and Altered States Theories; Challenge Theories. There are many subcategories, especially for Materialism Theories. Each explanation is self-described by its adherents, critique is minimal and only for clarification, and there is no attempt to adjudicate among theories. The implications of consciousness explanations or theories are assessed with respect to four questions: meaning/purpose/value (if any); AI consciousness; virtual immortality; and survival beyond death. A Landscape of Consciousness, I suggest, offers perspective.},
 authors = {['Robert Lawrence Kuhn']},
 journal = {Progress in Biophysics and Molecular Biology},
 keywords = {['Consciousness', 'Mind-body problem', 'Materialism', 'Monism', 'Dualism', 'Idealism']},
 title = {A landscape of consciousness: Toward a taxonomy of explanations and implications},
 year = {2024}
}

@Filtered Article{ca2911f3-719a-40c6-825f-b5000229c06a,
 abstract = {Despite the popularity of PID (Proportional-Integral-Derivative) controllers, their tuning aspect continues to present challenges for researches and plant operators. Various control design methodologies have been proposed in the literature, such as auto-tuning, self-tuning, and pattern recognition. The main drawback of these methodologies in the industrial environment is the number of tuning parameters to be selected. In this paper, the design of a PID controller, based on the universal model of the plant, is derived, in which there is only one parameter to be tuned. This is an attractive feature from the viewpoint of plant operators. Fuzzy and neural approaches – bio-inspired methods in the field of computational intelligence – are used to design and assess the efficiency of the PID controller design based on differential evolution optimization in nonlinear plants. The numerical results presented herein indicate that the proposed bio-inspired design is effective for the nonlinear control of nonlinear plants.},
 authors = {['Rodrigo Rodrigues Sumar', 'Antonio Augusto Rodrigues Coelho', 'Leandro dos Santos Coelho']},
 journal = {Information Sciences},
 keywords = {['PID control', 'Nonlinear systems', 'Fuzzy systems', 'Neural networks', 'Differential evolution', 'Optimization']},
 title = {Computational intelligence approach to PID controller design using the universal model},
 year = {2010}
}

@Filtered Article{ca4d9289-2113-419e-bf1c-add61b09d600,
 abstract = {Within the domain of psychology, Optimal Experimental Design (OED) principles have been used to model how people seek and evaluate information. Despite proving valuable as computational-level methods to account for people’s behaviour, their descriptive and explanatory powers remain largely unexplored. In a series of experiments, we used a naturalistic crime investigation scenario to examine how people evaluate queries, as well as outcomes, in probabilistic contexts. We aimed to uncover the psychological strategies that people use, not just to assess whether they deviated from OED principles. In addition, we explored the adaptiveness of the identified strategies across both one-shot and stepwise information search tasks. We found that people do not always evaluate queries strictly in OED terms and use distinct strategies, such as by identifying a leading contender at the outset. Moreover, we identified aspects of zero-sum thinking and risk aversion that interact with people’s information search strategies. Our findings have implications for building a descriptive account of information seeking and evaluation, accounting for factors that currently lie outside the realm of information-theoretic OED measures, such as context and the learner’s own preferences.},
 authors = {['Alice Liefgreen', 'Toby Pilditch', 'David Lagnado']},
 journal = {Cognitive Psychology},
 keywords = {['Information search', 'OED framework', 'Utility functions', 'Inquiry', 'Question asking', 'Strategies', 'Probabilistic reasoning', 'Bayesian Networks']},
 title = {Strategies for selecting and evaluating information},
 year = {2020}
}

@Filtered Article{ca7be52d-b2b5-48d8-9ed1-04cc85f8c335,
 abstract = {The k-method is the isogeometric method based on splines (or NURBS, etc.) with maximum regularity. When implemented following the paradigms of classical finite element methods, the computational resources required by the k-method are prohibitive even for moderate degree. In order to address this issue, we propose a matrix-free strategy combined with weighted quadrature, which is an ad-hoc strategy to compute the integrals of the Galerkin system. Matrix-free weighted quadrature (MF-WQ) speeds up matrix operations, and, perhaps even more important, greatly reduces memory consumption. Our strategy also requires an efficient preconditioner for the linear system iterative solver. In this work we deal with an elliptic model problem, and adopt a preconditioner based on the Fast Diagonalization method, an old idea to solve Sylvester-like equations. Our numerical tests show that the isogeometric solver based on MF-WQ is faster than standard approaches (where the main cost is the matrix formation by standard Gaussian quadrature) even for low degree. But the main achievement is that, with MF-WQ, the k-method gets orders of magnitude faster by increasing the degree, given a target accuracy. Therefore, we are able to show the superiority, in terms of computational efficiency, of the high-degree k-method with respect to low-degree isogeometric discretizations. What we present here is applicable to more complex and realistic differential problems, but its effectiveness will depend on the preconditioner stage, which is as always problem-dependent. This situation is typical of modern high-order methods: the overall performance is mainly related to the quality of the preconditioner.},
 authors = {['G. Sangalli', 'M. Tani']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Isogeometric analysis', '-method', 'Matrix-free', 'Weighted quadrature', 'Preconditioner']},
 title = {Matrix-free weighted quadrature for a computationally efficient isogeometric k-method},
 year = {2018}
}

@Filtered Article{cab13368-5b26-44a0-bee1-d50976d00dc9,
 abstract = {There is growing attention on the importance of building intelligent systems where humans and Artificial Intelligence-based systems (AIs) form teams exploiting the potentially synergistic relationships between humans and automation. In the last decade, the computational modeling of empathy has gained increasing attention. Empowering interactive agents with empathic capabilities leads, on the human’s side, to more trust, increases engagement, and thus interaction length, helps cope with stress. These findings suggest that agents endowed with empathy may enhance social interaction in educational applications, artificial companions, medical assistants, and gaming applications. This article focuses on modeling the empathic behavior of virtual agents interacting with humans. We propose a formal model that enables virtual agents to exhibit empathic, emotional behavior. Specifically, we extend the modeling of empathy via behavior trees with a new type of node allowing the specification of various kinds of empathy. Using the proposed extension, we show how different agents’ reactive behavior can be modeled.},
 authors = {['Pierangelo Dell’Acqua', 'Stefania Costantini', 'Abeer Dyoub', 'Giovanni De Gasperis', 'Andrea Monaldini', 'Andrea Rafanelli']},
 journal = {Procedia Computer Science},
 keywords = {['Behavior Trees', 'Affective Computing', 'Decision Making']},
 title = {Empathy-Aware Behavior Trees for Social Care Decision Systems},
 year = {2024}
}

@Filtered Article{cadbe0a3-c28d-4718-a78c-9d8dfda5cb9c,
 abstract = {The $-calculus is the extension of the π-calculus, built around the central notion of cost and allowing infinity in its operators. We propose the $-calculus as a more complete model for problem solving to provide a support to handle intractability and undecidability. It goes beyond the Turing Machine model. We define the semantics of the $-calculus using a novel optimization method (the kΩ-optimization), which approximates a nonexisting universal search algorithm and allows the simulation of many other search methods. In particular, the notion of total optimality has been utilized to provide an automatic way to deal with intractability of problem solving by optimizing together the quality of solutions and search costs. The sufficient conditions needed for completeness, optimality and total optimality of problem solving search are defined. A very flexible classification scheme of problem solving methods into easy, hard and solvable in the limit classes has been proposed. In particular, the third class deals with non-recursive solutions of undecidable problems. The approach is illustrated by solutions of some intractable and undecidable problems. We also briefly overview two possible implementations of the $-calculus.},
 authors = {['Eugene Eberbach']},
 journal = {Theoretical Computer Science},
 keywords = {['Problem solving', 'Process algebras', 'Anytime algorithms', 'SuperTuring models of computation', 'Bounded rational agents', '$-calculus', 'Intractability', 'Undecidability', 'Completeness', 'Optimality', 'Search optimality', 'Total optimality']},
 title = {The $-calculus process algebra for problem solving: A paradigmatic shift in handling hard computational problems},
 year = {2007}
}

@Filtered Article{cb3d3f1a-8ed1-4aa1-b572-81431a8ac1ee,
 abstract = {The relationship between stress-tolerance mechanisms (cell-wall structure, metabolism, morphology, etc.) of individual fungi and the physiochemistry of their food environment selects for a small group of specific spoilage organisms (SSOs). However, common process deviations and post-processing contamination widen the lens of potentially relevant spoilage fungi. For example, although heat-resistant molds are considered the SSOs in thermally processed foods, unintended events (deviations, post-processing contamination) lead to spoilage by other propagules, notably yeast. The frequency of these unintended events changes our assessments of which spoilage fungi are relevant to a given food system. Consequently, a framework using probabilistic and systems-based thinking is needed to understand spoilage risk. Toward that goal, simple molecular tools for identification and subtyping are required.},
 authors = {['Abigail B Snyder']},
 journal = {Current Opinion in Food Science},
 title = {The role of heat resistance in yeast spoilage of thermally processed foods: highlighting the need for a probabilistic, systems-based approach to microbial quality},
 year = {2022}
}

@Filtered Article{cb7cb601-5a9c-4c1d-96cb-e63a6c2c19ae,
 abstract = {The demand for processing vast volumes of data has surged dramatically due to the advancement of machine learning technology. Large-scale data processing necessitates substantial computational resources, prompting individuals and enterprises to turn to cloud services. Accompanying this trend is a growing concern regarding data leakage and misuse. Homomorphic encryption (HE) is one solution for safeguarding data privacy, enabling encrypted data to be processed securely in the cloud. However, the encryption and decryption routines of some HE schemes require considerable computational resources, presenting non-trivial work for clients. In this paper, we propose an outsourced decryption protocol for the prevailing RLWE-based fully homomorphic encryption schemes. The protocol splits the original decryption into two routines, with the computationally intensive part executed remotely by the cloud. Its security relies on an invariant of the NTRU-search problem with a newly designed blinding key distribution. Cryptographic analyses are conducted to configure protocol parameters across varying security levels. Our experiments demonstrate that the proposed protocol achieves up to a 67% acceleration in the client-side computation, accompanied by a 50% reduction in space usage.},
 authors = {['Xirong Ma', 'Chuan Li', 'Yuchang Hu', 'Yunting Tao', 'Yali Jiang', 'Yanbin Li', 'Fanyu Kong', 'Chunpeng Ge']},
 journal = {Journal of Information Security and Applications},
 keywords = {['Privacy-preserving computation', 'Outsourced computing', 'Homomorphic encryption']},
 title = {Secure outsourced decryption for FHE-based privacy-preserving cloud computing},
 year = {2024}
}

@Filtered Article{cb852aa8-d0d9-4d71-86fa-ed63475e4bff,
 abstract = {Complexity has radically changed human understanding of the world environment and continues challenging our best scientific theories. In a rapidly changing research landscape, historical and philosophical insights into Complexity can heighten awareness of the proper theoretical perspectives scientists should adopt to advance the study of biocomplexity, including ecological complexity. The present work aims to deepen this awareness and disclose how researchers should generally approach, scientifically and philosophically, the question of what Complexity is, which is of great importance not only to the scientific community but also far beyond. First, this article reviews some critical historical turning points that led to Complexity. Second, the paper discusses philosophical-scientific approaches to the emergence as one of the most critical features of complex systems. The critical ideas behind attempts to understand the generators of complexity in nature are then presented, focusing on the living world. Finally, the review focuses on understanding the ecosystem- and organism-oriented perspectives of biocomplexity. We conclude that the genuine problem of the origin of complexity theory and biocomplexity will continue to inspire generations of researchers to search for new, more comprehensive mathematical and computational frameworks to explain biological hierarchies in order to further advance the scientific understanding of life.},
 authors = {['Srdjan Kesić']},
 journal = {Ecological Complexity},
 keywords = {['Cybernetics', 'General systems theory', 'Complexity', 'Modeling', 'Biocomplexity', 'Emergence', 'Autopoiesis']},
 title = {Complexity and biocomplexity: Overview of some historical aspects and philosophical basis},
 year = {2024}
}

@Filtered Article{cb8b8d9f-34d7-469d-bfb8-3bccb3d6a6ff,
 abstract = {Fundamental observations and principles derived from traditional physiological studies of multisensory integration have been difficult to reconcile with computational and psychophysical studies that share the foundation of probabilistic (Bayesian) inference. We review recent work on multisensory integration, focusing on experiments that bridge single-cell electrophysiology, psychophysics, and computational principles. These studies show that multisensory (visual–vestibular) neurons can account for near-optimal cue integration during the perception of self-motion. Unlike the nonlinear (superadditive) interactions emphasized in some previous studies, visual–vestibular neurons accomplish near-optimal cue integration through subadditive linear summation of their inputs, consistent with recent computational theories. Important issues remain to be resolved, including the observation that variations in cue reliability appear to change the weights that neurons apply to their different sensory inputs.},
 authors = {['Dora E Angelaki', 'Yong Gu', 'Gregory C DeAngelis']},
 journal = {Current Opinion in Neurobiology},
 title = {Multisensory integration: psychophysics, neurophysiology, and computation},
 year = {2009}
}

@Filtered Article{cc30bf02-23f4-47b6-aee3-0f5dd081e6a8,
 abstract = {This paper introduces the emergent computational paradigm, discusses its applicability and potential in ecosystem management, and reviews the literature. Emergent computation is significantly different from the “classic” computational paradigm, where control is top-down and centralized. In emergent systems, overall system dynamics emerge from the local interactions of independent agents. In such systems, overall global control is minimized or eliminated altogether. Applications in ecosystem management include use of “artificial ecosystems” as surrogate experimental systems, and genetics-based machine learning systems to evolve management rule-sets for complex domains. Cellular automata, neural networks, genetic algorithms and classifier systems are discussed as examples of the emergent approach. Finally, an in-depth literature review of artificial ecosystems is provided.},
 authors = {['Richard L. Olson', 'Ronaldo A. Sequeira']},
 journal = {Computers and Electronics in Agriculture},
 keywords = {['Emergent computation', 'Ecosystem dynamics', 'Ecosystem management']},
 title = {Emergent computation and the modeling and management of ecological systems},
 year = {1995}
}

@Filtered Article{cc366201-e66c-45c8-9bdb-fc34d60c014e,
 abstract = {The purpose of the current study was to (i) investigate the malleability of children’s spatial thinking, and (ii) the extent to which training-related gains in spatial thinking generalize to mathematics performance. Sixty-one 6- to 8-year-olds were randomly assigned to either computerized mental rotation training or literacy training. Training took place on iPad devices over a 6-week period as part of regular classroom activity. Results revealed that in comparison to the control group, children who received spatial training demonstrated significant gains on two measures of mental rotation and marginally significant improvements on an untrained mental transformation task; a finding that suggests that training may have had a general effect on children’s spatial ability. However, contrary to theoretical claims and prior empirical findings, there was no evidence that spatial training transferred to mathematics performance.},
 authors = {['Zachary Hawes', 'Joan Moss', 'Beverly Caswell', 'Daniel Poliszczuk']},
 journal = {Trends in Neuroscience and Education},
 keywords = {['Spatial thinking', 'Mental rotation', 'Spatial training', 'Computerized cognitive training', 'Mathematics education', 'STEM']},
 title = {Effects of mental rotation training on children’s spatial and mathematics performance: A randomized controlled study},
 year = {2015}
}

@Filtered Article{cc371134-799c-4ac7-8188-76b733baa1be,
 abstract = {Design creativity techniques encourage divergent thinking. But how well do the existing generative design techniques support this requirement? How can these general techniques be augmented for supporting design exploration and creativity? This paper investigates these questions through a review of five different generative design techniques used in architectural design that includes cellular automata, genetic algorithms, L-systems, shape grammars, and swarm intelligence. Based on the literature on design cognition and the recent theoretical works on digital design thinking, this paper proposes the need for an integrated generative design framework to enhance design exploration support for human designers. Potential challenges and strategies towards developing such an integrated framework are discussed.},
 authors = {['Vishal Singh', 'Ning Gu']},
 journal = {Design Studies},
 keywords = {['generative design', 'architectural design', 'digital design', 'design cognition', 'reflective practise']},
 title = {Towards an integrated generative design framework},
 year = {2012}
}

@Filtered Article{cc3a2bb3-67af-404e-b596-5494360d44c2,
 abstract = {Tourism is a very important sector and has a major influence on development and national income. Moreover, Indonesia has thousands of tourist destinations that are very beautiful and interesting to visit, both for Indonesians and foreigners. It's just that, there are still many local tours, such as tourist villages, which are still not well known by most people. In fact, there are many cultures, customs, places of recreation, or characteristics of an area that need to be seen and introduced to outsiders, even to Indonesians themselves. Therefore, this study aims to explain the problems that occur in the field of tourism, as well as provide solutions in the form of tourism applications that aim to help promote local Indonesian tourism, as well as make it easy for travel enthusiasts to organize their travel plans. The process of making this travel application is also carried out through various research and interviews with potential users and IT people in order to produce an attractive and effective application. This study uses the design thinking method. Researchers collected data sources from literature studies and surveys through questionnaires, where the results of the data obtained from the questionnaires were numerical or quantitative. The aim is to determine the level of public interest in tourism, as well as determine the level of potential users of this tourism application. That way, the goals of this application will be achieved and effective in helping solve tourism problems.},
 authors = {['Daniella Oktalina Manalu', 'Yudhistya Ayu Kusumawati', 'Cuk Tho']},
 journal = {Procedia Computer Science},
 keywords = {['mobile application', 'tourism', 'local tourism']},
 title = {Developing Nusantara Mobile Application to Support Local Tourism in Indonesia},
 year = {2023}
}

@Filtered Article{cc59d9e8-4dc6-4200-9adb-5012d89990ff,
 abstract = {Multi-scale dense connection has been widely used in the biomedical image community to enhance the segmentation performance. In this way, features from all or most scales are aggregated or iteratively fused. However, by analyzing the details, we discover that some connections involving distant scales may not contribute to, or even harm, the performance, while they always introduce a noticeable increase in computational cost. In this paper, we propose constrained multi-scale dense connections (CMDC) for biomedical image segmentation. In contrast to current general lightweight approaches, we first introduce two methods, a naive method and a network architecture search (NAS)-based method, to remove redundant connections and verify the optimal connection configuration, thereby improving overall efficiency and accuracy. The results demonstrate that the two approaches obtain a similar optimal configuration in which most features at the adjacent scales are connected. Then, we applied the optimal configuration to various backbone networks to build constrained multi-scale dense networks (CMD-Net). Experimental results evaluated on eight image segmentation datasets covering biomedical images and natural images demonstrate the effectiveness of CMD-Net across a variety of backbone networks (FCN, U-Net, DeepLabV3, SegNet, FCNsa, ConvUNeXt) with a much lower increase in computational cost. Furthermore, CMD-Net achieves state-of-the-art performance on four publicly available datasets. We believe that the CMDC method can offer valuable insight for ways to engage in dense connectivity at multiple scales within communities. The source code has been made available at https://github.com/JerRuy/CMD-Net.},
 authors = {['Jiawei Zhang', 'Yanchun Zhang', 'Hailong Qiu', 'Tianchen Wang', 'Xiaomeng Li', 'Shanfeng Zhu', 'Meiping Huang', 'Jian Zhuang', 'Yiyu Shi', 'Xiaowei Xu']},
 journal = {Pattern Recognition},
 keywords = {['Multi-scale dense connections', 'Image segmentation', 'Network architecture search', 'Feature fusion']},
 title = {Constrained multi-scale dense connections for biomedical image segmentation},
 year = {2025}
}

@Filtered Article{ccc3206e-8652-489c-90ce-9677af65f445,
 abstract = {In a historical account recently published in this journal Dhein argues that the current debate whether insects like bees and ants use cognitive maps (centralized map hypothesis) or other means of navigation (decentralized network hypothesis) largely reflects the classical debate between American experimental psychologists à la Tolman and German ethologists à la Lorenz, respectively. In this dichotomy we, i.e., the proponents of the network hypothesis, are inappropriately placed on the Lorenzian line. In particular, we argue that in contrast to Dhein's claim our concepts are not based on merely instinctive or peripheral modes of information processing. In general, on the one side our approaches have largely been motivated by the early biocybernetics way of thinking. On the other side they are deeply rooted in studies on the insect's behavioral ecology, i.e., in the ecological setting within which the navigational strategies have evolved and within which the animal now operates. Following such a bottom-up approach we are not “anti-cognitive map researchers” but argue that the results we have obtained in ants, and also the results of some decisive experiments in bees, can be explained and simulated without the need of invoking metric maps.},
 authors = {['Rüdiger Wehner', 'Thierry Hoinville', 'Holk Cruse']},
 journal = {Studies in History and Philosophy of Science},
 keywords = {['Insect navigation', 'Cognitive map', 'Neural network model', 'Path integration', 'Landmark guidance', 'Ants', 'Bees']},
 title = {On the ‘cognitive map debate’ in insect navigation},
 year = {2023}
}

@Filtered Article{cd24804d-fa75-4f27-98a8-ded0a16bf2c0,
 abstract = {It has become increasingly apparent that nonlinearity and complexity are the norm in human physiological systems, the relevance of which is informing an enhanced understanding of basic pathological processes such as inflammation, the host response to severe trauma, and critical illness. This article will explore how an understanding of nonlinear systems and complexity might inform the study of the pathophysiology of deaths of medicolegal interest, and how ‘complexity thinking’ might usefully be incorporated into modern forensic medicine and forensic pathology research, education and practice.},
 authors = {['Richard Martin Jones']},
 journal = {Forensic Science International},
 keywords = {['Complexity', 'Chaos', 'Nonlinear', 'Pathophysiology', 'Forensic pathology', 'Forensic medicine']},
 title = {Complexity and forensic pathology},
 year = {2015}
}

@Filtered Article{cd56dd91-0708-413d-adaa-d6b48669b836,
 abstract = {Is consciousness based in prefrontal circuits involved in cognitive processes like thought, reasoning, and memory or is it based in sensory areas in the back of the neocortex? The no-report paradigm has been crucial to this debate because it aims to separate the neural basis of the cognitive processes underlying post-perceptual decision and report from the neural basis of conscious perception itself. However, the no-report paradigm is problematic because, even in the absence of report, subjects might engage in post-perceptual cognitive processing. Therefore, to isolate the neural basis of consciousness, a no-cognition paradigm is needed. Here, I describe a no-cognition approach to binocular rivalry and outline how this approach can help to resolve debates about the neural basis of consciousness.},
 authors = {['Ned Block']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['consciousness', 'perception', 'rivalry', 'frontal', 'global workspace', 'higher order']},
 title = {What Is Wrong with the No-Report Paradigm and How to Fix It},
 year = {2019}
}

@Filtered Article{cd706419-9a17-432d-8e07-4014a4d95a35,
 abstract = {As humans, we rely on intuitive reasoning for most of our decisions. However, when there is a novel or atypical decision to be made, we must rely on a slower and more deliberative thought process—analytical reasoning. As we gain experience with these novel or atypical decisions, our reasoning shifts from analytical to intuitive, which parallels a reduction in the need for cognitive control. Here, we sought to confirm this claim by employing electroencephalographic (EEG) measures of cognitive control as participants performed a simple perceptual decision-making task. Specifically, we had participants categorize “blobs” into families based on their visual attributes so we could examine how their reasoning changed with learning. In a key manipulation, halfway through the experiment we introduced novel blob families to categorize, thus temporarily increasing the need for analytical reasoning (i.e., cognitive control). Congruent with past research, we focused our EEG analyses on frontal theta activity as it has been linked to cognitive control and analytical thinking. As hypothesized, we found a transition from analytical to intuitive decision-making systems with learning as indexed by a decrease in frontal theta power. Further, when the novel blobs were introduced at the midpoint of the experiment, we found that decisions about these stimuli recruited analytical reasoning as indicated by increased theta power in comparison to decisions about well-practiced stimuli. We propose our findings to reflect prediction errors to decision demands—a monitoring process that determines whether our expectations of demands are met. Shifting from analytical to intuitive reasoning thus reflects the stabilization of our expectations of decision demands, which can be violated with unexpected demands when encountering novel stimuli.},
 authors = {['Chad C. Williams', 'Cameron D. Hassall', 'Olave E. Krigolson']},
 journal = {Cortex},
 keywords = {['Reasoning', 'Prediction errors', 'Cognitive control', 'Theta', 'EEG']},
 title = {Stabilizing expectations when shifting from analytical to intuitive reasoning: The role of prediction errors in reasoning},
 year = {2023}
}

@Filtered Article{cd9f617e-568f-43dd-ac61-da8fa7b5ee53,
 abstract = {Cognitive hierarchy theory, a collection of structural models of non-equilibrium thinking, in which players’ best responses rely on heterogeneous beliefs on others’ strategies including naïve behavior, proved powerful in explaining observations from a wide range of games. We propose an inclusive cognitive hierarchy model, in which players do not rule out the possibility of facing opponents at their own thinking level. Our theoretical results show that inclusiveness is crucial for asymptotic properties of deviations from equilibrium behavior in expansive games. We show that the limiting behaviors are categorized in three distinct types: naïve, Savage rational with inconsistent beliefs, and sophisticated. We test the model in a laboratory experiment of collective decision-making. The data suggests that inclusiveness is indispensable with regard to explanatory power of the models of hierarchical thinking.},
 authors = {['Yukio Koriyama', 'Ali I. Ozkes']},
 journal = {Journal of Economic Behavior & Organization},
 keywords = {['Cognitive hierarchy', 'Collective decision-making', 'Level- model', 'Strategic thinking']},
 title = {Inclusive cognitive hierarchy},
 year = {2021}
}

@Filtered Article{cdc3eecc-8c7e-42d4-a84a-a8175fa64b09,
 abstract = {Economic dispatch (ED) problem is a nonlinear and non-smooth optimization problem when valve-point effects, multi-fuel effects and prohibited operating zones (POZs) have been considered. This paper presents an efficient evolutionary method for a constrained ED problem using the new adaptive particle swarm optimization (NAPSO) algorithm. The original PSO has difficulties in premature convergence, performance and the diversity loss in optimization process as well as appropriate tuning of its parameters. In the proposed algorithm, to improve the global searching capability and prevent the convergence to local minima, a new mutation is integrated with adaptive particle swarm optimization (APSO). In APSO, the inertia weight is tuned by using fuzzy IF/THEN rules and the cognitive and the social parameters are self-adaptively adjusted. The proposed NAPSO algorithm is validated on test systems consisting of 6, 10, 15, 40 and 80 generators with the objective functions possessing prohibited zones, multi-fuel effects and valve-point loading effects. The research results reveal the effectiveness and applicability of the proposed algorithm to the practical ED problem.},
 authors = {['Taher Niknam', 'Hasan Doagou Mojarrad', 'Hamed Zeinoddini Meymand']},
 journal = {Applied Soft Computing},
 keywords = {['Economic dispatch', 'New adaptive particle swarm optimization (NAPSO)', 'Mutation operator', 'Multi-fuel effects', 'Self-adaptive parameter control']},
 title = {Non-smooth economic dispatch computation by fuzzy and self adaptive particle swarm optimization},
 year = {2011}
}

@Filtered Article{ce21632e-62e0-4c4c-a235-dc1ff1ce72aa,
 abstract = {The study presents ten different exercises covering various computational tools. These exercises are practical applications presented to improve the understanding and skills of students in important concepts of chemical-aided process synthesis. A few exercises aim to build a foundation in computational techniques for chemical engineering undergraduates. The exercises are based on a spreadsheet that covers the design of regression analysis to find the optimum Antoine constants, array calculation for multicomponent distillation material balance, and the generation of a Gantt chart to plan and study the activities of batch processes. The other exercises included an introduction to process simulation, simulation, and reactor rating, and a simulation of multicomponent shortcut distillation. These exercises provide students with hands-on experience in utilizing process simulation software essential for analysing and optimizing chemical processes in real-world scenarios. The exercises also included the design of a heat exchanger network and solving a linear programming problem. An anonymous survey was collected from the cohort that had undergone the exercises, and the practical grades were compared with the batch that did not study the proposed exercises. Additionally, student feedback on practical exercises was collected. Based on the experience of the course coordinator and the collected feedback from participants, it was clear that the exercises helped students to inculcate critical thinking and self-learning abilities. An article essentially sheds light on the computer-aided practical exercises that enable chemical engineering graduates to engage in lifelong learning.},
 authors = {['Krunal J. Suthar', 'Aesha Mehta', 'Swapna Rekha Panda', 'Hitesh Panchal', 'Rakesh Sinha']},
 journal = {Education for Chemical Engineers},
 keywords = {['computational tools', 'lifelong learning', 'laboratory learning', 'process synthesis']},
 title = {Practical exercises of computer-aided process synthesis for chemical engineering undergraduates},
 year = {2024}
}

@Filtered Article{ceb5c280-9afb-4ae3-a37a-2f72f2d322e2,
 abstract = {Large-scale data clustering needs an approximate approach for improving computation efficiency and data scalability. In this paper, we propose a novel method for ensemble clustering of large-scale datasets that uses the Random Sample Partition and Clustering Approximation (RSPCA) to tackle the problems of big data computing in cluster analysis. In the RSPCA computing framework, a big dataset is first partitioned into a set of disjoint random samples, called RSP data blocks that remain distributions consistent with that of the original big dataset. In ensemble clustering, a few RSP data blocks are randomly selected, and a clustering operation is performed independently on each data block to generate the clustering result of the data block. All clustering results of selected data blocks are aggregated to the ensemble result as an approximate result of the entire big dataset. To improve the robustness of the ensemble result, the ensemble clustering process can be conducted incrementally using multiple batches of selected RSP data blocks. To improve computation efficiency, we use the I-niceDP algorithm to automatically find the number of clusters in RSP data blocks and the k-means algorithm to determine more accurate cluster centroids in RSP data blocks as inputs to the ensemble process. Spectral and correlation clustering methods are used as the consensus functions to handle irregular clusters. Comprehensive experiment results on both real and synthetic datasets demonstrate that the ensemble of clustering results on a few RSP data blocks is sufficient for a good global discovery of the entire big dataset, and the new approach is computationally efficient and scalable to big data.},
 authors = {['Mohammad Sultan Mahmud', 'Hua Zheng', 'Diego Garcia-Gil', 'Salvador García', 'Joshua Zhexue Huang']},
 journal = {Pattern Recognition},
 keywords = {['Clustering approximation', 'Ensemble clustering', 'Incremental clustering', 'Ensemble learning']},
 title = {RSPCA: Random Sample Partition and Clustering Approximation for ensemble learning of big data},
 year = {2025}
}

@Filtered Article{cec464b1-eb21-433e-b91e-0f539614d44d,
 abstract = {This work explores the architecture of a context-dependent probabilistic model. We identify opportunities for providing reminders to operators in their environment as a means to address information overload. Hence, there is a need to represent a state of knowledge and help them stay vigilant during their jobs. Along with the architectural improvements, which further specialize information flows and develop a data-driven approach, continual learning techniques covered events in a probabilistic graphical model called Context-Dependent Recommendation Systems (CD-RS). We demonstrated, as a result, the use of statistical thinking and Design of Experiments (DoE), which are most clear in conducting a suitable experiment. Moreover, the validation of the model and experiments of the novel architecture based on the collected data from a real case study demonstrates the value of the proposed methods.},
 authors = {['Márcio J. {da Silva}', 'Gustavo Künzel', 'Carlos E. Pereira']},
 journal = {IFAC-PapersOnLine},
 keywords = {['Data Mining', 'Predictive Situation', 'Context Testing', 'Industrial Alarm System', 'Recommendation Systems']},
 title = {A Predictive, Context-Dependent Stochastic Model for Engineering Applications},
 year = {2022}
}

@Filtered Article{cf198617-21a6-4288-b93f-affac2cc020c,
 abstract = {The optimal stenting technique for coronary artery bifurcations is still debated. With additional advances computational simulations can soon be used to compare stent designs or strategies based on verified structural and hemodynamics results in order to identify the optimal solution for each individual’s anatomy. In this study, patient-specific simulations of stent deployment were performed for 2 cases to replicate the complete procedure conducted by interventional cardiologists. Subsequent computational fluid dynamics (CFD) analyses were conducted to quantify hemodynamic quantities linked to restenosis. Patient-specific pre-operative models of coronary bifurcations were reconstructed from CT angiography and optical coherence tomography (OCT). Plaque location and composition were estimated from OCT and assigned to models, and structural simulations were performed in Abaqus. Artery geometries after virtual stent expansion of Xience Prime or Nobori stents created in SolidWorks were compared to post-operative geometry from OCT and CT before being extracted and used for CFD simulations in SimVascular. Inflow boundary conditions based on body surface area, and downstream vascular resistances and capacitances were applied at branches to mimic physiology. Artery geometries obtained after virtual expansion were in good agreement with those reconstructed from patient images. Quantitative comparison of the distance between reconstructed and post-stent geometries revealed a maximum difference in area of 20.4%. Adverse indices of wall shear stress were more pronounced for thicker Nobori stents in both patients. These findings verify structural analyses of stent expansion, introduce a workflow to combine software packages for solid and fluid mechanics analysis, and underscore important stent design features from prior idealized studies. The proposed approach may ultimately be useful in determining an optimal choice of stent and position for each patient.},
 authors = {['Claudio Chiastra', 'Wei Wu', 'Benjamin Dickerhoff', 'Ali Aleiou', 'Gabriele Dubini', 'Hiromasa Otake', 'Francesco Migliavacca', 'John F. LaDisa']},
 journal = {Journal of Biomechanics},
 keywords = {['Mathematical model', 'Finite element analysis', 'Computational fluid dynamics', 'Coronary bifurcation', 'Stent']},
 title = {Computational replication of the patient-specific stenting procedure for coronary artery bifurcations: From OCT and CT imaging to structural and hemodynamics analyses},
 year = {2016}
}

@Filtered Article{cf51e07f-18c6-4cf2-a04e-b929fc199ba5,
 authors = {['Albert Y Zomaya', 'Fikret Ercal', 'El-ghazali Talbi']},
 journal = {Parallel Computing},
 title = {Parallel and nature-inspired computational paradigms and applications},
 year = {2004}
}

@Filtered Article{cf8c0cf4-1f52-4961-9522-6dcf58b48ea9,
 abstract = {Some of the major computer vision techniques make use of neural nets. In this paper we present a novel model based on neural networks denominated lateral interaction in accumulative computation (LIAC). This model is based on a series of neuronal models in one layer, namely the local accumulative computation model, the double time scale model and the recurrent lateral interaction model. The LIAC model usefulness in the general task of motion detection may be appreciated by means of some significant examples of object detection in indefinite sequences of synthetic and real images.},
 authors = {['Antonio Fernández-Caballero', 'José Mira Mira', 'Ana E. Delgado', 'Miguel A. {Fernández Graciani}']},
 journal = {Neurocomputing},
 keywords = {['Accumulative computation', 'Lateral interaction', 'Double time scale', 'Motion detection', 'Image sequences']},
 title = {Lateral interaction in accumulative computation: a model for motion detection},
 year = {2003}
}

@Filtered Article{cfb291d3-189a-405e-a210-9e7d8459ed7c,
 abstract = {The business resilience system (BRS) with its risk atom and processing data point is based on fuzzy logic and cloud computation in real time. Its purpose and objectives define a clear set of expectations for organizations and enterprises, so their network system and supply chain are totally resilient and protected against cyberattacks, man-made threats, and natural disasters. These enterprises include financial, organizational, homeland security, and supply chain operations with multipoint manufacturing across the world. Market share and marketing advantages are expected to result from the implementation of the system. The collected information and defined objectives provide the basis to monitor and analyze the data through cloud computation and will guarantee the success of their survivability against any unexpected threats. Putting this kind of operation in place allows the executive and stakeholders within those organizations and enterprises to make the right decision when encountering threats that interrupt their normal day-to-day operations, as well as, in cases such as defense and homeland security, to predict the next move of an adversary. Given the fact that the BRS, as part of its functionality, processes the incoming data and information if not real time, then near real time with the help of superartificial intelligence in place, this gives the stakeholder an edge against and threats as well as predicting issues with operational intelligence. Artificial intelligence (AI) is one of those technologies that seem to be expanding in every direction. This technology will take center stage at Think 2018. Resilience thinking is inevitably systems thinking, at least as much as sustainable development is. In fact, “when considering systems of humans and nature (social-ecological systems), it is important to consider the system as a whole.” The term “resilience” originated in the 1970s in the field of ecology from the research of C.S. Holling, who defined resilience as “a measure of the persistence of systems and of their ability to absorb change and disturbance and still maintain the same relationships between populations or state variables.” In short, resilience is best defined as “the ability of a system to absorb disturbances and still retain its basic function and structure.” In this chapter, we explain the BRS and how it works. Please note that the with minor editing and manipulation, the materials presented in this chapter have been borrowed from the book published from Zohuri and Moghaddam10 with permission from both authors and publisher as well.},
 authors = {['Bahman Zohuri', 'Farhang Mossavar-Rahmani', 'Farahnaz Behgounia']},
 journal = {Academic Press},
 keywords = {['Artificial intelligence', 'Data analysis and information', 'Market and market share', 'Predictive analytics', 'Super artificial intelligence']},
 title = {Chapter 2 - A general approach to business resilience system (BRS)},
 year = {2022}
}

@Filtered Article{d0066219-715d-4f4b-bb9b-0f6dbb78fd30,
 abstract = {Landscape approaches are integrated place-based approaches and provide cross-sectoral opportunities to facilitate sustainability transformations. The COVID-19 outbreak has profound ramifications for multiple dimensions of landscapes, ranging from mobility and lifestyle to value to environment and society. Therefore, integrated approaches to “health” have been more vigorously promoted in the policy arena dealing with human–nature interactions. The ecosystem principles of the Convention on Biological Diversity, which resonate with landscape approaches, are generally aligned with integrated approaches to health. However, commonalities and distinctions between these integrated approaches in both political and scientific domains have not been clarified. Drawing on a narrative review of the literature on “One Health,” “Ecohealth,” and “Planetary Health” as major health-oriented approaches in comparison with landscape approaches, the aspects of landscape approaches to be complemented in addressing health-related challenges were examined in this study. In addition to the review on the intellectual roots and evolutionary pathways, a comparative analysis of these relevant approaches was conducted in terms of three realms including theoretical assumptions, knowledge bases, and research paradigms. The results of the comparative review show that all approaches share systems thinking, interdisciplinarity, cross-sectoral collaboration, and holistic paradigm but differ with respect to their focused management problems, disciplines, and sectors as well as ontological and epistemological underpinnings. Pointing to the recent theoretical and methodological development in integrating health in placemaking, the results of this study suggest that pragmatic landscape approaches could be strengthened by using health-related research paradigms to achieve better constructivism–positivism meeting grounds regarding health–landscape intersections.},
 authors = {['Maiko Nishi', 'Shizuka Hashimoto']},
 journal = {Environmental Science & Policy},
 keywords = {['Landscape approaches', 'One Health', 'Ecohealth', 'Planetary Health', 'Social-ecological systems', 'Sustainability transformation']},
 title = {Health and landscape approaches: A comparative review of integrated approaches to health and landscape management},
 year = {2022}
}

@Filtered Article{d008b746-65af-4972-afa8-27520ea152d0,
 abstract = {Discovering the processes and types of knowledge organization which are involved in the creative process is a challenge up to this date. Human creativity is usually measured by psychological tests, such as the Remote Associates Test (RAT). In this paper, an approach based on a specific type of knowledge organization and processes which enables automatic solving of RAT queries is implemented (comRAT) as a part of a more general cognitive theoretical framework for creative problem-solving (CreaCogs). This aims to study: (a) whether a convergence process can be used to solve such queries and (b) if frequency of appearance of the test items in language data may influence knowledge association or discovery in solving such problems. The comRAT uses a knowledge base of language data extracted from the Corpus of Contemporary American English. The results obtained are compared to results obtained in empirical tests with humans. In order to explain why some answers might be preferred over others, frequencies of appearance of the queries and solutions are analyzed. The difficulty encountered by humans when solving RAT queries is expressed in response times and percentage of participants solving the query, and a significant moderate correlation between human data on query difficulty and the data provided by this approach is obtained.},
 authors = {['Ana-Maria Olteţeanu', 'Zoe Falomir']},
 journal = {Pattern Recognition Letters},
 keywords = {['Computational creativity', 'Remote Associates Test', 'Cognitive systems', 'Knowledge base', 'Language corpus', 'Cognitive modeling']},
 title = {comRAT-C: A computational compound Remote Associates Test solver based on language data and its comparison to human performance},
 year = {2015}
}

@Filtered Article{d03d7328-f51b-4849-aa91-eeea30435dd7,
 abstract = {A quantity value, such as 5 kg, consists of a number and a reference (often an International System of Units (SI) unit) that together express the magnitude of a quantity. Many software libraries, packages, and ontologies that implement “quantities and units” functions are available. Although all of them begin with SI and associated practices, they differ in how they address issues such as ad hoc counting units, ratios of two quantities of the same kind, and uncertainty. This short article describes an architecture that addresses the complete set of functions in a simple and consistent fashion. Its goal is to encourage more convergent thinking about the functions and the underlying concepts so that the many disparate implementations, present and future, will become more consistent with one another.},
 authors = {['David Flater']},
 journal = {Computer Standards & Interfaces},
 keywords = {['SI', 'Quantity', 'Unit', 'Uncertainty', 'Value', 'Unit 1']},
 title = {Architecture for software-assisted quantity calculus},
 year = {2018}
}

@Filtered Article{d05d9e7d-d820-4c6c-84cd-351d78fb8403,
 abstract = {This paper reviews the state-of-the-art methodologies for automating design with intelligent human–machine integration from the perspectives of ontology and epistemology. The human–machine integrated automating design paradigm is reviewed systematically based on a proposed prototype of human–machine integrated design, from the aspects of ontology-based knowledge management with local-to-global ontology transitions, and epistemology-based upward-spiral cognitive process of coupled design ideation. Particularly, imaginal thinking frame is proposed as the foundation of intelligent human–machine interaction that puts human and machine on an equal platform. Further, this paper presents implementations and applications of the automating design paradigm and concludes with the identification of future trend.},
 authors = {['Yue H. Yin', 'Andrew Y.C. Nee', 'S.K. Ong', 'Jian Y. Zhu', 'Pei H. Gu', 'Lien J. Chen']},
 journal = {CIRP Annals},
 keywords = {['Design automation', 'Human–machine integration', 'Intelligent design', 'Imaginal thinking', 'Ontology']},
 title = {Automating design with intelligent human–machine integration},
 year = {2015}
}

@Filtered Article{d0966a2d-fe04-4a7d-a0e8-cf5b64909a8d,
 abstract = {Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students.
ABSTRACT
Undergraduate genetics courses have historically focused on simple genetic models, rather than taking a more multifactorial approach where students explore how traits are influenced by a combination of genes, the environment, and gene-by-environment interactions. While a focus on simple genetic models can provide straightforward examples to promote student learning, they do not match the current scientific understanding and can result in deterministic thinking among students. In addition, undergraduates are often interested in complex human traits that are influenced by the environment, and national curriculum standards include learning objectives that focus on multifactorial concepts. This research aims to discover to what extent multifactorial genetics is currently being assessed in undergraduate genetics courses. To address this, we analyzed over 1,000 assessment questions from a commonly used undergraduate genetics textbook; published concept assessments; and open-source, peer-reviewed curriculum materials. Our findings show that current genetics assessment questions overwhelmingly emphasize the impact of genes on phenotypes and that the effect of the environment is rarely addressed. These results indicate a need for the inclusion of more multifactorial genetics concepts, and we suggest ways to introduce them into undergraduate courses.},
 authors = {['Kelly M. Schmid', 'Dennis Lee', 'Monica Weindling', 'Awais Syed', 'Stephanie-Louise Yacoba Agyemang', 'Brian Donovan', 'Gregory Radick', 'Michelle K. Smith', 'L. Kate Wright']},
 journal = {Journal of Microbiology & Biology Education},
 keywords = {['assessment', 'curriculum', 'environment', 'genes', 'genetics', 'undergraduate']},
 title = {Mendelian or Multifactorial? Current Undergraduate Genetics Assessments Focus on Genes and Rarely Include the Environment},
 year = {2022}
}

@Filtered Article{d0aa7638-4342-4bf0-bcd7-45c409307992,
 abstract = {This chapter revisits the concept of disasters as failures introduced in Chapter 2. This entails approaches to consider the many factors and causes. System thinking is introduced as a means of preventing or reducing the damage of disasters. Systematic approaches must make use of various tools, including regulatory measures; economic incentives; property rights; infrastructure installment; public education; for international projects, international inspections; and cooperation. Disaster prevention and mitigation must integrate planning and engineering approaches, especially thoughtful land use. This also requires an understanding of how people expect to meet basic and advanced needs, as exemplified by Maslow's hierarchy. Other tools include optimizing information technologies and interoperability. The good news is that with education and consideration of past disasters, the implementation of rules and regulations there can be a reduction of number and severity of disasters; e.g., decrease in oil tanker spills over the past half century.},
 authors = {['Daniel A. Vallero', 'Trevor M. Letcher']},
 journal = {Elsevier},
 keywords = {['Deconstructing disasters', 'Failure', 'Integrated pest management (IPM)', 'Land use', "Maslow's hierarchy of needs", 'Information technology', 'Interoperability', 'Systems thinking', 'Spills', 'Life-cycle analysis', 'Design for the environment (DfE)', 'Design for disassembly (DfD)', "Aesop's fables", 'Tragedy of the Commons', 'Triple bottom line', 'Categorical imperative']},
 title = {Chapter 20 - Future},
 year = {2024}
}

@Filtered Article{d0e5c1d5-5684-41b0-9cad-a8347df16c00,
 abstract = {This study explores the complex relationship between traffic noise and school children's cognition, acknowledging existing empirical inconsistencies and aiming to contribute to a richer understanding of this pivotal issue. Schools adjacent to noisy roads were selected, and outdoor noise levels were measured employing a Kimo dB300 sound level meter, focusing on noise level indices LAeq, L10, and L90. Subsequent calculations were performed to determine the noise pollution level (Lnp), noise climate (NC), and traffic noise index (TNI), revealing a severe noise exposure when compared to standard guidelines. A perception questionnaire for various noise and acoustic factors influencing cognition was developed, and 1524 student responses were collected. Data analysis incorporated Principal Component Analysis (PCA) and Exploratory Factor Analysis (EFA) for dimension reduction, revealing three latent factors labelled 'annoyance,' 'behaviour,' and 'cognition'. Further, Structural Equation Modeling (SEM) was utilized to explore multivariate relationships between variables and latent factors. Resultant path coefficients were obtained as 0.12, 0.98, and 0.10 for the impact of 'behaviour' and 'annoyance' on 'cognition' and the correlation between 'annoyance' and 'behaviour', respectively. Findings underscore a potent positive impact of annoyance, stemming from acute ambient noise exposure, on the deterioration of children's cognition. While suggesting that ambient noise may be correlated with adverse health impacts due to its influence on cognition, this study emphasizes the pressing necessity for noise mitigation in roadside schools and stringent enforcement of noise pollution guidelines in academic zones.},
 authors = {['Avnish Shukla', 'Bhaven N. Tandel']},
 journal = {Environmental Research},
 keywords = {['School children', 'Cognition', 'Traffic noise index (TNI)', 'Exploratory factor analysis (EFA)', 'Structural equation modeling (SEM)']},
 title = {Association of road traffic noise exposure and school childrens’ cognition: A structural equation model approach},
 year = {2024}
}

@Filtered Article{d1521a56-2a47-459a-bb28-7a2b347b78bf,
 abstract = {The history of perceptual control theory's growing influence in the field of Industrial-Organizational Psychology is described. This history began in the early 1980's and included mostly conceptual work that described how control theory concepts might be used to understand applied phenomena. Both conceptual and empirical work on control theory ideas continued throughout the 1990's despite a substantial backlash against the theory by prominent scholars in the field. However, it was conceptual and empirical work in the 21st century that defined its potential integrative value and its theoretical rigor. Moreover, research regarding self-efficacy demonstrated how informal theories of human behavior might be better understood from a control theory perspective. Much of the current work with perceptual control theory involves the construction and testing of computational models that represent the links among perceptual, learning, and thinking modes of self-regulation and control.},
 authors = {['Jeffrey B. Vancouver']},
 journal = {Academic Press},
 keywords = {['Control theory', 'Self-regulation', 'Self-efficacy', 'Computational modeling']},
 title = {Chapter 12 - Perceptions of control theory in industrial-organizational psychology: disturbances and counter-disturbances},
 year = {2020}
}

@Filtered Article{d1775fc7-42d9-46c8-b199-56fcf013f271,
 abstract = {The brain circuitry of saccadic eye movements, from brainstem to cortex, has been extensively studied during the last 30 years. The wealth of data gathered allowed the conception of numerous computational models. These models proposed descriptions of the putative mechanisms generating this data, and, in turn, made predictions and helped to plan new experiments. In this article, we review the computational models of the five main brain regions involved in saccade generation: reticular formation saccadic burst generators, superior colliculus, cerebellum, basal ganglia and premotor cortical areas. We present the various topics these models are concerned with: location of the feedback loop, multimodal saccades, long-term adaptation, on the fly trajectory correction, strategy and metrics selection, short-term spatial memory, transformations between retinocentric and craniocentric reference frames, sequence learning, to name the principle ones. Our objective is to provide a global view of the whole system. Indeed, narrowing too much the modelled areas while trying to explain too much data is a recurrent problem that should be avoided. Moreover, beyond the multiple research topics remaining to be solved locally, questions regarding the operation of the whole structure can now be addressed by building on the existing models.},
 authors = {['B. Girard', 'A. Berthoz']},
 journal = {Progress in Neurobiology},
 keywords = {['Saccade generation circuitry', 'Computational models', 'Brainstem', 'Superior colliculus', 'Cerebellum', 'Basal ganglia', 'Cortex']},
 title = {From brainstem to cortex: Computational models of saccade generation circuitry},
 year = {2005}
}

@Filtered Article{d19f2763-87e1-4883-ad2b-17ee9d6392c7,
 abstract = {In order to meet rapidly growing demand of liquid hydrogen in the future hydrogen industry and energy structure, the continuous catalytic ortho-para hydrogen conversion technology (CCOPHCT) has been once again proposed and has become an important choice to improve the hydrogen liquefaction units (HLUs). The origin, concept, and research progress of the CCOPHCT are systematically reviewed for the first time in this paper. However, the research depth and breadth of the CCOPHCT are insufficient to support its current application. To solve it, for the continuous catalytic ortho-para hydrogen conversion plate-fin heat exchanger (CCOPHC-PFHE) with better comprehensive performances, this paper comprehensively summarizes the research achievements in the related fields from the perspective of the unit analysis, including the ortho-para hydrogen conversion (OPHC), packing layer and plate-fin heat exchanger (PFHE), which to provide further thinking for the development of the CCOPHC-PFHE. Further, some suggestions for the CCOPHCT are proposed based on the existed research foundations, including preparing the effective ortho-para hydrogen conversion catalyst (OPHCC), developing the accurate OPHC dynamical model, revealing the coupling mechanism in the packing layer filled with the OPHCC and establishing an effective design method and standard of the CCOPHC-PFHE. In addition, considering the special conditions on the CCOPHC-PFHE, the importance of the experimental research is emphasized. And based on the established hydrogen experimental platform with the comprehensive supporting implementations, the experimental device of the CCOPHC-PFHE has been completed and a series of experimental tests are currently in progressing.},
 authors = {['Pan Xu', 'Jian Wen', 'Ke Li', 'Simin Wang', 'Yanzhong Li']},
 journal = {International Journal of Hydrogen Energy},
 keywords = {['Hydrogen liquefaction', 'Continuous catalytic ortho-para hydrogen conversion technology', 'Ortho-para hydrogen conversion catalyst', 'Packing layer', 'Plate-fin heat exchanger']},
 title = {Review of the continuous catalytic ortho-para hydrogen conversion technology for hydrogen liquefaction},
 year = {2024}
}

@Filtered Article{d28f7ee5-4bcf-4329-afd8-aed131d31f6a,
 abstract = {Generative linguistics is often claimed by Chomsky to have a 'Galilean style', which is intended to position linguistics as a science continuous with standard practise in the natural sciences. These claims, however, are more suggestive than explanatory. The paper will, first, explain just what a Galilean style is. It will then be argued that its application to two key notions in generative linguistics - the competence/performance distinction (with reference to centre-embedding) and the notion of computation - demands a departure from what we might expect of a Galilean style. In this sense, the epithet is misleading. It will also be shown, however, that the 'Galilean' label is appropriate once we factor in the difference between a science concerned with kinematics (the relations between objects in space and time) and one concerned with language.},
 authors = {['John Collins']},
 journal = {Language Sciences},
 keywords = {['Chomsky', 'Centre-embedding', 'Competence/performance', 'Computation', 'Galilean style', 'Galileo']},
 title = {Generative linguistics: ‘Galilean style’},
 year = {2023}
}

@Filtered Article{d2ab13d8-f014-45a9-8920-d09c0c681f2a,
 abstract = {The cognitive mechanisms underlying scientific thinking and discovery have been investigated using approaches from cognitive psychology, cognitive science, and artificial intelligence. In this article, six overlapping approaches are discussed. First, historical analyses and interviews have provided important information on the types of thinking involved in particular discoveries or used by individual scientists. Second, scientific reasoning has been thought of as a form of inductive thinking, and as a form of problem solving. Researchers using this approach have delineated some of the problem solving and inductive reasoning strategies used in science. Third, much research on errors in scientific reasoning, particularly on the topic of ‘confirmation bias’ has revealed some of the circumstances under which science can go awry. Fourth, many researchers have investigated how children's thinking is similar to, or different from, that of scientists. A fifth approach has been to investigate scientists reasoning live or ‘in vivo’ in their own labs. This work has shown how processes such as analogy, distributed cognition, and specific types of inductive and deductive reasoning strategies are used together by scientists. Finally, the incorporation of cognitive mechanisms into computer programs that make discoveries is seen as an important development in the cognitive psychology of scientific thinking.},
 authors = {['K. Dunbar']},
 journal = {Pergamon},
 title = {Scientific Reasoning and Discovery, Cognitive Psychology of},
 year = {2001}
}

@Filtered Article{d2db70cb-2dc8-4522-850c-8b56506db516,
 abstract = {This article evaluates the reliability, efficiency, and effectiveness of Linguistic Inquiry and Word Count (LIWC; Boyd et al., 2022) for the analysis of a white nationalist forum. This is important because LIWC has been the computational tool of choice for scores of studies generally and many examining extremist content in a forensic or security context. Our purpose, therefore, is to understand whether LIWC can be depended upon for large-scale analyses; we initially examine this here using a small sample of posts from a set of just eight users and manually checking the program's automated codings of a subset of categories. Our results show that the LIWC coding cannot be relied upon – precision falls to as low as 49.6 % and recall as low as 41.7 % for some categories. It would be possible to engage in considerable manual correction of these results, but this undermines its purported efficiency for large datasets.},
 authors = {['Madison Hunter', 'Tim Grant']},
 journal = {Applied Corpus Linguistics},
 keywords = {['LIWC', 'Reliability', 'Computerized text analysis', 'Forensic linguistics', 'Discourse analysis']},
 title = {Is LIWC reliable, efficient, and effective for the analysis of large online datasets in forensic and security contexts?},
 year = {2025}
}

@Filtered Article{d2e3a16c-d7ae-400a-85c9-45442dd3faaf,
 abstract = {The work presents two new numerical techniques devised for modeling propagating material failure, i.e. cracks in fracture mechanics or slip-lines in soil mechanics. The first one is termed crack-path-field technique and is conceived for the identification of the path of those cracks, or slip-lines, represented by strain-localization based solutions of the material failure problem. The second one is termed strain-injection, and consists of a procedure to insert, during specific stages of the simulation and in selected areas of the domain of analysis, goal oriented specific strain fields via mixed finite element formulations. In the approach, a first injection, of elemental constant strain modes (CSM) in quadrilaterals, is used, in combination of the crack-path-field technique, for obtaining reliable information that anticipates the position of the crack-path. Based on this information, in a subsequent stage, a discontinuous displacement mode (DDM) is efficiently injected, ensuring the required continuity of the crack-path across sides of contiguous elements. Combination of both techniques results in an efficient and robust procedure based on the staggered resolution of the crack-path-field and the mechanical failure problems. It provides the classical advantages of the “intra-elemental” methods for capturing complex propagating displacement discontinuities in coarse meshes, as E-FEM or X-FEM methods, with the non-code-invasive character of the crack-path-field technique. Numerical representative simulations of a wide range of benchmarks, in terms of the type of material and the failure problem, show the broad applicability, accuracy and robustness of the proposed methodology. The finite element code used for the simulations is open-source and available at http://www.cimne.com/compdesmat/.},
 authors = {['J. Oliver', 'I.F. Dias', 'A.E. Huespe']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Fracture', 'Computational material failure', 'Strong discontinuities', 'Crack-path field', 'Strain injection', 'Finite elements with embedded discontinuities']},
 title = {Crack-path field and strain-injection techniques in computational modeling of propagating material failure},
 year = {2014}
}

@Filtered Article{d2e8201d-a4d1-4a7a-8a52-a32d28a6c917,
 abstract = {Summary
Consideration of compound drivers and impacts are often missing from applications within the Disaster Risk Reduction (DRR) cycle, leading to poorer understanding of risk and benefits of actions. The need to include compound considerations is known, but lack of guidance is prohibiting practitioners from including these considerations. This article makes a step toward practitioner guidance by providing examples where consideration of compound drivers, hazards, and impacts may affect different application domains within disaster risk management. We discern five DRR categories and provide illustrative examples of studies that highlight the role of “compound thinking” in early warning, emergency response, infrastructure management, long-term planning, and capacity building. We conclude with a number of common elements that may contribute to the development of practical guidelines to develop appropriate applications for risk management.},
 authors = {['Bart J.J.M. {van den Hurk}', 'Christopher J. White', 'Alexandre M. Ramos', 'Philip J. Ward', 'Olivia Martius', 'Indiana Olbert', 'Kathryn Roscoe', 'Henrique M.D. Goulart', 'Jakob Zscheischler']},
 journal = {iScience},
 keywords = {['Earth sciences', 'Social sciences', 'Decision science']},
 title = {Consideration of compound drivers and impacts in the disaster risk reduction cycle},
 year = {2023}
}

@Filtered Article{d4ad82bd-fc73-4631-9c1e-534b93aae4ec,
 abstract = {One cognitive process that impacts dieters’ decision to indulge is the activation of compensatory beliefs. Compensatory beliefs (CBs) are convictions that the consequences of engaging in an indulgent behaviour (eating cake) can be neutralized by the effects of another behaviour (skipping dinner). Using experience sampling methodology, this study hypothesized that, in addition to the cognitive processes associated with restraint and disinhibition, compensatory thinking contributes to the prediction of caloric intake. Results indicated that higher scores on CB, CI and TFEQ-D predicted a greater number of portions eaten signifying that, along with disinhibition, compensatory thinking predicts caloric intake in dieters.},
 authors = {['Ilana Kronick', 'Randy P. Auerbach', 'Christine Stich', 'Bärbel Knäuper']},
 journal = {Appetite},
 keywords = {['Compensatory beliefs', 'Compensatory intentions', 'Restraint', 'Disinhibition', 'Caloric intake', 'Experience sampling methodology']},
 title = {Compensatory beliefs and intentions contribute to the prediction of caloric intake in dieters},
 year = {2011}
}

@Filtered Article{d4b651cc-316d-4e36-a814-df406b6da902,
 abstract = {Introduction:
Commuting is an integral part of modern life for many people, shaping daily routines and impacting overall well-being. With various transportation options, including driving, public transport, walking, and cycling, commuters encounter various experiences and challenges in their everyday journeys. Understanding how different modes of commuting affect stress levels is essential for improving public health and informing transportation planning. This study develops advanced machine-learning techniques to explore the connection between commuting methods and stress levels.
Methods:
This research examines how different commuting modes affect stress levels using machine learning methods. The study analyses data collected from 45 individuals who regularly commute to work, focusing on driving, cycling, and public transport modes. Non-invasive wearable sensors were utilised to gather electroencephalography (EEG), blood pressure (BP), and heart rate (HR) data for five consecutive days for each participant. Additionally, qualitative data was collected using the Positive and Negative Affect Schedule (PANAS) questionnaire to assess participants’ emotional responses before and after their commute. The research focuses on developing a machine learning-based model to predict the commute’s impact and monitor the stress level due to the commute mode. In research, objective and subjective factors shape the research process and outcomes. Understanding the interaction between these factors is essential for conducting thorough and reliable research that produces valid results. Our study utilises datasets incorporating qualitative and quantitative data from questionnaires and human bio-signals.
Results:
This research developed various machine learning algorithms to detect stress levels based on commuting mode. The results indicate that the Linear Discriminant Analysis technique achieved an accuracy of 88%, while Logistic Regression reached 90.66% accuracy. The Boosted Tree algorithm produced the best performance, with an accuracy of 91.11%. Furthermore, incorporating personalised parameters into the data improved the accuracy of these algorithms in detecting stress levels. Cross-validation was also utilised to mitigate the risk of overfitting, ensuring robust and reliable model performance.
Conclusion:
The findings reveal that human bio-signals tend to increase following commuting, irrespective of the mode, with driving identified as the most stressful option. Commuters using passive modes of transport experience elevated stress levels compared to those using active modes. This research underscores the importance of understanding the connection between commuting modes and stress, providing key insights into the potential health impacts of daily travel. The development of an intelligent model to predict stress levels based on commuting mode offers valuable contributions to public health and transportation planning, with the goal of enhancing well-being and improving commuters’ quality of life.},
 authors = {['Mhd Saeed Sharif', 'Madhav Raj Theeng Tamang', 'Cynthia Fu', 'Ahmed Ibrahim Alzahrani', 'Fahad Alblehai']},
 journal = {Journal of Transport & Health},
 keywords = {['Stress assessment', 'Blood pressure', 'Wearable sensors', 'Commuting', 'Intelligent transport system', 'Machine learning']},
 title = {Innovative computation to detect stress in working people based on mode of commute},
 year = {2025}
}

@Filtered Article{d4db3b14-cd1f-4c4e-898e-5f749c2b755e,
 abstract = {To foster the development of pedagogically potent and ethically sound AI-integrated learning landscapes, it is pivotal to critically explore the perceptions and experiences of the users immersed in these contexts. In this study, we perform a thorough qualitative content analysis across four key social media platforms. Our goal is to understand the user experience (UX) and views of early adopters of ChatGPT across different educational sectors. The results of our research show that ChatGPT is most commonly used in the domains of higher education, K-12 education, and practical skills training. In social media dialogues, the topics most frequently associated with ChatGPT are productivity, efficiency, and ethics. Early adopters' attitudes towards ChatGPT are multifaceted. On one hand, some users view it as a transformative tool capable of amplifying student self-efficacy and learning motivation. On the other hand, there is a degree of apprehension among concerned users. They worry about a potential overdependence on the AI system, which they fear might encourage superficial learning habits and erode students’ social and critical thinking skills. This dichotomy of opinions underscores the complexity of Human-AI Interaction in educational contexts. Our investigation adds depth to this ongoing discourse, providing crowd-sourced insights for educators and learners who are considering incorporating ChatGPT or similar generative AI tools into their pedagogical strategies.},
 authors = {['Reza {Hadi Mogavi}', 'Chao Deng', 'Justin {Juho Kim}', 'Pengyuan Zhou', 'Young {D. Kwon}', 'Ahmed {Hosny Saleh Metwally}', 'Ahmed Tlili', 'Simone Bassanelli', 'Antonio Bucchiarone', 'Sujit Gujar', 'Lennart E. Nacke', 'Pan Hui']},
 journal = {Computers in Human Behavior: Artificial Humans},
 keywords = {['Artificial intelligence (AI)', 'Generative AI', 'ChatGPT', 'Education', 'Human-computer interaction (HCI)', '', 'Early adopters', 'Social media', 'Qualitative research']},
 title = {ChatGPT in education: A blessing or a curse? A qualitative study exploring early adopters’ utilization and perceptions},
 year = {2024}
}

@Filtered Article{d4ddcbd2-8e6c-43a2-945c-c17b21e633db,
 abstract = {Electrochemical synthesis is well established for production of bulk commodities such as copper, aluminium, or ethylene oxide, but electrosynthesis could play an increasingly important role also in a broader range of organic and pharmaceutical syntheses. Electrochemical transformations linked to renewable electricity offer a low-carbon low-waste alternative to traditional chemical reactions (sustainability), although more work is needed to establish processes and reactor technology for easy implementation (practicality). Here, the application of interdigitated microband array electrodes (in conjunction with computational methods) is discussed/contrasted as a tool to (i) avoid the use of added supporting electrolyte, (ii) achieve anode–cathode process pairing, and (iii) allow very simple reactor technology to be introduced compatible with existing chemical reactionware.},
 authors = {['Tingran Liu', 'Taku Suzuki-Osborne', 'James E. Taylor', 'Frank Marken']},
 journal = {Current Opinion in Electrochemistry},
 title = {Interdigitated microband electrode arrays in paired organic electrosyntheses: Sustainability and practicality},
 year = {2025}
}

@Filtered Article{d50e0a00-8f9a-412a-a5e6-0387654d369b,
 abstract = {Atrial Flutter (AFL) termination by ablating the path responsible for the arrhythmia maintenance is an extended practice. However, the difficulty associated with the identification of the circuit in the case of atypical AFL motivates the development of diagnostic techniques. We propose body surface phase map analysis as a noninvasive tool to identify AFL circuits. Sixty seven lead body surface recordings were acquired in 9 patients during AFL (i.e. 3 typical, 6 atypical). Computed body surface phase maps from simulations of 5 reentrant behaviors in a realistic atrial structure were also used. Surface representation of the macro-reentrant activity was analyzed by tracking the singularity points (SPs) in surface phase maps obtained from band-pass filtered body surface potential maps. Spatial distribution of SPs showed significant differences between typical and atypical AFL. Whereas for typical AFL patients 70.78 ± 16.17% of the maps presented two SPs simultaneously in the areas defined around the midaxialliary lines, this condition was only satisfied in 5.15 ± 10.99% (p < 0.05) maps corresponding to atypical AFL patients. Simulations confirmed these results. Surface phase maps highlights the reentrant mechanism maintaining the arrhythmia and appear as a promising tool for the noninvasive characterization of the circuit maintaining AFL. The potential of the technique as a diagnosis tool needs to be evaluated in larger populations and, if it is confirmed, may help in planning ablation procedures.},
 authors = {['A. Liberos', 'M. Rodrigo', 'I. Hernandez-Romero', 'A. Quesada', 'F. Fernandez-Aviles', 'F. Atienza', 'A.M. Climent', 'M.S. Guillem']},
 journal = {Computers in Biology and Medicine},
 keywords = {['Atrial flutter', 'Phase map', 'Cardiac model', 'Body surface potential mapping']},
 title = {Phase singularity point tracking for the identification of typical and atypical flutter patients: A clinical-computational study},
 year = {2019}
}

@Filtered Article{d510785a-21c4-4b46-81d3-ddcfe28ca7b4,
 abstract = {Summary
Prior studies examining genomic variants suggest that some proteins contribute to both neurodevelopmental disorders (NDDs) and cancer. While there are several potential etiologies, here, we hypothesize that missense variation in proteins occurs in different clustering patterns, resulting in distinct phenotypic outcomes. This concept was first explored in 1D protein space and expanded using 3D protein structure models. Missense de novo variants were examined from 39,883 families with NDDs and missense somatic variants from 10,543 sequenced tumors covering five The Cancer Genome Atlas (TCGA) cancer types and two Catalog of Somatic Mutations in Cancer (COSMIC) pan-cancer aggregates of tissue types. We find 18 proteins with differential missense variation clustering in NDDs compared to cancers and 19 in cancers relative to NDDs. These proteins may be important for detailed assessments in thinking of future prognostic and therapeutic applications. We establish a framework for interpreting missense patterns in NDDs and cancer, using advances in 3D protein structure prediction.},
 authors = {['Jeffrey K. Ng', 'Yilin Chen', 'Titilope M. Akinwe', 'Hillary B. Heins', 'Elvisa Mehinovic', 'Yoonhoo Chang', 'David H. Gutmann', 'Christina A. Gurnett', 'Zachary L. Payne', 'Juana G. Manuel', 'Rachel Karchin', 'Tychele N. Turner']},
 journal = {Cell Genomics},
 keywords = {['neurodevelopmental disorders', 'cancer', 'clustering algorithm', '3D protein structure models', 'missense', '', 'somatic', 'variant interpretation', 'protein']},
 title = {Proteome-wide assessment of differential missense variant clustering in neurodevelopmental disorders and cancer},
 year = {2025}
}

@Filtered Article{d55f7284-e3d2-4330-ba6e-de440d2174d2,
 abstract = {We propose a new approach for multiverse analysis based on computational complexity, which leads to a new family of “computational” measure factors. By defining a cosmology as a space–time containing a vacuum with specified properties (for example small cosmological constant) together with rules for how time evolution will produce the vacuum, we can associate global time in a multiverse with clock time on a supercomputer which simulates it. We argue for a principle of “limited computational complexity” governing early universe dynamics as simulated by this supercomputer, which translates to a global measure for regulating the infinities of eternal inflation. The rules for time evolution can be thought of as a search algorithm, whose details should be constrained by a stronger principle of “minimal computational complexity”. Unlike previously studied global measures, ours avoids standard equilibrium considerations and the well-known problems of Boltzmann Brains and the youngness paradox. We also give various definitions of the computational complexity of a cosmology, and argue that there are only a few natural complexity classes.},
 authors = {['Frederik Denef', 'Michael R. Douglas', 'Brian Greene', 'Claire Zukowski']},
 journal = {Annals of Physics},
 keywords = {['Computational complexity', 'String theory', 'Multiverse', 'Measures']},
 title = {Computational complexity of the landscape II—Cosmological considerations},
 year = {2018}
}

@Filtered Article{d5657a4a-bee0-40de-b599-3096ad46b2a7,
 abstract = {Starting from the study of an architect who designs in the absence of sight, we question to what extent prevailing notions of design may be complemented with alternative articulations. In doing so, we point to the cognitivist understanding of human cognition underlying design researchers' strong attention to ‘visual thinking’, and contrast this with more situated understandings of human cognition. The ontological and epistemological differences between both raise questions about how design research is produced, and consequently what design can also be. By accounting for how a blind architect re-articulates prevailing notions of design, we invite researchers to keep the discussion open and call for an ontological and epistemological re-articulation in design research.},
 authors = {['Ann Heylighen', 'Greg Nijs']},
 journal = {Design Studies},
 keywords = {['design cognition', 'design research', 'epistemology']},
 title = {Designing in the absence of sight: Design cognition re-articulated},
 year = {2014}
}

@Filtered Article{d5660f8a-586c-4132-9fcf-3e45de11d206,
 abstract = {There is a growing body of road safety research that seeks to identify crash contributory factors beyond road users, their vehicles, and the immediate road environment. Although cyclist safety represents a critical research area, this ‘systems thinking’ approach has received less attention in bicycle crash analysis. This article presents the findings from a systematic literature review which aimed to synthesise the peer reviewed literature regarding bicycle crash contributory factors (defined as factors which play a contributory role in bicycle crashes, as opposed to risk factors which are factors which may increase the probability of crashes). Crash contributory factors were extracted from included articles and mapped onto a systems thinking framework comprising seven hierarchical road transport system levels. The findings show that a majority of the included studies identified contributory factors relating to the road environment, cycling infrastructure, and cyclist and driver behaviour. No studies identified contributory factors outside of cyclists and road users, bicycles and vehicles, and the road environment and few specifically examined causal relationships between contributory factors. It is concluded that there are gaps in the knowledge base regarding the broader transport system features that play a role in bicycle crashes and how contributory factors interact to create crashes. We argue that more expansive research into the systemic factors involved in bicycle crashes is required and that initial work should focus on the development of new data sources and analysis methods.},
 authors = {['Paul M. Salmon', 'Mitch Naughton', 'Adam Hulme', 'Scott McLean']},
 journal = {Safety Science},
 keywords = {['Cyclists', 'Cyclist crashes', 'Systems thinking', 'Road safety', 'Crash causation']},
 title = {Bicycle crash contributory factors: A systematic review},
 year = {2022}
}

@Filtered Article{d58a1ecd-9459-4234-a476-d0572954b231,
 abstract = {A computational model for the calculation of the bulk magnetic properties of rare-earth ferrimagnets and antiferromagnets was developed and justified theoretically in the framework of mean-field theory. To demonstrate its utility, the model was applied to calculate the anisotropic Heisenberg exchange constants of CeTe2 by fitting magnetization curves numerically, and to derive analytical expressions for the spontaneous magnetization as well as the Neél temperature by considering only the crystal-field (CF) ground-state doublet. It turns out that the temperature dependencies of the magnetization and the specific heat calculated with the formulas in absence of an external field are identical with the plots obtained directly with the full lowest CF J-multiplet, manifesting the strong role of the Kramers doublet in the magnetic process at low temperatures. Finally, the model was applied to investigate the effects of the quadrupolar and magneto-elastic (QM) interactions on the magnetic properties of the system.},
 authors = {['Z.-S. Liu', 'M. Diviš', 'V. Sechovský']},
 journal = {Physica B: Condensed Matter},
 keywords = {['Intermetallic compounds', 'Crystal field']},
 title = {A computational model for rare-earth ferrimagnets and antiferromagnets},
 year = {2005}
}

@Filtered Article{d5eeb503-8ffe-40f4-91c2-48ebe4afc884,
 abstract = {Introduction
Modern computers often use programs that incorporate a programming technique called Object Oriented Programming (OOP), allowing users to manipulate complex ‘computational objects’ such as menus, screen windows, etc with very little effort, say the click of a mouse. OOP deals with structures called objects and allows time and computational effort saving devices such as inheritance, polymorphism and encapsulation. We examine whether the brain itself may use OOP and if representation of objects suffers a breakdown in schizophrenia.
Review of literature
Previous models fail to provide a unifying explanation with a computational basis that could explain the psychopathology in schizophrenia. METHODS Using the object oriented programming language JavaTM we designed a system of self-objects named ‘hand’, ‘action monitor’ etc interacting with non-self objects ‘scissors’, ‘hammer’, ‘wall’, etc. In computational experiments, we allow the ‘action monitor’ to fail; the features of disparate objects are allowed to merge, some features of an object are allowed to be shared with other objects, etc.
Results
By transposing only a few lines of code, it is possible to duplicate various features of the psychopathology of schizophrenia.
Discussion
Our model can demonstrate overinclusion (overabstraction), concrete thinking (underabstraction), loss of ego boundaries (conjoining of disparate objects), delusions (misattribution of object function), lack of insight (poor monitoring of object activity) and passivity (loss of monitoring and misattribution of object activity).
Conclusion
The brain must use the OOP model in its computations. Failure of object representation and manipulation must lie at the core of the psychopathology of schizophrenia.},
 authors = {['C.P. Arun']},
 journal = {European Psychiatry},
 title = {P03-116 Damage to object oriented programming in the brain explains many of the psychopathological features of schizophrenia},
 year = {2009}
}

@Filtered Article{d5f30ef0-de97-40e5-99be-1aad030931e6,
 abstract = {Computer-aided drug design (CADD) comprises a broad range of theoretical and computational approaches that are part of modern drug discovery. CADD methods have made key contributions to the development of drugs that are in clinical use or in clinical trials. Such methods have emerged and evolved along with experimental approaches used in drug design. In this chapter we discuss the major CADD methods and examples of recent applications to drugs that have advanced in clinical trials or that have been approved for clinical use. We also comment on representative trends in current drug discovery that are shaping the development of novel methods, such as computer-aided drug repurposing. Similarly we present emerging concepts and technologies in molecular modeling and chemoinformatics. Furthermore, this chapter discusses the authors’ point of view of the challenges of traditional and novel CADD methods to increase their positive impact in drug discovery.},
 authors = {['Fernando D. Prieto-Martínez', 'Edgar López-López', 'K. {Eurídice Juárez-Mercado}', 'José L. Medina-Franco']},
 journal = {Academic Press},
 keywords = {['Artificial intelligence', 'Big data', 'Chemical space', 'Chemoinformatics', 'Deep learning', 'Molecular modeling', 'Polypharmacology', 'SmART', 'Target fishing', 'Virtual screening']},
 title = {Chapter 2 - Computational Drug Design Methods—Current and Future Perspectives},
 year = {2019}
}

@Filtered Article{d67ae57d-1c92-4801-ae46-354cfa04043d,
 abstract = {Based on multiple levels of granularity, the notion of sequential three-way granular computing focuses on a multiple stages of thinking, problem-solving, and information processing in threes. This paper interprets, represents, and implements sequential three-way granular computing by a framework of temporal-spatial multi-granularity learning, which is described with the temporality of data and the spatiality of parameters. In real-world decision-making, such a sequential approach is useful to make faster decisions for some objects with the lower cost of decision process and the acceptable accuracy when information is insufficient or unavailable. However, the cost of time-consuming computation for hierarchical multilevel granularity is our concern. To address this issue, we utilize a local strategy to accelerate a sequence of neighborhood-based granulation induced by Gaussian kernel function. Subsequently, local three-way decision rules are investigated based on the Bayesian minimum risk criterion. Moreover, by the construction of a novel local trisection model, we propose a local sequential approach of three-way granular computing under a temporal-spatial multilevel granular structure. Finally, a series of comparative experiments between global and local perspectives is carried out to verify the effectiveness of our proposed models.},
 authors = {['Xin Yang', 'Yingying Zhang', 'Hamido Fujita', 'Dun Liu', 'Tianrui Li']},
 journal = {Information Sciences},
 keywords = {['Three-way granular computing', 'Sequential three-way decision', 'Local neighborhood', 'Temporal-spatial', 'Multi-granularity']},
 title = {Local temporal-spatial multi-granularity learning for sequential three-way granular computing},
 year = {2020}
}

@Filtered Article{d6b558e5-53f9-40c8-80e8-cd6f18b02ae5,
 abstract = {Previous studies have reported mixed results regarding the relationship between students’ use of self-regulated learning (SRL) strategies and their performance in introductory programming courses. These studies were constrained by their reliance on self-report questionnaires as a means of collecting and analysing data. To address this limitation, this study aimed to employ eye-tracking and retrospective think-aloud techniques to identify differences in SRL strategy use for program comprehension tasks between high-performing students (N = 31) and low-performing students (N = 31) in an undergraduate programming course. All participants attended individual eye-tracking sessions to comprehend two Python program codes with different constructs. Their eye-tracking data and video-recalled retrospective think-aloud data were captured and recorded for analysis. The findings reveal that higher-order cognitive skills, such as elaboration and critical thinking, were mostly adopted by high-performing students, while basic cognitive and resource management strategy, such as rehearsal and help-seeking, were mostly employed by low-performing students when comprehending the program codes. This study not only demonstrates the design of combining eye-tracking and retrospective think-aloud data to explore students’ use of SRL strategies but also provides evidence to support the notion that program comprehension is a complex process that cannot be effectively addressed by employing merely rudimentary strategies, such as repetitively reading the same code segment. In the future, researchers could explore the possibility of using a webcam to monitor and assess students’ online programming processes and provide feedback based on their eye movements. They could also examine the effects of SRL strategies training on students’ motivation, engagement, and performance in various types of programming activities.},
 authors = {['Gary Cheng', 'Di Zou', 'Haoran Xie', 'Fu Lee Wang']},
 journal = {Computers & Education},
 keywords = {['Introductory programming', 'Self-regulated learning strategies', 'Eye tracking', 'Retrospective think aloud', 'Higher education']},
 title = {Exploring differences in self-regulated learning strategy use between high- and low-performing students in introductory programming: An analysis of eye-tracking and retrospective think-aloud data from program comprehension},
 year = {2024}
}

@Filtered Article{d6c211af-f1d7-462f-b51d-e3ca3191bfaa,
 abstract = {Despite the growing number of digital apps designed to teach coding skills to young children, we know little about their effectiveness. To formally explore this question, we conducted a naturalistic observation of a one-week program designed to teach foundational coding skills (i.e., sequencing, conditions, loops) to young children (N = 28, Mage = 5.15 years) using two tablet applications: Daisy the Dinosaur and Kodable. Pre- and post-assessments measured familiarity with technology, appeal of coding apps, knowledge of Daisy commands, ability to play Kodable, and conceptual understanding of coding. Participants improved in their knowledge of Daisy commands (i.e., move, grow, jump) and Kodable gameplay (i.e., placing arrows in the correct sequence to move a character through a maze), but did not improve in their ability to verbally explain what coding is. Appeal of the games was significantly related to children's learning of Daisy commands, but child gender was not related to either Daisy or Kodable learning outcomes. Results suggest that young children can learn foundational coding skills via apps, especially when the apps are appealing to children.},
 authors = {['Sarah Pila', 'Fashina Aladé', 'Kelly J. Sheehan', 'Alexis R. Lauricella', 'Ellen A. Wartella']},
 journal = {Computers & Education},
 keywords = {['Apps', 'Coding', 'Computational thinking', 'Digital games', 'Educational technology', 'STEM']},
 title = {Learning to code via tablet applications: An evaluation of Daisy the Dinosaur and Kodable as learning tools for young children},
 year = {2019}
}

@Filtered Article{d6e3c44b-97a3-46dd-a669-778df3439a8f,
 abstract = {In the philosophy of the social sciences, atomism is the view that human beings can be thinking, rational beings independently of social relations. Holism, by contrast, is the view that social relations are essential to human beings insofar as they are thinking, rational beings. This article first provides an overview of different sorts of atomism and holism (see Section Types of Atomism and Holism). It then briefly sketches the historical background of these notions in modern philosophy (Section The Historical Background of Atomism and Holism). The main part is a systematic characterization of atomism and holism (see Section A Characterization of Atomism and Holism) and a summary of the most important arguments for both these positions (see Section Arguments for Atomism and Holism).},
 authors = {['Michael Esfeld']},
 journal = {Elsevier},
 keywords = {['Atomism', 'Collectivism', 'Confirmation', 'Externalism', 'Holism', 'Human nature', 'Individualism', 'Internalism', 'Meaning', 'Ontological dependence', 'Rationality', 'Rule-following', 'Thought']},
 title = {Atomism and Holism: Philosophical Aspects},
 year = {2015}
}

@Filtered Article{d7082d92-b268-4cce-a60e-15f2cdf06005,
 abstract = {This study highlights the potential benefits of integrating Large Language Models (LLMs) into chemical engineering education. In this study, Chat-GPT, a user-friendly LLM, is used as a problem-solving tool. Chemical engineering education has traditionally focused on fundamental knowledge in the classroom with limited opportunities for hands-on problem-solving. To address this issue, our study proposes an LLMs-assisted problem-solving procedure. This approach promotes critical thinking, enhances problem-solving abilities, and facilitates a deeper understanding of core subjects. Furthermore, incorporating programming into chemical engineering education prepares students with vital Industry 4.0 skills for contemporary industrial practices. During our experimental lecture, we introduced a simple example of building a model to calculate steam turbine cycle efficiency, and assigned projects to students for exploring the possible use of LLMs in solving various aspect of chemical engineering problems. Although it received mixed feedback from students, it was found to be an accessible and practical tool for improving problem-solving efficiency. Analyzing the student projects, we identified five common difficulties and misconceptions and provided helpful suggestions for overcoming them. Our course has limitations regarding using advanced tools and addressing complex problems. We further provide two additional examples to better demonstrate how to integrate LLMs into core courses. We emphasize the importance of universities, professors, and students actively embracing and utilizing LLMs as tools for chemical engineering education. Students must develop critical thinking skills and a thorough understanding of the principles behind LLMs, taking responsibility for their use and creations. This study provides valuable insights for enhancing chemical engineering education's learning experience and outcomes by integrating LLMs.},
 authors = {['Meng-Lin Tsai', 'Chong Wei Ong', 'Cheng-Liang Chen']},
 journal = {Education for Chemical Engineers},
 keywords = {['Engineering education', 'Industry 4.0 skill', 'Programming in chemical engineering', 'Problem-solving', 'Large language models (LLMs)', 'Chat-GPT']},
 title = {Exploring the use of large language models (LLMs) in chemical engineering education: Building core course problem models with Chat-GPT},
 year = {2023}
}

@Filtered Article{d72019d6-874d-438c-a717-e83f6a11b457,
 abstract = {Designers often face situations where the only way forward is through the exploration of possibilities. However, there is a critical disconnect between understanding of how designer’s think and act in such situations. We address this disconnect by proposing and testing (via protocol analysis) the cognitive co-evolution model. Our model comprises a new approach to co-evolutionary design theory by explaining both the progression of the process itself and the creation of design outputs via an interplay between metacognitive perceived uncertainty, cognition, and the external world. We thus connect explanations of how designers think with descriptions of how they act. We provide a foundation for connecting to other theories, models, and questions in design research via common links to cognition and metacognition.},
 authors = {['Philip Cash', 'Milene Gonçalves', 'Kees Dorst']},
 journal = {Design Studies},
 keywords = {['co-evolution', 'design process(es)', 'design cognition', 'design thinking', 'creativity']},
 title = {Method in their madness: Explaining how designers think and act through the cognitive co-evolution model},
 year = {2023}
}

@Filtered Article{d75d2d48-8cde-420b-a882-b202a2881921,
 abstract = {In Malaysia and also elsewhere in the world the demands for graduates who have employability skills such as ability to think critically, solve problems and can communicate are highly sought in the workplace. In the early 2006, the development of such skills was recognized as integral goals of undergraduate education at Universiti Teknologi Malaysia. Since then rigorous efforts have been made to inculcate these skills amongst the undergraduates. In this paper, we will share some of our experiences in coping with the challenges of changing our teaching practices to accommodate this quest though focusing on communication. For mathematics learning to occur, we believed that students should participate actively in the knowledge construction and be able to take charge of their own learning. Taking these aspects into consideration, we had developed a framework of active learning and used it to guide our instruction in engineering mathematics at UTM. Here we will discuss the strategies that we had designed and employed in engaging students with the subject matter as well as to initiate and support student's thinking and communication in the language of mathematics. Some student's responses that gave indications of their struggle, progress and growth encountered in the research implementation will also be presented.},
 authors = {['Roselainy Abdul Rahman', 'Yudariah Mohammad Yusof', 'Hamidreza Kashefi', 'Sabariah Baharun']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Communication', 'Mathematical Thinking', 'Multivariable Calculus', "Student's Obstacles"]},
 title = {Developing Mathematical Communication Skills of Engineering Students},
 year = {2012}
}

@Filtered Article{d77fbc75-9fd4-4dbb-9f1d-620e915a79a6,
 abstract = {This chapter discusses various aspects of biological perspectives on creativity. Some of the research on creativity as of late involves the brain and biological correlates of originality, novelty, and insight. Handedness is sometimes used as an indication of hemispheric dominance or hemisphericity, with right-handed people being compared with left-handed people. There are several reports of left-handed persons outnumbering the right-handed in creative and eminent samples. Hemisphericity and other important brain structures and processes contributing to creative thinking and behavior have been studied with electroencephalogram (EEG), positron emission topography (PET), cerebral blood flow, and magnetic resonance imaging (MRI) techniques. Numerous EEG studies suggest that there are particular brain wave patterns and brain structures that are associated with creative problem solving, or at least specific phases within the problem solving process. EEGs suggest a complex kind of activity while individuals work on divergent thinking tasks. The complexity disappears when those same individuals work on convergent thinking tasks. It is found that the role of the prefrontal cortex in creative thinking and behavior comes from several sources and uses different methodologies.},
 authors = {['Mark A. Runco']},
 journal = {Academic Press},
 keywords = {['Split brain', 'Corpus callosum', 'Pre-frontal cortex', 'Cerebellum', 'Altered states of consciousness', 'Exercise', 'Stress', 'Dreams', 'Drugs', 'Genetics', 'Dopamine', 'Adoption studies', 'Genealogies']},
 title = {Chapter 3 - Biological Perspectives on Creativity},
 year = {2014}
}

@Filtered Article{d79e3bde-53a8-41e3-8bf1-4a05e37b97a5,
 abstract = {Aims
To investigate graduating nursing students' nursing and professional competencies and the predictors of their competencies.
Background
Across Asian countries, there is a paucity of literature that explores graduating nursing students' competency and professional competence during the ongoing COVID-19 pandemic.
Design
Descriptive, cross-sectional, and predictive approaches.
Method
Convenience sampling was used among graduating nursing students from the six Asian countries (n = 375). The STROBE guidelines for cross-sectional studies were used. Two self-report instruments were utilized to collect data. We conducted multiple linear regression analyses to assess the predictors of nursing competency and professional competence domains.
Results
Country of residence and general point average (GPA) showed statistically significant multivariate effects. Value-based nursing care and critical thinking and reasoning domains recorded the highest in professional competence and competency inventory for nursing students, respectively. Country of residence, GPA, and preferred nursing major were significant predictors of graduating nursing students' nursing competency and professional competence domains.
Conclusion
Our study's findings revealed a high level of diversity among nursing students regarding ethical care obligations, caring pedagogies, and lifelong learning, all of which may be ascribed to their distinct culture, background, and belief systems.},
 authors = {['Rizal Angelo N. Grande', 'Daniel Joseph E. Berdida', 'Tantut Susanto', 'Anwar Khan', 'Wanpen Waelveerakup', 'Zahrah Saad']},
 journal = {Nurse Education Today},
 keywords = {['Asian countries', 'Competency', 'Graduating nursing students', 'Nursing competency inventory', 'Professional competence']},
 title = {Nursing competency inventory and professional competence of graduating students in six Asian countries: A cross-sectional study},
 year = {2022}
}

@Filtered Article{d7a4084a-b433-4720-9dbb-03a49270c0aa,
 abstract = {Much recent scholarly investigation has been focused on the promise of digitalization and the new ways of working and organizing it makes possible. In this paper, we analyze how the COVID-19 pandemic has acted as a natural breaching experiment that has challenged taken-for-granted expectations about digitalization and revealed four important issues: uneven access to digital infrastructures, the persistence of the analog in digitalization, the brittleness of unchecked digitalization, and panoptical surveillance. The sudden shift to digital work has exposed taken-for-granted assumptions about the universality of digital access. The crisis has also revealed that many highly digitalized processes still rely on analog elements. The pandemic has also exposed that many algorithms used in digitalized inter-organizational processes are brittle due to overreliance on historic patterns. Finally, the pandemic has breached fundamental expectations of privacy when organizational surveillance was extended into private and public spaces. Thus, the pandemic has laid bare fundamental challenges in digitalization and has exposed the limits of rose‑tinted thinking about the relation between technology and organizing.},
 authors = {['Samer Faraj', 'Wadih Renno', 'Anand Bhardwaj']},
 journal = {Information and Organization},
 keywords = {['COVID', 'Digitalization', 'Technology', 'Organizing', 'Breaching experiment']},
 title = {Unto the breach: What the COVID-19 pandemic exposes about digitalization},
 year = {2021}
}

@Filtered Article{d803905c-cd4a-4aed-802f-31226115cff0,
 abstract = {Publisher Summary
Mental models are related to the concept of meaning and language comprehension; in other words, comprehending a linguistic message means that an appropriate mental model has been formed. The manipulation of mental models corresponds to thinking, and it is the manipulation that generates emergent ideas. The chapter discusses the importance of considering the ways ideas combine and presents the data from two experiments that illustrate the combination of ideas. The chapter illustrates the major implications for the theories of mental models. The first implication is that the computational theories cannot account for the data. The second implication is that something like embodiment is needed, and the chapter outlines one account of embodied mental models. The third implication is the most important and most controversial. It is that the human cognition is not a computational phenomenon.},
 authors = {['Arthur Glenberg']},
 journal = {North-Holland},
 title = {4 Why mental models must be embodied},
 year = {1999}
}

@Filtered Article{d80a105e-2709-413a-9285-3cb02728b998,
 abstract = {Purpose
Reliability and maintainability are the key system effectiveness measures in process and manufacturing industries, and treatment plants, especially in E-waste management plants. The present work is proposed with a motto to develop a stochastic framework for the e-waste management plant to optimize its availability integrated with reliability, availability, maintainability, and dependability (RAMD) measures and Markovian analysis to estimate the steady-state availability of the E-waste management plant. In the analysis an effort is also made to identify the best performing algorithm for availability optimization of the e-waste plant.
Methodology
A stochastic model for a particular plant is developed and its availability is optimized using various metaheuristic approaches like a genetic algorithm (GA), particle swarm optimization (PSO), and differential evolutions (DE). The most sensitive component is identified using RAMD methodology while the effect of deviation in various failure and repair rates are observed by the proposed model. The failure and repair rates follow an exponential distribution. All time-dependent random variables are statistically independent.
Originality/Novelties
A novel stochastic model is presented for an e-waste management plant and optimum availability is obtained using metaheuristic approaches. The proposed methodology is not so far discussed in the reliability analysis of process industries.
Findings
The numerical results of the proposed model compared to identify the most efficient algorithm. It is observed that genetic algorithm provides the maximum value (0.92330969) of availability at a population size 2500 after 500 iterations. PSO algorithm attained the maximum value (0.99996744) of availability just after 50 iterations and 100 population size. So, its rate of convergence is faster than GA. The optimum value of availability is 0.99997 using differential evolution after 500 iterations and population size of more than 1000. These findings are very beneficial for system designers.
Practical Implications
The proposed methodology can be utilized to find the reliability measures of other process industries.},
 authors = {['Naveen Kumar', 'Deepak Sinwar', 'Monika Saini', 'Dinesh Kumar Saini', 'Ashish Kumar', 'Manjit Kaur', 'Dilbag Singh', 'Heung-No Lee']},
 journal = {Journal of King Saud University - Computer and Information Sciences},
 keywords = {['E-waste management plant', 'Availability', 'Maintainability', 'Genetic Algorithm', 'Differential Evolution', 'Particle Swarm Optimization', 'Markov Birth-Death Process']},
 title = {Efficient computational stochastic framework for performance optimization of E-waste management plant},
 year = {2022}
}

@Filtered Article{d83a6cc8-969e-4a63-8c92-27ddd961d637,
 abstract = {Convolutional neural networks (CNNs) have evolved from the initial LeNet to date, and network models have become increasingly deep and comprehensive. It has been proven that deeper networks have better fitting effects, but the corresponding parameter size and computational complexity increase rapidly. With the continuous development of mobile Internet technology, portable devices have been rapidly popularized, and users have put forward more and more demands. Thus, how to design efficient and high-performance lightweight convolutional neural networks (CNNs) is the key to solve this challenging problem. Recently, this type of convolutional neural networks (CNNs)--lightweight convolutional neural networks (CNNs), which adopt the design concept of compression networks and maintain high accuracy with fewer parameters, has attracted increasing attention. SqueezeNet is a lightweight CNN adapting to edge device deployment. Its number of parameters is only 1/50 of AlexNet, but it achieves the same accuracy as AlexNet. In order to make the network more lightweight, inspired by SqueezeNet, MobileNet, SENet, SKNet, AlexNet, etc., in this paper we propose StereoSqueezeNet, using much fewer parameters but achieving even better accuracy than SqueezeNet.},
 authors = {['Qiaoyan Sun', 'Jianfei Chen']},
 journal = {Neurocomputing},
 keywords = {['SSNet', 'Stereo', 'SqueezeNet', 'Network compression', 'Network parameters']},
 title = {StereoSqueezeNet: With fewer parameters but higher accuracy than SqueezeNet},
 year = {2025}
}

@Filtered Article{d8b25262-0a7b-4d53-86b3-60fcf514832c,
 abstract = {Over the last few years, the integration of coding activities for children in K-12 education has flourished. In addition, novel technological tools and programming environments have offered new opportunities and increased the need to design effective learning experiences. This paper presents a design-based research (DBR) approach conducted over two years, based on constructionism-based coding experiences for children, following the four stages of DBR. Three iterations (cycles) were designed and examined in total, with participants aged 8–17 years old, using mixed methods. Over the two years, we conducted workshops in which students used a block-based programming environment (i.e., Scratch) and collaboratively created a socially meaningful artifact (i.e., a game). The study identifies nine design principles that can help us to achieve higher engagement during the coding activity. Moreover, positive attitudes and high motivation were found to result in the better management of cognitive load. Our contribution lies in the theoretical grounding of the results in constructionism and the emerging design principles. In this way, we provide both theoretical and practical evidence of the value of constructionism-based coding activities.},
 authors = {['Sofia Papavlasopoulou', 'Michail N. Giannakos', 'Letizia Jaccheri']},
 journal = {Computers in Human Behavior},
 keywords = {['Constructionism', 'Coding', 'Computational thinking', 'Engagement', 'Children', 'Design-based research']},
 title = {Exploring children's learning experience in constructionism-based coding activities through design-based research},
 year = {2019}
}

@Filtered Article{d8d114ff-9c77-4d53-91ff-36b01b1ce293,
 abstract = {Objective
Predicting outcomes after intracerebral hemorrhage (ICH) may help improve patient outcomes. We developed and validated a machine learning prediction model for post-rehabilitation functional outcomes after ICH. Patient selection and explanatory variable settings were based on clinical significance. Functional outcomes were predicted using ternary classification.
Methods
The subjects were patients aged > 18 years without pre-onset severe disability who developed primary putaminal and/or thalamic hemorrhage and underwent an inpatient rehabilitation program. As explanatory variables, 43 values related to patient background, imaging-related findings, systemic conditions, neurological findings, and blood tests were acquired within 10 days of onset. As an objective variable, the functional outcome at discharge to home or nursing home was acquired using a ternary classification. The dataset consisting of the collected information was split into a training dataset and a test dataset with a ratio of 2:1. A predictive model using a balanced random forest algorithm was created using supervised learning from the training dataset. The predictive performance was validated using a test dataset.
Results
Between January 2018 and June 2019, 100 consecutive patients were included in the study. The areas under the receiver operating characteristic curves for predictions of good, moderate, and poor outcomes were 0.952, 0.790, and 0.921, respectively.
Conclusions
The predictive performance of the model was comparable to that of previous models. Patient selection and variable settings from a clinical perspective may contribute to accurate and detailed predictions. These study designs are based on design thinking and may meet the needs of clinical practice.},
 authors = {['Shinya Sonobe', 'Tetsuo Ishikawa', 'Kuniyasu Niizuma', 'Eiryo Kawakami', 'Takuya Ueda', 'Eichi Takaya', 'Carlos {Makoto Miyauchi}', 'Junya Iwazaki', 'Ryuzaburo Kochi', 'Toshiki Endo', 'Arun Shastry', 'Vijayananda Jagannatha', 'Ajay Seth', 'Atsuhiro Nakagawa', 'Masahiro Yoshida', 'Teiji Tominaga']},
 journal = {Interdisciplinary Neurosurgery},
 keywords = {['Intracerebral hemorrhage', 'Machine learning prediction', 'Post-rehabilitation functional outcome', 'Design thinking']},
 title = {Development and validation of machine learning prediction model for post-rehabilitation functional outcome after intracerebral hemorrhage},
 year = {2022}
}

@Filtered Article{d8ef650f-755c-42cc-b2fb-847dfd76a369,
 abstract = {This paper is an overview of some of the methods developed by the Team for Advanced Flow Simulation and Modeling (T★AFSM) [http://www.mems.rice.edu/TAFSM/] to support flow simulation and modeling in a number of “Targeted Challenges”. The “Targeted Challenges” include unsteady flows with interfaces, fluid–object and fluid–structure interactions, airdrop systems, and air circulation and contaminant dispersion. The methods developed include special numerical stabilization methods for compressible and incompressible flows, methods for moving boundaries and interfaces, advanced mesh management methods, and multi-domain computational methods. We include in this paper a number of numerical examples from the simulation of complex flow problems.},
 authors = {['Tayfun Tezduyar', 'Yasuo Osawa']},
 journal = {Parallel Computing},
 keywords = {['Computational fluid dynamics', 'Flow simulation', 'Stabilization methods', 'Compressible flow', 'Incompressible flow', 'Multidomain computational methods']},
 title = {Methods for parallel computation of complex flow problems},
 year = {1999}
}

@Filtered Article{d8f351d2-81a5-4d87-9754-8ca041ed1d38,
 abstract = {With the progress of the times and the improvement of science and technique, network message technique has occupied a vital position in people's lives. At the same time, society has been implementing university English education reform in recent years, and the “internet plus” wisdom education model is the product of the improvement of the times. This new education model has gradually integrated into the education of various subjects. Introducing the concept of message technique and wisdom education into university translation education can innovate education mode, optimize education content, and integrate high-quality education resources. Cultivating applied translators has become the trend of educational reform. Based on deep learning, this paper studies translation education skills in universities. In-depth education enables learners to acquire systematic knowledge, critical spirit, creative thinking, etc. This kind of learning fully taps individual potential to cultivate a complete personality. According to the research in this paper, wisdom education is 12% better than traditional education, and it is suitable to be widely put into practice.},
 authors = {['Yan Liu', 'Shuhua Li', 'Dan Cui']},
 journal = {Computers in Human Behavior},
 keywords = {['Deep learning', 'Colleges and universities', 'Translation education', 'Machine learning applications', 'Teaching strategy']},
 title = {Analysis of translation teaching skills in colleges and universities based on deep learning},
 year = {2024}
}

@Filtered Article{d93982d3-1723-4666-a856-343c0f39af9e,
 abstract = {Intuition is the human capacity to make decisions under novel, complex situations where knowledge is incomplete and of variable levels of certainty. We take the view that intuition can be modeled as a rational and deductive mode of information processing which is suited to novel, complex situations. In this research, a computational algorithm, or “intuitive reasoner”, is proposed which mimics some aspects of human intuition by combining established mathematical tools, such as fuzzy set theory, and some novel innovations. A rule-based scheme is followed and a rule-learning module that allows rules to be learned from incomplete datasets is developed. The input and the rules drawn by the reasoner are allowed to be fuzzy, multi-valued, and low in certainty. A measure of the certainty level, Strength of Belief, is attached to each input as well as each rule. Solutions are formulated through iterations of consolidating intermediate reasoning results, during which the Strength of Belief of corroborating intermediate results is combined. An experimental implementation of the proposed intuitive reasoner is reported, in which the reasoner was used to solve a classification problem. The results showed that, when given increasingly sparse input data, the rule-learning module generated more rules of lower associated certainty than when presented with more complete data. The intuitive reasoner was able to make use of these low-certainty rules to solve the classification problems with an accuracy that compared favorably to that of traditional methods based on complete datasets.},
 authors = {['Yung-Chien Sun', 'Grant Clark']},
 journal = {Expert Systems with Applications},
 keywords = {['Artificial Intelligence', 'Intuition', 'Knowledge acquisition', 'Limited certainty']},
 title = {A computational model of an intuitive reasoner for ecosystem control},
 year = {2009}
}

@Filtered Article{d969a8c5-3492-49b2-bb7c-64b87e653c08,
 abstract = {Publisher Summary
This chapter examines the challenges and some of the recent advances in computational systems biology. Research in computational systems biology has moved beyond interaction networks based simply on clustering and correlation. There are two paradigms in computational systems biology: the iterative cycle of biochemical model—mathematical model—computational model, and integration of novel data and legacy knowledge to develop context-specific biochemical, mathematical, and computational models. Challenges in building biochemical models include the complexity of proteomic states and interactions, integration of diverse data to infer biochemical interactions, and the temporal state of biochemical models. Challenges in building mathematical models include incorporating statistical/probabilistic information into analytical models, using qualitative constraints in mathematical models, and incomplete knowledge and coarse-graining. Challenges in computational modeling include the absence of knowledge about model parameters such as rate constants, local versus global concentrations of species and multiple scales of distance and time, and variation among different cell types and subpopulation variability, or variability among biological repeats. Advanced research in coarse graining will pave the way for progress in the development of multiscale multidomain modeling that can connect fundamental research in network biology to clinical research.},
 authors = {['Mano Ram Maurya', 'Shankar Subramaniam']},
 journal = {Academic Press},
 title = {Chapter 8 - Computational Challenges in Systems Biology},
 year = {2010}
}

@Filtered Article{d99053c7-af6d-4755-b4df-4a00dc97cda0,
 abstract = {Creativity is related to a higher flexible semantic memory structure, which could explain greater fluency of ideas. Extensive research has identified a positive connection between creativity and bi-/multilingualism mainly in contexts where two languages or more concur in daily communicative interactions. Yet, creativity has received scant attention in regard to L2 (second or foreign language) acquisition that mainly takes place in classroom situations. The scarce research points to a positive relationship between creativity and L2 fluency – understood as the number of words produced. We apply computational network science analysis and Forward Flow methods to examine lexical organization patterns of a low creativity (LC) and high creativity (HC) group of 12th grade Spanish English as a Foreign Language (EFL) learners. The participants completed two fluency tasks, where they generated animal names in their L2, and also L1 – used here as a control measure. EFL proficiency was controlled. Our analyses revealed that the HC individuals were more fluent in L1 and L2, generated more remote responses, and exhibited a more flexible and efficiently structured semantic memory in both languages, with a greater effect of creativity in L2. Contrary to previous research, the L2 semantic memory network exhibited a less random organization. Differences in the L2 learning conditions are adduced as likely causes of this result.},
 authors = {['Almudena Fernández-Fontecha', 'Yoed N. Kenett']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Creativity', 'Semantic network', 'L2', 'Bilingualism', 'Semantic fluency']},
 title = {Examining the relations between semantic memory structure and creativity in second language},
 year = {2022}
}

@Filtered Article{d9a6e85a-5c34-45b2-b0a5-8c28928bd545,
 abstract = {This article analyses the perceived role of Finnish education experts working in development cooperation for education. We interviewed 31 education experts working in international organisations representing Finland. A theoretically pluralist approach is utilised combining complexity thinking with a multiple streams approach. The analysis demonstrates that the context of educational development cooperation is ambiguous and complex. Influencing policymaking is a strategic, non-linear task which takes time, resources, and personal skills. Policy entrepreneurs need to understand the dynamics of development cooperation, identify actors that trust them, and recognise when policy windows are likely to open.},
 authors = {['Íris Santos', 'Elias Pekkola']},
 journal = {International Journal of Educational Development},
 keywords = {['Development cooperation for education', 'Influence', 'Finnish education experts', 'Complexity', 'Multiple streams approach']},
 title = {Policy entrepreneurs in the global education complex: The case of Finnish education experts working in international organisations},
 year = {2023}
}

@Filtered Article{d9d6d78b-bff5-4c8b-8515-ee5c0dd78599,
 abstract = {The encyclopedic inventory of the first half of the twentieth century, “Anthropology Today”, published in 1953, gave little inkling that within a few decades developing trends in social theory, in field experience, in electronic data processing, and in mathematics would combine to bring to prominence a distinctive theoretical approach using a quite formal network model for social systems. Now, sophisticated mathematics and computer programming permit sophisticated network models — networks seen as sets of links, networks seen as generated structures, and networks seen as flow processes. Although network thinking has shown a dramatic rise from the “Anthropology Today” of 1953 to the current anthropology of 1978, it is predicted to soar in the next quarter century, much of the weighty burden of network analysis having been lifted from us by ever more rapid electronic data processing.},
 authors = {['Alvin W. Wolfe']},
 journal = {Social Networks},
 title = {The rise of network thinking in anthropology},
 year = {1978}
}

@Filtered Article{d9de9250-64c8-40f0-9162-4a150bd07eed,
 abstract = {Flood risk management is a significant concern for many regions. To reduce the flood impact, it is essential to increase residents' knowledge about this risk and in its management. Despite the many tools and methods available to raise awareness of flood risk, none of them fully meet the challenges of effective communication on flood and flood management by: integrating the perspective of local people, by providing information that is clear and easy to understand, by encouraging debate, discussion and reflection and by positioning flood mitigation measure at the center (positive vision on the risk). To answer this need, this article proposes an innovative approach that combines several methods, including sketch maps, agent-based simulation, and serious games. This combination enables to benefit from these three approaches: the expressiveness of sketch maps and the ability to analyze participants' spatial representations, the capacity of agent-based simulations to aid users in comprehending complex phenomena and dynamics, and the experimental and motivational environment provided by games. To implement this approach, we developed the DYSMA model, which bridges the gap between sketch maps and agent-based simulations by integrating drawn elements as agents, providing a dynamic sketch map. Additionally, we developed the Draw and Flood game, designed to engage the general public in thinking about flood management through the use of dynamic sketch maps. This approach is applied to an illustrative application dedicated to flooding in a small French city.},
 authors = {['Franck Taillandier', 'Patrick Taillandier', 'Pénélope Brueder', 'Noé Brosse']},
 journal = {International Journal of Disaster Risk Reduction},
 keywords = {['Urban flood', 'Game', 'Sketch map', 'Agent-based simulation']},
 title = {The dynamic sketch map to support reflection on urban flooding},
 year = {2025}
}

@Filtered Article{d9ef7ef4-d3a5-4c00-a65c-1f26913f0304,
 abstract = {Investment in agricultural conservation practices (CPs) to address Lake Erie's re-eutrophication may offer benefits that extend beyond the lake, such as improved habitat conditions for fish communities throughout the watershed. If such conditions are not explicitly considered in Lake Erie nutrient management strategies, however, this opportunity might be missed. Herein, we quantify the potential for common CPs that will be used to meet nutrient management goals for Lake Erie to simultaneously improve stream biological conditions throughout the western Lake Erie basin (WLEB) watershed. To do so, we linked a high-resolution watershed-hydrology model to predictive biological models in a conservation scenario framework. Our modeling simulations showed that the implementation of CPs on farm acres in critical and moderate need of treatment, representing nearly half of the watershed, would be needed to reduce spring/early summer total phosphorus loads from the WLEB watershed to acceptable levels. This widespread CP implementation also would improve potential stream biological conditions in >11,000km of streams and reduce the percentage of streams where water quality is limiting biological conditions, from 31% to 20%. Despite these improvements, we found that even with additional treatment of acres in low need of CPs, degraded water quality conditions would limit biological conditions in >3200streamkm. Thus, while we expect CPs to play an important role in mitigating eutrophication problems in the Lake Erie ecosystem, additional strategies and emerging technologies appear necessary to fully reduce water quality limitation throughout the watershed.},
 authors = {['S. Conor Keitzer', 'Stuart A. Ludsin', 'Scott P. Sowa', 'Gust Annis', 'Jeff G. Arnold', 'Prasad Daggupati', 'August M. Froehlich', 'Matt E. Herbert', 'Mari-Vaughn V. Johnson', 'Anthony M. Sasson', 'Haw Yen', 'Mike J. White', 'Charles A. Rewa']},
 journal = {Journal of Great Lakes Research},
 keywords = {['Best management practices', 'SWAT', 'Non-point source pollution', 'Great Lakes', 'Ecosystem-based management', 'Index of Biotic Integrity']},
 title = {Thinking outside of the lake: Can controls on nutrient inputs into Lake Erie benefit stream conservation in its watershed?},
 year = {2016}
}

@Filtered Article{da83bb32-1a70-4e51-b702-b97652d42800,
 abstract = {Cancer mutations that are recurrently observed among patients are known as hotspots. Hotspots are highly relevant because they are, presumably, likely functional. Known hotspots in BRAF, PIK3CA, TP53, KRAS, IDH1 support this idea. However, hundreds of hotspots have never been validated experimentally. The detection of hotspots nevertheless is challenging because background mutations obscure their statistical and computational identification. Although several algorithms have been applied to identify hotspots, they have not been reviewed before. Thus, in this mini-review, we summarize more than 40 computational methods applied to detect cancer hotspots in coding and non-coding DNA. We first organize the methods in cluster-based, 3D, position-specific, and miscellaneous to provide a general overview. Then, we describe their embed procedures, implementations, variations, and differences. Finally, we discuss some advantages, provide some ideas for future developments, and mention opportunities such as application to viral integrations, translocations, and epigenetics.},
 authors = {['Emmanuel Martinez-Ledesma', 'David Flores', 'Victor Trevino']},
 journal = {Computational and Structural Biotechnology Journal},
 keywords = {['Mutations', 'Cancer', 'Hotspots', 'Recurrent mutations', 'Algorithms', 'Genomics', 'Sequencing', 'Exome', 'Whole genome sequencing']},
 title = {Computational methods for detecting cancer hotspots},
 year = {2020}
}

@Filtered Article{db26d117-5ef4-4073-a10d-2ee11ee0e253,
 abstract = {Isolating blocks may be used as computational tools to search for the invariant manifolds of orbits and hyperbolic invariant sets associated with libration points while also giving additional insight into the dynamics of the flow in these regions. We use isolating blocks to investigate the dynamics of objects entering the Earth–Moon system in the circular restricted three-body problem with energies close to the energy of the L2 libration point. Specifically, the stable and unstable manifolds of Lyapunov orbits and the hyperbolic invariant set around the libration points are obtained by numerically computing the way orbits exit from an isolating block in combination with a bisection method. Invariant spheres of solutions in the spatial problem may then be located using the resulting manifolds.},
 authors = {['Rodney L. Anderson', 'Robert W. Easton', 'Martin W. Lo']},
 journal = {Physica D: Nonlinear Phenomena},
 keywords = {['Circular restricted three-body problem', 'Isolating blocks', 'Invariant manifolds', 'Invariant 3-sphere']},
 title = {Isolating blocks as computational tools in the circular restricted three-body problem},
 year = {2017}
}

@Filtered Article{db5a3eb6-4008-4d1d-af97-6bc77730e842,
 abstract = {Integrating renewables into existing energy infrastructure to construct hybrid energy systems (HES) plays a vital role for advancing energy sustainability. While various approaches, such as energy systems analysis and linear or non-linear optimisation, have been employed to achieve energy sustainability mainly at the national or city level, there has been a lack of focus on achieving energy sustainability in the residential sector through a holistic optimal decision-making approach for efficient HES design. This study focuses on developing a multi-stage optimisation-based decision-making framework that models, quantifies, and optimises the performance indicators of HES, allowing for an assessment of the trade-off between benefits and systems costs under various design scenarios. The initial step involves designing the HES model and constructing scenarios that cater to the electrification requirements of water, energy, and food elements in the residential sector by using a systematic thinking approach. Then, a preliminary evaluation of the modelled scenarios is conducted to assess energy sustainability in terms of technical and economic aspects. Afterwards, an optimal decision-making setup is established by integrating a multi-objective HES model into the NSGA-II algorithm, which approximates the Pareto optimal solutions. These solutions are then ranked by using a multi-criteria decision-making method. According to the findings, the Quetta region in Pakistan contains the best optimal solution. The results underscore the utility of the developed framework in facilitating the optimal design of renewables-integrated HES for the residential sector. Furthermore, intergovernmental organizations can leverage this framework to formulate effective policies aimed at encouraging residents to invest in HES installation.},
 authors = {['Aamir Mehmood', 'Long Zhang', 'Jingzheng Ren']},
 journal = {Sustainable Futures},
 keywords = {['Hybrid energy system', 'System thinking approach', 'Genetic algorithm', 'Multi-criteria decision-making', 'Energy sustainability']},
 title = {A multi-stage optimisation-based decision-making framework for sustainable hybrid energy system in the residential sector},
 year = {2023}
}

@Filtered Article{db9e1045-be83-41b7-b3b6-4dd478f44a5f,
 abstract = {As computing power increases, more complex computational models are utilized for biomass supply system studies. The paper describes three commonly used modeling methods in this context, geographic information systems, life-cycle assessment, and discrete-time simulation and presents bibliometric analysis of work using these three study methods. Of the 498 publications identified in searches of the Scopus and Web of Science databases, 17 reported on combinations of methods: 10 on life-cycle assessment and geographic information systems, six on joint use of life-cycle assessment and discrete-time simulation, and one on use of geographic information systems jointly with discrete-time simulation. While no articles dealt directly with simultaneous use of all three methods, several acknowledged the potential of this. The authors discuss numerous challenges identified in the review that arise in combining methods, among them computational load, the increasing number of assumptions, guaranteeing coherence between the models used, and the large quantities of data required. Discussion of issues such as the complexity of reporting and the need for standard procedures and terms becomes more critical as repositories bring together research materials, including entire models, from various sources. Efforts to mitigate many of modeling’s challenges have involved phase-specific modeling and use of such methods as expressions or uncertainty analysis in place of a complex secondary model. The authors conclude that combining modeling methods offer considerable potential for taking more variables into account; improving the results; and benefiting researchers, decision–makers, and operation managers by producing more reliable information.},
 authors = {['Mika Aalto', 'Raghu KC', 'Olli-Jussi Korpinen', 'Kalle Karttunen', 'Tapio Ranta']},
 journal = {Applied Energy},
 keywords = {['Biomass', 'Supply chain', 'Life cycle assessment', 'Geographical information system', 'Agent-based modeling and simulation', 'Discrete-event simulation']},
 title = {Modeling of biomass supply system by combining computational methods – A review article},
 year = {2019}
}

@Filtered Article{dc496c2b-939a-43d0-92b3-2104142adef7,
 abstract = {Action allows us to shape the world around us. But to act effectively we need to accurately sense what we can and cannot control. Classic theories across cognitive science suppose that this ‘sense of agency’ is constructed from the sensorimotor signals we experience as we interact with our surroundings. But these sensorimotor signals are inherently ambiguous, and can provide us with a distorted picture of what we can and cannot influence. Here we investigate one way that agents like us might overcome the inherent ambiguity of these signals: by combining noisy sensorimotor evidence with prior beliefs about control acquired through explicit communication with others. Using novel tools to measure and model control decisions, we find that explicit beliefs about the controllability of the environment alter both the sensitivity and bias of agentic choices; meaning that we are both better at detecting and more biased to feel control when we are told to expect it. These seemingly paradoxical effects on agentic choices can be captured by a computational model where expecting to be in control exaggerates the sensitivity or ‘gain’ of the mechanisms we use to detect our influence over our surroundings – making us increasingly sensitised to both true and illusory signs of agency. In combination, these results reveal a cognitive and computational mechanism that allows public communication about what we can and cannot influence to reshape our private sense of control.},
 authors = {['George Blackburne', 'Chris D. Frith', 'Daniel Yon']},
 journal = {Cognition},
 keywords = {['Agency', 'Control', 'Expectation', 'Prediction', 'Communication']},
 title = {Communicated priors tune the perception of control},
 year = {2025}
}

@Filtered Article{dc768ba4-6348-45e0-a7e6-de0e3ce34470,
 abstract = {The arising-from-chair task assessment is a key aspect of the evaluation of movement disorders in Parkinson's disease (PD). However, common scale-based clinical assessment methods are highly subjective and dependent on the neurologist's expertise. Alternate automated methods for arising-from-chair assessment can be established based on quantitative susceptibility mapping (QSM) images with multiple-instance learning. However, performance stability for such methods can be typically undermined by the presence of irrelevant or spuriously-relevant features that mask the intrinsic causal features. Therefore, we propose a QSM-based arising-from-chair assessment method using a causal graph-neural-network framework, where counterfactual and debiasing strategies are developed and integrated into this framework for capturing causal features. Specifically, the counterfactual strategy is proposed to suppress irrelevant features caused by background noise, by producing incorrect predictions when dropping causal parts. The debiasing strategy is proposed to suppress spuriously relevant features caused by the sampling bias and it comprises a resampling guidance scheme for selecting stable instances and a causal invariance constraint for improving stability under various interferences. The results of extensive experiments demonstrated the superiority of the proposed method in detecting arising-from-chair abnormalities. Its clinical feasibility was further confirmed by the coincidence between the selected causal features and those reported in earlier medical studies. Additionally, the proposed method was extensible for another motion task of leg agility. Overall, this study provides a potential tool for automated arising-from-chair assessment in PD patients, and also introduces causal counterfactual thinking in medical image analysis. Our source code is publicly available at https://github.com/SJTUBME-QianLab/CFGNN-PDarising.},
 authors = {['Xinlu Tang', 'Rui Guo', 'Chencheng Zhang', 'Xiaohua Qian']},
 journal = {Medical Image Analysis},
 keywords = {["Parkinson's disease", 'Arising-from-chair', 'Graph neural network', 'Causal inference', 'Counterfactual thinking']},
 title = {A causal counterfactual graph neural network for arising-from-chair abnormality detection in parkinsonians},
 year = {2024}
}

@Filtered Article{dca51eb1-c657-4924-9b71-8550792361f4,
 abstract = {Understanding the molecular and physical complexity of the tissue microenvironment (TiME) in the context of its spatiotemporal organization has remained an enduring challenge. Recent advances in engineering and data science are now promising the ability to study the structure, functions, and dynamics of the TiME in unprecedented detail; however, many advances still occur in silos that rarely integrate information to study the TiME in its full detail. This review provides an integrative overview of the engineering principles underlying chemical, optical, electrical, mechanical, and computational science to probe, sense, model, and fabricate the TiME. In individual sections, we first summarize the underlying principles, capabilities, and scope of emerging technologies, the breakthrough discoveries enabled by each technology and recent, promising innovations. We provide perspectives on the potential of these advances in answering critical questions about the TiME and its role in various disease and developmental processes. Finally, we present an integrative view that appreciates the major scientific and educational aspects in the study of the TiME.},
 authors = {['Rishyashring R. Iyer', 'Catherine C. Applegate', 'Opeyemi H. Arogundade', 'Sushant Bangru', 'Ian C. Berg', 'Bashar Emon', 'Marilyn Porras-Gomez', 'Pei-Hsuan Hsieh', 'Yoon Jeong', 'Yongdeok Kim', 'Hailey J. Knox', 'Amir Ostadi Moghaddam', 'Carlos A. Renteria', 'Craig Richard', 'Ashlie Santaliz-Casiano', 'Sourya Sengupta', 'Jason Wang', 'Samantha G. Zambuto', 'Maria A. Zeballos', 'Marcia Pool', 'Rohit Bhargava', 'H. Rex Gaskins']},
 journal = {Heliyon},
 keywords = {['Bioengineering', 'Interdisciplinary research', 'Bioimaging', 'Biomaterials', 'Biosensing', 'Computational biology', 'Biomedical devices', 'Biotechnology']},
 title = {Inspiring a convergent engineering approach to measure and model the tissue microenvironment},
 year = {2024}
}

@Filtered Article{dd0f7f64-f6fa-4640-a9d5-4c5ee5b159d9,
 abstract = {In this paper, we demonstrate how studying the rhetorics of ChatGPT prompt writing on social media can promote critical AI literacies. Prompt writing is the process of writing instructions for generative AI tools like ChatGPT to elicit desired outputs and there has been an upsurge of conversations about it on social media. To study this rhetorical activity, we build on four overlapping traditions of digital writing research in computers and composition that inform how we frame literacies, how we study social media rhetorics, how we engage iteratively and reflexively with methodologies and technologies, and how we blend computational methods with qualitative methods. Drawing on these four traditions, our paper shows our iterative research process through which we gathered and analyzed a dataset of 32,000 posts (formerly known as tweets) from X (formerly Twitter) about prompt writing posted between November 2022 to May 2023. We present five themes about these emerging AI literacy practices: (1) areas of communication impacted by prompt writing, (2) micro-literacy resources shared for prompt writing, (3) market rhetoric shaping prompt writing, (4) rhetorical characteristics of prompts, and (5) definitions of prompt writing. In discussing these themes and our methodologies, we highlight takeaways for digital writing teachers and researchers who are teaching and analyzing critical AI literacies.},
 authors = {['Anuj Gupta', 'Ann Shivers-McNair']},
 journal = {Computers and Composition},
 keywords = {['AI', 'ChatGPT', 'Prompt writing', 'Prompt engineering', 'Machine learning', 'Computational methods', 'Algorithms', 'Critical AI literacy', 'Digital rhetoric']},
 title = {“Wayfinding” through the AI wilderness: Mapping rhetorics of ChatGPT prompt writing on X (formerly Twitter) to promote critical AI literacies},
 year = {2024}
}

@Filtered Article{dd86d49b-0f74-4ecb-a4a1-0ebd75109a97,
 abstract = {The current view posits that objects, despite changes in appearance, are uniquely encoded by ‘expert’ cells. This view is untenable. First, even if cell ensemble responses are invariant and unique, we are consciously aware of all of the objects’ details. Second, in addition to detail preservation, data show that the current hypothesis fails to account for uniqueness and invariance. I present an alternative view whereby objects’ representation and recognition are based on parallel representation of space by primary visual cortex (V1) responses. Information necessary for invariance and other attributes is handled in series by other cortical areas through integration, interpolation, and hierarchical convergence. The parallel and serial mechanisms combine to enable our flexible space perception. Only in this alternative view is conscious perception consistent with the underlying architecture.},
 authors = {['Moshe Gur']},
 journal = {Trends in Neurosciences},
 keywords = {['vision', 'object representation', 'recognition', 'conscious perception', 'parallel processing']},
 title = {Space reconstruction by primary visual cortex activity: a parallel, non-computational mechanism of object representation},
 year = {2015}
}

@Filtered Article{dd9b5592-4bce-4ab2-9518-ee9864e5810c,
 abstract = {It is important to assess the cumulative effects of technology on student achievement captured in the last 30 years of technologyenhanced mathematics instruction. Synthesizing the thousands of articles and gray literature on this subject is necessary but would require a considerable commitment of academic resources. A second-order metaanalysis or meta-analysis of meta-analyses is an alternative that is reasonable and effective. Thus, a second-order meta-analysis of 19 prior meta-analyses with minimum overlap between primary studies was conducted. The results represent 663 primary studies (approximately 141,733 participants) and 1,263 effect sizes. The random effects' mean effect size of .38 was statistically significantly different from zero. The results provide a historical and contextualized summary of 30 years of meta-analytic research, which supports meta-analytic thinking and better interpretation of future effect sizes. Results indicate that technology function and study quality are major contributors to effect size variation. Specifically, computation enhancement technologies were most effective, while studies that examine combinations of enhancements were least effective. Implications for technology-enhanced mathematics instruction and meta-analytic research are provided.},
 authors = {['Jamaal Young']},
 journal = {Educational Research Review},
 keywords = {['Meta-analysis', 'Mathematics achievement', 'Technology', 'Calculators', 'Computer-assisted instruction']},
 title = {Technology-enhanced mathematics instruction: A second-order meta-analysis of 30 years of research},
 year = {2017}
}

@Filtered Article{de6cf2fe-3fee-488a-a244-14657ac5bdf0,
 abstract = {Background
As COVID-19 poses different levels of threat to people of different ages, health communication regarding prevention measures such as social distancing and isolation may be strengthened by understanding the unique experiences of various age groups.
Objective
The aim of this study was to examine how people of different ages (1) experienced the impact of the COVID-19 pandemic and (2) their respective rates and reasons for compliance or noncompliance with social distancing and isolation health guidance.
Methods
We fielded a survey on social media early in the pandemic to examine the emotional impact of COVID-19 and individuals’ rates and reasons for noncompliance with public health guidance, using computational and content analytic methods of linguistic analysis.
Results
A total of 17,287 participants were surveyed. The majority (n=13,183, 76.3%) were from the United States. Younger (18-31 years), middle-aged (32-44 years and 45-64 years), and older (≥65 years) individuals significantly varied in how they described the impact of COVID-19 on their lives, including their emotional experience, self-focused attention, and topical concerns. Younger individuals were more emotionally negative and self-focused, while middle-aged people were other-focused and concerned with family. The oldest and most at-risk group was most concerned with health-related terms but were lower in anxiety (use of fewer anxiety-related terms) and higher in the use of emotionally positive terms than the other less at-risk age groups. While all groups discussed topics such as acquiring essential supplies, they differentially experienced the impact of school closures and limited social interactions. We also found relatively high rates of noncompliance with COVID-19 prevention measures, such as social distancing and self-isolation, with younger people being more likely to be noncompliant than older people (P<.001). Among the 43.1% (n=7456) of respondents who did not fully comply with health orders, people differed substantially in the reasons they gave for noncompliance. The most common reason for noncompliance was not being able to afford to miss work (n=4273, 57.3%). While work obligations proved challenging for participants across ages, younger people struggled more to find adequate space to self-isolate and manage their mental and physical health; middle-aged people had more concerns regarding childcare; and older people perceived themselves as being able to take sufficient precautions.
Conclusions
Analysis of natural language can provide insight into rapidly developing public health challenges like the COVID-19 pandemic, uncovering individual differences in emotional experiences and health-related behaviors. In this case, our analyses revealed significant differences between different age groups in feelings about and responses to public health orders aimed to mitigate the spread of COVID-19. To improve public compliance with health orders as the pandemic continues, health communication strategies could be made more effective by being tailored to these age-related differences.},
 authors = {['Ryan C Moore', 'Angela Y Lee', 'Jeffrey T Hancock', 'Meghan C Halley', 'Eleni Linos']},
 journal = {JMIR Human Factors},
 keywords = {['COVID-19', 'natural language processing', 'public health messaging', 'social distancing compliance', 'age differences', 'older adults', 'younger adults', 'age', 'NLP', 'public health', 'elderly', 'youth', 'adult', 'emotion', 'compliance', 'guideline']},
 title = {Age-Related Differences in Experiences With Social Distancing at the Onset of the COVID-19 Pandemic: A Computational and Content Analytic Investigation of Natural Language From a Social Media Survey},
 year = {2021}
}

@Filtered Article{de6e0dcb-a1c0-4357-9060-f7cc94aede04,
 abstract = {A framework is proposed for facilitating the exploration process during the early design phase through computational sketch synthesis and interactive 3D reconstruction. In that phase, designers concentrate on developing concepts through numerous alternatives. Therefore, they constantly sketch so that they can rapidly visualize their ideas. Recently, the design industry has attempted to streamline the design process by implementing 3D model generation in the early design phase so that ideas may be more thoroughly explored, thus improving concept and final design conformance; however, efficiency issues have arisen. In this study, a 3D computational sketch synthesis framework was developed comprising two major components. First, a robust method was proposed to synthesize design alternatives by interpolating an input sketch with sketches in a database so that unvisited combinations may be explored. Secondly, a novel interactive 3D model reconstruction method was developed to facilitate the shape transition of design elements so that designers can quickly evaluate the potential of a large number of design variations. Finally, an interface for design refinement was developed so that designs may be embodied by sketching over the 3D model. To test the proposed methodology, expert designers were recruited for a validation experiment with two conditions followed up by in-depth interviews. In the first condition, the participants were asked to sketch based on a design brief in their current working manner. In the second condition, they were asked to create designs using the proposed framework. It was tested whether there was a difference in the design outcomes. It was demonstrated that the proposed framework resulted in more satisfactory and higher-quality designs and generated design alternatives faster and in greater quantities. All participants agreed that the framework could be useful in the early design phase and responded that the proposed system provides more design inspiration than traditional design methods. Most importantly, it was demonstrated that the proposed framework could enhance the reevaluation potential of design concepts and assist in making better-informed design decisions.},
 authors = {['Seonghoon Ban', 'Kyung Hoon Hyun']},
 journal = {Computer-Aided Design},
 keywords = {['Intelligent design system', 'Assisted creativity', 'Sketch-based modeling', 'Computational design', 'Virtual reality']},
 title = {3D Computational Sketch Synthesis Framework: Assisting Design Exploration Through Generating Variations of User Input Sketch and Interactive 3D Model Reconstruction},
 year = {2020}
}

@Filtered Article{de80b48d-c486-4a06-a9de-30882d8476a3,
 abstract = {While research has shown that argument based systems (ABSs) can be used to improve aspects of individual thinking and learning, relatively few studies have shown that ABSs improve decision performance in real world tasks. In this article, we strive to improve the value-proposition of ABSs for decision makers by showing that individuals can, with minimal training, use a novel ABS called Pendo to improve their ability to predict housing market trends. Pendo helps to weight and aggregate evidence through a computational engine to support evidence-based reasoning, a well-documented deficiency in human decision-making. It also supports individuals in the creation of knowledge artifacts that can be used to solve similar problems in the same domain. An unexpected finding and one of the major contributions of this work is that individual unaided decision-making performance was not predictive of an individual's performance with Pendo, even though the average performance of assisted individuals was higher. We infer that the skills activated when using the tool are substantially different than those enacted to solve the same problem without that tool. We discuss the implications this result has for the design and application of ABSs to decision-making, and possibly other decision support technologies.},
 authors = {['Joshua Introne', 'Luca Iandoli']},
 journal = {Decision Support Systems},
 keywords = {['Computer-supported argumentation', 'Evidence-based reasoning', 'Dempster–Shafer belief aggregation', 'Housing market prediction']},
 title = {Improving decision-making performance through argumentation: An argument-based decision support system to compute with evidence},
 year = {2014}
}

@Filtered Article{df145b92-85d1-4556-9601-91f038636b98,
 abstract = {Artificial intelligence (AI) remains a relatively unfamiliar concept for many, but its significance in the biomedical field is gaining recognition as the world undergoes transformative changes. Furthermore, AI possesses the potential to emulate critical thinking, reasoning, problem-solving abilities, and logical capacities of machines. Additionally, in the realm of gut microbiota research, AI emerges as a valuable asset. The synergy between gut microbes and AI not only holds promise for treating diverse gastroenterological diseases but also aids in comprehending the intricate relationships between gut microbes and microbes of resides into the other body parts. Moreover, AI facilitates a deeper understanding of different facets within gut-microbes interaction research. These direct communications are governed by chemical messengers, hormones, and neurotransmitters, detectable through biosensor chips employing machine learning (ML). Additionally, the indirect regulation of gut function by the brain via the hypothalamic-pituitary-adrenal (HPA) axis can be analysed using different computational models. This promising prospect remains largely unexplored, and in this chapter, our aim is to delve into and harness the potential of AI in gut microbial research.},
 authors = {['Vaibhav Mishra', 'Chhavi Atri', 'Raj Pandey', 'Akanksha Srivastava']},
 journal = {Academic Press},
 keywords = {['Artificial intelligence', 'Gut microbes', 'Microbiology', 'Gastroenterology', 'Machine learning', 'Deep learning']},
 title = {Chapter Twelve - Unravelling the gut microbiome: Connecting with AI for deeper insights},
 year = {2024}
}

@Filtered Article{df56163a-a825-4d92-805e-74dc76a628bb,
 abstract = {Squamous cell carcinoma of the lung is remarkable for the extent to which the same chromosomal abnormalities are detected in individual tumours. We have used next generation sequencing at low coverage to produce high resolution copy number karyograms of a series of 89 non-small cell lung tumours specifically of the squamous cell subtype. Because this methodology is able to create karyograms from formalin-fixed paraffin-embedded material, we were able to use archival stored samples for which survival data were available and correlate frequently occurring copy number changes with disease outcome. No single region of genomic change showed significant correlation with survival. However, adopting a whole-genome approach, we devised an algorithm that relates to total genomic damage, specifically the relative ratios of copy number states across the genome. This algorithm generated a novel index, which is an independent prognostic indicator in early stage squamous cell carcinoma of the lung.},
 authors = {['Ornella Belvedere', 'Stefano Berri', 'Rebecca Chalkley', 'Caroline Conway', 'Fabio Barbone', 'Federica Pisa', 'Kenneth MacLennan', 'Catherine Daly', 'Melissa Alsop', 'Joanne Morgan', 'Jessica Menis', 'Peter Tcherveniakov', 'Kostas Papagiannopoulos', 'Pamela Rabbitts', 'Henry M. Wood']},
 journal = {Genomics},
 keywords = {['Lung cancer', 'Copy number', 'Survival', 'Next-generation sequencing']},
 title = {A computational index derived from whole-genome copy number analysis is a novel tool for prognosis in early stage lung squamous cell carcinoma},
 year = {2012}
}

@Filtered Article{df9e07a8-98ba-4f84-a459-d90f64560597,
 abstract = {Underground construction work is heavily affected by surrounding hydrogeology, adjacent pipelines, and existing subway lines, which can lead to a high degree of uncertainty and generate safety risk on site. In order to overcome rigid thinking of causal factors within a structured framework and incorporate features of different accidents, this study adopted grounded theory for the investigation on factors contributing to workplace accidents in subway construction. The deep reinforcement learning model of double deep Q-network (DDQN) was developed for predicting subway construction safety risk, which integrated the advantage of reinforcement learning in decision making with the advantage of deep learning in objection perception. The findings denoted that DDQN performed better than other machine learning models inclusive of random forest, extreme gradient boosting, k-nearest neighbor, and support vector machine. Contributing factors relevant to subway construction accidents were quantitatively analyzed using permutation importance of attributes. It was beneficial for determining how the 37 contributing factors had negative effects on subway construction safety risk. Safety measures for risk reduction and controlling could be optimized according to permutation importance of individual contributing factor, which paved a new way for the promotion of safety management performance at subway construction sites.},
 authors = {['Zhipeng Zhou', 'Wen Zhuo', 'Jianqiang Cui', 'Haiying Luan', 'Yudi Chen', 'Dong Lin']},
 journal = {Reliability Engineering & System Safety},
 keywords = {['Subway construction safety', 'Grounded theory', 'Deep reinforcement learning', 'Double deep Q-network', 'Permutation importance']},
 title = {Developing a deep reinforcement learning model for safety risk prediction at subway construction sites},
 year = {2025}
}

@Filtered Article{dfa1ecdc-d305-4ab1-acf5-17a0b0bc86eb,
 abstract = {There are two fundamental models to understanding the phenomenon of natural life. One is the computational model, which is based on the symbolic thinking paradigm. The other is the biological organism model. The common difficulty attributed to these paradigms is that their reductive tools allow the phenomenological aspects of experience to remain hidden behind yes/no responses (behavioral tests), or brain ‘pictures’ (neuroimaging). Hence, one of the problems regards how to overcome methodological difficulties towards a non-reductive investigation of conscious experience. It is our aim in this paper to show how cooperation between Eastern and Western traditions may shed light for a non-reductive study of mind and life. This study focuses on the first-person experience associated with cognitive and mental events. We studied phenomenal data as a crucial fact for the domain of living beings, which, we expect, can provide the ground for a subsequent third-person study. The intervention with Jhana meditation, and its qualitative assessment, provided us with experiential profiles based upon subjects' evaluations of their own conscious experiences. The overall results should move towards an integrated or global perspective on mind where neither experience nor external mechanisms have the final word.},
 authors = {['Inês Hipólito', 'Jorge Martins']},
 journal = {Progress in Biophysics and Molecular Biology},
 keywords = {['Conscious experience', 'Qualitative study', 'Meditation', '', 'Mind-life continuity thesis']},
 title = {Mind-life continuity: A qualitative study of conscious experience},
 year = {2017}
}

@Filtered Article{dfd79de5-a044-472d-b669-8a8ac7548218,
 abstract = {We introduce Virtual Leashing,11The techniques described in this paper are protected by U.S. patents, both granted and pending. a new technique for software protection and control. The leashing process removes small fragments of code, pervasive throughout the application, and places them on a secure server. The secure server provides the missing functionality, but never the missing code. Reverse engineering the missing code, even with full tracing of the program's execution and its communication with the server, is computationally hard. Moreover, the server provides the missing functionality asynchronously: the application's performance is independent (within reason) of the secure server's speed. For example, the server might reside on a slow inexpensive chip or a remote Internet server. Leashing makes only modest demands on communication bandwidth, space, and computation.},
 authors = {['Ori Dvir', 'Maurice Herlihy', 'Nir N. Shavit']},
 journal = {Journal of Parallel and Distributed Computing},
 keywords = {['Digital rights management', 'Virtual leashing']},
 title = {Virtual Leashing: Creating a computational foundation for software protection},
 year = {2006}
}

@Filtered Article{dfd8e2fb-fb6e-4e01-a74f-14a86915a29b,
 abstract = {Toxicology in the 21st Century has seen a shift from chemical risk assessment based on traditional animal tests, identifying apical endpoints and doses that are “safe”, to the prospect of Next Generation Risk Assessment based on non-animal methods. Increasingly, large and high throughput in vitro datasets are being generated and exploited to develop computational models. This is accompanied by an increased use of machine learning approaches in the model building process. A potential problem, however, is that such models, while robust and predictive, may still lack credibility from the perspective of the end-user. In this commentary, we argue that the science of causal inference and reasoning, as proposed by Judea Pearl, will facilitate the development, use and acceptance of quantitative AOP models. Our hope is that by importing established concepts of causality from outside the field of toxicology, we can be “constructively disruptive” to the current toxicological paradigm, using the “Causal Revolution” to bring about a “Toxicological Revolution” more rapidly.},
 authors = {['Nicoleta Spînu', 'Mark T.D. Cronin', 'Judith C. Madden', 'Andrew P. Worth']},
 journal = {Computational Toxicology},
 keywords = {['Model credibility', 'Adverse Outcome Pathway', 'qAOP', 'Causality', 'Next Generation Risk Assessment']},
 title = {A matter of trust: Learning lessons about causality will make qAOPs credible},
 year = {2022}
}

@Filtered Article{e0241ccb-1959-4a99-a64f-6f5425c70be5,
 abstract = {ABSTRACT
Background: Alzheimer's disease (AD) is characterized by multiple cognitive deficits and affects functional competency to perform daily activities (ADL). As this may contribute to the patient's overall disability, it is important to identify factors that compromise competency. Objective: The relationship between different cognitive domains and functional activities in AD was studied. Methods: The functional competency of 73 Japanese AD patients, most with mild dementia, was assessed using a 27-item relative/carer-rating scale covering 7 ADL: managing finances, using transportation, taking precautions, self-care, housekeeping, communication and taking medicine. Cognitive assessment used 16 neuropsychological tests from the Japanese version of the WAIS-R and COGNISTAT, covering 9 cognitive domains: orientation, attention, episodic memory, semantic memory, language, visuoperceptual and construction abilities, computational ability, abstract thinking, and psychomotor speed. Results: Multiple regression analysis by the stepwise method indicated that functional competency could, for the most part, be predicted from test scores for orientation, abstract thinking and psychomotor speed. Discussion: The results of this study suggest that impairment of these three cognitive domains plays an important role in the functional deterioration of AD.},
 authors = {['Osamu Matsuda', 'Masahiko Saito']},
 journal = {International Psychogeriatrics},
 keywords = {["Alzheimer's disease", 'cognitive deficits', 'functional competency']},
 title = {Functional competency and cognitive ability in mild Alzheimer's Disease: relationship between ADL assessed by a relative/ carer-rated scale and neuropsychological performance},
 year = {2005}
}

@Filtered Article{e0278a5b-5b1e-4c15-a5d4-d839b23fb77a,
 abstract = {This paper compares the longitudinal effect of instructional tasks on algebra learning that used a Standards-based curriculum [Connected Mathematics Project (CMP)] to that of classrooms that used a traditional curriculum (non-CMP). CMP was developed based on the National Council of Teachers of Mathematics (NCTM) Standards and can be characterized as a problem-based curriculum. CMP teachers were more than three times as likely to implement high-level instructional tasks than non-CMP teachers. Increases in the cognitive demand were associated with enhanced growth rates in problem solving, computation, and equation solving. Notably, when controlling for the cognitive demand of the instructional tasks, the advantage of the CMP curriculum over the non-CMP curricula on students’ growth in problem solving disappeared. However, non-CMP curricula had an advantage on students’ growth over the CMP curriculum after the cognitive demand of the instructional tasks was controlled.},
 authors = {['Jinfa Cai', 'John C. Moyer', 'Chuang Wang', 'Ning Wang', 'Bikai Nie']},
 journal = {International Journal of Educational Research},
 keywords = {['Longitudinal study', 'Problem solving', 'Instructional tasks', 'Mathematics learning', 'Curricular effect']},
 title = {Student learning and instructional tasks in different curricular contexts: A longitudinal study},
 year = {2024}
}

@Filtered Article{e08b2a29-3053-4fbf-9a1c-44febdcf9be0,
 abstract = {This article considers some of the connections between genetic algorithms (GAs)—search procedures based on the mechanics of natural selection and natural genetics—and human innovation. Simply stated, innovation has been a source of inspiration for thinking about genetic algorithms, and as the algorithms have improved, GAs have become increasingly interesting computational models of the processes of innovation. The article reviews the basics of genetic algorithm operation and connects the basic mechanics to two processes of innovation: continual improvement and discontinuous change. Thereafter, some of the technical lessons of genetic algorithm processing are reviewed and their implications are briefly explored in the context of organizational change.},
 authors = {['David E Goldberg']},
 journal = {Technological Forecasting and Social Change},
 title = {The Design of Innovation: Lessons from Genetic Algorithms, Lessons for the Real World},
 year = {2000}
}

@Filtered Article{e0a480ea-a12e-4255-a324-edffaf23a769,
 abstract = {Four principal factors contribute to grain-boundary strengthening: (a) the grain boundaries act as barriers to plastic flow; (b) the grain boundaries act as dislocation sources; (c) elastic anisotropy causes additional stresses in grain-boundary surroundings; (d) multislip is activated in the grain-boundary regions, whereas grain interiors are initially dominated by single slip, if properly oriented. As a result, the regions adjoining grain boundaries harden at a rate much higher than grain interiors. A phenomenological constitutive equation predicting the effect of grain size on the yield stress of metals is discussed and extended to the nanocrystalline regime. At large grain sizes, it has the Hall–Petch form, and in the nanocrystalline domain the slope gradually decreases until it asymptotically approaches the flow stress of the grain boundaries. The material is envisaged as a composite, comprised of the grain interior, with flow stress σfG, and grain boundary work-hardened layer, with flow stress σfGB. The predictions of this model are compared with experimental measurements over the mono, micro, and nanocrystalline domains. Computational predictions are made of plastic flow as a function of grain size incorporating differences of dislocation accumulation rate in grain-boundary regions and grain interiors. The material is modeled as a monocrystalline core surrounded by a mantle (grain-boundary region) with a high work hardening rate response. This is the first computational plasticity calculation that accounts for grain size effects in a physically-based manner. A discussion of statistically stored and geometrically necessary dislocations in the framework of strain-gradient plasticity is introduced to describe these effects. Grain-boundary sliding in the nanocrystalline regime is predicted from calculations using the Raj–Ashby model and incorporated into the computations; it is shown to predispose the material to shear localization.},
 authors = {['H.-H. Fu', 'D.J. Benson', 'M.A. Meyers']},
 journal = {Acta Materialia},
 keywords = {['Nanocrystalline materials', 'Grain size', 'Hall–Petch']},
 title = {Analytical and computational description of effect of grain size on yield stress of metals},
 year = {2001}
}

@Filtered Article{e0b4d9da-3d14-4225-95c8-79c5d0b98a66,
 abstract = {Leaders and their employees must navigate competing yet interrelated demands and processes when developing and implementing creative ideas. They have to engage in divergent and convergent thinking, challenge existing assumptions and accept them, plan and persist while remaining spontaneous and adaptive. We explore how, why, and when adopting a paradox approach to navigating such tensions enhances creativity and innovation. Rather than seeking to eliminate the discomfort associated with tensions by prioritizing one demand or process over the other, the paradox approach sees tensions as an opportunity for growth and learning. When adopting a paradox approach, people feel comfortable with the discomfort as tensions arise and recognize that by engaging in one process they enable the seemingly opposing process. We review research on paradoxical frames, mindset, and leadership, and offer a comprehensive theoretical model that delineates the related cognitive, affective, motivational, and social pathways, as well as contextual and cultural boundary conditions. We conclude by identifying promising future directions for research.},
 authors = {['Matthew Rubin', 'Ella Miron-Spektor', 'Joshua Keller']},
 journal = {Academic Press},
 keywords = {['Creativity', 'Innovation', 'Paradox', 'Mindset', 'Paradoxical leadership', 'Culture']},
 title = {Chapter 9 - Unlocking creative tensions with a paradox approach},
 year = {2023}
}

@Filtered Article{e0bd5650-3ab0-4aab-b44f-28beb1a88152,
 abstract = {We introduce a model of inductive inference, or learning, that extends the conventional Bayesian approach by explicitly considering the computational cost of formulating predictions to be tested. We view the learner as a scientist who must divide her time between doing experiments and deducing predictions from promising theories, and we wish to know how she can do so most effectively. We explore several approaches based on the cost of making a prediction relative to the cost of performing an experiment. The resulting strategies share many qualitative characteristics with "real" science. This model is significant for the following reasons: •It allows us to study how a scientist might go about acquiring knowledge in a world where (as in real life) both performing experiments and making predictions from theories require time and effort.•It lays the foundation for a rigorous machine-implementable notion of "subjective probability." Good (1959, , 443-447) argues persuasively that subjective probability is at the heart of probability theory. Previous treatments of subjective probability do not handle the complication that the learner′s subjective probabilities may change as the result of pure thinking; our model captures this and other effects in a realistic manner. In addition, we begin to answer the question of how to trade off versus -a question that is fundamental for computers that must exist in the world and learn from their experience.},
 authors = {['R.L. Rivest', 'R.H. Sloan']},
 journal = {Information and Computation},
 title = {On Choosing between Experimenting and Thinking when Learning},
 year = {1993}
}

@Filtered Article{e0ef3606-dfe8-4386-aa0f-f891add01a5a,
 abstract = {Background and aim:
In recent days, the research on student’s intelligence level modelling is a challenging Artificial Intelligence (AI) task, which gains more attraction because it provides actionable insights to the tutor by analysing the intelligence level of the learners. Each learner’s knowledge, comprehension, and intellectual capacities are unique. It is critical to identify these capacities and provide learners, particularly slow learners, with the necessary knowledge. Cognitive Performance Test (CPT) is an essential component for assessing the knowledge level of students. The reasoning level or coefficient deals with the analysis of the thinking capability in a logical way. It also reflects the child’s learning potential. The main aim of the proposed system is to design a Cognitive Knowledge Representation Model (CKRM), which fuses Cognitive Performance Metrics (CPM) calculation and Reasoning Coefficient Calculation (RCC) algorithms to assess the student’s intelligence level. The result of the proposed system is stratification of students to three different ranges of reasoning coefficient.
Methods:
The CKRM consists of the following phases: data collection, statistical Exploratory Data Analysis (EDA), model building and analysis, which involve the assessment of the knowledge level using CPT and calculation of reasoning coefficient using First Order Logic (FOL), and finally model evaluation using cognitive evaluation metrics. CPM and RCC algorithms have been proposed in this paper to calculate the student’s reasoning coefficient by using the forward chaining FOL inference engine. The dataset is a real time data which consists of the academic and cognitive performance details of school students from classes 1 to 6 for the year 2019 to 2020. The academic data are collected from the Educational Management Information System (EMIS) maintained by the school. The cognitive performance data are collected by conducting the tests for the students using the memory training application called Lumosity.
Results:
The proposed system’s performance is evaluated using ten Machine Learning (ML) algorithms in which the Quadratic Discriminant Analysis achieved an accuracy of 0.97 for classes 1, 2, and 3. For classes 4, 5, and 6, nearly twelve ML algorithms are evaluated in which Random Forest (RF) Classifier achieved an accuracy of 0.98. Six math expert committee teachers concluded that the reasoning coefficient value was acceptable with an average accuracy of 0.92 for classes 1, 2, 3 and 0.9 for classes 4, 5, 6. In comparison to the pre-existing models employed in the prior research, it was determined that the created CKRM (academic and cognitive) was superior. The cognitive metrics such as taskability, Response Time (RT), knowledge capacity and utilization has also been evaluated. The average values of taskability, RT, knowledge capacity and knowledge utilization are 0.85, 0.81, 0.55, and 0.44.
Conclusion:
The ultimate goal is to make customized teaching easier; hence, this article involves determining a student’s cognitive level by estimating their reasoning coefficient. The suggested approach analyses and categorizes students’ cognitive abilities, such as memory, reasoning, problem solving, thinking, and logical reasoning, using three different reasoning coefficients. This approach assists teachers in determining the degree of intelligence of their students.},
 authors = {['Srivani M.', 'Abirami Murugappan']},
 journal = {Expert Systems with Applications},
 keywords = {['Customized AI based teaching', 'Cognitive performance test', 'Reasoning coefficient', 'Cognition level', 'Knowledge representation', 'Cognitive metrics']},
 title = {Design of a Cognitive Knowledge Representation Model to Assess the Reasoning Levels of Primary School Children},
 year = {2023}
}

@Filtered Article{e0f1350a-a807-4133-b289-7c78d49fe945,
 abstract = {A select history of dichlorocarbene chemistry between 1950 and 2010 will be presented. This is not a comprehensive review; rather, it is a personal perspective on the contributions of two respected colleagues, the reactive intermediate that spanned their research efforts, and their important contributions to organic synthesis and mechanistic thinking.},
 authors = {['Matthew S. Platz']},
 journal = {Journal of Physical Organic Chemistry},
 keywords = {['carbene', 'dichlorocarbene', 'Jack Hine', 'Robert Moss']},
 title = {Dichlorocarbene: From Jack Hine to Robert Moss},
 year = {2024}
}

@Filtered Article{e13d9f7e-e8e3-4607-81c3-0edef3aae0c9,
 abstract = {Leadership 4.0 focuses on leaders developing their digital transformation strategy and ensuring alignment with the organization’s business and development ambitions. This is accomplished by successfully displaying disruptive digital leadership characteristics, which include emotional and social intelligence abilities such as empathy and relationship management, cognitive preparedness, critical thinking, inventive thinking, agility, and resilience. Academics and consultants increasingly use Leadership 4.0 to describe the new leadership style required for the Fourth Industrial Revolution (Industry 4.0). It strategically addresses people’s concerns, which are crucial for the effective integration of Industry 4.0, and plays a significant and crucial role in integrating Industry 4.0 into modern workplaces. The primary purpose of this paper is to explore Leadership 4.0 and its needs. Several quality characteristics associated with Digital Leadership 4.0 are investigated, and two-dimensional style matrix presentations for Leadership 4.0 are briefed. Finally, this study identifies and addresses the role of Leadership 4.0 in upcoming industrial management systems. Because digital technologies now impact the entire business, advancing digital strategies requires strong leadership at all levels. With the increasing prevalence of digital transformation in the business sector and the intensification of the "battle for talent," organizations need to consider a more methodical approach to building a solid leadership pipeline with the capabilities required to lead in the digital era. They may place future leaders in positions that require them to go beyond their current competencies and skills to instruct and motivate them to promptly acquire new digital skills. In a new working setting, effectively managing the dynamic interactions between machines, technology, and people is essential for influential digital leaders. Leadership 4.0 is expected to foster an open and innovative culture that welcomes change and progress. This will encourage and inspire their teams to adapt to the ongoing changes in the market.},
 authors = {['Abid Haleem', 'Mohd Javaid', 'Ravi Pratap Singh']},
 journal = {Journal of Industrial Safety},
 keywords = {['Leadership 4.0', 'Industry 4.0', 'Industrial Safety', 'Technologies', 'Management']},
 title = {Perspective of leadership 4.0 in the era of fourth industrial revolution: A comprehensive view},
 year = {2024}
}

@Filtered Article{e148fd9f-8a6f-4643-8c4e-dbbab3b3b29c,
 abstract = {The proliferation of Artificial Intelligence (AI) in everyday life raises concerns for children, other marginalized groups, and the general public. As new AI implementations continue to emerge, it is crucial to enable children to engage critically with AI. Critical literacy objectives and practices can encourage children to question, critique, and transform the social, political, cultural, and ethical implications of AI. As an initial step towards critical AI education, we conducted a 10-year scoping review to identify publications reporting on activities that engage children, between the ages of 5 and 18, to address the critical implications of AI. Our review identifies a wide range of participants, content, and pedagogical approaches. Through framework synthesis guided by an established critical literacy model, we examine the critical literacy learning objectives embedded in the reported activities and propose a critical AI literacy framework. This paper outlines future opportunities for critical AI literacies in the field of child–computer interaction including inspiring new learning activities, encouraging inclusive perspectives, and supporting pragmatic curriculum integration.},
 authors = {['Annemiek Veldhuis', 'Priscilla Y. Lo', 'Sadhbh Kenny', 'Alissa N. Antle']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['Artificial intelligence', 'Critical literacy', 'AI ethics', 'AI literacy', 'Computational empowerment', 'Literature review']},
 title = {Critical Artificial Intelligence literacy: A scoping review and framework synthesis},
 year = {2025}
}

@Filtered Article{e17344c4-e422-46a0-aa11-aadf48003dbb,
 abstract = {To examine the impact of emotion on creative potential, experimental studies have typically focused on the impact of induced or spontaneous mood states on creative performance. In this report the relationship between the perceived pleasantness of tasks (using divergent thinking and story writing tasks) and creative performance was examined. Overall perceived pleasantness did not differ between tasks. However, results indicate that the perceived pleasantness of the story writing task increased during task completion whereas the perceived pleasantness of divergent thinking tasks remained stable during task performance. The number of generated ideas in a divergent thinking task (fluency) was significantly related to overall perceived pleasantness of the task.},
 authors = {['Franck Zenasni', 'Todd Lubart']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Creativity', 'Perceived pleasantness', 'Emotion', 'Story writing', 'Divergent thinking']},
 title = {Pleasantness of creative tasks and creative performance},
 year = {2011}
}

@Filtered Article{e1968f0a-afcd-49e7-b7e4-06d65758b7c2,
 abstract = {Debates about freedom of will and action and their connections with moral responsibility have raged for centuries, but the opposing sides might disagree because they use different concepts of freedom. Based on previous work, we hypothesized that people who assert freedom in a determined (D) or counterfactual-intervener (CI) scenario assert this because they are thinking about freedom from constraint and not about freedom from determination (in D) or from inevitability (in CI). We also hypothesized that people who deny that freedom in D or in CI deny this because they are thinking about freedom from determination or from inevitability, respectively, and not about freedom from constraint. To test our hypotheses, we conducted two main online studies. Study I supported our hypotheses that people who deny freedom in D and CI are thinking about freedom from determinism and from inevitability, respectively, but these participants seemed to think about freedom from constraint when they were later considering modified scenarios where acts were not determined or inevitable. Study II investigated a contrary bypassing hypothesis that those who deny freedom in D denied this because they took determinism to exclude mental causation and hence to exclude freedom from constraint. We found that participants who took determinism to exclude freedom generally did not deny causation by mental states, here represented by desires and decisions. Their responses regarding causation by desires and decisions at most weakly mediated the relation between determinism and freedom or responsibility among this subgroup of our participants. These results speak against the bypassing hypothesis and in favor of our hypothesis that these participants were not thinking about freedom from constraint.},
 authors = {['Claire Simmons', 'Paul Rehren', 'John-Dylan Haynes', 'Walter Sinnott-Armstrong']},
 journal = {Consciousness and Cognition},
 title = {Freedom from what? Separating lay concepts of freedom},
 year = {2022}
}

@Filtered Article{e241be93-eab7-45e0-8a25-b89b8ccf86ef,
 abstract = {Based on an idealized model with six distinguishing criteria of geodesign projects -- large areas, complex issues, and multi-person teams; digital computing, algorithmic processes, and communications technologies; collaborative, information-based projects; timely feedback about impacts and implications of proposals; dynamic modeling and simulation; and systems thinking -- the technological supports required for each of these criteria are described.},
 authors = {['Stephen M. Ervin']},
 journal = {Landscape and Urban Planning},
 keywords = {['Geodesign', 'Algorithmic processes', 'Collaboration', 'Dynamic modeling', 'Simulation', 'Systems thinking']},
 title = {Technology in geodesign},
 year = {2016}
}

@Filtered Article{e2772450-aead-49ce-9e03-e0c4a828e12e,
 abstract = {Developing creative Promote higher-order thinking processes Give learners specific ability to think on their wide variety and innovative of the original. It led to the discovery and creation of new inventions or form new ideas. Consistent with the educational goals of the program.This research aim to study a guild line of using Scratch Computer Program that leading to creativity. And study the effects of media on the Scratch programming capabilities creativity. The sample consisted of 60 students who were studying in semester 1. 2013 academic year, using purposive sampling (Purposive Sampling) tool used in this research is a lesson plan. Scratch and computer media test innovative ideas. Statistics used Data analysis were percentage, mean, standard deviation and Dependent t-test. The findings indicated that First, Mediums Scratch program can be used as a medium for learning activities. The adoption includes a multimedia interactive media as a tool to support learning. Second, Scratch media performance of computer programs is equal according to the criteria set 82.46/82.25 E1/E2 is 80/80. Creativity of students. Received instruction from the learning activities through the medium of a computer program Scratch by elements of creativity is an idea ingenious ideas flexibility. Initiatives and ideas census. Higher posttest than pretest statistically significant at the .05 level of performance, computer media Scratch equals 82.46/82.25 according to defined criteria E1/E2 is 80 /80. In conclusion the computer program Scratch media can lead creative development of students through the learning activities that promote innovative education that cause the learners’ desirable.},
 authors = {['Worarit Kobsiripat']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Scratch programming', 'Higher-order Thinking', 'Creative Thinking', 'Computer Multimedia;']},
 title = {Effects of the Media to Promote the Scratch Programming Capabilities Creativity of Elementary School Students},
 year = {2015}
}

@Filtered Article{e283ce8c-3bce-42f0-8c01-9194f08e0871,
 abstract = {Drug discovery, a multifaceted process from compound identification to regulatory approval, historically plagued by inefficiencies and time lags due to limited data utilization, now faces urgent demands for accelerated lead compound identification. Innovations in biological data and computational chemistry have spurred a shift from trial-and-error methods to holistic approaches to medicinal chemistry. Computational techniques, particularly artificial intelligence (AI), notably machine learning (ML) and deep learning (DL), have revolutionized drug development, enhancing data analysis and predictive modeling. Natural products (NPs) have long served as rich sources of biologically active compounds, with many successful drugs originating from them. Advances in information science expanded NP-related databases, enabling deeper exploration with AI. Integrating AI into NP drug discovery promises accelerated discoveries, leveraging AI’s analytical prowess, including generative AI for data synthesis. This perspective illuminates AI’s current landscape in NP drug discovery, addressing strengths, limitations, and future trajectories to advance this vital research domain.},
 authors = {['Amit Gangwal', 'Antonio Lavecchia']},
 journal = {Journal of Medicinal Chemistry},
 title = {Artificial Intelligence in Natural Product Drug Discovery: Current Applications and Future Perspectives},
 year = {2025}
}

@Filtered Article{e2ca1f62-4651-422b-b637-bc7455939d3a,
 abstract = {The activity of collections of synchronizing neurons can be represented by weakly coupled nonlinear phase oscillators satisfying Kuramoto’s equations. In this article, we build such neural-oscillator models, partly based on neurophysiological evidence, to represent approximately the learning behavior predicted and confirmed in three experiments by well-known stochastic learning models of behavioral stimulus–response theory. We use three Kuramoto oscillators to model a continuum of responses, and we provide detailed numerical simulations and analysis of the three-oscillator Kuramoto problem, including an analysis of the stability points for different coupling conditions. We show that the oscillator simulation data are well-matched to the behavioral data of the three experiments.},
 authors = {['P. Suppes', 'J. Acacio {de Barros}', 'G. Oas']},
 journal = {Journal of Mathematical Psychology},
 keywords = {['Learning', 'Neural oscillators', 'Three-oscillator Kuramoto model', 'Stability points of the Kuramoto model', 'Stimulus–response theory', 'Phase representation', 'Continuum of responses']},
 title = {Phase-oscillator computations as neural models of stimulus–response conditioning and response selection},
 year = {2012}
}

@Filtered Article{e2ca61bc-ffbd-4e8b-8121-f00fe3d5b9f5,
 abstract = {Problem posing is increasingly being considered in the field of education, with many experts exploring its positive effects on student learning outcomes. In this case, different perspectives have emerged regarding the impact of the intervention, claiming the overall effect remains uncertain. Therefore, this study aims to explore the effects of a problem posing instructional intervention on student learning outcomes at the cognitive and non-cognitive levels from 2000 to 2023, using a three-level meta-analysis. 32 studies and 4,068 participants were included to compare the classrooms with and without problem posing instructional interventions in elementary to higher education. The results showed a moderate-positive and small positive effect on students cognitive (Hedges' g = 0.681, 95 % CI [0.552, 0.810], p < 0.001) and non-cognitive (Hedges' g = 0.367, 95 % CI [0.113, 0.620], p = 0.003) levels, respectively. Based on the moderator analysis, there were differences in the learning outcomes among students across various task formats. Notably, tasks that included specific information and involved problem posing in context demonstrated significantly better performance. In conclusion, these results indicate the importance of problem posing instructional interventions in promoting student's development and their impact on cognitive and non-cognitive dimensions.},
 authors = {['Cheng Zhang', 'Ying Zhou', 'Tommy Tanu Wijaya', 'Jihe Chen', 'Yimin Ning']},
 journal = {Thinking Skills and Creativity},
 keywords = {['Problem posing', 'Learning outcomes', 'Three-level meta-analysis', 'Instructional interventions']},
 title = {Effects of a problem posing instructional interventions on student learning outcomes: A three-level meta-analysis},
 year = {2024}
}

@Filtered Article{e2d0fda0-e718-4cef-ba50-8acd39f5c34a,
 abstract = {Background
Schizophrenia is a complex neuropsychiatric disorder characterized by positive symptoms, negative symptoms, cognitive deficits, and co-occurring mood symptoms. Network analysis offers a novel approach to investigate the intricate relationships between these symptom dimensions, potentially informing personalized treatment strategies.
Methods
A cross-sectional study was conducted from November 2019 to October 2021, involving 1285 inpatients with schizophrenia in Liaoning Province, China. Symptom severity was assessed using the Positive and Negative Syndrome Scale (PANSS), Hamilton Depression Rating Scale (HAMD-17), Hamilton Anxiety Rating Scale (HAMA-14), and Montreal Cognitive Assessment (MoCA). Network analysis was conducted to investigate the network structure, central symptoms, and bridge symptoms.
Results
The network analysis uncovered profound interconnectivity between core symptoms and the anxiety-depression community. Central symptoms, such as psychic anxiety, poor rapport, delusions, and attention, were identified as potential therapeutic targets. Bridge symptoms, including insomnia, depressed mood, anxiety-somatic, conceptual disorganization, and stereotyped thinking, emerged as key nodes facilitating interactions between symptom communities. The stability and reliability of the networks were confirmed through bootstrapping procedures.
Discussion
The findings highlight the complex interplay between schizophrenia symptoms, emphasizing the importance of targeting affective symptoms and cognitive impairment in treatment. The identification of central and bridge symptoms suggests potential pathways for personalized interventions aimed at disrupting self-reinforcing symptom cycles. The study underscores the need for a transdiagnostic, personalized approach to schizophrenia treatment.},
 authors = {['Yucheng Wang', 'Yixiao Xu', 'Peiyi Wu', 'Yang Zhou', 'Huanrui Zhang', 'Zijia Li', 'Yanqing Tang']},
 journal = {Schizophrenia Research},
 keywords = {['Schizophrenia', 'Core symptoms', 'Mood symptoms', 'Network analysis', 'Symptom interactions']},
 title = {Exploring the interplay between core and mood symptoms in schizophrenia: A network analysis},
 year = {2024}
}

@Filtered Article{e3140c6a-fb1d-4fe1-8652-9ef3d7201e22,
 abstract = {Cell-free synthetic biology is emerging as a powerful approach aimed to understand, harness, and expand the capabilities of natural biological systems without using intact cells. Cell-free systems bypass cell walls and remove genetic regulation to enable direct access to the inner workings of the cell. The unprecedented level of control and freedom of design, relative to in vivo systems, has inspired the rapid development of engineering foundations for cell-free systems in recent years. These efforts have led to programmed circuits, spatially organized pathways, co-activated catalytic ensembles, rational optimization of synthetic multi-enzyme pathways, and linear scalability from the micro-liter to the 100-liter scale. It is now clear that cell-free systems offer a versatile test-bed for understanding why nature's designs work the way they do and also for enabling biosynthetic routes to novel chemicals, sustainable fuels, and new classes of tunable materials. While challenges remain, the emergence of cell-free systems is poised to open the way to novel products that until now have been impractical, if not impossible, to produce by other means.},
 authors = {['C. Eric Hodgman', 'Michael C. Jewett']},
 journal = {Metabolic Engineering},
 keywords = {['Cell-free biology', 'protein synthesis', 'Metabolic engineering', 'Synthetic biology', 'Synthetic enzymatic pathways', 'Biocatalysis']},
 title = {Cell-free synthetic biology: Thinking outside the cell},
 year = {2012}
}

@Filtered Article{e3647b1f-69b9-4dec-bdf8-1125453a7790,
 abstract = {Michael Polanyi’s idea of tacit knowing and Martin Heidegger’s concept of pre-theoretical shared practice are presented as providing a strong rationale for the notion of practice based knowledge. Artificial Intelligence (AI) approaches such as Artificial Neural Networks (ANN), Case Based Reasoning (CBR) and Grounded Theory (with Interval Probability Theory) are able to model these philosophical concepts related to practice based knowledge. The AI techniques appropriate for modeling Polanyi’s and Heidegger’s ideas should be founded more on a connectionist rather than a cognitivist paradigm. Examples from engineering practice are used to demonstrate how the above techniques can capture, structure and make available such knowledge to practitioners.},
 authors = {['W.P.S. Dias']},
 journal = {Knowledge-Based Systems},
 keywords = {['Practice based knowledge', 'Connectionist AI techniques', 'Tacit knowing', 'Shared practice']},
 title = {Philosophical grounding and computational formalization for practice based engineering knowledge},
 year = {2007}
}

@Filtered Article{e39ad62e-6a11-4260-ac02-9ed5f4e3c96c,
 abstract = {This paper studies the role of cognition and affect in decision-making as well as notions of Type 1 and 2 processes and behaviors typically used in dual process theories. In order to demonstrate that there is no 1:1 correspondence between types of observed behavior and internal processes causing them, and that Type 1 and Type 2 processes can be produced by a single system, we implemented a computational model integrating affective and cognitive processing. Our model is based on the model of Marinier, Laird, and Lewis (2009). We modified it by increasing the agent’s visual field, adding a GOFAI-style cognitive module (sub-goal management) and expanding the environment by a high-threat tile, to which the agent responds with a hard-wired automatic reaction. This allowed us to generate and observe different types of behavior and study interesting interactions between cognitive and affective control. By comparing our re-implementation to the modified agent, we demonstrated clear cases of Type 1 (fast, automatic) and Type 2 (slow, deliberative) behavior, providing further evidence for the “single-system, two processes” hypothesis.},
 authors = {['Michael Anton Palkovics', 'Martin Takáč']},
 journal = {Cognitive Systems Research},
 keywords = {['Affective computing', 'Dual process theory', 'Decision-making']},
 title = {Exploration of cognition–affect and Type 1–Type 2 dichotomies in a computational model of decision making},
 year = {2016}
}

@Filtered Article{e3e49268-000e-40e1-8f43-1e925c407552,
 abstract = {Facebook and Instagram are currently the most popular Social Network Sites (SNS) for young adults. A large amount of research examined the relationship between these SNS and well-being, and possible intermediate constructs such as social comparison, self-esteem, and repetitive negative thinking (RNT). However, most of these studies have cross-sectional designs and use self-report indicators of SNS use. Therefore, their conclusions should be interpreted cautiously. Consequently, the goal of the current experience sampling study was to examine the temporal dynamics between objective indicators of SNS use, and self-reports of social comparison, RNT, and daily fluctuations in negative affect. More specifically, we assessed 98 participants 6 times per day during 14 days to examine reciprocal relationships between SNS use, negative affect, emotion regulation, and key psychological constructs. Results indicate that (1) both Facebook and Instagram use predicted reduced well-being, and (2) self-esteem and RNT appear to be important intermediate constructs in these relationships. Future longitudinal and experimental studies are needed to further support and extend the current research findings.},
 authors = {['Lien Faelens', 'Kristof Hoorelbeke', 'Bart Soenens', 'Kyle {Van Gaeveren}', 'Lieven {De Marez}', 'Rudi {De Raedt}', 'Ernst H.W. Koster']},
 journal = {Computers in Human Behavior},
 keywords = {['Social media', 'Social comparison', 'Self-esteem', 'Repetitive negative thinking', 'Negative affect']},
 title = {Social media use and well-being: A prospective experience-sampling study},
 year = {2021}
}

@Filtered Article{e41e86ae-52f2-4ccf-a0ae-bf796b6f911e,
 abstract = {Abstract dialectical frameworks (ADFs) have recently been proposed as a versatile generalization of Dung's abstract argumentation frameworks (AFs). In this paper, we present a comprehensive analysis of the computational complexity of ADFs. Our results show that while ADFs are one level up in the polynomial hierarchy compared to AFs, there is a useful subclass of ADFs which is as complex as AFs while arguably offering more modeling capacities. As a technical vehicle, we employ the approximation fixpoint theory of Denecker, Marek and Truszczyński, thus showing that it is also a useful tool for complexity analysis of operator-based semantics.},
 authors = {['Hannes Strass', 'Johannes Peter Wallner']},
 journal = {Artificial Intelligence},
 keywords = {['Abstract dialectical frameworks', 'Computational complexity', 'Approximation fixpoint theory']},
 title = {Analyzing the computational complexity of abstract dialectical frameworks via approximation fixpoint theory},
 year = {2015}
}

@Filtered Article{e4481bce-29cb-4b68-8e2a-880d636171f2,
 abstract = {The emergence of nontrivial embedded sensor units and cyber-physical systems and the Internet of Things has made possible the design and implementation of sophisticated applications where large amounts of real-time data are collected, possibly to constitute a big data picture as time passes. Within this framework, intelligence mechanisms based on machine learning, neural networks, and brain computing approaches play a key role to provide systems with advanced functionalities. Intelligent mechanisms are needed to guarantee appropriate performances within an evolving, time-variant environment, optimally harvest the available energy and manage the residual energy, reduce the energy consumption of the whole system, identify and mitigate occurrence of faults, and provide shields against cyberattacks. The chapter introduces the above aspects of intelligence, whose functionalities are needed to boost the next generation of cyber-physical and Internet of Things applications, and the smart world generation whose footprint is already around us.},
 authors = {['Cesare Alippi', 'Seiichi Ozawa']},
 journal = {Academic Press},
 keywords = {['Brain computing', 'Cyber-physical systems', 'Cybersecurity', 'Embedded systems', 'Neurodynamics', 'IoT', 'Machine learning', 'Neural networks']},
 title = {Chapter 12 - Computational Intelligence in the Time of Cyber-Physical Systems and the Internet of Things},
 year = {2019}
}

@Filtered Article{e4533735-b9f6-4fa3-b5b1-e26e573be870,
 abstract = {This paper explores that natural relationships between Pragmatic theory of knowing, the dynamic structuring of the mind and thinking suggested by connectionist theory, and the way information is distributed and organized through the world wide web (www). We suggest that these three “innovations” can be brought together to offer a better understanding of the way the human mind works. The internet and the information revolution may finally offer the opportunity to use and develop inductive learning practices and information based social inquiry in ways Pragmatic philosophers envisioned a hundred years ago, while the recent rise of connectionist and cognitive architecture works provides a concrete context for such developments. This confluence of process represents the type of synergy that only history can offer. The information revolution – exemplified by both the rise of connectionism and the internet – is the apotheosis of the Pragmatic revolution – bringing together radical empiricism and democratization of information in community practice. We offer three important realizations in our understanding of how information is organized and thinking progresses made possible by burgeoning virtual communities on the internet – open source thinking, scale-free networks, and interrelationships in the development of blogs to illustrate our thesis.},
 authors = {['Michael Glassman', 'Min Ju Kang']},
 journal = {Computers in Human Behavior},
 keywords = {['Internet', 'Dewey', 'Connectionism', 'Democracy']},
 title = {Pragmatism, connectionism and the internet: A mind’s perfect storm},
 year = {2010}
}

@Filtered Article{e46105ac-4ffa-4b3f-8fe5-d84c4d8e9cf2,
 abstract = {Current clinical trials are based on rigid designs and drug-centric approaches that can stifle flexibility and innovation. With advances in molecular biology and technology, there is an urgent call to revitalize trial designs to meet these evolving demands. We propose a reshaped, prismatic vision of clinical trials combining different knowledge layers, synergized with modern computational approaches. This paradigm based on iterative learning will enable a more adaptive and precise framework for oncology drug development.},
 authors = {['Grégoire Marret', 'Mercedes Herrera', 'Lillian L. Siu']},
 journal = {Cancer Cell},
 title = {Turning the kaleidoscope: Innovations shaping the future of clinical trial design},
 year = {2025}
}

@Filtered Article{e47697be-7733-434c-bb0c-7f66f8f44445,
 abstract = {While smart city initiatives have netted energy savings and carbon reductions, many cities and nations are still falling short of their sustainability targets and climate goals. This chapter draws on transitions concepts and behavior change theory to explore how smaller-scale and localized examples of existing and speculative urban technology projects that combine spatial design thinking and physical computing can help to anchor sustainability culture within everyday citizen's lives. It discusses the ethical significance of designing interactive urban technology projects that aim to nudge behavior change in relation to sustainability awareness.},
 authors = {['Nicole Gardner']},
 journal = {Elsevier},
 keywords = {['Behavior change', 'Cyber-physical system', 'Interaction', 'Physical computing', 'Persuasive technology', 'Nudge theory', 'Responsibilization', 'Smart city', 'Sustainability', 'Transition design', 'Urban design', 'Urban technology']},
 title = {Chapter 6 - Smart design for sustainable behaviors},
 year = {2024}
}

@Filtered Article{e4dc7b35-4ff8-46fa-8e5f-e3889851d38c,
 abstract = {All design problems in telecommunications can be formulated as optimization problems, and thus may be tackled by some optimization techniques. However, these problems can be extremely challenging due to the stringent time requirements, complex constraints, and a high number of design parameters. Solution methods tend to use conventional methods such as Lagrangian duality and fractional programming in combination with numerical solvers, while new trends tend to use evolutionary algorithms and swarm intelligence. This chapter provides a summary review of the bio-inspired optimization algorithms and their applications in telecommunications. We also discuss key issues in optimization and some active topics for further research.},
 authors = {['Xin-She Yang', 'Su Fong Chien', 'Tiew On Ting']},
 journal = {Morgan Kaufmann},
 keywords = {['Algorithm', 'Ant algorithm', 'Bee algorithm', 'Bat algorithm', 'Bio-inspired computation', 'Cuckoo search', 'Firefly algorithm', 'Harmony search', 'Particle swarm optimization', 'Metaheuristics', 'Swarm intelligence', 'Telecommunications']},
 title = {Chapter 1 - Bio-Inspired Computation and Optimization: An Overview},
 year = {2015}
}

@Filtered Article{e4f4f577-ffc6-42dc-8349-3ce2069076e0,
 abstract = {This paper adds to the growing body of research happening in multivariable calculus by examining scalar and vector line integrals. This paper contributes in two ways. First, this paper provides a conceptual analysis for both types of line integrals in terms of how theoretical ways of thinking about definite integrals summarized from the research literature might be applied to understanding line integrals specifically. Second, this paper provides an initial investigation of students’ understandings of line integral expressions, and connects these understanding to the theoretical ways of thinking drawn from the literature. One key finding from the empirical part is that several students appeared to understand individual pieces of the integral expression based on one way of thinking, such as adding up pieces or anti-derivatives, while trying to understand the overall integral expression through a different way of thinking, such as area under a curve.},
 authors = {['Steven R. Jones']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Multivariable calculus', 'Definite integrals', 'Line integrals', 'Conceptual analysis', 'Student understanding']},
 title = {Scalar and vector line integrals: A conceptual analysis and an initial investigation of student understanding},
 year = {2020}
}

@Filtered Article{e528973d-e6c1-4494-852c-9c9e1950e6d4,
 abstract = {Swarm intelligence (SI) and bio-inspired computing in general have attracted great interest in almost every area of science, engineering, and industry over the last two decades. In this chapter, we provide an overview of some of the most widely used bio-inspired algorithms, especially those based on SI such as cuckoo search, firefly algorithm, and particle swarm optimization. We also analyze the essence of algorithms and their connections to self-organization. Furthermore, we highlight the main challenging issues associated with these metaheuristic algorithms with in-depth discussions. Finally, we provide some key, open problems that need to be addressed in the next decade.},
 authors = {['Xin-She Yang', 'Mehmet Karamanoglu']},
 journal = {Elsevier},
 keywords = {['Algorithm', 'ant algorithm', 'bee algorithm', 'bat algorithm', 'bio-inspired', 'cuckoo search', 'firefly algorithm', 'harmony search', 'particle swarm optimization', 'swarm intelligence', 'metaheuristics']},
 title = {1 - Swarm Intelligence and Bio-Inspired Computation: An Overview},
 year = {2013}
}

@Filtered Article{e536c85d-e851-4232-bdaa-18822cb026ab,
 abstract = {Exposome-related data will come from a myriad of sources. The huge amounts of data must be organized in some manner that allows appropriate interpretations and associations to be drawn. Models and maps are often used to provide organization to complex data sets. Maps are quite appropriate for exposome research as the location of the sources and exposures is a critical component, and spatial statistics could play a major role in exposome data organization. The complex types of data will undoubtedly require mathematical approaches, including bioinformatics, computational, and systems biology-based techniques. This chapter reviews some of the possible strategies that can be used to keep track of the diverse and massive data sets that will result from exposome research.},
 authors = {['Gary W. Miller']},
 journal = {Academic Press},
 keywords = {['Bioinformatics', 'systems biology', 'computational biology', 'machine learning', 'Bayesian methods']},
 title = {Chapter 5 - Managing and Integrating Exposome Data: Maps, Models, Computation, and Systems Biology},
 year = {2014}
}

@Filtered Article{e582986d-5085-40bb-ab11-39a46e5252ff,
 abstract = {In recent years there has been an increasing interest in the role of brain networks supporting creative thinking. This chapter provides a summary of the literature on the network neuroscience of creativity, providing a twofold argument by separately detailing research in domain-general and domain-specific creativity. The first section will concern two main lines of research on domain-general creativity: (1) the neurocognitive mechanisms of creative cognition (how brain networks map onto specific cognitive processes involved in creative thinking), and (2) the individual differences in brain network connectivity and creative ability (how brain networks relate to differences in creative abilities). The second section, on domain-specific creativity, will then consider three domains of artistic creativity: (1) music improvisation, (2) figural creativity, and (3) literary creativity. Throughout this chapter we discuss common themes and shared findings between domain-general and domain-specific creativity. We will then conclude by outlining some of the limitations in the literature and by providing some directions for future research.},
 authors = {['Simone Luchini', 'Roger E. Beaty']},
 journal = {Academic Press},
 keywords = {['Creativity', 'Default network', 'Divergent thinking', 'Executive control network', 'Functional connectivity', 'Network neuroscience']},
 title = {Chapter 13 - Brain networks of creative cognition},
 year = {2023}
}

@Filtered Article{e61c7db9-dd36-4e97-ad44-efa183be57b7,
 abstract = {Energy concepts are taught in many schools, but children rarely have an opportunity to grapple with energy problems and work on their own solutions. This study explores the impacts of Connect Science, a service-learning (SL) program developed to enhance elementary students' energy literacy in the United States. Program impacts were explored within the context of a randomized controlled trial. Teachers in the SL intervention group were provided with professional development, coaching and curricular materials. Each fourth grade class chose an energy problem to address, and designed projects to test out a solution. Teachers in a waitlist control group taught their typical energy unit. Upon completion of the unit, students were asked to write about a problem related to energy production or use and propose a potential solution. Inductive content analysis was used to code 703 student responses (377 from control group and 326 from SL group). The majority of students expressed concerns about wasting or using too much electricity or the use of nonrenewable energy sources. Solutions focused on energy conservation and the use of renewable or clean resources were mentioned most frequently overall. Students in the SL group were significantly more likely to mention environmental impacts of various energy sources and to suggest energy conservation solutions or educating others. Conversely, the control group student responses more often focused on electric circuits or electrical safety. Results from this study suggest the promise of environmental SL programs to advance energy literacy and promote critical thinking about how to address energy problems.},
 authors = {['Eileen G. Merritt', 'Andrea E. Weinberg', 'Candace Lapan', 'Sara E. Rimm-Kaufman']},
 journal = {Energy Research & Social Science},
 keywords = {['Environmental service-learning', 'Energy literacy', 'Elementary students', 'Science education', 'Randomized controlled trial']},
 title = {Igniting kid power: The impact of environmental service-learning on elementary students' awareness of energy problems and solutions},
 year = {2024}
}

@Filtered Article{e672927d-4664-4a7c-9032-9bccc3abd642,
 abstract = {Recent advancements in medical imaging and therapeutic technologies have propelled innovative strategies in brain tumor treatment. This study introduces a comprehensive methodology merging Quantum Dots (QDs) and Real-Time Imaging-Guided Therapeutics (RIGT) to refine the precision of brain tumor radiotherapy. Focusing on the synthesis of near-infrared quantum dots (NIR-QDs), the study emphasizes the critical role of meticulous surface functionalization in achieving biocompatibility and stability. The methodology integrates 3D brain MRI images into the ingeniously devised Real-Time Imaging-Guided Therapeutics (RIGT) system, facilitating precise tumor localization and adaptive treatment protocol adjustments. Novel hybrid architecture is introduced for real-time MRI data analysis, enabling intricate tumor segmentation, feature extraction, localization, and synthetic image generation. This fusion of technologies, empowered by artificial intelligence, equips healthcare professionals with comprehensive insights into tumor intricacies and potential treatment outcomes. The proposed methodology's transformative objective seeks to redefine brain tumor treatment by seamlessly integrating advanced imaging modalities, cutting-edge nanotechnology, and AI-driven precision therapeutics. It envisions establishing a new paradigm in brain tumor treatment, promising heightened efficacy and minimized risks for patients. The study's numerical findings showcase the AI-powered image analysis capabilities of the Hybrid CNN-GAN network. Demonstrating superior performance in tumor segmentation, the results exhibit an Intersection over Union (IoU) of 0.89, Dice Coefficient of 0.95, F1-score of 0.94, and Structural Similarity Index (SSI) of 0.91. Additionally, computational efficiency is evident, with a short processing time of 65 ms and balanced CPU and GPU usage at 80 % and 90 %, respectively. In summary, this study presents an innovative methodology for brain tumor treatment, underpinned by exceptional numerical results validating its efficacy and computational efficiency.},
 authors = {['A. Karthik', 'S. {Shiek Aalam}', 'M. Sivakumar', 'M.V. {Rama Sundari}', 'J. {Dafni Rose}', 'Muniyandy Elangovan', 'A. Rajaram']},
 journal = {Biomedical Signal Processing and Control},
 keywords = {['NIR- quantum dots', 'Magnetic resonance imaging', 'Radiotherapy', 'Brain Tumor']},
 title = {Improving brain tumor treatment with better imaging and real-time therapy using quantum dots},
 year = {2024}
}

@Filtered Article{e6bd89d3-a520-4b0e-b69c-b4d8cd3f037d,
 abstract = {The method of caustics is a powerful experimental method in elasticity and particularly in fracture mechanics for crack problems. The related method of pseudocaustics is also of interest. Here we apply the computational method of quantifier elimination implemented in the computer algebra system Mathematica in order to determine (i) the non-parametric equation and two properties of the caustic at a crack tip and especially (ii) the illuminated and the dark regions related to caustics and pseudocaustics in plane elasticity and plate problems. The present computations concern: (i) The derivation of the non-parametric equation of the classical caustic about a crack tip through the elimination of the parameter involved (here the polar angle) as well as two geometrical properties of this caustic. (ii) The derivation of the inequalities defining the illuminated region on the screen in the problem of an elastic half-plane loaded normally by a concentrated load with the boundary of this illuminated region related to some extent to the caustic formed. (iii) Similarly for the problem of a clamped circular plate under a uniform loading with respect to the caustic and the pseudocaustic formed. (iv) Analogously for the problem of an equilateral triangular plate loaded by uniformly distributed moments along its whole boundary, which defines the related pseudocaustic. (v) The determination of quantities of interest in mechanics from the obtained caustics or pseudocaustics. The kind of computations in the applications (ii) to (iv), i.e. the derivation of inequalities defining the illuminated region on the screen, seems to be completely new independently of the use here of the method of quantifier elimination. Additional applications are also possible, but some of them require the expansion of the present somewhat limited power of the quantifier elimination algorithms in Mathematica. This is expected to take place in the future.},
 authors = {['Nikolaos I. Ioakimidis']},
 journal = {Optics and Lasers in Engineering},
 keywords = {['Caustics', 'Pseudocaustics', 'Illuminated and dark regions', 'Cracks', 'Plates', 'Elasticity']},
 title = {Caustics, pseudocaustics and the related illuminated and dark regions with the computational method of quantifier elimination},
 year = {2017}
}

@Filtered Article{e6f38aec-e120-4a78-9bba-e76d4744f538,
 abstract = {Stereotyping is a ubiquitous feature of social cognition, yet surprisingly little is known about how group-related beliefs influence the acquisition of person knowledge. Accordingly, in combination with computational modeling (i.e., Reinforcement Learning Drift Diffusion Model analysis), here we used a probabilistic selection task to explore the extent to which gender stereotypes impact instrumental learning. Several theoretically interesting effects were observed. First, reflecting the impact of cultural socialization on person construal, an expectancy-based preference for stereotype-consistent (vs. stereotype-inconsistent) responses was observed. Second, underscoring the potency of unexpected information, learning rates were faster for counter-stereotypic compared to stereotypic individuals, both for negative and positive prediction errors. Collectively, these findings are consistent with predictive accounts of social perception and have implications for the conditions under which stereotyping can potentially be reduced.},
 authors = {['Johanna K. Falbén', 'Marius Golubickis', 'Dimitra Tsamadi', 'Linn M. Persson', 'C. Neil Macrae']},
 journal = {Cognition},
 keywords = {['Stereotyping', 'Person perception', 'Reinforcement learning', 'Prediction errors', 'Drift diffusion model']},
 title = {The power of the unexpected: Prediction errors enhance stereotype-based learning},
 year = {2023}
}

@Filtered Article{e7032a3f-2f07-401e-8083-6ccf09537648,
 abstract = {Exact computation is assumed in most algorithms in computational geometry. In practice, implementors perform computation in some fixed-precision model, usually the machine floating-point arithmetic. Such implementations have many well-known problems, here informally called “robustness issues”. To reconcile theory and practice, authors have suggested that theoretical algorithms ought to be redesigned to become robust under fixed-precision arithmetic. We suggest that in many cases, implementors should make robustness a non-issue by computing exactly. The advantages of exact computation are too many to ignore. Many of the presumed difficulties of exact computation are partly surmountable and partly inherent with the robustness goal. This paper formulates the theoretical framework for exact computation based on algebraic numbers. We then examine the practical support needed to make the exact approach a viable alternative. It turns out that the exact computation paradigm encompasses a rich set of computational tactics. Our fundamental premise is that the traditional “BigNumber” package that forms the work-horse for exact computation must be reinvented to take advantage of many features found in geometric algorithms. Beyond this, we postulate several other packages to be built on top of the BigNumber package.},
 authors = {['Chee-Keng Yap']},
 journal = {Computational Geometry},
 title = {Towards exact geometric computation},
 year = {1997}
}

@Filtered Article{e70604a6-786e-4ab0-8679-ba5cf5fdc6dd,
 abstract = {A new method is presented to generate reduced order models (ROMs) in Fluid Dynamics problems. The method is based on the expansion of the flow variables on a Proper Orthogonal Decomposition (POD) basis, calculated from a limited number of snapshots, which are obtained via Computational Fluid Dynamics (CFD). Then, the POD-mode amplitudes are calculated as minimizers of a properly defined overall residual of the equations and boundary conditions. The residual can be calculated using only a limited number of points in the flow field, which can be scattered either all over the whole computational domain or over a smaller projection window. This means that the process is both computationally efficient (reconstructed flow fields require less than 1% of the time needed to compute a full CFD solution) and flexible (the projection window can avoid regions of large localized CFD errors). Also, various definitions of the residual are briefly discussed, along with the number and distribution of snapshots, the number of retained modes, and the effect of CFD errors, to conclude that the method is numerically robust. This is because the results are largely insensitive to the definition of the residual, to CFD errors, and to the CFD method itself, which may contain artificial stabilizing terms. Thus, the method is amenable for practical engineering applications.},
 authors = {['D. Alonso', 'A. Velazquez', 'J.M. Vega']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 keywords = {['Reduced order model', 'Proper Orthogonal Decomposition', 'Incompressible nonisothermal flow']},
 title = {A method to generate computationally efficient reduced order models},
 year = {2009}
}

@Filtered Article{e733811f-b7ce-41e5-b0b0-4f0a54c4734e,
 abstract = {We are developing a rigorous methodology to analyse experimental computation, by which we mean the idea of computing a set or function by experimenting with some physical equipment. Here we consider experimental computation by kinematic systems under both Newtonian and relativistic kinematics. An experimental procedure, expressed in a language similar to imperative programming languages, is applied to equipment, having the form of a bagatelle, and is interpreted using the two theories. We prove that for any set A of natural numbers there exists a two-dimensional kinematic system BA with a single particle P whose observable behaviour decides n∈A for all n∈N. The procedure can operate under (a) Newtonian mechanics or (b) relativistic mechanics. The proofs show how any information (coded by some A) can be embedded in the structure of a simple kinematic system and retrieved by simple observations of its behaviour. We reflect on the methodology, which seeks a formal theory for performing abstract experiments with physical restrictions on the construction of systems. We conclude with some open problems.},
 authors = {['E.J. Beggs', 'J.V. Tucker']},
 journal = {Applied Mathematics and Computation},
 keywords = {['Foundations of computation', 'Computable functions and sets', 'Newtonian kinematic systems', 'Relativistic kinematic systems', 'Foundations of mechanics', 'Theory of Gedanken experiments', 'Non-computable physical systems']},
 title = {Computations via Newtonian and relativistic kinematic systems},
 year = {2009}
}

@Filtered Article{e795795a-518c-431c-92d4-44ea67823040,
 abstract = {In the last decade many publications have appeared on degrowth as a strategy to confront environmental and social problems. We undertake a systematic review of their content, data and methods. This involves the use of computational linguistics to identify main topics investigated. Based on a sample of 561 studies we conclude that: (1) content covers 11 main topics; (2) the large majority (almost 90%) of studies are opinions rather than analysis; (3) few studies use quantitative or qualitative data, and even fewer ones use formal modelling; (4) the first and second type tend to include small samples or focus on non-representative cases; (5) most studies offer ad hoc and subjective policy advice, lacking policy evaluation and integration with insights from the literature on environmental/climate policies; (6) of the few studies on public support, a majority concludes that degrowth strategies and policies are socially-politically infeasible; (7) various studies represent a “reverse causality” confusion, i.e. use the term degrowth not for a deliberate strategy but to denote economic decline (in GDP terms) resulting from exogenous factors or public policies; (8) few studies adopt a system-wide perspective – instead most focus on small, local cases without a clear implication for the economy as a whole. We illustrate each of these findings for concrete studies.},
 authors = {['Ivan Savin', 'Jeroen {van den Bergh}']},
 journal = {Ecological Economics},
 keywords = {['Economic growth', 'Environmental policy', 'GDP', 'Political feasibility', 'Post-growth']},
 title = {Reviewing studies of degrowth: Are claims matched by data, methods and policy analysis?},
 year = {2024}
}

@Filtered Article{e7cc1310-fca4-446d-b012-e071ebc7d427,
 abstract = {Experimental cell biology, biochemistry, and structural biology have provided a wealth of information about microtubule function and mechanism, but we are reaching a limit as to what can be understood from experiment alone. Standard biochemical approaches are not sufficient to make quantitative predictions about microtubule behavior, and they are limited in their ability to test existing conceptual models of microtubule mechanism. Because microtubules are so complex, achieving a deep understanding of microtubule behavior and mechanism will require the input of mathematical and computational modeling. However, this type of analysis can be daunting to the uninitiated. The purpose of this chapter is to provide a straightforward introduction to the various types of modeling and how they can be used to study microtubule function, dynamics, and mechanism.},
 authors = {['Holly V. Goodson', 'Ivan V. Gregoretti']},
 journal = {Academic Press},
 title = {Chapter 10 - Using Computational Modeling to Understand Microtubule Dynamics: A Primer for Cell Biologists},
 year = {2010}
}

@Filtered Article{e7e839da-97af-4961-b117-780a57a35861,
 abstract = {Teaching programming logic to students who do not have a background in computer science is challenging, as the instructor has to awaken problem-solving, critical thinking, and logical reasoning skills Several programming tools have been created to teach coding concepts to computer science students of different ages. However, these tools are not well designed to meet the challenge of teaching programming to new developers who come with school training in other areas such as accounting, management, etc. Therefore, this research focused on analyzing the importance of the application of online programming tools to students starting college who come with a school background in an area other than technology. A pre/post experimental design was carried out with 82 first-level students of the Technical University of Ambato in the careers of Systems and Electronics. The results revealed that 45% of the students increased their levels of application and analysis in programming processes. In addition, the research revealed that students who come from a background other than computer science agree with the integration of online programming tools from the first level of university entrance since this method helps to improve their learning capacity.},
 authors = {['Fernando Ibarra-Torres', 'Gustavo Caiza', 'Marcelo V. García', 'Valeria Barona-Pico']},
 journal = {Procedia Computer Science},
 keywords = {['Programming logic', 'programming tools', 'software development', 'undergraduate student']},
 title = {Use of basic programming tools to foster programming logic in university students with school preparation other than computer science},
 year = {2024}
}

@Filtered Article{e80a98e6-e320-42d9-b523-f3686023bc9c,
 abstract = {The increasing demand for computational skills in economics necessitates the integration of programming into undergraduate economics curricula in the UK. This paper argues for a systematic incorporation of programming courses tailored to economics students, addressing the limitations of current approaches and highlighting the benefits of such integration. We propose a sequence of introductory and intermediate-level integrated courses, and argue that this curriculum change will enhance students’ understanding of economic concepts, improve their employment prospects, and better prepare them for postgraduate studies. This paper aims to initiate a discussion and exchange of ideas and experiences on this subject at the national level.},
 authors = {['Nigar Hashimzade', 'Oleg Kirsanov', 'Tatiana Kirsanova']},
 journal = {International Review of Economics Education},
 title = {Integrating Programming into the Modern Undergraduate Economics Curriculum},
 year = {2025}
}

@Filtered Article{e832c4c6-05ea-4211-845b-24a9c4acc03c,
 abstract = {In this chapter, the ALE formulation of the finite volume method is proposed for the simulation of compressible fluid flow. Two major topics of the discrete geometric conservation lawgeometric conservation law and mesh deformationmesh deformation algorithm in this chapter are to handle the moving boundary problem accurately and efficiently. Three examples are given to verify the effectiveness of the presented methods in multiphysics simulation for aerospace engineering problems.},
 journal = {Academic Press},
 keywords = {['aerospace engineering', 'finite volume method', 'ALE formulation', 'discrete geometric conservation law', 'mesh deformation', 'remeshing', 'flapping wing', 'store separation', 'wing flutter']},
 title = {10 - Computational fluid dynamics in aerospace field and CFD-based multidisciplinary simulations},
 year = {2016}
}

@Filtered Article{e84f02c2-c9a1-483e-9a1f-253cae207ab3,
 abstract = {Fuzzy logic and fuzzy system models have become popular tools in the field of management as they enable efficient handling of uncertainty. We present a tool based on the authors´ original approach focused on solving complex managerial problems affected by the vagueness or uncertainty caused by the human factor. For this purpose, we show the connection between the functioning principle of the tool and processes occurring in the human mind including a description of its structure as perceived by an external observer. This is followed by an overview of selected fragments of fuzzy propositional logic, the theory of fuzzy sets, and the conclusions derived from it. The main part consists of formulating an algebraic description of the computational process of multi-criteria evaluation of the considered alternative performed by a fuzzy system, which serves as the executive unit of a Fuzzy calculator. This is supplemented by a flowchart diagram illustrating the algorithm of its functioning. The Fuzzy calculator distinguishes itself from other fuzzy systems by standardizing all linguistic variables, regardless of the number of linguistic values, into a unified framework comprising three terms L, M, and H, which are represented using trapezoidal fuzzy numbers, ensuring precise mathematical characterization. During the transformation, the original linguistic terms are preserved by incorporating the positions of their support intervals, thereby maintaining the specificity of the input information. This approach establishes the Fuzzy calculator as a universal and highly adaptable tool, capable of addressing a wide range of practical managerial problems with improved consistency and control.},
 authors = {['Simona Hašková', 'Petr Šuleř', 'Martin Smrt']},
 journal = {Journal of Computational Science},
 keywords = {['Fuzzy calculator', 'Computer program', 'Multi-criteria evaluation', 'Fuzzy logic']},
 title = {Fuzzy calculator – A tool for management needs},
 year = {2025}
}

@Filtered Article{e8abb124-f485-420d-9dfc-c9828fbfecd4,
 abstract = {The transformative power of artificial intelligence (AI) is reshaping radiology, medicine, and healthcare, marking radiology as a pioneering specialty in AI adoption. The digital nature of radiological data and standardized data formats positioned radiology as the ideal testing ground for clinical AI integration. While initial enthusiasm led to inflated expectations, fueled by linear thinking and the planning fallacy, AI has now matured into tools that augment, rather than replace, radiologists’ expertise. Radiologists’ role is evolving from image interpreters to diagnostic orchestrators in a multimodal era. The integration of imaging data with diverse sources such as genomics, pathology, and wearable sensors necessitates a shift to a systems-level perspective. This transformation demands not only technical literacy but also interdisciplinary collaboration to effectively synthesize AI-driven insights and mitigate cognitive overload. Radiologists must navigate uncertainty, adopt structured workflows, and communicate AI-supported findings clearly to maintain trust in diagnostics. The emergence of generative AI, particularly large language models, further streamlines AI adoption by enabling intuitive, human-centered interfaces. However, addressing the growing knowledge gap is crucial. Traditional radiology training must be overhauled to incorporate data science, bioinformatics, and systems biology, ensuring radiologists are prepared to lead multimodal diagnostics. Radiologists are uniquely positioned to spearhead this transition, leveraging AI to integrate diverse data streams, improve patient care, and foster collaboration across specialties. Proactive adaptation will secure radiologists’ central role in AI-driven medicine, safeguarding the human element in healthcare while advancing diagnostic precision.},
 authors = {['Felix Nensa']},
 journal = {European Journal of Radiology Artificial Intelligence},
 keywords = {['Multimodal AI', 'GenAI', 'LLM', 'Radiology', 'Superdiagnostics']},
 title = {The Future of Radiology: The Path Towards Multimodal AI and Superdiagnostics},
 year = {2025}
}

@Filtered Article{e8b35ca0-ba26-4490-a263-21cdf9fff547,
 abstract = {Digital design and its growing impact on design and production practices have resulted in the need for a re-examination of current design theories and methodologies in order to explain and guide future research and development. The present research postulates the requirements for a conceptual framework and theoretical basis of digital design; reviews the recent theoretical and historical background; and defines a generic schema of design characteristics through which the paradigmatic classes of digital design are formulated. The implication of this research for the formulation of ‘digital design thinking’ is presented and discussed.},
 authors = {['Rivka Oxman']},
 journal = {Design Studies},
 keywords = {['digital design', 'design theory', 'design methodology', 'design thinking']},
 title = {Theory and design in the first digital age},
 year = {2006}
}

@Filtered Article{e8eb3743-3ae6-40dc-839e-40e8ff5b105a,
 abstract = {The self-assembly behavior of shape-anisotropic particles at curved fluid interfaces is computationally investigated by diffuse interface field approach (DIFA). A Gibbs–Duhem-type thermodynamic formalism is introduced to treat heterogeneous pressure within the phenomenological model, in agreement with Young–Laplace equation. Computer simulations are performed to study the effects of capillary forces (interfacial tension and Laplace pressure) on particle self-assembly at fluid interfaces in various two-dimensional cases. For isolated particles, it is found that the equilibrium liquid interface remains circular and particles of different shapes do not disturb the homogeneous curvature of liquid interface, while the equilibrium position, orientation and stability of a particle at the liquid interface depend on its shape and initial location with respect to the liquid interface. For interacting particles, the curvature of local liquid interfaces is different from the apparent curvature of the particle shell; nevertheless, irrespective of the particle shapes, a particle-coated droplet always tends to deform into a circular morphology under positive Laplace pressure, loses mechanical stability and collapses under negative Laplace pressure, while adapts to any morphology and stays in neutral equilibrium under zero Laplace pressure. Finally, the collective behaviors of particles and Laplace pressure evolution in bicontinuous interfacially jammed emulsion gels (bijels) are investigated.},
 authors = {['Tian-Le Cheng', 'Yu U. Wang']},
 journal = {Journal of Colloid and Interface Science},
 keywords = {['Capillary forces', 'Surface tension', 'Laplace pressure', 'Diffuse interface field approach', 'Gibbs–Duhem relation', 'Shape anisotropy', 'Pickering emulsions']},
 title = {Shape-anisotropic particles at curved fluid interfaces and role of Laplace pressure: A computational study},
 year = {2013}
}

@Filtered Article{ea756748-a424-4700-9891-ffba2f1f5806,
 abstract = {Some genes produce noncoding transcripts that function directly as structural, regulatory, or even catalytic RNAs 1, 2. Unlike protein-coding genes, which can be detected as open reading frames with distinctive statistical biases, noncoding RNA (ncRNA) gene sequences have no obvious inherent statistical biases [3]. Thus, genome sequence analyses reveal novel protein-coding genes, but any novel ncRNA genes remain invisible. Here, we describe a computational comparative genomic screen for ncRNA genes. The key idea is to distinguish conserved RNA secondary structures from a background of other conserved sequences using probabilistic models of expected mutational patterns in pairwise sequence alignments. We report the first whole-genome screen for ncRNA genes done with this method, in which we applied it to the “intergenic” spacers of Escherichia coli using comparative sequence data from four related bacteria. Starting from >23,000 conserved interspecies pairwise alignments, the screen predicted 275 candidate structural RNA loci. A sample of 49 candidate loci was assayed experimentally. At least 11 loci expressed small, apparently noncoding RNA transcripts of unknown function. Our computational approach may be used to discover structural ncRNA genes in any genome for which appropriate comparative genome sequence data are available.},
 authors = {['Elena Rivas', 'Robert J. Klein', 'Thomas A. Jones', 'Sean R. Eddy']},
 journal = {Current Biology},
 title = {Computational identification of noncoding RNAs in E. coli by comparative genomics},
 year = {2001}
}

@Filtered Article{ea8b3b75-037d-48bb-929e-168b426c7b39,
 abstract = {As hospitality enters the metaverse era, artificial empathy becomes essential for developing of artificial intelligence (AI) agents. Using the empathy cycle model, computational empathy frameworks and interdisciplinary research, this conceptual paper proposes a model explaining how artificial empathy will evolve in the hospitality metaverse era. The paper also addresses customer empathy and responses towards AI agents and other human actors with in the hospitality context. It explores how metaverse characteristics such as immersiveness, sociability, experiential nature, interoperability, blended virtual and physical environments as well as environmental fidelity will shape computational models and evolution of artificial empathy. Findings suggests that metaverse enables AI agents to form a seamless cycle of detection, resonation, and response to consumers’ affective states, facilitating the evolution of artificial empathy. Additionally, the paper outlines conditions under which the artificial empathy cycle may be disrupted and proposes future research questions that can advance our understanding of artificial empathy.},
 authors = {['Ioannis Assiouras', 'Cornelia Laserer', 'Dimitrios Buhalis']},
 journal = {International Journal of Hospitality Management},
 keywords = {['Empathy', 'Artificial empathy', 'Artificial intelligence', 'Metaverse', 'Hospitality', 'Artificial intelligence agents']},
 title = {The evolution of artificial empathy in the hospitality metaverse era},
 year = {2025}
}

@Filtered Article{ea9398a7-a876-4b88-a0e9-1289fc402f93,
 abstract = {Dealing with molecular-state transitions for radiative transfer purposes involves two successive steps that both reach the complexity level at which physicists start thinking about statistical approaches: (1) constructing line-shaped absorption spectra as the result of very numerous state-transitions, (2) integrating over optical-path domains. For the first time, we show here how these steps can be addressed simultaneously using the null-collision concept. This opens the door to the design of Monte Carlo codes directly estimating radiative transfer observables from spectroscopic databases. The intermediate step of producing accurate high-resolution absorption spectra is no longer required. A Monte Carlo algorithm is proposed and applied to six one-dimensional test cases. It allows the computation of spectrally integrated intensities (over 25cm−1 bands or the full IR range) in a few seconds, regardless of the retained database and line model. But free parameters need to be selected and they impact the convergence. A first possible selection is provided in full detail. We observe that this selection is highly satisfactory for quite distinct atmospheric and combustion configurations, but a more systematic exploration is still in progress.},
 authors = {['Mathieu Galtier', 'Stéphane Blanco', 'Jérémi Dauchet', 'Mouna {El Hafi}', 'Vincent Eymet', 'Richard Fournier', 'Maxime Roger', 'Christophe Spiesser', 'Guillaume Terrée']},
 journal = {Journal of Quantitative Spectroscopy and Radiative Transfer},
 keywords = {['Radiative transfer', 'Monte Carlo method', 'Null-collision', 'Line sampling', 'Statistical approach', 'Spectroscopic databases']},
 title = {Radiative transfer and spectroscopic databases: A line-sampling Monte Carlo approach},
 year = {2016}
}

@Filtered Article{eaaad179-ee68-4feb-8e14-d83e0120f7d9,
 abstract = {This paper considers the problem of identifying the profiles and capabilities of attackers injecting adversarial inputs to a cyber-physical system. The system in question interacts with attackers of different levels of intelligence, each employing different feedback controllers against the system. Principles of behavioral game theory – specifically the concept of level-k thinking – is employed to construct a database of potential attack vectors. By observing the state trajectories under sequential interactions with different adversaries, the defender adaptively estimates both the number and profiles of the different attack signals using an online deterministic annealing approach. This information is used to dynamically estimate the level of intelligence of the attackers. Simulation results showcase the efficacy of the proposed method.},
 authors = {['Christos N. Mavridis', 'Aris Kanellopoulos', 'Kyriakos G. Vamvoudakis', 'John S. Baras', 'Karl Henrik Johansson']},
 journal = {IFAC-PapersOnLine},
 title = {Attack Identification for Cyber-Physical Security in Dynamic Games under Cognitive Hierarchy},
 year = {2023}
}

@Filtered Article{eae2964c-fe9f-473d-b15b-da62196cee5a,
 abstract = {Summary
Psychiatry has increasingly adopted explanations for psychopathology that are based on neurobiological reductionism. With the recognition of health disparities and the realisation that someone's postcode can be a better predictor of health outcomes than their genetic code, there are increasing efforts to ensure cultural and social–structural competence in psychiatric practice. Although neuroscientific and social–cultural approaches in psychiatry remain largely separate, they can be brought together in a multilevel explanatory framework to advance psychiatric theory, research, and practice. In this Personal View, we outline how a cultural–ecosocial systems approach to integrating neuroscience in psychiatry can promote social–contextual and systemic thinking for more clinically useful formulations and person-centred care.},
 authors = {['Ana Gómez-Carrillo', 'Laurence J Kirmayer', 'Neil Krishan Aggarwal', 'Kamaldeep S Bhui', 'Kenneth Po-Lun Fung', 'Brandon A Kohrt', 'Mitchell G Weiss', 'Roberto Lewis-Fernández']},
 journal = {The Lancet Psychiatry},
 title = {Integrating neuroscience in psychiatry: a cultural–ecosocial systemic approach},
 year = {2023}
}

@Filtered Article{eb361b4d-3ce6-4760-b38e-5f3a755699a9,
 abstract = {The evolution of stochastic dynamical systems is governed by Fokker-Planck equations if the response process is Markovian. Analytical solutions for the transient response of multidimensional systems exist only for the simplest dynamical systems. The evolution of the transition probability density function over the phase space has been solved numerically for various low dimensional systems subjected to additive and multiplicative white noise excitations using the finite element method. Systems of higher order, however, pose difficulty when using standard finite element formulations due to memory requirements and computational expense. Direct Monte Carlo simulation (MCS), while often regarded as less elegant than other methods, can be used to solve problems of significantly higher complexity. The number of realizations required to accurately produce the transition probability density function over the entire phase space, especially in the tails, is large, but since each realization is entirely independent of the others, the Monte Carlo simulation is easily and efficiently adapted to parallel computation. The advent of high-speed, massively-parallel computers permits a large number of realizations of a complex dynamical system to be simultaneously determined. Consequently, Monte Carlo simulation may be more efficient for higher-dimensional systems than other solution methods currently in use. This investigation will examine some of these observations and compare the performance of MCS on various platforms, in the context of a four-dimensional linear oscillator and a Duffing oscillator subjected to band-limited white noise.},
 authors = {['E.A. Johnson', 'S.F. Wojtkiewicz', 'L.A. Bergman', 'B.F. Spencer']},
 journal = {International Journal of Non-Linear Mechanics},
 title = {Observations with regard to massively parallel computation for Monte Carlo simulation of stochastic dynamical systems},
 year = {1997}
}

@Filtered Article{eb488546-9d91-47ce-b43d-6e38ba54a351,
 abstract = {We study a new approach to the problem of transparent boundary conditions for the Helmholtz equation in unbounded domains. Our approach is based on the minimization of an integral functional arising from a volume integral formulation of the radiation condition. The index of refraction does not need to be constant at infinity and may have some angular dependency as well as perturbations. We prove analytical results on the convergence of the approximate solution. Numerical examples for different shapes of the artificial boundary and for non-constant indexes of refraction will be presented.},
 authors = {['Giulio Ciraolo', 'Francesco Gargano', 'Vincenzo Sciacca']},
 journal = {Journal of Computational Physics},
 keywords = {['Helmholtz equation', 'Transparent boundary conditions', 'Minimization of integral functionals']},
 title = {A computational method for the Helmholtz equation in unbounded domains based on the minimization of an integral functional},
 year = {2013}
}

@Filtered Article{eb6ec834-ad34-4520-bc4d-a662dd4a7129,
 abstract = {The Connection Machine Supercomputer system is described with emphasis on the solution to large scale physics problems. Numerous parallel algorithms as well as their implementation are given that demonstrate the use of the Connection Machine for physical simulations. Applications discussed include classical mechanics, quantum mechanics, electromagnetism, fluid flow, statistical physics and quantum field theories. The visualization of physical phenomena is also discussed and in the lectures video tapes demonstrating this capability are shown. Connection Machine performance and I/O characteristics are also described as well as the CM-2 software.},
 authors = {['John L. Richardson']},
 journal = {Physics Reports},
 title = {Computational physics on the CM-2 supercomputer},
 year = {1991}
}

@Filtered Article{eb8072b7-02cf-4ac4-a478-51bb3ea3c01c,
 abstract = {Summary
Decision-making is a cognitive process of central importance for the quality of our lives. Here, we ask whether a common factor underpins our diverse decision-making abilities. We obtained 32 decision-making measures from 830 young people and identified a common factor that we call “decision acuity,” which was distinct from IQ and reflected a generic decision-making ability. Decision acuity was decreased in those with aberrant thinking and low general social functioning. Crucially, decision acuity and IQ had dissociable brain signatures, in terms of their associated neural networks of resting-state functional connectivity. Decision acuity was reliably measured, and its relationship with functional connectivity was also stable when measured in the same individuals 18 months later. Thus, our behavioral and brain data identify a new cognitive construct that underpins decision-making ability across multiple domains. This construct may be important for understanding mental health, particularly regarding poor social function and aberrant thought patterns.},
 authors = {['Michael Moutoussis', 'Benjamín Garzón', 'Sharon Neufeld', 'Dominik R. Bach', 'Francesco Rigoli', 'Ian Goodyer', 'Edward Bullmore', 'Peter Fonagy', 'Peter Jones', 'Tobias Hauser', 'Rafael Romero-Garcia', 'Michelle {St Clair}', 'Petra Vértes', 'Kirstie Whitaker', 'Becky Inkster', 'Gita Prabhu', 'Cinly Ooi', 'Umar Toseeb', 'Barry Widmer', 'Junaid Bhatti', 'Laura Villis', 'Ayesha Alrumaithi', 'Sarah Birt', 'Aislinn Bowler', 'Kalia Cleridou', 'Hina Dadabhoy', 'Emma Davies', 'Ashlyn Firkins', 'Sian Granville', 'Elizabeth Harding', 'Alexandra Hopkins', 'Daniel Isaacs', 'Janchai King', 'Danae Kokorikou', 'Christina Maurice', 'Cleo McIntosh', 'Jessica Memarzia', 'Harriet Mills', 'Ciara O’Donnell', 'Sara Pantaleone', 'Jenny Scott', 'Pasco Fearon', 'John Suckling', 'Anne-Laura {van Harmelen}', 'Rogier Kievit', 'Marc Guitart-Masip', 'Raymond J. Dolan']},
 journal = {Neuron},
 keywords = {['decision acuity', 'computational psychiatry', 'functional connectivity', 'adolescence', 'development']},
 title = {Decision-making ability, psychopathology, and brain connectivity},
 year = {2021}
}

@Filtered Article{ebc49479-6c3f-46b5-a5e4-dc6d8bab1e06,
 abstract = {There is a widespread stated desire amongst both public and private organizations worldwide to engage in more significant “evidence-based reasoning” and to be more “data-driven.” We argue that these two goals are proxies for the often-unstated goal of improving the exploration of possible futures as foresights that could lead to better strategic decisions and improved business outcomes. From this perspective, data and analytics hold great promise and are necessary—but not sufficient—for improving strategic decision-making. Something more is needed to realize this potential. We specify how to fill this gap using an integration framework between technology and decision-makers, which is especially appropriate in complex and/or volatile environments. Our solution—which comprises a methodology as well as a software architecture—therefore unifies not only human decision makers to technology but each other and also integrates several disciplines that have been hitherto unnecessarily separated. Thereby, it could help organizations to address increasing challenges better as well as improve the exploration of possible futures.},
 authors = {['Lorien Pratt', 'Christophe Bisson', 'Thierry Warin']},
 journal = {Futures},
 keywords = {['Strategic decision making', 'Decision intelligence', 'Corporate foresight', 'Artificial intelligence', 'Complexity', 'Kahneman’s systems thinking']},
 title = {Bringing advanced technology to strategic decision-making: The Decision Intelligence/Data Science (DI/DS) Integration framework},
 year = {2023}
}

@Filtered Article{ebea4cfa-652f-4988-b9a4-3581e9ca0bbb,
 abstract = {Process systems engineering (PSE) has been an active research field for almost 50 years. Its major achievements include methodologies and tools to support process modeling, simulation and optimization (MSO). Mature, commercially available technologies have been penetrating all fields of chemical engineering in academia as well as in industrial practice. MSO technologies have become a commodity, they are not a distinguishing feature of the PSE field any more. Consequently, PSE has to reassess and to reposition its future research agenda. Emphasis should be put on model-based applications in all PSE domains including product and process design, control and operations. Furthermore, systems thinking and systems problem solving have to be prioritized rather than the mere application of computational problem solving methods. This essay reflects on the past, present and future of PSE from an academic and industrial point of view. It redefines PSE as an active and future-proof research field which can play an active role in providing enabling technologies for product and process innovations in the chemical industries and beyond.},
 authors = {['Karsten-Ulrich Klatt', 'Wolfgang Marquardt']},
 journal = {Computers & Chemical Engineering},
 keywords = {['Review', 'Modeling', 'Design', 'Optimization', 'Control', 'Operations', 'Numerical algorithms', 'Software', 'Computer-aided process engineering (CAPE)']},
 title = {Perspectives for process systems engineering—Personal views from academia and industry},
 year = {2009}
}

@Filtered Article{ec908489-fc8c-4845-99c5-9b78bbd285ea,
 abstract = {This study examines the integration of generative AI tools into digital multimodal composition (DMC) within a multicultural context, examining their impact on students’ motivation, writing processes, and outcomes. Eleven culturally diverse students from two high schools in Hong Kong participated in the study. The study developed and employed a novel pedagogical framework, IDEA (Interpret, Design, Evaluate, and Articulate), to seamlessly incorporate generative AI into DMC practices. Data-collection methods included analysis of generative AI tool-usage history, classroom video observations, surveys, and interviews. The findings reveal that students leveraged generative AI’s capabilities across five key areas: content generation, feedback and revision, multilingual support, critical thinking, and visual representation. The integration of AI tools followed distinct stages in the composition process, resulting in enhancements to the vocabulary, grammar, and structural elements of students’ work. This research contributes to the growing body of knowledge on the intersection of generative AI, education, and multimodal literacy, with a particular emphasis on human-AI collaboration in multicultural settings. It also offers valuable insights for educators seeking to enhance students’ DMC skills through the thoughtful integration of generative AI tools, potentially increasing engagement, motivation, and creative expression among learners from diverse cultural backgrounds.},
 authors = {['Chin-Hsi Lin', 'Keyi Zhou', 'Lanqing Li', 'Lanfang Sun']},
 journal = {Computers and Composition},
 keywords = {['Generative AI', 'Multimodal composing', 'Multicultural education']},
 title = {Integrating generative AI into digital multimodal composition: A study of multicultural second-language classrooms},
 year = {2025}
}

@Filtered Article{ec9fcf0b-e137-4cf3-8263-fe3c7d197bae,
 abstract = {Dreams have long captivated human curiosity, but empirical research in this area has faced significant methodological challenges. Recent interdisciplinary advances have now opened up new opportunities for studying dreams. This review synthesizes these advances into three methodological frameworks and describes how they overcome historical barriers in dream research. First, with observable dreaming, neural decoding and real-time reporting offer more direct measures of dream content. Second, with dream engineering, targeted stimulation and lucidity provide routes to experimentally manipulate dream content. Third, with computational dream analysis, the generation and exploration of large dream-report databases offer powerful avenues to identify patterns in dream content. By enabling researchers to systematically observe, engineer, and analyze dreams, these innovations herald a new era in dream science.},
 authors = {['Remington Mallett', 'Karen R. Konkoly', 'Tore Nielsen', 'Michelle Carr', 'Ken A. Paller']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['sleep', 'dreams', 'memory', 'neuroscience', 'natural language processing']},
 title = {New strategies for the cognitive science of dreaming},
 year = {2024}
}

@Filtered Article{ecc1ad5c-1a26-4de0-af06-cdb52bb32064,
 abstract = {Semi-supervised learning (SSL) methods, which exploit both the labeled and unlabeled data, have attracted a lot of attention. One of the major categories of SSL methods, graph-based semi-supervised learning (GBSSL) learns labels of unlabeled data on an adjacency graph, where neighborhood sparse graph is often used to reduce computational complexity. However, the neighborhood size is difficult to set. Instead of assigning a concrete value of neighborhood size, we propose a new label propagation algorithm called multi granularity based label propagation (MGLP) and developed from the view of granular computing. In MGLP, labels of unlabeled data are learned by two classic label propagation processes with diverse neighborhood size k, where granular computing delivers a guiding strategy to leverage multiple level neighborhood information granules, and three-way decision acts as an active learning strategy to select the unlabeled data for further annotating. Through the iterative procedures of label propagating, data annotating and data subset updating, the ultimate pseudo label accuracy of unlabeled data may be higher. Theoretically, the accuracy of pseudo labels is enhanced in some scenarios. Experimentally, the results of simulation studies on ten benchmark datasets, show that the proposed method MGLP can rise pseudo labels accuracy by 8.6% than LP (label propagation), 6.5% than LNP (linear neighborhood propagation), 6.4% than LPSN (label propagation through sparse neighborhood), 4.5% than Adaptive-NP (adaptive neighborhood propagation) and 4.6% than CRLP (consensus rate-based label propagation). It also provides a novel way to annotate data.},
 authors = {['Shengdan Hu', 'Duoqian Miao', 'Witold Pedrycz']},
 journal = {Expert Systems with Applications},
 keywords = {['Semi-supervised learning', 'Granular computing', 'Multi granularity', 'Label propagation', 'Active learning', 'Three-way decision']},
 title = {Multi granularity based label propagation with active learning for semi-supervised classification},
 year = {2022}
}

@Filtered Article{ecd2f7c5-35ee-4bd2-9517-c50097dda8c2,
 abstract = {This study examined conceptions of algebra held by 30 preservice elementary teachers. In addition to exploring participants’ general “definitions” of algebra, this study examined, in particular, their analyses of tasks designed to engage students in relational thinking or a deep understanding of the equal sign as well as student work on these tasks. Findings from this study suggest that preservice elementary teachers’ conceptions of algebra as subject matter are rather narrow. Most preservice teachers equated algebra with the manipulation of symbols. Very few identified other forms of reasoning – in particular, relational thinking – with the algebra label. Several participants made comments implying that student strategies that demonstrate traditional symbol manipulation might be valued more than those that demonstrate relational thinking, suggesting that what is viewed as algebra is what will be valued in the classroom. This possibility, along with implications for mathematics teacher education, will be discussed.},
 authors = {['Ana C. Stephens']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Algebra', 'Elementary mathematics', 'Teacher conceptions']},
 title = {What “counts” as algebra in the eyes of preservice elementary teachers?},
 year = {2008}
}

@Filtered Article{ed823964-77af-4036-b96a-ad46bfe0b9b8,
 abstract = {A theory of early motion processing in the human and primate visual system is presented which is based on the idea that spatio-temporal retinal image data is represented in primary visual cortex by a truncated 3D Taylor expansion that we refer to as a jet vector. This representation allows all the concepts of differential geometry to be applied to the analysis of visual information processing. We show in particular how the generalised Stokes theorem can be used to move from the calculation of derivatives of image brightness at a point to the calculation of image brightness differences on the boundary of a volume in space–time and how this can be generalised to apply to integrals of products of derivatives. We also provide novel interpretations of the roles of direction selective, bi-directional and pan-directional cells and of type I and type II cells in V5/MT.},
 authors = {['A. Johnston', 'P.W. McOwan', 'C.P. Benton']},
 journal = {Journal of Physiology-Paris},
 keywords = {['Optic flow', 'Cortex', 'Differential forms', 'Vision', 'Motion']},
 title = {Biological computation of image motion from flows over boundaries},
 year = {2003}
}

@Filtered Article{ed928f56-2eeb-4796-8504-2a97af14da9d,
 abstract = {Successful materials innovations can transform society. However, materials research often involves long timelines and low success probabilities, dissuading investors who have expectations of shorter times from bench to business. A combination of emergent technologies could accelerate the pace of novel materials development by ten times or more, aligning the timelines of stakeholders (investors and researchers), markets, and the environment, while increasing return on investment. First, tool automation enables rapid experimental testing of candidate materials. Second, high-performance computing concentrates experimental bandwidth on promising compounds by predicting and inferring bulk, interface, and defect-related properties. Third, machine learning connects the former two, where experimental outputs automatically refine theory and help define next experiments. We describe state-of-the-art attempts to realize this vision and identify resource gaps. We posit that over the coming decade, this combination of tools will transform the way we perform materials research, with considerable first-mover advantages at stake.},
 authors = {['Juan-Pablo Correa-Baena', 'Kedar Hippalgaonkar', 'Jeroen {van Duren}', 'Shaffiq Jaffer', 'Vijay R. Chandrasekhar', 'Vladan Stevanovic', 'Cyrus Wadia', 'Supratik Guha', 'Tonio Buonassisi']},
 journal = {Joule},
 keywords = {['accelerated materials development', 'machine learning', 'artificial intelligence', 'energy materials']},
 title = {Accelerating Materials Development via Automation, Machine Learning, and High-Performance Computing},
 year = {2018}
}

@Filtered Article{edb4335c-15a3-4776-b1a8-6163fe0f2d26,
 abstract = {Radio Frequency Identification (RFID) uses wireless radio frequency technology to automatically identify tagged objects. Despite the extensive development of RFID technology, tag collisions still remains a major drawback. The collision issue can be solved by using anti-collision techniques. While existing research has focused on improving anti-collision methods alone, it is also essential that a suitable type of anti-collision algorithm is selected for the specific circumstance. In this work, we evaluate anti-collision techniques and perform a comparative analysis in order to find the advantages and disadvantages of each approach. To identify the best anti-collision selection method in various scenarios, we have proposed two strategies for selective anti-collision technique management: a “Novel Decision Tree Strategy” and a “Six Thinking Hats Strategy”. We have shown that the selection of the correct technique for specific scenarios improve the quality of the data collection which, in turn, will increase the integrity of the data after being transformed, aggregated, and used for event processing.},
 authors = {['Prapassara Pupunwiwat', 'Peter Darcy', 'Bela Stantic']},
 journal = {Procedia Computer Science},
 keywords = {['Radio Frequency Identification (RFID)', 'Anti-Collision', 'Decision Tree', 'Six Thinking Hats']},
 title = {Conceptual Selective RFID Anti-Collision Technique Management},
 year = {2011}
}

@Filtered Article{ee2a6a49-552e-49c1-a263-d1dc8212f8ae,
 abstract = {We examined the influences of pre-solving pause time, algebraic knowledge, mathematics self-efficacy, and mathematics anxiety on middle-schoolers' strategy efficiency in an algebra learning game. We measured strategy efficiency using (a) the number of steps taken to complete a problem, (b) the proportion of problems completed on the initial attempt, and (c) the number of resets prior to completing the problems. Using the log data from the game, we found that longer pre-solving pause time was associated with more efficient strategies, as indicated by fewer solution steps, higher initial completion rate, and fewer resets. Higher algebraic knowledge was associated with higher initial completion rate and fewer resets. Mathematics self-efficacy and mathematics anxiety was not associated with any measures of strategy efficiency. The results suggest that pause time may be an indicator of student thinking before problem-solving, and provide insights into using data from online learning platforms to examine students' problem-solving processes.},
 authors = {['Jenny Yun-Chen Chan', 'Erin R. Ottmar', 'Ji-Eun Lee']},
 journal = {Learning and Individual Differences},
 keywords = {['Pause time', 'Strategy efficiency', 'Algebra problem-solving', 'Online learning environment', 'Metacognitive skills']},
 title = {Slow down to speed up: Longer pause time before solving problems relates to higher strategy efficiency},
 year = {2022}
}

@Filtered Article{ee8a6509-fce2-4c0a-afa9-e7fbe56f87bd,
 abstract = {Prior investigations of functional specialization have focused on the response profiles of particular brain regions. Given the growing emphasis on regional covariation, we propose to reframe these questions in terms of brain ‘networks’ (collections of regions jointly engaged by some mental process). Despite the challenges that investigations of the language network face, a network approach may prove useful in understanding the cognitive architecture of language. We propose that a language network plausibly includes a functionally specialized ‘core’ (brain regions that coactivate with each other during language processing) and a domain-general ‘periphery’ (a set of brain regions that may coactivate with the language core regions at some times but with other specialized systems at other times, depending on task demands). Framing the debate around network properties such as this may prove to be a more fruitful way to advance our understanding of the neurobiology of language.},
 authors = {['Evelina Fedorenko', 'Sharon L. Thompson-Schill']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['domain specificity', 'domain generality', 'language network', 'cognitive control', 'fMRI']},
 title = {Reworking the language network},
 year = {2014}
}

@Filtered Article{eeb580c6-3d50-4407-bf71-539ab8ed1a27,
 abstract = {People frequently entertain counterfactual thoughts, or mental simulations about alternative ways the world could have been. But the perceived plausibility of those counterfactual thoughts varies widely. The current article interfaces research in the philosophy and semantics of counterfactual statements with the psychology of mental simulations, and it explores the role of perceived similarity in judgments of counterfactual plausibility. We report results from seven studies (N = 6405) jointly supporting three interconnected claims. First, the perceived plausibility of a counterfactual event is predicted by the perceived similarity between the possible world in which the imagined situation is thought to occur and the actual world. Second, when people attend to differences between imagined possible worlds and the actual world, they think of the imagined possible worlds as less similar to the actual world and tend to judge counterfactuals in such worlds as less plausible. Lastly, when people attend to what is identical between imagined possible worlds and the actual world, they think of the imagined possible worlds as more similar to the actual world and tend to judge counterfactuals in such worlds as more plausible. We discuss these results in light of philosophical, semantic, and psychological theories of counterfactual thinking.},
 authors = {['Felipe {De Brigard}', 'Paul Henne', 'Matthew L. Stanley']},
 journal = {Cognition},
 keywords = {['Imagination', 'Counterfactual thinking', 'Plausibility', 'Similarity', 'Possible worlds']},
 title = {Perceived similarity of imagined possible worlds affects judgments of counterfactual plausibility},
 year = {2021}
}

@Filtered Article{eec56992-949b-4c5d-a31d-cd76e92f813f,
 abstract = {This paper explores the hydrological history of the Hunter River and Estuary (Newcastle, Australia), to identify pathways for incorporating climate-sensitive adaptation approaches into urban development and planning. The research method utilises mapping as a methodological discovery tools to visually articulate the correlation of pre-colonial hydrological landscapes, the transformation of the estuary over two centuries, the areas identified as at risk, and the opportunities for developing a climate-resilient estuary. This research aims to contribute to the redefinition of the discourse on the role of estuary planning for changing climate, focusing on four critical aspects: identify the impacts of urbanisation and industrialisation on ecosystems and its correlation with climate hazard at the estuary; visualise such transformations over time and space to identify critical spatial and climate factors threatening inhabitation; propose strategic spatial practices towards adaptation and resilience; and synthesising the options to foster reflective thinking and establish a correlation with novel policies, governance and practices. The study highlights that adopting new urbanism aligned with cultural and ecological principles can mitigate future climate impacts through re-naturalisation and urban adaptation to sea-level rise by focusing on proactive approaches to building resilient communities. This paper also acknowledges the need for site-specific adaptive design and planning strategies at multiple scales and governance levels.},
 authors = {['Irene {Perez Lopez}', 'Sandra Carrasco', 'Cesar {Mariscal Madrigal}']},
 journal = {Cities},
 keywords = {['Ecological design', 'Estuary urbanism', 'Climate adaptation', 'Living infrastructures', 'Hunter River Australia']},
 title = {Cartographic analysis as spatial determinant for climate change adaptation in the Hunter River Estuary, Australia},
 year = {2024}
}

@Filtered Article{eedcfa9c-f0e6-457a-9319-107bedd5e9e1,
 abstract = {The research aimed to know the effect of parental leadership represented by (benevolent leadership, moral leadership, and authoritarian leadership) found in the research sample, in the organizational familiarity (employee morale, empowerment, and objective merit), the research relied on the questionnaire as a key instrument to collect the necessary data to meet its goal. As (60) forms were distributed to find the level of availability of parental leadership and organizational harmony, while (56) forms were retrieved. A set of statistical methods were used, represented by normal distribution, stability factor (Alpha Kronbach), reliability, arithmetic mean, standard deviation, and coefficient Simple correlation Pearson, multiple regression coefficient. The results showed that there is a positive correlation and effect relationship with statistically significant between parental leadership with its dimensions (benevolent leadership, moral leadership, authoritarian leadership) and organizational affiliation with its dimensions (employee morale, empowerment, and merit's Objectivity), and the research showed a direct impact relationship between parental leadership and the organizational affiliation of the studied sample. Accordingly, the research concluded that the study sample should pay attention to the nature and type of empowering workers in order to give them freedom and independence in making decisions regarding the tasks assigned to them.},
 authors = {['Marwan {Thakir Abed}']},
 journal = {Materials Today: Proceedings},
 keywords = {['Parental leadership', 'Organizational familiarity', 'Empowerment', 'Iraqi Airways']},
 title = {The computation intelligent system of role of parental leadership in organizational familiarity in Iraqi Airways employees},
 year = {2023}
}

@Filtered Article{ef1ebba4-6892-448d-a514-dac88d057bc9,
 authors = {['Eugene Garfield']},
 journal = {Computer Compacts},
 title = {Artificial intelligence: Using computers to think about thinking, Part 2: Some practical applications of Al},
 year = {1984}
}

@Filtered Article{ef1ec865-8d07-4f61-a4cb-3fc9c88bc468,
 abstract = {The technological waves of super artificial intelligence, big data, algorithms, and machine learning continue to impact our thinking and actions, thereby affecting the ways individuals, professions and institutions make judgments. On the one hand, there is an argument that more data and knowledge together with the cyber physical system of industry4.0 will automatically push society along some track toward a better world for all. On the other hand, we hear worrying voices of the imponderable downsides of powerful new cyber-, bio-, Nano-technologies, and synthetic biology. In the age of uncertainties, big data and the algorithm, how is the decision and judgment making process being affected?},
 authors = {['Karamjit S Gill']},
 journal = {IFAC-PapersOnLine},
 keywords = {['algorithms', 'artificial intelligence', 'big data', 'calculation', 'decision', 'judgment', 'wisdom']},
 title = {Data to Decision and Judgment Making – a Question of Wisdom},
 year = {2018}
}

@Filtered Article{ef1ffdd0-602f-472d-b9bb-488bcb71b4eb,
 abstract = {Many social systems have a set of opposite interactions such as friend/enemy, cooperation/competition and support/opposition. In these signed systems, there exist functional imbalances from the system-level view because of the existence of unbalanced interactions. However, it is difficult to compute the unbalance degree and transform unbalanced factors to balanced ones in real signed systems. Earlier studies tackled these two issues separately and gave a unique solution, and thus cannot be well applied to real applications with constraints. In this paper, we devise a decomposition-based and network-specific multi-objective optimization algorithm to solve the balance computation and transformation of signed networks simultaneously. The devised algorithm aims at finding a set of optimal balance transformation solutions, and each of which is the trade-off between the twin objectives (i.e., the minimization of inter-cluster positive links and the minimization of intra-cluster negative links). Of these solutions, the one with the fewest unbalanced links corresponds to the solution to the balance computation. And each trade-off solution corresponds to an optimal balance transformation way under a certain transformation cost. Extensive experiments on four social networks demonstrate the effectiveness of the devised algorithm on both the computation and the transformation of structural balance. They also show that the devised algorithm can provide multiple optimal solutions at the same transformation cost.},
 authors = {['Lijia Ma', 'Maoguo Gong', 'Jianan Yan', 'Fuyan Yuan', 'Haifeng Du']},
 journal = {Information Sciences},
 keywords = {['Structural balance computation', 'Balance transformation', 'Multi-objective optimization', 'Decomposition', 'Signed networks']},
 title = {A decomposition-based multi-objective optimization for simultaneous balance computation and transformation in signed networks},
 year = {2017}
}

@Filtered Article{ef432635-6bd3-42db-8f55-88a5ffbb0921,
 abstract = {This paper presents a small number of MATLAB APPs and livescript files designed to help students both understand and implement frequency response tools into feedback design. The paper presents the thinking behind the use of MATLAB and the topic itself before then describing the proposed resources in detail.},
 authors = {['J.A. Rossiter']},
 journal = {IFAC-PapersOnLine},
 keywords = {['Control101 toolbox', 'frequency response', 'lead and lag compensation', 'virtual laboratories', 'independent learning']},
 title = {MATLAB files to support learning of simple frequency response design},
 year = {2024}
}

@Filtered Article{ef4a84c9-0de3-4760-b935-d47f183b5e39,
 abstract = {Background
The ability to construct simple computer programs (coding) is being progressively recognized as a life skill. Coding is now being taught to primary-school children worldwide, but current medical students usually lack coding skills, and current measures of computer literacy for medical students focus on the use of software and internet safety. There is a need to train a cohort of doctors who can both practice medicine and engage in the development of useful, innovative technologies to increase efficiency and adapt to the modern medical world.
Objective
The aim of the study was to address the following questions: (1) is it possible to teach undergraduate medical students the basics of computer coding in a 2-day course? (2) how do students perceive the value of learning computer coding at medical school? and (3) do students see computer coding as an important skill for future doctors?
Methods
We developed a short coding course to teach self-selected cohorts of medical students basic coding. The course included a 2-day introduction on writing software, discussion of computational thinking, and how to discuss projects with mainstream computer scientists, and it was followed on by a 3-week period of self-study during which students completed a project. We explored in focus groups (FGs) whether students thought that coding has a place in the undergraduate medical curriculum.
Results
Our results demonstrate that medical students who were complete novices at coding could be taught enough to be able to create simple, usable clinical programs with 2 days of intensive teaching. In addition, 6 major themes emerged from the FGs: (1) making sense of coding, (2) developing the students’ skill set, (3) the value of coding in medicine, research, and business, (4) role of teaching coding in medical schools, (5) the concept of an enjoyable challenge, and (6) comments on the course design.
Conclusions
Medical students can acquire usable coding skills in a weekend course. They valued the teaching and identified that, as well as gaining coding skills, they had acquired an understanding of its potential both for their own projects and in health care delivery and research. They considered that coding skills teaching should be offered as an optional part of the medical curriculum.},
 authors = {['Caroline E Morton', 'Susan F Smith', 'Tommy Lwin', 'Michael George', 'Matt Williams']},
 journal = {JMIR Medical Education},
 keywords = {['coding', 'medical education', 'undergraduate curriculum']},
 title = {Computer Programming: Should Medical Students Be Learning It?},
 year = {2019}
}

@Filtered Article{f01c2fec-8a85-456b-95fa-d07c7282cb2c,
 abstract = {We present computational methods for 3D simulation of fluid–object interactions in spatially periodic flows. These methods include a stabilized space-time finite element formulation for incompressible flows with spatial periodicity, automatic mesh generation and update techniques for fluid–object mixtures with spatial periodicity, and parallel implementations. The methods can be applied to uni-periodic (i.e., periodic in one direction), bi-periodic, or tri-periodic flows. The methods are described here in the context of tri-periodic flows with fluid–object interactions, and are applied to the simulation of sedimentation of particles in a fluid. We present several case studies where the results obtained provide notable insight into the behavior of fluid–particle mixtures during sedimentation.},
 authors = {['Andrew Johnson', 'Tayfun Tezduyar']},
 journal = {Computer Methods in Applied Mechanics and Engineering},
 title = {Methods for 3D computation of fluid–object interactions in spatially periodic flows},
 year = {2001}
}

@Filtered Article{f04b88b9-4da0-4398-ae54-d47f3fb60de5,
 abstract = {The predominant, but largely untested, assumption in research on food choice is that people obey the classic commandments of rational behavior: they carefully look up every piece of relevant information, weight each piece according to subjective importance, and then combine them into a judgment or choice. In real world situations, however, the available time, motivation, and computational resources may simply not suffice to keep these commandments. Indeed, there is a large body of research suggesting that human choice is often better accommodated by heuristics—simple rules that enable decision making on the basis of a few, but important, pieces of information. We investigated the prevalence of such heuristics in a computerized experiment that engaged participants in a series of choices between two lunch dishes. Employing MouselabWeb, a process-tracing technique, we found that simple heuristics described an overwhelmingly large proportion of choices, whereas strategies traditionally deemed rational were barely apparent in our data. Replicating previous findings, we also observed that visual stimulus segments received a much larger proportion of attention than any nutritional values did. Our results suggest that, consistent with human behavior in other domains, people make their food choices on the basis of simple and informationally frugal heuristics.},
 authors = {['Michael Schulte-Mecklenbeck', 'Matthias Sohn', 'Emanuel {de Bellis}', 'Nathalie Martin', 'Ralph Hertwig']},
 journal = {Appetite},
 keywords = {['Food choice', 'Heuristics', 'Process tracing', 'Rational choice', 'MouselabWeb']},
 title = {A lack of appetite for information and computation. Simple heuristics in food choice},
 year = {2013}
}

@Filtered Article{f0f12e19-db40-46d4-86e9-0a9589af8e96,
 abstract = {Is the brain at rest during the so-called resting state? Ongoing experiences in the resting state vary in unobserved and uncontrolled ways across time, individuals, and populations. However, the role of self-generated thoughts in resting-state fMRI remains largely unexplored. In this study, we collected real-time self-generated thoughts during “resting-state” fMRI scans via the think-aloud method (i.e., think-aloud fMRI), which required participants to report whatever they were currently thinking. We first investigated brain activation patterns during a think-aloud condition and found that significantly activated brain areas included all brain regions required for speech. We then calculated the relationship between divergence in thought content and brain activation during think-aloud and found that divergence in thought content was associated with many brain regions. Finally, we explored the neural representation of self-generated thoughts by performing representational similarity analysis (RSA) at three neural scales: a voxel-wise whole-brain searchlight level, a region-level whole-brain analysis using the Schaefer 400-parcels, and at the systems level using the Yeo seven-networks. We found that “resting-state” self-generated thoughts were distributed across a wide range of brain regions involving all seven Yeo networks. This study highlights the value of considering ongoing experiences during resting-state fMRI and providing preliminary methodological support for think-aloud fMRI.},
 authors = {['Hui-Xian Li', 'Bin Lu', 'Yu-Wei Wang', 'Xue-Ying Li', 'Xiao Chen', 'Chao-Gan Yan']},
 journal = {NeuroImage},
 keywords = {['Self-generated thoughts', 'Think-aloud fMRI', 'Natural language processing', 'Representational similarity analysis']},
 title = {Neural representations of self-generated thought during think-aloud fMRI},
 year = {2023}
}

@Filtered Article{f1b0af05-0e81-4f17-870d-b906ba1a9b72,
 abstract = {Chagas disease is a neglected tropical disease caused by the protozoan parasite Trypanosoma cruzi, with approximately 6–7 million people infected worldwide, becoming a public health problem in tropical countries, thus generating an increasing demand for the development of more effective drugs, due to the low efficiency of the existing drugs. Aiming at the development of a new antichagasic pharmacological tool, the density functional theory was used to calculate the reactivity descriptors of amentoflavone, a biflavonoid with proven anti-trypanosomal activity in vitro, as well as to perform a study of interactions with the enzyme cruzain, an enzyme key in the evolutionary process of T-cruzi. Structural properties (in solvents with different values of dielectric constant), the infrared spectrum, the frontier orbitals, Fukui analysis, thermodynamic properties were the parameters calculated from DFT method with the monomeric structure of the apigenin used for comparison. Furthermore, molecular docking studies were performed to assess the potential use of this biflavonoid as a pharmacological antichagasic tool. The frontier orbitals (HOMO-LUMO) study to find the band gap of compound has been extended to calculate electron affinity, ionization energy, electronegativity electrophilicity index, chemical potential, global chemical hardness and global chemical softness to study the chemical behaviour of compound. The optimized structure was subjected to molecular Docking to characterize the interaction between amentoflavone and cruzain enzyme, a classic pharmacological target for substances with anti-gas activity, where significant interactions were observed with amino acid residues from each one's catalytic sites enzyme. These results suggest that amentoflavone has the potential to interfere with the enzymatic activity of cruzain, thus being an indicative of being a promising antichagasic agent.},
 authors = {['Márcia M. Marinho', 'Francisco Wagner Q. Almeida-Neto', 'Emanuelle M. Marinho', 'Leonardo P. {da Silva}', 'Ramon R.P.P.B. Menezes', 'Ricardo P. {dos Santos}', 'Emmanuel S. Marinho', 'Pedro {de Lima-Neto}', 'Alice M.C. Martins']},
 journal = {Heliyon},
 keywords = {['Antichagasic agent', 'Biflavonoid', 'DFT', 'Fukui analysis', 'NLO']},
 title = {Quantum computational investigations and molecular docking studies on amentoflavone},
 year = {2021}
}

@Filtered Article{f1b7b916-91f9-4acd-af18-c255c0e13bae,
 abstract = {Promoter prediction is essential for analyzing gene structures, understanding regulatory networks, transcription mechanisms, and precisely controlling gene expression. Recently, computational and deep learning methods for promoter prediction have gained attention. However, there is still room to improve their accuracy. To address this, we propose the HybProm model, which uses DNA2Vec to transform DNA sequences into low-dimensional vectors, followed by a CNN-BiLSTM-Attention architecture to extract features and predict promoters across species, including E. coli, humans, mice, and plants. Experiments show that HybProm consistently achieves high accuracy (90%-99%) and offers good interpretability by identifying key sequence patterns and positions that drive predictions.},
 authors = {['Rentao Luo', 'Jiawei Liu', 'Lixin Guan', 'Mengshan Li']},
 journal = {Methods},
 keywords = {['Promoter', 'Deep learning', 'Attention', 'Gene sequences', 'Bioinformatics']},
 title = {HybProm: An attention-assisted hybrid CNN-BiLSTM model for the interpretable prediction of DNA promoter},
 year = {2025}
}

@Filtered Article{f1e2eaf7-5cbd-441e-9869-4cb9bef49e44,
 abstract = {Creativity, transforming imaginative thinking into reality, is a mental imagery simulation in essence. It can be incorporeal, concerns sophisticated and/or substantial thinking, and involves objects. In the present study, a mental imagery task consisting of creating a scene using familiar (FA) or abstract (AB) physical or virtual objects in real (RMI) and augmented reality (VMI) environments, and an execution task involving effectively creating a scene in augmented reality (VE), were utilised. The beta and gamma neural oscillations of healthy participants were recorded via a 32 channel wireless 10/20 international EGG system. In real and augmented environments and for both the mental imagery and execution tasks, the participants displayed a similar cortico-cortical neural signature essentially based on synchronous vs asynchronous beta and gamma oscillatory activities between anterior (i.e. frontal) and posterior (i.e. parietal, occipito-parietal and occipito-temporal) areas bilaterally. The findings revealed a transient synchronised neural architecture that appears to be consistent with the hypothesis according to which, creativity, because of its inherent complexity, cannot be confined to a single brain area but engages various interconnected networks.},
 authors = {['I. Giannopulu', 'G. Brotto', 'T.J. Lee', 'A. Frangos', 'D. To']},
 journal = {Heliyon},
 keywords = {['Creativity', 'Synchronisation', 'Mental imagery', 'Real environment', 'Augmented reality', 'Complexity']},
 title = {Synchronised neural signature of creative mental imagery in reality and augmented reality},
 year = {2022}
}

@Filtered Article{f2175463-57c6-490a-b35b-a578522ef54c,
 abstract = {In this paper we address some problems concerning an approximate Dirichlet domain. We show that under some assumptions an approximate Dirichlet domain can work equally well as an exact Dirichlet domain. In particular, we consider a problem of tiling a hyperbolic ball with copies of the Dirichlet domain. This problem arises in the construction of the length spectrum algorithm which is implemented by the computer program SnapPea. Our result explains the empirical fact that the program works surprisingly well despite it does not use exact data. Also we demonstrate a rigorous verification whether two words of the fundamental group of a hyperbolic 3-manifold are the same or not.},
 authors = {['Maria Trnková']},
 journal = {Topology and its Applications},
 keywords = {['Length spectrum', 'Dirichlet domain', 'Hyperbolic 3-manifold']},
 title = {Rigorous computations with an approximate Dirichlet domain},
 year = {2019}
}

@Filtered Article{f23b1ebc-f472-4525-8694-b5e8881cad1f,
 abstract = {Globular PDZ domains typically serve as protein–protein interaction modules that regulate a wide variety of cellular functions via recognition of short linear motifs (SLiMs). Often, PDZ mediated-interactions are essential components of macromolecular complexes, and disruption affects the entire scaffold. Due to their roles as linchpins in trafficking and signaling pathways, PDZ domains are attractive targets: both for controlling viral pathogens, which bind PDZ domains and hijack cellular machinery, as well as for developing therapies to combat human disease. However, successful therapeutic interventions that avoid off-target effects are a challenge, because each PDZ domain interacts with a number of cellular targets, and specific binding preferences can be difficult to decipher. Over twenty-five years of research has produced a wealth of data on the stereochemical preferences of individual PDZ proteins and their binding partners. Currently the field lacks a central repository for this information. Here, we provide this important resource and provide a manually curated, comprehensive list of the 271 human PDZ domains. We use individual domain, as well as recent genomic and proteomic, data in order to gain a holistic view of PDZ domains and interaction networks, arguing this knowledge is critical to optimize targeting selectivity and to benefit human health.},
 authors = {['Jeanine F. Amacher', 'Lionel Brooks', 'Thomas H. Hampton', 'Dean R. Madden']},
 journal = {Journal of Structural Biology: X},
 keywords = {['Protein-protein interactions', 'PDZ', 'Peptide-binding domains', 'Therapeutic targets']},
 title = {Specificity in PDZ-peptide interaction networks: Computational analysis and review},
 year = {2020}
}

@Filtered Article{f27007c1-069c-485f-addf-9743cc02d48d,
 abstract = {This chapter presents a cognitive computational view of decision making as the search for, and accumulation of, evidence for options under consideration. It is based on existing models that have been successful in traditional decision tasks involving preferential choice. The model assumes shifting attention over time that determines momentary inputs to an evolving preference state. In this chapter, the cognitive model is extended to illustrate how links from the motor system may be incorporated. These links can basically be categorized into one of three influences: modifying the subjective evaluation of choice options, restricting attention, and altering the options that are to be found in the choice set. The implications for the formal model are introduced and preliminary evidence is drawn from the extant literature.},
 authors = {['Joseph G. Johnson']},
 journal = {Elsevier},
 keywords = {['attention', 'decision making', 'motor system']},
 title = {Embodied cognition of movement decisions: a computational modeling approach},
 year = {2009}
}

@Filtered Article{f2c1a5a9-84b9-4dcf-b373-89dfd73d3d60,
 abstract = {Rarely do academics reveal the ‘backend’ of their research; the hours of labour invested into question generation, ethics compliance, transcription, translation, and data analysis, and in our case, coding. Further, when working collaboratively, the conversations that occur between collaborators seldom appear in final publications either. The development of the mobile application QualNotes, provided a productive digital space for collaboration across disciplines. In this paper, we use three vignettes to explicate the ‘backend’ of the development of that mobile application. We chart howour collaborative cross-disciplinary ethnography revealed the generative potential of thinking-with the digital and across disciplinary divides. This paper's contribution is in revealing ‘how’ we work using our disciplinary expertise, but at the same time at the edges of those disciplines too, where we contest, argue, adapt, understand, and, where we learn.},
 authors = {['Danielle Drozdzewski', 'Jose Oriol Lopez Berengueres']},
 journal = {Digital Geography and Society},
 keywords = {['QualNotes', 'Collaborative ethnography', 'Cross-disciplinary', 'Mobile application', 'Digital']},
 title = {Developing QualNotes: A collaborative and cross-disciplinary ethnography},
 year = {2024}
}

@Filtered Article{f305ceac-580c-4fd0-9398-75080489d9c7,
 abstract = {The role of aesthetics in determining usability of interactive systems has come under focus in recent time. The issue is relevant for Graphical User Interfaces (GUI) containing elements of widely varying nature. It is important to evaluate GUI aesthetically to determine their acceptability to the users. Computational models have been reported in the literature to perform objective assessment of interface aesthetics. However, the existing models only consider geometric features at the highest level, without considering the content inside the geometry. To address this issue, we propose a computational model to evaluate aesthetics of textual contents present on a GUI. The proposed model is based on empirical data collected from user studies. The model is a weighted sum of six features characterizing text: chromatic contrast, luminance contrast, font size, letter spacing, line height and word spacing. A separate validation study demonstrates the feasibility and potential of the model (showing 87% accuracy in model prediction), which is expected to be useful in predicting usability of a web page in a more refined way. Such modeling has its obvious implications in the context of engineering interactive systems. The proposed model along with the user studies are presented in this paper.},
 authors = {['Ranjan Maity', 'Akshay Madrosiya', 'Samit Bhattacharya']},
 journal = {Procedia Computer Science},
 keywords = {['Aesthetics', 'web page', 'text elements', 'aesthetic score', 'categorization']},
 title = {A Computational Model to Predict Aesthetic Quality of Text Elements of GUI},
 year = {2016}
}

@Filtered Article{f35c862f-6712-4434-98a3-1d1a1b2fe030,
 abstract = {13C metabolic flux analysis (MFA) has become an important and powerful tool for the quantitative analysis of metabolic networks in the framework of metabolic engineering. Isotopically instationary 13C MFA under metabolic stationary conditions is a promising refinement of classical stationary MFA. It accounts for the experimental requirements of non-steady-state cultures as well as for the shortening of the experimental duration. This contribution extends all computational methods developed for classical stationary 13C MFA to the instationary situation by using high-performance computing methods. The developed tools allow for the simulation of instationary carbon labeling experiments (CLEs), sensitivity calculation with respect to unknown parameters, fitting of the model to the measured data, statistical identifiability analysis and an optimal experimental design facility. To explore the potential of the new approach all these tools are applied to the central metabolism of Escherichia coli. The achieved results are compared to the outcome of the stationary counterpart, especially focusing on statistical properties. This demonstrates the specific strengths of the instationary method. A new ranking method is proposed making both an a priori and an a posteriori design of the sampling times available. It will be shown that although still not all fluxes are identifiable, the quality of flux estimates can be strongly improved in the instationary case. Moreover, statements about the size of some immeasurable pool sizes can be made.},
 authors = {['Katharina Nöh', 'Aljoscha Wahl', 'Wolfgang Wiechert']},
 journal = {Metabolic Engineering},
 keywords = {['Instationary C metabolic flux analysis', 'C labeling experiment', 'C labeling dynamics', 'Parameter identifiability', 'Optimal experimental design', '']},
 title = {Computational tools for isotopically instationary 13C labeling experiments under metabolic steady state conditions},
 year = {2006}
}

@Filtered Article{f365a4b6-4806-4665-8d50-70c3437de267,
 abstract = {In recent years, deep learning methods have been shown to have strong potential and superiority in reducing channel state information (CSI) feedback overhead and further improving feedback accuracy to maximize the performance benefits of massive Multiple-Input Multiple-Output (MIMO) in frequency division duplex (FDD) mode. As the CSI matrices are transformed into sequences for input to the Transformer model, the rearrangement leads to the loss of the original physical location relationships. Based on this problem, this paper proposes a transformer decoder based on spatio-temporal joint (ST-T). We employ a spatial attention mechanism to compensate for this information loss and focus on key spatial features more accurately, further exploiting the potential of single- and two-layer transformers in reconstructing CSI matrices. The results are validated by simulations based on DCRNet and CLNet encoders, which show that higher performance can be achieved with lower computational load compared to other lightweight models.},
 authors = {['Linyu Wang', 'Yize Cao', 'Jianhong Xiang']},
 journal = {Physical Communication},
 keywords = {['Massive MIMO', 'CSI feedback', 'Deep learning', 'Lightweighting', 'Transformer', 'Attention']},
 title = {ST-TNet: An spatio-temporal joint transformer network for CSI feedback in FDD-MIMO systems},
 year = {2025}
}

@Filtered Article{f3b1cf73-84f5-4861-a2ca-e4eed2f81e7e,
 abstract = {To increase the phase field model’s computational effectiveness, an efficient and robust adaptive incremental solution scheme is presented in this work. Firstly, a time field change criterion is established based on the variation of phase field variable and its increment, so that the pseudo time increment and load increment can be adaptively regulated with the solution of displacement and phase fields, which cuts down on computation time and the number of iterations. Secondly, the implementation of the scheme is introduced. Finally, the effectiveness of proposed solution scheme is tested through some numerical examples. The results showcase that the proposed strategy can not only acquire accurate load–displacement responses and crack patterns, but also significantly reduce the computational cost. By comparing with the current standard staggered strategy and the monolithic BFGS strategy, the computation time of the presented solution scheme is about 1% of that of the standard strategy, and less than 1/3 of the time of the BFGS strategy. Meanwhile, the presented scheme also exhibits excellent convergence properties.},
 authors = {['Yuanfeng Yu', 'Chi Hou', 'Timon Rabczuk', 'Meiying Zhao']},
 journal = {Engineering Fracture Mechanics},
 keywords = {['Fracture', 'Phase field model', 'Solution scheme', 'Adaptive increment', 'Computational time']},
 title = {An adaptive incremental solution scheme for the phase field model of fracture},
 year = {2025}
}

@Filtered Article{f442d08a-441e-493d-b33c-f17a2c83e3e8,
 abstract = {Graduate student teaching assistants (GTAs) usually teach introductory level courses at the undergraduate level. Since GTAs constitute the majority of future mathematics faculty, their image of effective teaching and preparedness to lead instructional improvements will impact future directions in undergraduate mathematics curriculum and instruction. In this paper, we argue for the need to support GTAs in improving their mathematical meanings of foundational ideas and their ability to support productive student thinking. By investigating GTAs’ meanings for average rate of change, a key content area in precalculus and calculus, we found evidence that even mathematically sophisticated GTAs possess impoverished meanings of this key idea. We argue for the need, and highlight one approach, for supporting GTAs to improve their understanding of foundational mathematical ideas and how these ideas are learned.},
 authors = {['Stacy Musgrave', 'Marilyn P. Carlson']},
 journal = {The Journal of Mathematical Behavior},
 keywords = {['Graduate student teaching assistant', 'Mathematical meanings', 'Average rate of change', 'Precalculus']},
 title = {Understanding and advancing graduate teaching assistants’ mathematical knowledge for teaching},
 year = {2017}
}

@Filtered Article{f453993a-ca22-413d-8c9e-04036258f1f1,
 abstract = {The workshop “Bioinformatics for Biotechnology Applications (HavanaBioinfo 2012)”, held December 8–11, 2012 in Havana, aimed at exploring new bioinformatics tools and approaches for large-scale proteomics, genomics and chemoinformatics. Major conclusions of the workshop include the following: (i) development of new applications and bioinformatics tools for proteomic repository analysis is crucial; current proteomic repositories contain enough data (spectra/identifications) that can be used to increase the annotations in protein databases and to generate new tools for protein identification; (ii) spectral libraries, de novo sequencing and database search tools should be combined to increase the number of protein identifications; (iii) protein probabilities and FDR are not yet sufficiently mature; (iv) computational proteomics software needs to become more intuitive; and at the same time appropriate education and training should be provided to help in the efficient exchange of knowledge between mass spectrometrists and experimental biologists and bioinformaticians in order to increase their bioinformatics background, especially statistics knowledge.},
 authors = {['Yasset Perez-Riverol', 'Henning Hermjakob', 'Oliver Kohlbacher', 'Lennart Martens', 'David Creasy', 'Jürgen Cox', 'Felipe Leprevost', 'Baozhen Paul Shan', 'Violeta I. Pérez-Nueno', 'Michal Blazejczyk', 'Marco Punta', 'Klemens Vierlinger', 'Pedro A. Valiente', 'Kalet Leon', 'Glay Chinea', 'Osmany Guirola', 'Ricardo Bringas', 'Gleysin Cabrera', 'Gerardo Guillen', 'Gabriel Padron', 'Luis Javier Gonzalez', 'Vladimir Besada']},
 journal = {Journal of Proteomics},
 keywords = {['Bioinformatics workshop', 'Mass spectrometry', 'Course', 'Protein identification', 'Database searching', 'Proteomic repositories']},
 title = {Computational proteomics pitfalls and challenges: HavanaBioinfo 2012 Workshop report},
 year = {2013}
}

@Filtered Article{f4cef956-b0fa-48f7-a66d-c8a66f637158,
 abstract = {Recent research in artificial intelligence has developed computational theories of agents' involvements in their environments. Although inspired by a great diversity of formalisms and architectures, these research projects are unified by a common concern: using principled characterizations of agents' interactions with their environments to guide analysis of living agents and design of artificial ones. This article offers a conceptual framework for such theories, surveys several other fields of research that hold the potential for dialogue with these new computational projects, and summarizes the principal contributions of the articles in this special double volume. It also briefly describes a case study in these ideas—a computer program called Toast that acts as a short-order breakfast cook. Because its designers have discovered useful structures in the world it inhabits, Toast can employ an extremely simple mechanism to decide what to do next.},
 authors = {['Philip E. Agre']},
 journal = {Artificial Intelligence},
 title = {Computational research on interaction and agency},
 year = {1995}
}

@Filtered Article{f5447d85-ffe5-491a-b6d9-bfb0a60ed2af,
 abstract = {Background
Bipolar is a severe mental health condition affecting at least 2% of the global population, with clinical observations suggesting that individuals experiencing elevated mood states, such as mania or hypomania, may have an increased propensity for engaging in risk-taking behaviors, including hypersexuality. Hypersexuality has historically been stigmatized in society and in health care provision, which makes it more difficult for service users to talk about their behaviors. There is a need for greater understanding of hypersexuality to develop better evidence-based treatment, support, and training for health professionals.
Objective
This study aimed to develop and assess effective methodologies for identifying posts on Reddit related to hypersexuality posted by people with a self-reported bipolar diagnosis. Using natural language processing techniques, this research presents a specialized dataset, the Talking About Bipolar on Reddit Corpus (TABoRC). We used various computational tools to filter and categorize posts that mentioned hypersexuality, forming the Hypersexuality in Bipolar Reddit Corpus (HiB-RC). This paper introduces a novel methodology for detecting hypersexuality-related conversations on Reddit and offers both methodological insights and preliminary findings, laying the groundwork for further research in this emerging field.
Methods
A toolbox of computational linguistic methods was used to create the corpora and infer demographic variables for the Redditors in the dataset. The key psychological domains in the corpus were measured using Linguistic Inquiry and Word Count, and a topic model was built using BERTopic to identify salient language clusters. This paper also discusses ethical considerations associated with this type of analysis.
Results
The TABoRC is a corpus of 6,679,485 posts from 5177 Redditors, and the HiB-RC is a corpus totaling 2146 posts from 816 Redditors. The results demonstrate that, between 2012 and 2021, there was a 91.65% average yearly increase in posts in the HiB-RC (SD 119.6%) compared to 48.14% in the TABoRC (SD 51.2%) and an 86.97% average yearly increase in users (SD 93.8%) compared to 27.17% in the TABoRC (SD 38.7%). These statistics suggest that there was an increase in posting activity related to hypersexuality that exceeded the increase in general Reddit use over the same period. Several key psychological domains were identified as significant in the HiB-RC (P<.001), including more negative tone, more discussion of sex, and less discussion of wellness compared to the TABoRC. Finally, BERTopic was used to identify 9 key topics from the dataset.
Conclusions
Hypersexuality is an important symptom that is discussed by people with bipolar on Reddit and needs to be systematically recognized as a symptom of this illness. This research demonstrates the utility of a computational linguistic framework and offers a high-level overview of hypersexuality in bipolar, providing empirical evidence that paves the way for a deeper understanding of hypersexuality from a lived experience perspective.},
 authors = {['Daisy Harvey', 'Paul Rayson', 'Fiona Lobban', 'Jasper Palmier-Claus', 'Clare Dolman', 'Anne Chataigné', 'Steven Jones']},
 journal = {JMIR Infodemiology},
 keywords = {['bipolar', 'hypersexuality', 'natural language processing', 'Linguistic Inquiry and Word Count', 'LIWC', 'BERTopic', 'topic modeling', 'computational linguistics']},
 title = {Using Natural Language Processing Methods to Build the Hypersexuality in Bipolar Reddit Corpus: Infodemiology Study of Reddit},
 year = {2025}
}

@Filtered Article{f56dfa5f-6972-4133-860d-c5ca42e730ba,
 abstract = {The main aim of this paper is to present the machine learning (ML) extension to the authors’ original conceptual framework for implementing temporal big data analytics (TBDA) in organizations. The framework has been also supplemented with a ML-supported feedback loop aimed at ongoing verification of the organization's maturity for TBDA in light of changing needs, requirements, and the company's environment. Such extension is needed to make the TBDA more flexible and adaptable to market environment, thus augmenting organizational agility. The research has been carried following the Design Science Research in Information Systems (DSRIS) methodological approach with the addition of creative thinking. As a result, the extended framework is elaborated, and further improvements and research directions are identified.},
 authors = {['Maria Mach-Król', 'Bartłomiej Hadasik']},
 journal = {Procedia Computer Science},
 keywords = {['temporal big data analytics', 'temporal knowledge', 'machine learning', 'organizational agility', 'feedback loop']},
 title = {An ML-extended conceptual framework for implementing temporal big data analytics in organizations to support their agility},
 year = {2023}
}

@Filtered Article{f56f696e-4a37-4d50-83b5-e856d8a7c3c6,
 abstract = {It is underlined that typical nanosystems are adequately described only by the fundamental laws of theoretical physics. It is in particular argued that phenomenological models are in most cases not sophisticated enough. For the description of such nanosystems the theoretical and computational tools have to be selected carefully and have in particular to be improved in many cases. In this connection the interaction laws (potentials) between the atoms, forming a nanosystem, are critical functions because the structure and dynamics of such systems are very sensitive to small variations in the potentials. This point has been studied in detail. Various potential laws have been introduced and discussed in connection with applications. The most relevant simulations methods are quoted and their relevance for nanotechnology is discussed. In particular, the molecular dynamics method is described in detail. We give typical examples, which demonstrate the fact the molecular dynamics method is a powerful and reliable tool for the investigation of typical nanosystems with their large variety of structures and complex dynamical states. The examples deal with wear at the nanotechnological level and with metallic nanoclusters as building blocks.},
 authors = {['Wolfram Schommers']},
 journal = {Elsevier},
 keywords = {['Simulation methods', 'interaction potentials', 'anharmonicities', 'temperature effects', 'molecular dynamics', 'nanosystems', 'structures', 'dynamics']},
 title = {Chapter 2 - Theoretical and Computational Methods},
 year = {2019}
}

@Filtered Article{f5c3e231-3748-41b3-a69a-306e6cdb031e,
 abstract = {As computer programming and computational thinking (CT) become more integrated into K-12 instruction, content teachers and special educators need to understand how to provide instructional supports to a wide range of learners, including students with disabilities. This cross-case analysis study examined the supports that two students with disabilities, who were initially disengaged during computing activities, received during computing instruction. Data revealed that students' support needs during computing activities were not CT-specific. Rather, supports specific to these students' needs that were successful in other educational areas were also successful and sufficient in CT. Although additional studies would need to be conducted to ascertain the transferability of these findings to other contexts and students, our results contribute evidence that students with disabilities can and should participate in CT and be provided with the supports they need, just as in all other areas of the curriculum. We present a framework for evaluating student engagement to identify student-specific supports and, when needed, refine the emerging K-12 CT pedagogy to facilitate full participation of all students. We then offer a list of four implications for practice based on the findings.},
 authors = {['Melinda R. Snodgrass', 'Maya Israel', 'George C. Reese']},
 journal = {Computers & Education},
 keywords = {['Universal design for learning', 'Students with disabilities', 'Pedagogy', 'Supports']},
 title = {Instructional supports for students with disabilities in K-5 computing: Findings from a cross-case analysis},
 year = {2016}
}

@Filtered Article{f629b16b-0985-4814-ae3d-37b377b06555,
 abstract = {Language understanding and mathematics understanding are two fundamental forms of human thinking. Prior research has largely focused on the question of how language shapes mathematical thinking. The current study considers the converse question. Specifically, it investigates whether the magnitude representations that are thought to anchor understanding of number are also recruited to understand the meanings of graded words. These are words that come in scales (e.g., Anger) whose members can be ordered by the degree to which they possess the defining property (e.g., calm, annoyed, angry, furious). Experiment 1 uses the comparison paradigm to find evidence that the distance, ratio, and boundary effects that are taken as evidence of the recruitment of magnitude representations extend from numbers to words. Experiment 2 uses a similarity rating paradigm and multi-dimensional scaling to find converging evidence for these effects in graded word understanding. Experiment 3 evaluates an alternative hypothesis – that these effects for graded words simply reflect the statistical structure of the linguistic environment – by using machine learning models of distributional word semantics: LSA, word2vec, GloVe, counterfitted word vectors, BERT, RoBERTa, and GPT-2. These models fail to show the full pattern of effects observed of humans in Experiment 2, suggesting that more is needed than mere statistics. This research paves the way for further investigations of the role of magnitude representations in sentence and text comprehension, and of the question of whether language understanding and number understanding draw on shared or independent magnitude representations. It also informs the role of machine learning models in cognitive psychology research.},
 authors = {['Sashank Varma', 'Emily M. Sanford', 'Vijay Marupudi', 'Olivia Shaffer', 'R. {Brooke Lea}']},
 journal = {Cognitive Psychology},
 keywords = {['Magnitude representations', 'Numerical cognition', 'Graded words', 'Distributional word semantics', 'Multi-dimensional scaling', 'Machine learning']},
 title = {Recruitment of magnitude representations to understand graded words},
 year = {2024}
}

@Filtered Article{f6c567ee-0e4d-4094-b407-9ccd40e66ab3,
 abstract = {Teaching with knowledge of students’ thinking and learning styles increases its effectiveness. The YBRAINS test is developed to help school teachers to understand the thinking and learning readiness levels of their students in the process of providing effective teaching and learning activities. The test was established based on theories and brain experiment research evidences. This article reports the rationale of establishing the test and its validity and reliability.},
 authors = {['Chua Yan Piaw']},
 journal = {Procedia - Social and Behavioral Sciences},
 keywords = {['Brain style', 'thinking and learning', 'YBRAINS', 'validity and reliability']},
 title = {Establishing a Brain Styles Test: The YBRAINS Test},
 year = {2011}
}

@Filtered Article{f6e6e907-50e0-4114-bd5c-e824304a718b,
 abstract = {Understanding the cognitive processes of the human mind is necessary to further learn about design thinking processes. Cognitive studies are also significant in the research about design studio. The aim of this study is to examine the effect of designers intelligence quotient (IQ) on their designs. The statistical population in this study consisted of all Deylaman Institute of Higher Education architecture graduate students enrolled in 2011. Sixty of these students were selected via simple random sampling based on the finite population sample size calculation formula. The students’ IQ was measured using Raven’s Progressive Matrices. The students’ scores in Architecture Design Studio (ADS) courses from first grade (ADS-1) to fifth grade (ADS-5) and the mean scores of the design courses were used in determining the students’ design ability. Inferential statistics, as well as correlation analysis and mean comparison test for independent samples with SPSS, were also employed to analyze the research data. Results indicated that the students’ IQ, ADS-1 to ADS-4 scores, and the mean scores of the students’ design courses were not significantly correlated. By contrast, the students’ IQ and ADS-5 scores were significantly correlated. As the complexity of the design problem and designers’ experience increased, the effect of IQ on design seemingly intensified.},
 authors = {['Sajjad Nazidizaji', 'Ana Tomé', 'Francisco Regateiro']},
 journal = {Frontiers of Architectural Research},
 keywords = {['Architectural design studio', 'Intelligence quotient (IQ)', 'Design education', 'Human factors', 'Design thinking']},
 title = {Does the smartest designer design better? Effect of intelligence quotient on students’ design skills in architectural design studio},
 year = {2015}
}

@Filtered Article{f7079866-cda2-4943-aee9-ba52fe8ff1aa,
 abstract = {This article addresses three key questions related to the ethical facets of algorithmic literacy. First, it synthesizes existing literature to identify six core ethical components, including bias, privacy, transparency, accountability, accuracy, and non-maleficence. Second, a crosswalk maps the intersections of these principles across the Association of College and Research Libraries' Framework for Information Literacy for Higher Education and the Association of Computing Machinery's Code of Ethics and Professional Conduct and Joint Statement on Principles for Responsible Algorithmic Systems. This analysis reveals significant overlap on issues like unfairness and transparency, helping prioritize topics for instruction. Finally, case studies showcase pedagogical strategies for teaching ethical considerations, informed by the crosswalk. Workshops for diverse undergraduates and computer science students employed reallife instances of algorithmic bias to prompt reflection on unintended harm, contestability, and responsible development. Pre-post surveys indicated expanded critical perspectives after the interventions. By systematically examining shared values and testing instructional approaches, this study provides practical tools to shape ethical thinking on algorithms. It also demonstrates promising practices for responsibly advancing algorithmic literacy across disciplines. Ultimately, fostering interdisciplinary awareness and multipronged educational initiatives can empower students to question algorithmic authority and biases.},
 authors = {['Susan Gardner Archambault', 'Shalini Ramachandran', 'Elisa Acosta', 'Sheree Fu']},
 journal = {The Journal of Academic Librarianship},
 keywords = {['Algorithmic literacy', 'Information literacy', 'Algorithmic bias', 'AI ethics', 'Algorithmic fairness', 'Computer science education']},
 title = {Ethical dimensions of algorithmic literacy for college students: Case studies and cross-disciplinary connections},
 year = {2024}
}

@Filtered Article{f76ec265-4799-4444-9a5e-508b81e83059,
 abstract = {In this study, we apply a cognitive theoretical lens to investigate students’ opportunity to develop their analytic text-based writing skills (N = 35 fifth and sixth grade classrooms). Specifically, we examine the thinking demands of classroom text-based writing tasks and teachers’ written feedback on associated student work. Four text-based writing tasks with drafts of associated student work were collected from teachers across a school year. Results of qualitative analyses showed that about half of the classroom text-based writing tasks considered by teachers to be challenging guided students to express analytic thinking about what they read (n = 73). A minority of student work received written feedback focused on students’ use of evidence, expression of thinking, and text comprehension; or received feedback that provided guidance for strategies students could take to meet genre goals. Most teachers provided content-related, instructive, and/or localized feedback on at least one piece of student work. Only a small number of teachers, however, consistently provided content-related, instructive or localized feedback on their students’ essays. Overall, results suggest that students have few opportunities to practice analytic text-based writing and receive feedback that would be expected to advance their conceptual understanding and adaptive expertise for writing in this genre.},
 authors = {['Lindsay Clare Matsumura', 'Elaine Lin Wang', 'Richard Correnti', 'Diane Litman']},
 journal = {Assessing Writing},
 keywords = {['Feedback', 'Instruction', 'Tasks', 'Text-based writing']},
 title = {Tasks and feedback: An exploration of students’ opportunity to develop adaptive expertise for analytic text-based writing},
 year = {2023}
}

@Filtered Article{f7ec25a9-ff7b-4edd-88ab-9fbdf4945848,
 abstract = {COVID-19 was declared a pandemic by the World Health Organisation (WHO) on March 11th, 2020. With half of the world's countries in lockdown as of April due to this pandemic, monitoring and understanding the spread of the virus and infection rates and how these factors relate to behavioural and societal parameters is crucial for developing control strategies. This paper aims to investigate the effectiveness of masks, social distancing, lockdown and self-isolation for reducing the spread of SARS-CoV-2 infections. Our findings from an agent-based simulation modelling showed that whilst requiring a lockdown is widely believed to be the most efficient method to quickly reduce infection numbers, the practice of social distancing and the usage of surgical masks can potentially be more effective than requiring a lockdown. Our multivariate analysis of simulation results using the Morris Elementary Effects Method suggests that if a sufficient proportion of the population uses surgical masks and follows social distancing regulations, then SARS-CoV-2 infections can be controlled without requiring a lockdown.},
 authors = {['Kelvin K.F. Li', 'Stephen A. Jarvis', 'Fayyaz Minhas']},
 journal = {Computers in Biology and Medicine},
 keywords = {['COVID-19', 'Agent-based modelling', 'Coronavirus', 'Simulation', 'SARS-COV-2', 'netlogo', 'Python', 'Epidemiology', 'Survival', 'Infectious diseases', 'VIRUS', 'Stochastic processes', 'Stochasticity', 'Social distancing', 'Masks', 'Isolation', 'Lockdown']},
 title = {Elementary effects analysis of factors controlling COVID-19 infections in computational simulation reveals the importance of social distancing and mask usage},
 year = {2021}
}

@Filtered Article{f8acb17a-bcec-4597-b2f9-12beec5092cf,
 abstract = {Publisher Summary
Computational chemistry is a very vast field dealing with atomic and molecular systems, considered at different complexity levels either as discretized quantum mechanical systems, or as statistical ensembles, amenable to Monte Carlo and Molecular Dynamic treatments, or as continuous matter fluid-dynamical distributions, modeled with Navier– Stokes equations. The mainstream computational chemistry was bent to fully solve the correlation problem with a single “technology.” Computational chemistry became a must for more and more chemists, even if the computer users had less and less awareness of the computational details of computer programs, and hardly understood that the computed answer could be incorrect, because of limitations of the selected method. In this computer generation and even more in the following years, internet, communications, commercial computer programs, computer servers, personal computers, desktop, graphics, Window, and Linux were common words, memory and disk space seemed unlimited, price/performance improved yearly, but faith in the computer replaced knowledge of the instrument and its software. Computational chemistry was becoming a part of the global economy.},
 authors = {['Enrico Clementi']},
 journal = {Elsevier},
 title = {Chapter 6 - Computational chemistry: Attempting to simulate large molecular systems},
 year = {2005}
}

@Filtered Article{f90826f1-0d65-48a1-986d-63f344096aff,
 abstract = {In human and non-human animals, conceptual knowledge is partially organized according to low-dimensional geometries that rely on brain structures and computations involved in spatial representations. Recently, two separate lines of research have investigated cognitive maps, that are associated with the hippocampal formation and are similar to world-centered representations of the environment, and image spaces, that are associated with the parietal cortex and are similar to self-centered spatial relationships. We review evidence supporting cognitive maps and image spaces, and we propose a hippocampal–parietal network that can account for the organization and retrieval of knowledge across multiple reference frames. We also suggest that cognitive maps and image spaces may be two manifestations of a more general propensity of the mind to create low-dimensional internal models.},
 authors = {['Roberto Bottini', 'Christian F. Doeller']},
 journal = {Trends in Cognitive Sciences},
 keywords = {['conceptual knowledge', 'space', 'hippocampus', 'parietal lobe', 'conceptual metaphors', 'analogy']},
 title = {Knowledge Across Reference Frames: Cognitive Maps and Image Spaces},
 year = {2020}
}

@Filtered Article{f923f416-1591-447c-be12-ff53ee711792,
 abstract = {Computational fluid dynamics (CFD) is a powerful numerical tool that is becoming widely used to simulate many processes in the food industry. Recent progression in computing efficacy coupled with reduced costs of CFD software packages has advanced CFD as a viable technique to provide effective and efficient design solutions. This paper discusses the fundamentals involved in developing a CFD solution. It also provides a state-of-the-art review on various CFD applications in the food industry such as ventilation, drying, sterilisation, refrigeration, cold display and storage, and mixing and elucidates the physical models most commonly used in these applications. The challenges faced by modellers using CFD in the food industry are also discussed.},
 authors = {['Tomás Norton', 'Da-Wen Sun']},
 journal = {Trends in Food Science & Technology},
 title = {Computational fluid dynamics (CFD) – an effective and efficient design and analysis tool for the food industry: A review},
 year = {2006}
}

@Filtered Article{f9ab3acf-5c62-4884-b901-89937ec07dde,
 abstract = {Digital health is a promising tool to support people with an elevated risk for atherosclerotic cardiovascular disease (ASCVD) and patients with an established disease to improve cardiovascular outcomes. Many digital health initiatives have been developed and employed. However, barriers to their large-scale implementation have remained. This paper focuses on these barriers and presents solutions as proposed by the Dutch CARRIER (ie, Coronary ARtery disease: Risk estimations and Interventions for prevention and EaRly detection) consortium. We will focus in 4 sections on the following: (1) the development process of an eHealth solution that will include design thinking and cocreation with relevant stakeholders; (2) the modeling approach for two clinical prediction models (CPMs) to identify people at risk of developing ASCVD and to guide interventions; (3) description of a federated data infrastructure to train the CPMs and to provide the eHealth solution with relevant data; and (4) discussion of an ethical and legal framework for responsible data handling in health care. The Dutch CARRIER consortium consists of a collaboration between experts in the fields of eHealth development, ASCVD, public health, big data, as well as ethics and law. The consortium focuses on reducing the burden of ASCVD. We believe the future of health care is data driven and supported by digital health. Therefore, we hope that our research will not only facilitate CARRIER consortium but may also facilitate other future health care initiatives.},
 authors = {['Bart Scheenstra', 'Anke Bruninx', 'Florian {van Daalen}', 'Nina Stahl', 'Elizabeth Latuapon', 'Maike Imkamp', 'Lianne Ippel', 'Sulaika Duijsings-Mahangi', 'Djura Smits', 'David Townend', 'Inigo Bermejo', 'Andre Dekker', 'Laura Hochstenbach', 'Marieke Spreeuwenberg', 'Jos Maessen', "Arnoud {van 't Hof}", 'Bas Kietselaer']},
 journal = {JMIR Cardio},
 keywords = {['atherosclerotic cardiovascular disease', 'ASCVD', 'cardiovascular risk management', 'CVRM', 'eHealth', 'digital Health', 'personalized e-coach', 'big data', 'clinical prediction models', 'federated data infrastructure']},
 title = {Digital Health Solutions to Reduce the Burden of Atherosclerotic Cardiovascular Disease Proposed by the CARRIER Consortium},
 year = {2022}
}

@Filtered Article{fa1124c6-bc54-4e4d-a7d9-0b67ec519ab9,
 abstract = {Video summarization addresses generating video summaries to help watchers grasp the content of a video without watching it entirely. Many methods have engaged in automatic video summarization. Although these methods have performed well, they still suffer from limited training data and sparse reward problems. We propose a Progressive Reinforcement Learning Video Summarization structure (PRLVS) with an unsupervised reward. The reward measures the information and quality the selected frames convey without annotations. Striving to earn higher rewards, our PRLVS adopts a “T”-type human thinking paradigm: choosing some key frames and checking if their adjacent frames are better than them. To simulate this paradigm, we decompose the flat strategy into a hierarchical strategy consisting of a horizontal policy and a vertical policy. These two policies are optimized alternatively, which densifies the reward while reducing the exploration space. Their cooperation also makes the agent capture the context information of the whole video at every step. Extensive experimental results on two benchmark databases (i.e., SumMe, TVSum) show that our PRLVS outperforms the comparisons and approaches the supervised methods, which indicates that it is significant to integrate our unsupervised reward into the progressive reinforcement learning structure to address limited annotation and sparse reward problems.},
 authors = {['Guolong Wang', 'Xun Wu', 'Junchi Yan']},
 journal = {Information Sciences},
 keywords = {['Video summarization', 'Progressive reinforcement learning', 'Hierarchical strategy']},
 title = {Progressive reinforcement learning for video summarization},
 year = {2024}
}

@Filtered Article{fa81aecd-de01-41bb-b901-ca19ff1d9b98,
 abstract = {As automated journalism based on AI came into being, it is important to understand the algorithm competence possibilities and limitations for the institutional facilitating the human–machine collaboration. Meanwhile, videos become mainstream in the advertisement realm. To expand the scope of research from journalism to advertisement, from text news to video, a comparative study was conducted to examine how the users perceive the video created by AI and humans. There is no significant difference explicitly, but the implicit appraisals were in favor of human-generated video. The key discussion is the boundary thinking of AI in both the academic and industrial spheres.},
 authors = {['Jun Wang', 'Sichen Li', 'Ke Xue', 'Li Chen']},
 journal = {Displays},
 keywords = {['Artificial intelligence', 'Video', 'Information fluency', 'Perception']},
 title = {What is the competence boundary of Algorithms? An institutional perspective on AI-based video generation},
 year = {2022}
}

@Filtered Article{faaa7a11-7bf7-4f44-ab54-a408af8e2cf2,
 abstract = {Software caching and computation migration are mechanisms that satisfy remote references by either bringing a copy of the data to the computation or moving the computation to the data. We evaluate these mechanisms usingOlden, a system that, with minimal programmer annotations, provides parallelism for C programs that use recursively defined structures, such as trees, lists, and DAGs. We demonstrate that providing both software caching and computation migration can improve the performance of these programs, and provide a compile-time heuristic that selects between them for each pointer dereference. We have implemented the heuristic in Olden on the Thinking Machines CM-5. We describe our implementation and report on experiments with eleven benchmarks.},
 authors = {['Martin C. Carlisle', 'Anne Rogers']},
 journal = {Journal of Parallel and Distributed Computing},
 title = {Software Caching and Computation Migration in Olden},
 year = {1996}
}

@Filtered Article{fb305943-1806-4ace-9800-813dd66d764d,
 abstract = {The lackluster societal response to the climate crisis is partially attributed to the abstractness of people's mental construals of climate change given its vast spatial and temporal dimensions, which fail to evoke urgency to act. Prior efforts to measure mental construal levels of climate change are inconsistent, insufficient, and labor-intensive. This study developed and implemented learning experiences for integrating engineering design and climate fiction writing to engage 48 high school students in concrete climate change thinking. A novel measure of cognitive abstractness overcomes previous methodological shortcomings by automatically quantifying the linguistic abstractness of participant-authored stories using natural language processing. Comparing participant stories written at the beginning and end of the intervention reveals a significant decrease in linguistic abstractness (Cohen's d = 1.01, p = 0.03). This study contributes to the nascent movement for greater use of narratives as data sources in environmental psychology research, which may uncover new insights into human behavior and decision making.},
 authors = {['Brandon Reynante', 'Nicole M. Ardoin', 'Roy Pea']},
 journal = {Journal of Environmental Psychology},
 keywords = {['Artificial intelligence', 'Climate change education', 'Climate fiction']},
 title = {Reducing the cognitive abstractness of climate change through an “engineering fiction” learning experience: A natural language processing study},
 year = {2024}
}

@Filtered Article{fba2101c-3ef7-4d96-88b4-12549542a061,
 abstract = {This study attempts to investigate the effect of teaching code.org site on reflective thinking skills towards problem solving. More specifically, this study attempts to investigate whether there is a gender difference in terms of students’ reflective thinking skills towards problem solving. This triangulation study was conducted with 32 primary school students. The quantitative part of the study was conducted in pre-test/post-test comparison design of quasi-experimental design. The scores of reflective problem solving skills were gathered through the reflective thinking skill scale towards problem solving and the students’ performances in the code-org site were examined. In the qualitative part of the research, after the five-week experimental process, focus group interviews were conducted with ten students and a reflection paper from the IT teacher was analysed. According to the t-test results, teaching programming to primary school students in the code.org site did not cause any differences in reflective thinking skills towards problem solving. However, there is a slight increment in the means of female students’ reflective thinking skills towards problem solving over the males’ reflective thinking skills towards problem solving. On the other hand, qualitative data provided more information about the students’ experiences. Students developed a positive attitude towards programming, and female students showed that they were as successful as their male counterparts, and that programming could be part of their future plans.},
 authors = {['Filiz Kalelioğlu']},
 journal = {Computers in Human Behavior},
 keywords = {['Improving classroom teaching', 'Programming and programming languages', 'Elementary education']},
 title = {A new way of teaching programming skills to K-12 students: Code.org},
 year = {2015}
}

@Filtered Article{fba84e36-48f5-44aa-9112-aaeaae2541ad,
 abstract = {MCVT (Making Computing Visible and Tangible) Cards are a toolkit of paper-based computing cards intended for use in the codesign of inclusive computing education. Working with groups of teachers and students over multiple design sessions, we share our toolkit, design drivers and material considerations; and use cases drawn from a week-long codesign workshop where seven teachers made and adapted cards for their future classroom facilitation. Our findings suggest that teachers valued the MCVT toolkit as a resource for their own learning and perceived the cards to be useful for supporting new computational practices, specifically for learning through making and connecting to examples of everyday computing. Critically reviewed by teachers during codesign workshops, the toolkit however posed some implementation challenges and constraints for learning through making and troubleshooting circuitry. From teacher surveys, interviews, workshop video recordings, and teacher-constructed projects, we show how teachers codesigned new design prototypes and pedagogical activities while also adapting and extending paper-based computing materials so their students could take advantage of the unique technical and expressive affordances of MCVT Cards. Our design research contributes a new perspective on using interactive paper computing cards as a medium for instructional materials development to support more inclusive computing education.},
 authors = {['HyunJoo Oh', 'Sherry Hsi', 'Noah Posner', 'Colin Dixon', 'Tymirra Smith', 'Tingyu Cheng']},
 journal = {International Journal of Child-Computer Interaction},
 keywords = {['Paper-based computing', 'Codesign', 'Inclusive CS education', 'Learning through making']},
 title = {Making computing visible & tangible: A paper-based computing toolkit for codesigning inclusive computing education activities},
 year = {2023}
}

@Filtered Article{fbe24ddd-356f-4c60-baa9-a52645a6bb91,
 abstract = {Titanium flexible intramedullary nails have become far more prevalent for stabilization of pediatric femur fractures in recent years. While steel may be expected to have superior fracture stability due to its higher elastic modulus; titanium alloy has experimentally demonstrated improved biomechanical stability, as measured by gap closure and nail slippage. The purpose of this study was to verify these observations computationally, and thus, explain why titanium alloy may be better suited for surgical fixation of fractured femurs. A finite element model of a femur with complete mid-diaphyseal fracture and having two 3.5mm nails in a retrograde “C” pattern was created. Static analyses were run in which the nail material properties were titanium alloy or stainless steel, respectively. Gap closure for the stainless steel nails was 1.03mm; while the titanium alloy nails had 0.69mm of closure. Titanium alloy nails slipped slightly less at each loading increment than stainless steel nails. The titanium alloy nails distributed stress more evenly along the nail axis, resulting in lower peak magnitudes. These results agree with previously published clinical and biomechanical studies that reported increased gap closure and nail slippage with stainless steel nails. The increased deformation of the titanium alloy nail likely increases the contact area with the intramedullary canal wall, thus, increasing stability. Additionally, stainless steel nails had higher curve apex von Mises stresses, potentially inducing a stress-shielding effect which could hamper remodeling and consequently increase risk of re-fracture.},
 authors = {['Angel Perez', 'Andrew Mahar', 'Charles Negus', 'Peter Newton', 'Tom Impelluso']},
 journal = {Medical Engineering & Physics},
 keywords = {['Finite element method', 'Simulated pediatric femur fractures', 'Intramedullary nails', 'Biomechanical stability']},
 title = {A computational evaluation of the effect of intramedullary nail material properties on the stabilization of simulated femoral shaft fractures},
 year = {2008}
}

@Filtered Article{fc570428-22cc-481a-9b7a-52ac5f2285de,
 abstract = {Embodiment theory predicts that mental imagery of object words recruits neural circuits involved in object perception. The degree of visual imagery present in routine thought and how it is encoded in the brain is largely unknown. We test whether fMRI activity patterns elicited by participants reading objects' names include embodied visual-object representations, and whether we can decode the representations using novel computational image-based semantic models. We first apply the image models in conjunction with text-based semantic models to test predictions of visual-specificity of semantic representations in different brain regions. Representational similarity analysis confirms that fMRI structure within ventral-temporal and lateral-occipital regions correlates most strongly with the image models and conversely text models correlate better with posterior-parietal/lateral-temporal/inferior-frontal regions. We use an unsupervised decoding algorithm that exploits commonalities in representational similarity structure found within both image model and brain data sets to classify embodied visual representations with high accuracy (8/10) and then extend it to exploit model combinations to robustly decode different brain regions in parallel. By capturing latent visual-semantic structure our models provide a route into analyzing neural representations derived from past perceptual experience rather than stimulus-driven brain activity. Our results also verify the benefit of combining multimodal data to model human-like semantic representations.},
 authors = {['Andrew James Anderson', 'Elia Bruni', 'Alessandro Lopopolo', 'Massimo Poesio', 'Marco Baroni']},
 journal = {NeuroImage},
 keywords = {['Concept representation', 'Embodiment', 'Mental imagery', 'Perceptual simulation', 'Language', 'Multimodal semantic models', 'Representational similarity']},
 title = {Reading visually embodied meaning from the brain: Visually grounded computational models decode visual-object mental imagery induced by written text},
 year = {2015}
}

@Filtered Article{fc95b446-e48d-46a4-8119-e753a817dc48,
 abstract = {Testosterone has been implicated in the regulation of emotional responses and risky decision-making. However, the causal effect of testosterone upon emotional decision-making, especially in non-social settings, is still unclear. The present study investigated the role of testosterone in counterfactual thinking: regret is an intense negative emotion that arises from comparison of an obtained outcome from a decision against a better, non-obtained (i.e. counterfactual) alternative. Healthy male participants (n = 64) received a single-dose of 150 mg testosterone Androgel in a double-blind, placebo-controlled, between-participants design. At 180 min post-administration, participants performed the counterfactual thinking task. We applied a computational model derived from behavioral economic principles to uncover latent decision-making mechanisms that may be invisible in simple choice analyses. Our data showed that testosterone increased the ability to use anticipated regret to guide choice behavior, while reducing choice based on expected value. On affective ratings, testosterone increased sensitivity to both obtained and counterfactual outcomes. These findings provide evidence that testosterone causally modulates emotional decision-making, and highlight the role of testosterone in affective sensitivity.},
 authors = {['Yin Wu', 'Luke Clark', 'Samuele Zilioli', 'Christoph Eisenegger', 'Claire M. Gillan', 'Huihua Deng', 'Hong Li']},
 journal = {Psychoneuroendocrinology},
 keywords = {['Testosterone', 'Reward', 'Regret', 'Emotion', 'Human male', 'Dual process']},
 title = {Single dose testosterone administration modulates emotional reactivity and counterfactual choice in healthy males},
 year = {2018}
}

@Filtered Article{fca28a11-2b80-4fa8-972f-23f5675f9f60,
 abstract = {Developing and testing psychological interventions for primary caregivers of children with cancer at significant psychosocial risk is still needed. One psychological factor contributing to their emotional distress is repetitive negative thinking (RNT). This study conducted a randomized, multiple-baseline evaluation of the effect of an individual, online, 2-session, RNT-focused ACT intervention in 12 parents. Participants responded to daily measures of emotional symptoms, RNT, and progress in values during baseline, intervention, and the 2-month follow-up. These measures have shown adequate psychometric properties at the individual level in this study. All 12 participants completed the intervention. A Bayesian hierarchical model indicated that most participants showed reductions in emotional symptoms and RNT (10 of 11), and 8 of 12 participants showed increases in valued living. The design-comparable standardized mean difference was computed to estimate the intervention effect overall. The effect sizes were large for all variables (PHQ-4: d = 0.83, 95% CI [0.27, 1.40]; RNTQ-3: d = 0.81, 95% CI [0.34, 1.28]; VQ-3: d = 1.07, 95% CI [0.22, 1.91]). Participants evaluated the intervention as useful at the 2-month follow-up. In conclusion, a brief and online RNT-focused intervention showed promising results in parents of children with cancer at significant psychosocial risk.},
 authors = {['Ana B. Bautista', 'Francisco J. Ruiz', 'Juan C. Suárez-Falcón']},
 journal = {Journal of Contextual Behavioral Science},
 keywords = {['Acceptance and commitment therapy', 'Repetitive negative thinking', 'Childhood cancer', 'Parents', 'Single-case experimental design', 'Psychosocial risk']},
 title = {Acceptance and commitment therapy in parents of children with cancer at psychosocial risk: A randomized multiple baseline evaluation},
 year = {2023}
}

@Filtered Article{fcd4a51e-5cc2-4d95-ba2d-dec14256e508,
 abstract = {In this paper we present an algorithm for the integer linear programming (ILP) problem within an algebraic model of computation and use it to solve the following digital plane segment recognition problem: Given a set of points M={p1,p2,…,pm}⊆Rn, decide whether M is a portion of a digital hyperplane and, if so, determine its analytical representation. In our setting p1, p2, …,pm may be arbitrary points (possibly, with rational and/or irrational coefficients) and the dimension n may be any arbitrary fixed integer. We reduce this last problem to an ILP to which our general integer programming algorithm applies. It performs O(mlogD) arithmetic operations, where D is a bound on the norm of the domain elements. For the special case of problem dimension two, we propose an elementary algorithm that takes advantage of the specific geometry of the problem and appears to be optimal. It implies an efficient algorithm for digital line segment recognition.},
 authors = {['Valentin E. Brimkov', 'Stefan Dantchev']},
 journal = {Image and Vision Computing},
 keywords = {['Digital hyperplane', 'Digital plane recognition', 'Integer programming', 'Euclidean algorithm']},
 title = {Digital hyperplane recognition in arbitrary fixed dimension within an algebraic computation model},
 year = {2007}
}

@Filtered Article{fcf4fcfc-d29d-4546-bb25-7ad14901d54b,
 authors = {['Peter M.A. Sloot']},
 journal = {Journal of Computational Science},
 title = {Computational science: A kaleidoscopic view into science},
 year = {2010}
}

@Filtered Article{fd519bee-bd8f-4715-821c-8521d0ca9058,
 abstract = {Given that modern children have grown up with numerous digital interactive devices it is essential to understand how the digital environment might affect children's cognitive development. As an extension of previous studies, this research investigates the cognitive effects of tactile interaction on children's problem solving. In order to explore the cognitive development of children with respect to tactile interaction, we compared furniture arrangements by elementary school students of 3D blocks and pencils. A protocol analysis was adopted for examining the ways in which children used the two different tools. The results of this study show that tactile interaction supports children's problem solving. This research implies that children in early education need to experience a wide range of digital devices utilizing rich sensorial dimensions as such devices stimulate divergent thinking, affecting cognitive developmental trajectories.},
 authors = {['Mi Jeong Kim', 'Myung Eun Cho']},
 journal = {Thinking Skills and Creativity},
 keywords = {["Children's problem solving", 'Tactile interaction', 'Cognitive perspectives', 'Empirical study', 'Protocol analysis']},
 title = {Studying children's tactile problem-solving in a digital environment},
 year = {2014}
}

@Filtered Article{fd5875c7-27ee-4a15-a504-3d6694f69ea3,
 abstract = {Computational modelling has been used to address: (1) the variety of symptoms observed in schizophrenia using abstract models of behavior (e.g. Bayesian models – top-down descriptive models of psychopathology); (2) the causes of these symptoms using biologically realistic models involving abnormal neuromodulation and/or receptor imbalance (e.g. connectionist and neural networks – bottom-up realistic models of neural processes). These different levels of analysis have been used to answer different questions (i.e. understanding behavioral vs. neurobiological anomalies) about the nature of the disorder. As such, these computational studies have mostly supported diverging hypotheses of schizophrenia's pathophysiology, resulting in a literature that is not always expanding coherently. Some of these hypotheses are however ripe for revision using novel empirical evidence. Here we present a review that first synthesizes the literature of computational modelling for schizophrenia and psychotic symptoms into categories supporting the dopamine, glutamate, GABA, dysconnection and Bayesian inference hypotheses respectively. Secondly, we compare model predictions against the accumulated empirical evidence and finally we identify specific hypotheses that have been left relatively under-investigated.},
 authors = {['Vincent Valton', 'Liana Romaniuk', 'J. {Douglas Steele}', 'Stephen Lawrie', 'Peggy Seriès']},
 journal = {Neuroscience & Biobehavioral Reviews},
 keywords = {['Psychotic symptoms', 'Schizophrenia', 'Computational models', 'Computational psychiatry']},
 title = {Comprehensive review: Computational modelling of schizophrenia},
 year = {2017}
}

@Filtered Article{fd7f6fc8-6e7f-4ea1-bf67-94ed6bd11816,
 abstract = {Beyond words and gestures, people have a remarkable capacity to communicate indirectly through everyday objects: A hat on a chair can mean it is occupied, rope hanging across an entrance can mean we should not cross, and objects placed in a closed box can imply they are not ours to take. How do people generate and interpret the communicative meaning of objects? We hypothesized that this capacity is supported by social goal inference, where observers recover what social goal explains an object being placed in a particular location. To test this idea, we study a category of common ad-hoc communicative objects where a small cost is used to signal avoidance. Using computational modeling, we first show that goal inference from indirect physical evidence can give rise to the ability to use object placement to communicate. We then show that people from the U.S. and the Tsimane’—a farming-foraging group native to the Bolivian Amazon—can infer the communicative meaning of object placement in the absence of a pre-existing convention, and that people’s inferences are quantitatively predicted by our model. Finally, we show evidence that people can store and retrieve this meaning for use in subsequent encounters, revealing a potential mechanism for how ad-hoc communicative objects become quickly conventionalized. Our model helps shed light on how humans use their ability to interpret other people’s behavior to embed social meaning into the physical world.},
 authors = {['Michael Lopez-Brau', 'Julian Jara-Ettinger']},
 journal = {Cognition},
 keywords = {['Computational modeling', 'Social objects', 'Theory of Mind']},
 title = {People can use the placement of objects to infer communicative goals},
 year = {2023}
}

@Filtered Article{fdb40056-66cb-40ef-901e-73b415953955,
 abstract = {This chapter presents one way of incorporating computational intelligence into smart grid environment. We introduce an energy ecosystem, where contemporary technologies are used and by involving advanced methods of data analysis and optimization, we aim to ensure its effective operation. In order to schedule reliable energy supply, the prediction models for power load consumption and for energy spot prices are inevitable. We provide an overview of forecasting and optimization methods and propose solutions, which deal with stream and online processing as well as adaptivity of the proposed solutions. Several different prediction methods including statistical methods and computational intelligence methods, as well as our proposed ensemble and online SVR method are compared. We take into account the current trends of distributed energy generation from renewable sources and anticipate massive usage of electro vehicles in the near future, where the optimization of the whole environment is needed.},
 authors = {['Viera Rozinajová', 'Anna Bou Ezzeddine', 'Marek Lóderer', 'Jaroslav Loebl', 'Róbert Magyar', 'Petra Vrablecová']},
 journal = {Academic Press},
 keywords = {['Smart grid', 'Intelligent data analysis', 'Computational intelligence', 'Power load prediction', 'Optimization', 'Bio-inspired algorithms', 'Ensemble models', 'Support vector regression']},
 title = {Chapter 2 - Computational Intelligence in Smart Grid Environment},
 year = {2018}
}

@Filtered Article{fdf75af3-b7a2-42ad-8ef8-7d76482eb31e,
 abstract = {Publisher Summary
This introductory chapter discusses the main issues associated with Artificial Intelligence (AI) and the prospect of a machine that could think. AI is the study of intelligent behavior that is achieved through computational means. One striking aspect of intelligent behavior is that it is conditioned by knowledge. Knowledge representation and reasoning are the parts of AI that are concerned with how an agent uses what it knows in deciding what to do. It is the study of thinking as a computational process. The book introduces the symbolic structures invented for representing knowledge and the computational processes devised for reasoning with those symbolic structures. The reason why logic is relevant to knowledge representation and reasoning is that logic is the study of entailment relations—languages, truth conditions, and rules of inference… Despite the centrality of knowledge representation and reasoning to AI, there are alternate views. Some authors have claimed that human-level reasoning is not achievable via purely computational means. Others suggest that intelligence derives from computational mechanisms.},
 authors = {['Ronald J. Brachman', 'Hector J. Levesque']},
 journal = {Morgan Kaufmann},
 title = {Chapter 1 - Introduction},
 year = {2004}
}

@Filtered Article{fdff389d-4718-4612-87aa-eca77df3a8ac,
 abstract = {In this chapter we utilize results from game theory to model the interactions of the CPS operator with different types of adversarial agents. To approach accurate prediction of realistic attacks, we present and exploit results from behavioral game theory, namely level-k thinking and cognitive hierarchy. Finally, we propose a method of predicting future adversarial behavior of adapting, learning opponents.},
 authors = {['Aris Kanellopoulos', 'Lijing Zhai', 'Filippos Fotiadis', 'Kyriakos G. Vamvoudakis']},
 journal = {Academic Press},
 keywords = {['Bounded rationality', 'Differential games', 'Attack prediction']},
 title = {Chapter Five - Adversarial modeling},
 year = {2024}
}

@Filtered Article{fe21faab-22bd-4fe1-b033-08f402faf243,
 abstract = {Information graphics include a wide variety of static and dynamic visual representations of information such as diagrams, statistical graphics, and maps. These displays take different forms depending on the purpose for which the graphic is being constructed (e.g., for thinking or communication), characteristics of the data, potential visual display forms, and whether or not the information graphic is interactive. Choosing an appropriate method for representing data requires a basic understanding of the perceptual and cognitive processing that occurs in the human visual system. Information graphics can be constructed using a wide array of tools, but are increasingly constructed using computer code and distributed through the internet.},
 authors = {['Amy L. Griffin']},
 journal = {Elsevier},
 keywords = {['Big data', 'Cognition', 'Exploratory data analysis', 'Geovisualization', 'Graphics', 'Information dashboard', 'Information visualization', 'Interactivity', 'Perception', 'Semiotics', 'Storytelling', 'Visual analytics']},
 title = {Information Graphics},
 year = {2020}
}

@Filtered Article{fe3f52ed-bd81-4218-80b2-9c47530596c7,
 abstract = {Serotonin has been associated with a wide range of neural computations and behaviours, yet an overarching function of this neurotransmitter has been hard to pinpoint. Here, we combine recent theories and findings on serotonin and propose a framework where serotonin integrates information on resource availability and state value to represent a cost–benefit trade-off at the neural level. Critically, this framework supports meta-decision making, that is, the flexible allocation of resources to decision-making. We highlight a computational and neural implementation of this framework, and through this novel, lens interpret empirical findings in the domains of controllability and persistence.},
 authors = {['Renée S Koolschijn', 'Bertalan Polner', 'Julie M Hoomans', 'Roshan Cools', 'Eliana Vassena', 'Hanneke EM {den Ouden}']},
 journal = {Current Opinion in Behavioral Sciences},
 title = {Resources, costs and long-term value: an integrative perspective on serotonin and meta-decision making},
 year = {2024}
}

@Filtered Article{ff1331de-7a72-4551-bb58-873b49dc92fb,
 abstract = {Makerspace environments are becoming popular project-based learning spaces where students interact with physical objects and peer collaboration, while developing 21st century skills and engaging with science, technology, engineering, and math (STEM) topics. At the same time, augmented reality (AR) technology, which combines physical objects with digital visualizations, is becoming increasingly applicable for makerspace activities and has potential to address challenges for student learning in makerspaces. However, there is a lack of understanding of how to use and integrate AR in real makerspace environments. In this research we use a co-design methodology to address the following questions: (1) How can AR be useful for education in makerspaces? (2) How are students impacted by the process of co-designing AR technology? and (3) What are practical considerations for integrating AR in makerspaces? We engaged in a co-design process in a semester-long makerspace course attended by 18 students in a graduate school of education. Through this process, we generated six prototypes with seven student co-designers, exploring AR use in design, fabrication, programming, electronics, and training. We also identified areas where AR technology can benefit makerspaces, such as teaching STEM skills, facilitating construction activities, enhancing contextualization of learning, and debugging. We observed that students participating in co-design demonstrated improved understanding of technology design, enthusiasm for engaging with makerspaces and AR, and increased critical thinking about AR technology. These results suggest considerations and guidelines for integrating AR technology into makerspace environments.},
 authors = {['Iulian Radu', 'Josia Yuan', 'Xiaomeng Huang', 'Bertrand Schneider']},
 journal = {Computers & Education: X Reality},
 keywords = {['Augmented reality', 'Makerspaces', 'Co-design', 'STEM', 'Classroom integration']},
 title = {Charting opportunities and guidelines for augmented reality in makerspaces through prototyping and co-design research},
 year = {2023}
}

@Filtered Article{ff15011a-cd86-453b-a5bf-fb276b11c321,
 abstract = {Past research has independently examined the concepts of certainty and future thought. Here we combine these concepts by examining the cognitive and behavioral outcomes of certainty about the future during periods of societal uncertainty. Three studies (N = 1218) examined future certainty, defined as feeling certain about some future event or outcome, during two major societal events of uncertainty—the COVID-19 pandemic and the 2020 U.S. Presidential Election. In Study 1, certainty about positive or negative futures of COVID-19 (e.g., the pandemic will end soon; the pandemic will never end) predicted poorer information seeking—ignorance of medical experts, adherence to conspiratorial thinking, and lower objective knowledgeability about COVID-19. Building on these findings, in Study 2, future certainty predicted antisocial health behaviors, including failing to social distance. Study 3 extended these findings to the political domain—the 2020 Presidential Election. Future certainty that one's preferred candidate would win the election predicted poor information seeking and antisocial behaviors in terms of claiming that the election was rigged, endorsing violence if one's candidate lost, and, among Trump supporters, identifying with Capitol insurrectionists. These findings suggest that future certainty is linked to intellectual blindness and antisocial behaviors during important periods of societal uncertainty.},
 authors = {['Irmak {Olcaysoy Okten}', 'Anton Gollwitzer', 'Gabriele Oettingen']},
 journal = {Personality and Individual Differences},
 keywords = {['Subjective certainty', 'Future thinking', 'COVID-19', 'Elections', 'Information seeking', 'Uncertainty']},
 title = {When knowledge is blinding: The dangers of being certain about the future during uncertain societal events},
 year = {2022}
}

@Filtered Article{ff24ca13-5bcb-4800-b2e4-4e2b00f43301,
 abstract = {The present review summarizes the scientific content in the workshop “Current clinical status of IC/BPS and what the future holds in basic & translational science” at the International Continence Society (ICS) 2023, Toronto. In the workshop, clinicians and scientists from different disciplines and nationalities discussed the current clinical status of IC/BPS diagnostic and treatment. They defined the available trends in biomarker search and translational medicine. The recent contribution of computational science and bioinformatics, together with artificial intelligence and the recent improvements in the use of animal models were also explored. The search for diagnostic and predictive biomarkers for IC/BPS patients is important. The use of well-refined and characterized animal models and the use of bioinformatics and artificial intelligence will be useful in this search.},
 authors = {['Guldal Inal', 'Dick Janssen', 'Naside Mangir', 'Francisco Cruz', 'Ana Charrua']},
 journal = {Continence},
 keywords = {['IC/BPS', 'Biomarkers', 'Bioinformatics', 'Artificial intelligence', 'Animal models']},
 title = {Current clinical status of IC/BPS and what the future holds in basic & translational science},
 year = {2024}
}

@Filtered Article{ff4a94bd-d691-44c2-b12b-d9d1e10d594d,
 abstract = {We outline the basic connection between distributed computing and combinatorial topology in terms of two formal models: a conventional operational model, in which systems consist of communicating state machines whose behaviors unfold over time, and the combinatorial model, in which all possible behaviors are captured statically using topological notions. We start with one particular system model (shared memory) and focus on a restricted (but important) class of problems (so-called “colorless” tasks).},
 authors = {['Maurice Herlihy', 'Dmitry Kozlov', 'Sergio Rajsbaum']},
 journal = {Morgan Kaufmann},
 keywords = {['Configurations', 'Executions', 'Layered executions', 'Layered protocols', 'Processes', 'Protocols', 'Schedules', 'Tasks']},
 title = {Chapter 4 - Colorless Wait-Free Computation},
 year = {2014}
}

@Filtered Article{ffabedd1-5a56-452f-9d14-007a5b29aaca,
 abstract = {The search domain of meta-heuristic algorithms is always constantly changing, which make it difficult to adapt the diverse optimization issues. To overcome above issue, an evolutionary updating mechanism called Memory Backtracking Strategy (MBS) is proposed, which contains thinking stage, recall stage, and memory stage. Overall, the adoption of the MBS enhances the efficiency of MHSs by incorporating group memory, clue recall, and memory forgetting mechanisms. These strategies improve the algorithm's ability to explore the search space, optimize the search process, and escape local optima. MBS will be applied to three different types of MHS algorithms: evolutionary based (LSHADE_SPACMA), physical based (Stochastic Fractal Search, SFS), and biological based (Marine Predators Algorithmnm, MPA) to demonstrate the universality of MBS. In the experimental section including 57 engineering problems, algorithm complexity analysis, CEC2020 Friedman ranking, convergence curve, Wilcoxon statistical, and box plot. Among them, 21 algorithms participated in the Friedman experiment, including MBS_LSHADE_SPACMA ranked first, LSHADE_SPACMA ranked second, MBS_MPA ranked 6th, MPA ranked 8th, MBS_SFS ranked 9th and SFS ranked 12th. Combined with the analysis of "MBS testing analysis" and the experimental results of engineering problems, it has proven that MBS has universality and good ability to improve optimization algorithm performance. The source codes of the proposed MBS (MBS_MPA) can be accessed by https://github.com/luchenghao2022/Memory-Backtracking-Strategy},
 authors = {['Heming Jia', 'Chenghao Lu', 'Zhikai Xing']},
 journal = {Swarm and Evolutionary Computation},
 keywords = {['Memory backtracking strategies', 'New evolutionary updating strategy', 'Meta-heuristic optimization algorithm']},
 title = {Memory backtracking strategy: An evolutionary updating mechanism for meta-heuristic algorithms},
 year = {2024}
}

@Filtered Article{ffc57403-8e68-4cb3-98ba-c42853f6f880,
 abstract = {Increasingly, decision makers seek to harness “big data” to guide choices in management and policy settings as well as in professions that manufacture, build, and innovate. Scholars examining this trend tend to diagnose it at once as techno positivist in its insistence on design yoked to quantifiable variables and computational modeling and, alternatively, as an imperative integral to realizing ecologically sustainable innovation. This article investigates this tension. It reflects on the role of futurists, designers, architects, urban planners, social scientists, and artists in interpreting and utilizing comprehensiveness as a design frame. Among nine experimental foresight workshops at the inaugural Emerge conference at Arizona State University, many focused on producing physical objects or media, one modeled and expanded upon a method pioneered by architect and polymath R. Buckminster Fuller. At a time when many of the capabilities to realize Fuller's specifications for big data have matured, I investigate whether comprehensive design as framed by Fuller's method shows promise as a trend enabling ecologically sustainable innovations. A historical look at Fuller's Design Science and the reflection on it in the Emerge workshop marks an opportunity to highlight and interpret the resurgence of comprehensive thinking in design while navigating the contradictions this orientation engenders.},
 authors = {['Gretchen Gano']},
 journal = {Futures},
 keywords = {['Comprehensiveness', 'Big data', 'Design science', 'Buckminster Fuller', 'Worldviews Network']},
 title = {Starting with Universe: Buckminster Fuller's Design Science Now},
 year = {2015}
}

@Filtered Article{ffc7cb65-47ac-4940-9e66-d4cb3af29c32,
 abstract = {Experimental and theoretical research aimed at determining the structure and function of the family of intermediate filament proteins has made significant advances over the past 20 years. Much of this has either contributed to or relied on the amino acid sequence databases that are now available online, and the data mining approaches that have been developed to analyze these sequences. As the quality of sequence data is generally high, it follows that it is the design of the computational and graphical methodologies that are of especial importance to researchers who aspire to gain a greater understanding of those sequence features that specify both function and structural hierarchy. However, these techniques are necessarily subject to limitations and it is important that these be recognized. In addition, no single method is likely to be successful in solving a particular problem, and a coordinated approach using a suite of methods is generally required. A final step in the process involves the interpretation of the results obtained and the construction of a working model or hypothesis that suggests further experimentation. While such methods allow meaningful progress to be made it is still important that the data are interpreted correctly and conservatively. New data mining methods are continually being developed, and it can be expected that even greater understanding of the relationship between structure and function will be gleaned from sequence data in the coming years.},
 authors = {['David A.D. Parry']},
 journal = {Academic Press},
 keywords = {['IF chain assembly', 'Sequence periodicities', 'Heptad and hendecad substructure', 'Interchain ionic interactions', 'IF secondary and tertiary structure', 'Structural/functional motifs', 'Mutations']},
 title = {Chapter Ten - Using Data Mining and Computational Approaches to Study Intermediate Filament Structure and Function},
 year = {2016}
}

@Filtered Article{ffe07d22-41d8-481e-86c1-d9c316eef682,
 abstract = {Dynamic time warping (DTW) algorithm is widely used in diversified applications due to its excellent anti-deformation and anti-interference in measuring time-series based similarity. However, the high time complexity of DTW restrains the applicability of real-time case. The existing DTW acceleration studies suffer from a loss of accuracy. How to accelerate computation while maintaining satisfying computational accuracy remains challenging. Motivated by sequential three-way decisions, this paper develops a novel model with adaptive sequential three-way decisions for dynamic time warping (AS3-DTW). Firstly, we systematically summarize distance differences under the context of adjacent tripartite search spaces for DTW, and propose five patterns of granularity adjustments of the search spaces. Furthermore, we present the corresponding calculation method of DTW adjacent tripartite search spaces distances difference. Finally, we construct a novel dynamism on adaptively adjusting time warping by combining sequence-based multi-granularity with sequential three-way decisions. Experimental results show that AS3-DTW effectively achieves promising trade-off between computational speed and accuracy on multiple UCR datasets when compared with the state-of-the-art algorithms.},
 authors = {['Jianfeng Xu', 'Ruihua Wang', 'Yuanjian Zhang', 'Weiping Ding']},
 journal = {Information Sciences},
 keywords = {['Time-series data', 'Sequential three-way decisions', 'Dynamic time warping', 'Uncertainty']},
 title = {Adaptive sequential three-way decisions for dynamic time warping},
 year = {2025}
}
